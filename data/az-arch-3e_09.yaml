- en: 9\. Azure Big Data solutions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9. Azure 大数据解决方案
- en: In the previous chapter, you learned about the various security strategies that
    can be implemented on Azure. With a secure application, we manage vast amounts
    of data. Big data has been gaining significant traction over the last few years.
    Specialized tools, software, and storage are required to handle it. Interestingly,
    these tools, platforms, and storage options were not available as services a few
    years back. However, with new cloud technology, Azure provides numerous tools,
    platforms, and resources to create big data solutions easily. This chapter will
    detail the complete architecture for ingesting, cleaning, filtering, and visualizing
    data in a meaningful way.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了可以在 Azure 上实施的各种安全策略。通过安全的应用程序，我们可以管理大量数据。近年来，大数据获得了显著的关注。处理大数据需要专用的工具、软件和存储。有趣的是，几年前这些工具、平台和存储选项还不以服务形式提供。然而，随着新云技术的出现，Azure
    提供了众多工具、平台和资源，可以轻松创建大数据解决方案。本章将详细介绍数据的摄取、清理、过滤和可视化的完整架构。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Big data overview
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据概览
- en: Data integration
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集成
- en: '**Extract-Transform-Load** (**ETL**)'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**抽取-转换-加载** (**ETL**)'
- en: Data Factory
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据工厂
- en: Data Lake Storage
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据湖存储
- en: Tools ecosystems such as Spark, Databricks, and Hadoop
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像 Spark、Databricks 和 Hadoop 这样的工具生态系统
- en: Databricks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks
- en: Big data
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大数据
- en: 'With the influx of cheap devices—such as Internet of Things devices and hand-held
    devices—the amount of data that is being generated and captured has increased
    exponentially. Almost every organization has a great deal of data and they are
    ready to purchase more if needed. When large quantities of data arrive in multiple
    different formats and on an ever-increasing basis, then we can say we are dealing
    with big data. In short, there are three key characteristics of big data:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着廉价设备的涌入——如物联网设备和手持设备——生成和捕捉的数据量呈指数级增长。几乎每个组织都拥有大量数据，并且如果需要，他们也准备好购买更多。当大量数据以多种格式并以不断增加的速度到达时，我们就可以说我们在处理大数据。简而言之，大数据有三个关键特征：
- en: '**Volume**: By volume, we mean the quantity of data both in terms of size (in
    GB, TB, and PB, for instance) and in terms of the number of records (as in a million
    rows in a hierarchical data store, 100,000 images, half a billion JSON documents,
    and so on).'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**体量**：体量指的是数据的数量，包括大小（例如 GB、TB 和 PB）以及记录的数量（例如在层级数据存储中的百万行、10万张图片、5亿个 JSON
    文档等）。'
- en: '**Velocity**: Velocity refers to the speed at which data arrives or is ingested.
    If data does not change frequently or new data does not arrive frequently, the
    velocity of data is said to be low, while if there are frequent updates and a
    lot of new data arrives on an ongoing basis frequently, it is said to have high
    velocity.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度**：速度指的是数据到达或被摄取的速度。如果数据变化不频繁或新数据不常到达，则数据的速度被认为是低的；而如果频繁更新并且有大量新数据持续不断地到达，则数据的速度被认为是高的。'
- en: '**Variety**: Variety refers to different kinds and formats of data. Data can
    come from different sources in different formats. Data can arrive as structured
    data (as in comma-separated files, JSON files, or hierarchical data), as semi-structured
    databases (as in schema-less NoSQL documents), or as unstructured data (such as
    binary large objects (blobs), images, PDFs, and so on). With so many variants,
    it''s important to have a defined process for processing ingested data.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样性**：多样性指的是数据的不同种类和格式。数据可以来自不同来源，且格式各异。数据可以是结构化数据（如逗号分隔文件、JSON 文件或层级数据）、半结构化数据库（如无模式的
    NoSQL 文档），或是非结构化数据（如二进制大对象（blobs）、图片、PDF 等）。由于存在如此多的变体，因此必须有一个明确的处理流程来处理接收的数据。'
- en: In the next section, we will check out the general big data process.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将查看大数据的通用处理过程。
- en: Process for big data
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大数据处理过程
- en: 'When data comes from multiple sources in different formats and at different
    speeds, it is important to set out a process of storing, assimilating, filtering,
    and cleaning data in a way that helps us to work with that data more easily and
    make the data useful for other processes. There needs to be a well-defined process
    for managing data. The general process for big data that should be followed is
    shown in *Figure 9.1*:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据来自不同来源，格式不同，且速度不同的时候，制定一个存储、整合、过滤和清理数据的过程变得至关重要，这样可以帮助我们更轻松地处理数据，并使数据能够服务于其他流程。必须有一个明确的数据管理流程。大数据的通用处理流程应遵循
    *图 9.1* 所示：
- en: '![A flow diagram illustrating the big data process with four stages—Ingest,
    Store, Analyze, and Visualize.](img/B15432_09_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![一个流程图，说明大数据处理的四个阶段——摄入、存储、分析和可视化。](img/B15432_09_01.jpg)'
- en: 'Figure 9.1: Big data process'
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.1：大数据处理
- en: 'There are four main stages of big data processing. Let''s explore them in detail:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据处理有四个主要阶段。让我们详细探讨它们：
- en: '**Ingest**: This is the process of bringing and ingesting data into the big
    data environment. Data can come from multiple sources, and connectors should be
    used to ingest that data within the big data platform.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ingest**：这是将数据引入和摄入到大数据环境中的过程。数据可以来自多个来源，应使用连接器将该数据摄入到大数据平台中。'
- en: '**Store**: After ingestion, data should be stored in the data pool for long-term
    storage. The storage should be there for both historical as well as live data
    and must be capable of storing structured, semi-structured, and non-structured
    data. There should be connectors to read the data from data sources, or the data
    sources should be able to push data to storage.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Store**：摄入后，数据应存储在数据池中以进行长期存储。存储应包括历史数据和实时数据，并且必须能够存储结构化、半结构化和非结构化数据。应该有连接器从数据源读取数据，或者数据源应能够将数据推送到存储中。'
- en: '**Analysis**: After data is read from storage, it should be analyzed, a process
    that requires filtering, grouping, joining, and transforming data to gather insights.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Analysis**：从存储中读取数据后，应对其进行分析，这个过程需要过滤、分组、连接和转换数据以获取洞察。'
- en: '**Visualize**: The analysis can be sent as reports using multiple notification
    platforms or used to generate dashboards with graphs and charts.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Visualize**：分析结果可以作为报告通过多种通知平台发送，也可以用于生成带有图表和图形的仪表板。'
- en: Previously, the tools needed to capture, ingest, store, and analyze big data
    were not readily available for organizations due to the involvement of expensive
    hardware and large investments. Also, no platform was available to process them.
    With the advent of the cloud, it has become easier for organizations to capture,
    ingest, store, and perform big data analytics using their preferred choice of
    tools and frameworks. They can pay the cloud provider to use their infrastructure
    and avoid any capital expenditure. Moreover, the cost of the cloud is very cheap
    compared to any on-premises solution.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，由于涉及昂贵的硬件和大量投资，组织机构无法轻易获得捕获、摄入、存储和分析大数据所需的工具。此外，也没有平台可供处理它们。随着云计算的出现，组织机构现在可以更轻松地使用他们首选的工具和框架捕获、摄入、存储和执行大数据分析。他们可以支付云提供商使用其基础设施，避免任何资本支出。而且，与任何本地解决方案相比，云计算的成本非常便宜。
- en: Big data demands an immense amount of compute, storage, and network resources.
    Generally, the amount of resources required is not practical to have on a single
    machine or server. Even if, somehow, enough resources are made available on a
    single server, the time it takes to process an entire big data pool is considerably
    large, since each job is done in sequence and each step has a dependency upon
    the prior step. There is a need for specialized frameworks and tools that can
    distribute work across multiple servers and eventually bring back the results
    from them and present to the user after appropriately combining the results from
    all the servers. These tools are specialized big data tools that help in achieving
    availability, scalability, and distribution out of the box to ensure that a big
    data solution can be optimized to run quickly with built-in robustness and stability.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据需要大量的计算、存储和网络资源。通常，所需资源量不适合在单台计算机或服务器上拥有。即使在某种程度上，可以在单台服务器上提供足够的资源，处理整个大数据池所需的时间也相当长，因为每个作业按顺序完成，每个步骤都依赖于前一步骤。需要专门的框架和工具来分布工作到多个服务器，并最终从这些服务器返回结果，并在适当地结合所有服务器的结果后呈现给用户。这些工具是专门的大数据工具，帮助实现可用性、可扩展性和分发，以确保大数据解决方案能够通过内置的稳健性和稳定性快速运行。
- en: The two prominent Azure big data services are HD Insights and Databricks. Let's
    go ahead and explore the various tools available in the big data landscape.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 显著的 Azure 大数据服务包括 HD Insights 和 Databricks。让我们继续探索大数据领域中提供的各种工具。
- en: Big data tools
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大数据工具
- en: There are many tools and services in the big data space, and we are going to
    cover some of them in this chapter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据空间中有许多工具和服务，我们将在本章中介绍其中一些。
- en: Azure Data Factory
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Azure 数据工厂
- en: Azure Data Factory is the flagship ETL service in Azure. It defines incoming
    data (in terms of its format and schema), transforms data according to business
    rules and filters, augments existing data, and finally transfers data to a destination
    store that is readily consumable by other downstream services. It is able to run
    pipelines (containing ETL logic) on Azure, as well as custom infrastructure, and
    can also run SQL Server Integration Services packages.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Azure数据工厂是Azure中的旗舰ETL服务。它定义了传入数据（根据其格式和模式），根据业务规则和过滤器转换数据，增强现有数据，并最终将数据传输到目标存储，供其他下游服务轻松消费。它能够在Azure上运行管道（包含ETL逻辑），以及在自定义基础设施上运行，还可以运行SQL
    Server集成服务包。
- en: Azure Data Lake Storage
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Azure数据湖存储
- en: Azure Data Lake Storage is enterprise-level big data storage that is resilient,
    highly available, and secure out of the box. It is compatible with Hadoop and
    can scale to petabytes of data storage. It is built on top of Azure storage accounts
    and hence gets all of the benefits of storage account directly. The current version
    is called Gen2, after the capabilities of both Azure Storage and Data Lake Storage
    Gen1 were combined.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Azure数据湖存储是企业级的大数据存储，开箱即用即具备弹性、高可用性和安全性。它与Hadoop兼容，能够扩展到PB级别的数据存储。它建立在Azure存储帐户之上，因此直接获得所有存储帐户的优势。当前版本称为Gen2，源自Azure存储和数据湖存储Gen1的能力合并。
- en: Hadoop
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hadoop
- en: Hadoop was created by the Apache software foundation and is a distributed, scalable,
    and reliable framework for processing big data that breaks big data down into
    smaller chunks of data and distributes them within a cluster. A Hadoop cluster
    comprises two types of servers—masters and slaves. The master server contains
    the administrative components of Hadoop, while the slaves are the ones where the
    data processing happens. Hadoop is responsible for the logical partition data
    between slaves; slaves perform all transformation on data, gather insights, and
    pass them back to master nodes who will collate them to generate the final output.
    Hadoop can scale to thousands of servers, with each server providing compute and
    storage for the jobs. Hadoop is available as a service using the **HDInsight**
    service in Azure.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop由Apache软件基金会创建，是一个分布式、可扩展且可靠的框架，用于处理大数据。它将大数据拆分成较小的数据块并将它们分布在集群中。一个Hadoop集群由两种类型的服务器组成——主节点和从节点。主节点包含Hadoop的管理组件，而从节点是进行数据处理的地方。Hadoop负责在从节点之间进行逻辑数据分区；从节点执行所有数据转换、收集洞察并将其返回给主节点，主节点将它们整理成最终输出。Hadoop可以扩展到数千台服务器，每台服务器为作业提供计算和存储服务。Hadoop也可以通过Azure的**HDInsight**服务作为一种服务提供。
- en: 'There are three main components that make up the Hadoop core system:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop核心系统由三个主要组件组成：
- en: '**HDFS**: Hadoop Distributed File System is a file system for the storage of
    big data. It is a distributed framework that helps by breaking down large big
    data files into smaller chunks and placing them on different slaves in a cluster.
    HDFS is a fault-tolerant file system. This means that although different chunks
    of data are made available to different slaves in the cluster, there is also the
    replication of data between the slaves to ensure that in the event of any slave''s
    failure, that data will also be available on another server. It also provides
    fast and efficient access to data to the requestor.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**HDFS**：Hadoop分布式文件系统（HDFS）是一个用于存储大数据的文件系统。它是一个分布式框架，通过将大数据文件分解为较小的数据块并将其放置在集群中的不同从节点上来帮助处理数据。HDFS是一个容错的文件系统。这意味着，尽管不同的数据块分配给集群中的不同从节点，但数据在从节点之间也会进行复制，以确保在某个从节点失败时，数据也能在另一个服务器上可用。它还提供快速高效的数据访问。'
- en: '**MapReduce**: MapReduce is another important framework that enables Hadoop
    to process data in parallel. This framework is responsible for processing data
    stored within HDFS slaves and mapping them to the slaves. After the slaves are
    done processing, the "reduce" part brings information from each slave and collates
    them together as the final output. Generally, both HDFS and MapReduce are available
    on the same node, such that the data does not need to travel between slaves and
    higher efficiency can be achieved when processing them.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**MapReduce**：MapReduce是另一个重要的框架，使Hadoop能够并行处理数据。该框架负责处理存储在HDFS从节点中的数据并将其映射到从节点。在从节点完成处理后，“reduce”部分将来自每个从节点的信息汇总到一起，生成最终输出。通常，HDFS和MapReduce会在同一节点上运行，这样数据就不需要在从节点之间传输，从而提高处理效率。'
- en: '**YARN**: **Yet Another Resource Negotiator** (**YARN**) is an important Hadoop
    architectural component that helps in scheduling jobs related to applications
    and resource management within a cluster. YARN was released as part of Hadoop
    2.0, with many casting it as the successor to MapReduce as it is more efficient
    in terms of batch processing and resource allocation.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**YARN**：**Yet Another Resource Negotiator**（**YARN**）是一个重要的 Hadoop 架构组件，帮助在集群内调度与应用程序和资源管理相关的作业。YARN
    作为 Hadoop 2.0 的一部分发布，许多人将其视为 MapReduce 的继任者，因为它在批处理和资源分配方面更为高效。'
- en: Apache Spark
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Apache Spark
- en: Apache Spark is a distributed, reliable analytics platform for large-scale data
    processing. It provides a cluster that is capable of running transformation and
    machine learning jobs on large quantities of data in parallel and bringing a consolidated
    result back to the client. It comprises master and worker nodes, where the master
    nodes are responsible for dividing and distributing the actions within jobs and
    data between worker nodes, as well as consolidating the results from all worker
    nodes and returning the results to the client. An important thing to remember
    while using Spark is that the logic or calculations should be easily parallelized,
    and the amount of data is too large to fit on one machine. Spark is available
    in Azure as a service from HDInsight and Databricks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是一个分布式、可靠的大规模数据处理分析平台。它提供一个集群，能够并行运行转换和机器学习作业，并将汇总结果返回给客户端。它由主节点和工作节点组成，其中主节点负责在作业内划分和分发操作和数据到工作节点之间，并将所有工作节点的结果汇总并返回给客户端。在使用
    Spark 时要记住的一件重要事情是，逻辑或计算应易于并行化，并且数据量太大以至于无法适应一台机器。Spark 在 Azure 中作为 HDInsight
    和 Databricks 的服务提供。
- en: Databricks
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Databricks
- en: Databricks is built on top of Apache Spark. It is a Platform as a Service where
    a managed Spark cluster is made available to users. It provides lots of added
    features, such as a complete portal to manage Spark cluster and its nodes, as
    well as helping to create notebooks, schedule and run jobs, and provide security
    and support for multiple users.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 构建在 Apache Spark 之上。它是一个平台即服务，为用户提供管理的 Spark 集群。它提供许多附加功能，如完整的门户管理
    Spark 集群及其节点，以及帮助创建笔记本、调度和运行作业，以及为多个用户提供安全性和支持。
- en: Now, it's time to learn how to integrate data from multiple sources and work
    with them together using the tools we've been talking about.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候学习如何从多个源集成数据并使用我们讨论过的工具共同处理它们了。
- en: Data integration
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集成
- en: We are well aware of how integration patterns are used for applications; applications
    that are composed of multiple services are integrated together using a variety
    of patterns. However, there is another paradigm that is a key requirement for
    many organizations, which is known as data integration. The surge in data integration
    has primarily happened during the last decade, when the generation and availability
    of data has become incredibly high. The velocity, variety, and volume of data
    being generated has increased drastically, and there is data almost everywhere.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们深知集成模式如何用于应用程序；由多个服务组成的应用程序使用各种模式进行集成。然而，还有另一种范例对许多组织来说是关键需求，即被称为数据集成。数据集成的激增主要发生在过去十年间，当时数据的生成和可用性显著增加。数据生成的速度、多样性和数量大幅增加，数据几乎无处不在。
- en: Every organization has many different types of applications, and they all generate
    data in their own proprietary format. Often, data is also purchased from the marketplace.
    Even during mergers and amalgamations of organizations, data needs to be migrated
    and combined.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 每个组织都有许多不同类型的应用程序，它们都以自己的专有格式生成数据。通常，数据也从市场购买。即使在组织合并和合并期间，也需要迁移和组合数据。
- en: Data integration refers to the process of bringing data from multiple sources
    and generating a new output that has more meaning and usability.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集成是指从多个源带入数据并生成新输出的过程，该输出具有更多的意义和可用性。
- en: 'There is a definite need for data integration in the following scenarios:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下场景中明确需要数据集成：
- en: Migrating data from a source or group of sources to a target destination. This
    is needed to make data available in different formats to different stakeholders
    and consumers.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从源或一组源迁移数据到目标位置。这是为了让数据以不同的格式对各种利益相关者和消费者可用。
- en: Getting insights from data. With the rapidly increasing availability of data,
    organizations want to derive insights from it. They want to create solutions that
    provide insights; data from multiple sources should be merged, cleaned, augmented,
    and stored in a data warehouse.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据中获取洞察。随着数据的快速增加，组织希望从中获取洞察。它们希望创建提供洞察的解决方案；来自多个源的数据应该被合并、清洗、增强，并存储在数据仓库中。
- en: Generating real-time dashboards and reports.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成实时仪表盘和报告。
- en: Creating analytics solutions.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建分析解决方案。
- en: Application integration has a runtime behavior when users are consuming the
    application—for example, in the case of credit card validation and integration.
    On the other hand, data integration happens as a back-end exercise and is not
    directly linked to user activity.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序集成在用户使用应用程序时具有运行时行为——例如，在信用卡验证和集成的情况下。另一方面，数据集成是在后台执行的，并且不直接与用户活动相关联。
- en: Let's move on to understanding the ETL process with Azure Data Factory.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续了解如何使用Azure数据工厂理解ETL过程。
- en: ETL
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ETL
- en: 'A very popular process known as ETL helps in building a target data source
    to house data that is consumable by applications. Generally, the data is in a
    raw format, and to make it consumable, the data should go through the following
    three distinct phases:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常流行的过程称为ETL，它有助于构建目标数据源，以存储应用程序可以使用的数据。通常，数据是以原始格式存在的，为了使其可用，数据需要经过以下三个不同的阶段：
- en: '**Extract**: During this phase, data is extracted from multiple places. For
    instance, there could be multiple sources and they all need to be connected together
    in order to retrieve the data. Extract phases typically use data connectors consisting
    of connection information related to the target data source. They might also have
    temporary storage to bring the data from the data source and store it for faster
    retrieval. This phase is responsible for the ingestion of data.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提取**：在这个阶段，数据从多个地方提取。例如，可能有多个数据源，它们需要连接在一起以便检索数据。提取阶段通常使用包含目标数据源连接信息的数据连接器。它们还可能有临时存储，用于将数据从数据源提取并存储，以便更快地检索。这个阶段负责数据的摄取。'
- en: '**Transform**: The data that is available after the extract phase might not
    be directly consumable by applications. This could be for a variety of reasons;
    for example, the data might have irregularities, there might be missing data,
    or there might be erroneous data. Or, there might even be data that is not needed
    at all. Alternatively, the format of the data might not be conducive to consumption
    by the target applications. In all of these cases, transformation has to be applied
    to the data in such a way that it can be efficiently consumed by applications.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换**：在提取阶段之后的数据可能无法直接被应用程序使用。这可能是由于多种原因；例如，数据可能存在不规则性，可能有缺失的数据，或数据可能存在错误。或者，可能有一些数据根本不需要。或者，数据的格式可能不适合目标应用程序的使用。在所有这些情况下，必须对数据进行转换，使其能够高效地被应用程序使用。'
- en: '**Load**: After transformation, data should be loaded to the target data source
    in a format and schema that enables faster, easier, and performance-centric availability
    for applications. Again, this typically consists of data connectors for destination
    data sources and loading data into them.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加载**：在数据转换后，应该将数据加载到目标数据源中，格式和架构使其能够更快、更容易地提供给应用程序，以提高性能。通常，这包括目标数据源的数据连接器以及将数据加载到其中的过程。'
- en: Next, let's cover how Azure Data Factory relates to the ETL process.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们来了解一下Azure数据工厂如何与ETL过程相关联。
- en: A primer on Azure Data Factory
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure数据工厂简介
- en: Azure Data Factory is a fully managed, highly available, highly scalable, and
    easy-to-use tool for creating integration solutions and implementing ETL phases.
    Data Factory helps you to create new pipelines in a drag and drop fashion using
    a user interface, without writing any code; however, it still provides features
    to allow you to write code in your preferred language.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Azure数据工厂是一个完全托管、高可用、高可扩展、易于使用的工具，用于创建集成解决方案并实现ETL阶段。数据工厂帮助你以拖放方式通过用户界面创建新的管道，而无需编写任何代码；然而，它仍然提供功能，允许你用自己喜欢的编程语言编写代码。
- en: 'There are a few important concepts to learn about before using the Data Factory
    service, which we will be exploring in more detail in the following sections:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用数据工厂服务之前，有一些重要的概念需要了解，我们将在接下来的章节中更详细地探讨这些概念：
- en: '**Activities**: Activities are individual tasks that enable the running and
    processing of logic within a Data Factory pipeline. There are multiple types of
    activities. There are activities related to data movement, data transformation,
    and control activities. Each activity has a policy through which it can decide
    the retry mechanism and retry interval.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**活动**：活动是能够在数据工厂管道中运行和处理逻辑的单个任务。活动有多种类型，涉及数据移动、数据转换和控制活动。每个活动都有一个策略，可以通过该策略决定重试机制和重试间隔。'
- en: '**Pipelines**: Pipelines in Data Factory are composed of groups of activities
    and are responsible for bringing activities together. Pipelines are the workflows and
    orchestrators that enable the running of the ETL phases. Pipelines allow the weaving
    together of activities and allow the declaration of dependencies between them.
    By using dependencies, it is possible to run some tasks in parallel and other
    tasks in sequence.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道**：数据工厂中的管道由一组活动组成，负责将活动集合在一起。管道是工作流和调度器，能够实现 ETL 阶段的运行。管道允许将活动编织在一起，并允许声明它们之间的依赖关系。通过使用依赖关系，可以使某些任务并行执行，其他任务按顺序执行。'
- en: '**Datasets**: Datasets are the sources and destinations of data. These could
    be Azure storage accounts, Data Lake Storage, or a host of other sources.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集**：数据集是数据的来源和目的地。这些可以是 Azure 存储账户、数据湖存储或其他多个来源。'
- en: '**Linked services**: These are services that contain the connection and connectivity
    information for datasets and are utilized by individual tasks for connecting to
    them.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**链接服务**：这些是包含数据集连接和连接信息的服务，并由各个任务用于连接到它们。'
- en: '**Integration runtime**: The main engine that is responsible for the running
    of Data Factory is called the integration runtime. The integration runtime is
    available on the following three configurations:'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成运行时**：负责运行数据工厂的主要引擎叫做集成运行时。集成运行时有以下三种配置：'
- en: '**Azure**: In this configuration, Data Factory runs on the compute resources
    that are provided by Azure.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure**：在此配置下，数据工厂运行在 Azure 提供的计算资源上。'
- en: '**Self-hosted**: Data Factory, in this configuration, runs when you bring your
    own compute resources. This could be through on-premises or cloud-based virtual
    machine servers.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自托管**：在此配置下，数据工厂在你提供自己的计算资源时运行。这可以通过本地或基于云的虚拟机服务器来实现。'
- en: '**Azure SQL Server Integration Services** (**SSIS**): This configuration allows
    the running of traditional SSIS packages written using SQL Server.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure SQL Server 集成服务** (**SSIS**): 此配置允许运行使用 SQL Server 编写的传统 SSIS 包。'
- en: '**Versions**: Data Factory comes in two different versions. It is important
    to understand that all new developments will happen on V2, and that V1 will stay as
    it is, or fade out at some point. V2 is preferred for the following reasons:'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**版本**：数据工厂有两个不同的版本。需要理解的是，所有新开发将发生在 V2 上，V1 将保持原样，或者在某个时候逐渐淘汰。V2 优先的原因如下：'
- en: It provides the capability to run SQL Server integration packages.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它提供了运行 SQL Server 集成包的功能。
- en: It has enhanced functionalities compared to V1.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它相较于 V1 提供了增强的功能。
- en: It comes with enhanced monitoring, which is missing in V1.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它提供了 V1 中缺少的增强监控功能。
- en: Now that you have a fair understanding of Data Factory, let's get into the various
    storage options available on Azure.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对数据工厂有了较为清晰的理解，让我们来看一下 Azure 上可用的各种存储选项。
- en: A primer on Azure Data Lake
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure 数据湖入门
- en: Azure Data Lake provides storage for big data solutions. It is specially designed
    for storing the large amounts of data that are typically needed in big data solutions.
    It is an Azure-provided managed service. Customers need to bring their data and
    store it in a data lake.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 数据湖为大数据解决方案提供存储。它专门设计用于存储大数据解决方案中通常需要的大量数据。它是 Azure 提供的托管服务。客户需要将他们的数据带来并存储在数据湖中。
- en: 'There are two versions of Azure Data Lake Storage: version 1 (Gen1) and the
    current version, version 2 (Gen2). Gen2 has all the functionality of Gen1, but
    one particular difference is that it is built on top of Azure Blob storage.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 数据湖存储有两个版本：版本 1（Gen1）和当前版本，版本 2（Gen2）。Gen2 拥有 Gen1 的所有功能，但有一个特别的区别是它建立在
    Azure Blob 存储之上。
- en: As Azure Blob storage is highly available, can be replicated multiple times,
    is disaster-ready, and is low in cost, these benefits are transferred to Gen2
    Data Lake. Data Lake can store any kind of data, including relational, non-relational,
    file system–based, and hierarchical data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Azure Blob 存储具有高可用性、可以多次复制、具备灾难恢复能力并且成本较低，这些优势也被继承到 Gen2 Data Lake 中。Data
    Lake 可以存储任何类型的数据，包括关系型、非关系型、基于文件系统和层次化数据。
- en: Creating a Data Lake Gen2 instance is as simple as creating a new storage account.
    The only change that needs to be done is enabling the hierarchical namespace from
    the **Advanced** tab of your storage account. It is important to note that there
    is no direct migration or conversion from a general storage account to Azure Data
    Lake or vice versa. Also, storage accounts are for storing files, while Data Lake
    is optimized for reading and ingesting large quantities of data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 Data Lake Gen2 实例与创建新存储账户一样简单。唯一需要做的更改是在存储账户的 **高级** 选项卡中启用层次命名空间。需要注意的是，无法直接将通用存储账户迁移或转换为
    Azure Data Lake，反之亦然。此外，存储账户用于存储文件，而 Data Lake 则针对读取和摄取大量数据进行了优化。
- en: Next, we will look into the process and main phases while working with big data.
    These are distinct phases and each is responsible for different activities on
    data.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨处理大数据时的流程和主要阶段。这些是不同的阶段，每个阶段负责数据的不同操作。
- en: Migrating data from Azure Storage to Data Lake Storage Gen2
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据从 Azure 存储迁移到 Data Lake 存储 Gen2
- en: In this section, we will be migrating data from Azure Blob storage to another Azure
    container of the same Azure Blob storage instance, and we will also migrate data
    to an Azure Data Lake Gen2 instance using an Azure Data Factory pipeline. The
    following sections outline the steps that need to be taken to create such an end-to-end
    solution.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将把数据从 Azure Blob 存储迁移到同一 Azure Blob 存储实例中的另一个 Azure 容器，并且我们还将使用 Azure
    Data Factory 管道将数据迁移到 Azure Data Lake Gen2 实例。以下各节概述了创建此端到端解决方案所需的步骤。
- en: Preparing the source storage account
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备源存储账户
- en: Before we can create Azure Data Factory pipelines and use them for migration,
    we need to create a new storage account, consisting of a number of containers,
    and upload the data files. In the real world, these files and the storage connection
    would already be prepared. The first step for creating a new Azure storage account
    is to create a new resource group or choose an existing resource group within
    an Azure subscription.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以创建 Azure Data Factory 管道并将其用于迁移之前，需要创建一个新的存储账户，存储账户由多个容器组成，并上传数据文件。在实际操作中，这些文件和存储连接通常已经准备好。创建新
    Azure 存储账户的第一步是创建一个新的资源组，或者在 Azure 订阅内选择一个现有的资源组。
- en: Provisioning a new resource group
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置新的资源组
- en: 'Every resource in Azure is associated with a resource group. Before we provision
    an Azure storage account, we need to create a resource group that will host the
    storage account. The steps for creating a resource group are given here. It is
    to be noted that a new resource group can be created while provisioning an Azure
    storage account or an existing resource group can be used:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 中的每个资源都与一个资源组关联。在我们配置 Azure 存储账户之前，需要创建一个资源组来承载存储账户。创建资源组的步骤如下所示。需要注意的是，在配置
    Azure 存储账户时，可以创建一个新的资源组，或者使用现有的资源组：
- en: Navigate to the Azure portal, log in, and click on `Resource group`.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到 Azure 门户，登录并点击 `资源组`。
- en: Select **Resource group** from the search results and create a new resource
    group. Provide a name and choose an appropriate location. Note that all the resources
    should be hosted in the same resource group and location so that it is easy to
    delete them.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从搜索结果中选择 **资源组**，然后创建一个新的资源组。提供名称并选择合适的位置。请注意，所有资源应该托管在同一资源组和位置，这样删除时会更方便。
- en: After provisioning the resource group, we will provision a storage account within
    it.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置资源组之后，我们将在其中配置一个存储账户。
- en: Provisioning a storage account
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置存储账户
- en: 'In this section, we will go through the steps of creating a new Azure storage
    account. This storage account will fetch the data source from which data will
    be migrated. Perform the following steps to create a storage account:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍创建新的 Azure 存储账户的步骤。这个存储账户将从中获取数据源，并迁移数据。执行以下步骤来创建存储账户：
- en: Click on `Storage Account`. Select **Storage Account** from the search results and then
    create a new storage account.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 `存储账户`。从搜索结果中选择 **存储账户**，然后创建一个新的存储账户。
- en: Provide a name and location, and then select a subscription based on the resource
    group that was created earlier.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供名称和位置，然后选择基于之前创建的资源组的订阅。
- en: Select **StorageV2 (general purpose v2)** for **Account kind**, **Standard** for **Performance**,
    and **Locally-redundant storage (LRS)** for **Replication**, as demonstrated in
    *Figure 9.2*:![In the ‘Create storage account’ pane, entering details such as
    subscription, Resource group, Storage account name, Location, Performance, Account
    kind, and Replication.](img/B15432_09_02.jpg)
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**帐户类型**中选择**StorageV2 (通用目的 v2)**，在**性能**中选择**标准**，在**复制**中选择**本地冗余存储（LRS）**，如*图9.2*所示：![在“创建存储帐户”窗格中，输入订阅、资源组、存储帐户名称、位置、性能、帐户类型和复制等详细信息。](img/B15432_09_02.jpg)
- en: 'Figure 9.2: Configuring the storage account'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9.2：配置存储帐户
- en: Now create a couple of containers within the storage account. The`rawdata`container contains
    the files that will be extracted by the Data Factory pipeline and will act as
    the source dataset, while **finaldata** will contain files that the Data Factory
    pipelines will write data to and will act as the destination dataset:![Creating
    finaldata and rawdata containers in the storage account.](img/B15432_09_03.jpg)
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在在存储帐户中创建几个容器。`rawdata`容器包含将由数据工厂管道提取的文件，并将作为源数据集，而**finaldata**将包含数据工厂管道将写入数据的文件，并将作为目标数据集：![在存储帐户中创建finaldata和rawdata容器。](img/B15432_09_03.jpg)
- en: 'Figure 9.3: Creating containers'
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9.3：创建容器
- en: Upload a data file (this file is available with the source code) to the **rawdata** container,
    as shown in *Figure 9.4*:![Uploading the data file products.csv to the rawdata
    container.](img/B15432_09_04.jpg)
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据文件（该文件随源代码一起提供）上传到**rawdata**容器，如*图9.4*所示：![将数据文件products.csv上传到rawdata容器。](img/B15432_09_04.jpg)
- en: 'Figure 9.4: Uploading a data file'
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9.4：上传数据文件
- en: After completing these steps, the source data preparation activities are complete.
    Now we can focus on creating a Data Lake instance.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，源数据准备工作完成。现在我们可以专注于创建Data Lake实例。
- en: Provisioning the Data Lake Gen2 service
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置Data Lake Gen2服务
- en: 'As we already know, the Data Lake Gen2 service is built on top of the Azure
    storage account. Because of this, we will be creating a new storage account in
    the same way that we did earlier—with the only difference being the selection
    of **Enabled** for **Hierarchical** **namespace** in the **Advanced** tab of the new
    Azure storage account. This will create the new Data Lake Gen2 service:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，Data Lake Gen2服务是建立在Azure存储帐户之上的。因此，我们将以与之前相同的方式创建一个新的存储帐户——唯一的区别是在新Azure存储帐户的**高级**选项卡中选择**启用**，以支持**层次命名空间**。这将创建新的Data
    Lake Gen2服务：
- en: '![Configuring the settings in the ‘Create storage account’ pane.](img/B15432_09_05.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![在“创建存储帐户”窗格中配置设置。](img/B15432_09_05.jpg)'
- en: 'Figure 9.5: Creating a new storage account'
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9.5：创建新存储帐户
- en: After the creation of the data lake, we will focus on creating a new Data Factory
    pipeline.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建数据湖之后，我们将专注于创建一个新的Data Factory管道。
- en: Provisioning Azure Data Factory
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置Azure Data Factory
- en: 'Now that we have provisioned both the resource group and Azure storage account,
    it''s time to create a new Data Factory resource:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经配置了资源组和Azure存储帐户，是时候创建一个新的Data Factory资源了：
- en: Create a new Data Factory pipeline by selecting **V2** and by providing a name
    and location, along with a resource group and subscription selection.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过选择**V2**并提供名称和位置，以及资源组和订阅选择，创建一个新的Data Factory管道。
- en: 'Data Factory has three different versions, as shown in *Figure 9.6*. We''ve
    already discussed **V1** and **V2**:'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Data Factory有三个不同的版本，如*图9.6*所示。我们已经讨论了**V1**和**V2**：
- en: '![The ‘New data factory’ pane displaying the Name as DemoDataFactoryBook, Version
    as V2, Subscription as Microsoft Azure Sponsorship, Resource group as akscluster,
    and so on.](img/B15432_09_06.jpg)'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![“新建数据工厂”窗格显示名称为DemoDataFactoryBook，版本为V2，订阅为Microsoft Azure Sponsorship，资源组为akscluster等信息。](img/B15432_09_06.jpg)'
- en: 'Figure 9.6: Selecting the version of Data Factory'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9.6：选择数据工厂的版本
- en: Once the Data Factory resource is created, click on the **Author & Monitor** link
    from the central pane.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建完数据工厂资源后，点击中央窗格中的**Author & Monitor**链接。
- en: This will open another window, consisting of the Data Factory designer for the
    pipelines.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打开另一个窗口，其中包含用于管道设计的Data Factory设计器。
- en: The code of the pipelines can be stored in version control repositories such
    that it can be tracked for code changes and promote collaboration between developers.
    If you missed setting up the repository settings in these steps, that can be done
    later.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 管道的代码可以存储在版本控制仓库中，这样可以跟踪代码更改，并促进开发人员之间的协作。如果在这些步骤中错过了仓库设置，可以稍后进行设置。
- en: The next section will focus on configuration related to version control repository
    settings if your Data Factory resource was created without any repository settings
    being configured.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分将重点介绍与版本控制仓库设置相关的配置，如果你的数据工厂资源在没有配置任何仓库设置的情况下创建，我们将在这里进行相关配置。
- en: Repository settings
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 仓库设置
- en: 'Before creating any Data Factory artifacts, such as datasets and pipelines,
    it is a good idea to set up the code repository for hosting files related to Data
    Factory:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建任何数据工厂工件（如数据集和管道）之前，最好先设置代码仓库，以托管与数据工厂相关的文件：
- en: From the **Authoring** page, click on the **Manage** button and then **Git Configuration**
    in the left menu. This will open another pane; click on the **Set up code repository**
    button in this pane:![In the Data Factory window, navigating to the Git configuration
    blade and setting up a Git repository.](img/B15432_09_07.jpg)
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**创作**页面，点击**管理**按钮，然后在左侧菜单中选择**Git 配置**。这将打开另一个面板；在此面板中点击**设置代码仓库**按钮：![在数据工厂窗口中，导航到
    Git 配置面板并设置 Git 仓库。](img/B15432_09_07.jpg)
- en: 'Figure 9.7: Setting up a Git repository'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.7：设置 Git 仓库
- en: From the resultant blade, select any one of the types of repositories that you
    want to store Data Factory code files in. In this case, let's select **Azure DevOps
    Git**:![In the ‘Repository Settings’ pane, adding the details for Repository type,
    Azure AD, Azure DevOps Account, Project Name, Git repository name, Collaboration
    branch, and Root folder.](img/B15432_09_08.jpg)
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从结果面板中，选择你希望存储数据工厂代码文件的仓库类型。在本例中，我们选择**Azure DevOps Git**：![在“仓库设置”面板中，添加仓库类型、Azure
    AD、Azure DevOps 账户、项目名称、Git 仓库名称、协作分支和根文件夹的详细信息。](img/B15432_09_08.jpg)
- en: 'Figure 9.8: Selecting the appropriate Git repository type'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.8：选择合适的 Git 仓库类型
- en: Create a new repository or reuse an existing repository from Azure DevOps. You
    should already have an account in Azure DevOps. If not, visit, [https://dev.azure.com](https://dev.azure.com)
    and use the same account used for the Azure portal to login and create a new organization
    and project within it. Refer to *Chapter 13, Integrating Azure DevOps*, to learn
    more about creating organizations and projects in Azure DevOps.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的仓库，或从 Azure DevOps 重用现有仓库。你应该已经在 Azure DevOps 中有一个账户。如果没有，可以访问[https://dev.azure.com](https://dev.azure.com)，使用与你在
    Azure 门户中相同的账户登录，并在其中创建一个新的组织和项目。有关在 Azure DevOps 中创建组织和项目的更多信息，请参考*第13章，集成 Azure
    DevOps*。
- en: Now, we can move back to the Data Factory authoring window and start creating
    artifacts for our new pipeline.
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们可以回到数据工厂创作窗口，开始为我们的新管道创建工件。
- en: In the next section, we will prepare the datasets that will be used within our
    Data Factory pipelines.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分中，我们将准备将在数据工厂管道中使用的数据集。
- en: Data Factory datasets
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据工厂数据集
- en: 'Now we can go back to the Data Factory pipeline. First, create a new dataset
    that will act as the source dataset. It will be the first storage account that
    we create and upload the sample `product.csv` file to:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以返回到数据工厂管道。首先，创建一个新的数据集，作为源数据集。它将是我们创建的第一个存储账户，我们将在其中上传示例`product.csv`文件：
- en: Click on `AutoResolveIntegrationRuntime` is used for the runtime environment,
    which means Azure will provide the runtime environment on Azure-managed compute.
    Linked services provide multiple authentication methods, and we are using the **shared
    access signature (SAS) uniform resource locator (URI)** method. It is also possible to
    use an account key, service principal, and managed identity as authentication
    methods:![In the ‘New linked service’ pane, configuring the authentication method.](img/B15432_09_09.jpg)
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击`AutoResolveIntegrationRuntime`用于运行时环境，这意味着 Azure 将在 Azure 托管的计算上提供运行时环境。链接服务提供多种身份验证方法，我们正在使用**共享访问签名（SAS）统一资源定位符（URI）**方法。也可以使用账户密钥、服务主体和托管身份作为身份验证方法：![在“新建链接服务”面板中，配置身份验证方法。](img/B15432_09_09.jpg)
- en: 'Figure 9.9: Implementing the authentication method'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.9：实现身份验证方法
- en: Then, on the resultant lower pane in the **General** tab, click on the **Open
    properties** link and provide a name for the dataset:![In the properties pane,
    adding the dataset name as InputBlobStorageData.](img/B15432_09_10.jpg)
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在结果的下方窗格中的**常规**选项卡中，点击**打开属性**链接并为数据集提供一个名称：![在属性窗格中，将数据集名称添加为InputBlobStorageData。](img/B15432_09_10.jpg)
- en: 'Figure 9.10: Naming the dataset'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.10：为数据集命名
- en: From the **Connection** tab, provide details about the container, the blob file
    name in the storage account, the row delimiter, the column delimiter, and other
    information that will help Data Factory to read the source data appropriately.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**连接**选项卡中，提供有关容器、存储帐户中的 Blob 文件名、行分隔符、列分隔符和其他有助于 Data Factory 正确读取源数据的信息。
- en: 'The **Connection** tab, after configuration, should look similar to *Figure
    9.11*. Notice that the path includes the name of the container and the name of
    the file:'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 配置后的**连接**选项卡应类似于*图 9.11*。请注意，路径包括容器的名称和文件的名称：
- en: '![Navigating to the Connection tab and configuring the connection by filling
    the details in the fields Linked Service, File path, Compression type, Column
    delimiter, and so on.](img/B15432_09_11.jpg)'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![导航到连接选项卡，通过填写字段中的详细信息（如链接服务、文件路径、压缩类型、列分隔符等）来配置连接。](img/B15432_09_11.jpg)'
- en: 'Figure 9.11: Configuring the connection'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.11：配置连接
- en: At this point, if you click on the `product.csv` file. On the `ProductID` and
    `ProductPrice`. The schema helps in providing an identifier to the columns and
    also mapping the source columns in the source dataset to the target columns in
    the target dataset, when the names are not the same.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此时，如果点击`product.csv`文件。在`ProductID`和`ProductPrice`列上，架构有助于为列提供标识符，并在源数据集和目标数据集之间映射源列到目标列，即使列名不一致。
- en: Now that the first dataset is created, let's create the second one.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在第一个数据集已创建，让我们创建第二个数据集。
- en: Creating the second dataset
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建第二个数据集
- en: 'Create a new dataset and linked service for the destination blob storage account
    in the same way that you did before. Note that the storage account is the same
    as the source but the container is different. Ensure that the incoming data has
    schema information associated with it as well, as shown in *Figure 9.12*:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以之前相同的方式为目标 Blob 存储帐户创建一个新的数据集和关联服务。请注意，存储帐户与源相同，但容器不同。确保传入的数据也附带有架构信息，如*图 9.12*所示：
- en: '![Creating another dataset by filling in the details in the Connection tab.](img/B15432_09_12.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![通过填写连接选项卡中的详细信息创建另一个数据集。](img/B15432_09_12.jpg)'
- en: 'Figure 9.12: Creating the second dataset'
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.12：创建第二个数据集
- en: Next, we will create a third dataset.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建第三个数据集。
- en: Creating a third dataset
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建第三个数据集
- en: Create a new dataset for the Data Lake Gen2 storage instance as the target dataset.
    To do this, select the new dataset and then select **Azure Data Lake Storage Gen2
    (Preview)**.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为 Data Lake Gen2 存储实例创建一个新的数据集，作为目标数据集。为此，选择新数据集，然后选择**Azure Data Lake Storage
    Gen2（预览版）**。
- en: 'Give the new dataset a name and create a new linked service in the **Connection** tab.
    Choose **Use account key** as the authentication method and the rest of the configuration
    will be auto-filled after selecting the storage account name. Then, test the connection
    by clicking on the **Test connection** button. Keep the default configuration
    for the rest of the tabs, as shown in *Figure 9.13*:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 给新数据集命名，并在**连接**选项卡中创建一个新的关联服务。选择**使用帐户密钥**作为身份验证方法，选择存储帐户名称后，其余配置将自动填充。然后，通过点击**测试连接**按钮来测试连接。保持其他选项卡的默认配置，如*图
    9.13*所示：
- en: '![Setting the configuration as default for the third dataset and then clicking
    on ‘Test connection’ at the bottom.](img/B15432_09_13.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![将配置设置为第三个数据集的默认值，然后在底部点击“测试连接”。](img/B15432_09_13.jpg)'
- en: 'Figure 9.13: Configuration in Connection tabs'
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.13：连接选项卡中的配置
- en: Now that we have the connection to source data and also connections to both
    the source and destination data stores, it's time to create the pipelines that
    will contain the logic of the data transformation.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经连接到源数据，并且连接到源和目标数据存储的连接都已建立，是时候创建包含数据转换逻辑的管道了。
- en: Creating a pipeline
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建管道
- en: 'After all the datasets are created, we can create a pipeline that will consume
    those datasets. The steps for creating a pipeline are given next:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 创建所有数据集后，我们可以创建一个管道来消费这些数据集。创建管道的步骤如下：
- en: Click on the **+ Pipelines => New Pipeline** menu from the left menu to create
    a new pipeline. Then, drag and drop the **Copy Data** activity from the **Move
    & Transform** menu, as demonstrated in *Figure 9.14*:![In the ‘pipeline1’ pane,
    the ‘Copy data’ activity is dragged onto the pipeline.](img/B15432_09_14.jpg)
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从左侧菜单点击**+ 管道 => 新建管道**菜单以创建新管道。然后，将**复制数据**活动从**移动与转换**菜单拖放，如*图 9.14*所示：![在‘pipeline1’面板中，将‘复制数据’活动拖到管道中。](img/B15432_09_14.jpg)
- en: 'Figure 9.14: Pipeline menu'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.14：管道菜单
- en: The resultant **General** tab can be left as it is, but the **Source** tab should
    be configured to use the source dataset that we configured earlier:![Navigating
    to the Source tab and selecting Source Dataset as InputBlobStorageData, and selecting
    the checkbox for ‘Copy file recursively’.](img/B15432_09_15.jpg)
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果的**常规**标签可以保持不变，但**源**标签应配置为使用我们之前配置的源数据集：![导航到源标签并选择源数据集作为 InputBlobStorageData，并勾选“递归复制文件”复选框。](img/B15432_09_15.jpg)
- en: 'Figure 9.15: Source tab'
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.15：源标签
- en: The **Sink** tab is used to configure the destination data store and dataset,
    and it should be configured to use the target dataset that we configured earlier:![Navigating
    to the Sink tab and selecting Sink dataset as AzureBlob1 and copy behavior as
    None.](img/B15432_09_16.jpg)
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**接收**标签用于配置目标数据存储和数据集，应配置为使用我们之前配置的目标数据集：![导航到接收标签并选择接收数据集为 AzureBlob1，并将复制行为设置为无。](img/B15432_09_16.jpg)'
- en: 'Figure 9.16: Sink tab'
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.16：接收标签
- en: On the **Mapping** tab, map the columns from the source to the destination dataset
    columns, as shown in *Figure 9.17*:![Moving to the Mapping tab and mapping the
    columns from the source to the destination dataset.](img/B15432_09_17.jpg)
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**映射**标签中，将源数据集的列映射到目标数据集的列，如*图 9.17*所示：![移动到映射标签并将源列映射到目标数据集列。](img/B15432_09_17.jpg)
- en: 'Figure 9.17: Mapping tab'
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.17：映射标签
- en: Adding one more Copy Data activity
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加另一个复制数据活动
- en: 'Within our pipeline, we can add multiple activities, each responsible for a
    particular transformation task. The task looked at in this section is responsible
    for copying data from the Azure storage account to Azure Data Lake Storage:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的管道中，我们可以添加多个活动，每个活动负责特定的转换任务。本节讨论的任务负责将数据从 Azure 存储帐户复制到 Azure Data Lake
    Storage：
- en: Add another `product.csv` file.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加另一个`product.csv`文件。
- en: The sink configuration will target the Data Lake Gen2 storage account.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接收配置将目标指向 Data Lake Gen2 存储帐户。
- en: The rest of the configuration can be left in the default settings for the second
    Copy Data activity.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于第二个复制数据活动，其余配置可以保留为默认设置。
- en: After the authoring of the pipeline is complete, it can be published to a version
    control repository such as GitHub.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成管道的编写后，可以将其发布到版本控制库，如 GitHub。
- en: Next, we will look into creating a solution using Databricks and Spark.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨如何使用 Databricks 和 Spark 创建解决方案。
- en: Creating a solution using Databricks
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Databricks 创建解决方案
- en: Databricks is a platform for using Spark as a service. We do not need to provision
    master and worker nodes on virtual machines. Instead, Databricks provides us with
    a managed environment consisting of master and worker nodes and also manages them.
    We need to provide the steps and logic for the processing of data, and the rest
    is taken care of by the Databricks platform.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 是一个将 Spark 作为服务使用的平台。我们无需在虚拟机上配置主节点和工作节点。相反，Databricks 为我们提供了一个由主节点和工作节点组成的托管环境，并且对其进行管理。我们只需提供数据处理的步骤和逻辑，其余的都由
    Databricks 平台负责。
- en: In this section, we will go through the steps of creating a solution using Databricks.
    We will be downloading sample data to analyze.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过步骤介绍如何使用 Databricks 创建解决方案。我们将下载示例数据进行分析。
- en: 'The sample CSV has been downloaded from [https://ourworldindata.org/coronavirus-source-data](https://ourworldindata.org/coronavirus-source-data),
    although it is also provided with the code of this book. The URL mentioned before
    will have more up-to-date data; however, the format might have changed, and so
    it is recommended to use the file available with the code samples of this book:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 CSV 文件已从[https://ourworldindata.org/coronavirus-source-data](https://ourworldindata.org/coronavirus-source-data)下载，尽管它也可以在本书的代码中找到。前述网址会提供更为最新的数据，但格式可能发生变化，因此建议使用本书代码示例中提供的文件：
- en: The first step in creating a Databricks solution is to provision it from the
    Azure portal. There is a 14-day evaluation SKU available along with two other
    SKUs—standard and premium. The premium SKU has Azure Role-Based Access Control
    at the level of notebooks, clusters, jobs, and tables:![In the Azure portal, navigating
    to Databricks, then selecting the New option and navigating to the Azure Databricks
    pane, and finally provisioning the Databricks workspace.](img/B15432_09_19.jpg)
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 Databricks 解决方案的第一步是通过 Azure 门户进行预配。从 Azure 门户中可以选择 14 天评估版 SKU，以及标准和高级版两个其他
    SKU。高级版 SKU 在笔记本、集群、作业和表格级别提供 Azure 基于角色的访问控制：![在 Azure 门户中，导航到 Databricks，然后选择“新建”选项，进入
    Azure Databricks 面板，最后预配 Databricks 工作区。](img/B15432_09_19.jpg)
- en: 'Figure 9.19: Azure portal—Databricks service'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.19：Azure 门户—Databricks 服务
- en: After the Data bricks workspace is provisioned, click on the **Launch workspace**
    button from the **Overview** pane. This will open a new browser window and will
    eventually log you in to the Databricks portal.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Databricks 工作区预配完成后，从 **概述** 面板点击 **启动工作区** 按钮。这将打开一个新的浏览器窗口，并最终将你登录到 Databricks
    门户。
- en: From the Databricks portal, select **Clusters** from the left menu and create
    a new cluster, as shown in *Figure 9.20*:![In the Create cluster pane, adding
    cluster details such as Cluster Name, Cluster Mode, Pool, Databricks Runtime Version,
    and so on.](img/B15432_09_20.jpg)
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Databricks 门户中，从左侧菜单选择 **集群**，并创建一个新集群，如 *图 9.20* 所示：![在“创建集群”面板中，添加集群名称、集群模式、池、Databricks
    运行时版本等集群详细信息。](img/B15432_09_20.jpg)
- en: 'Figure 9.20: Creating a new cluster'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.20：创建一个新集群
- en: Provide the name, the Databricks runtime version, the number of worker types,
    the virtual machine size configuration, and the driver type server configuration.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供名称、Databricks 运行时版本、工作节点类型、虚拟机大小配置以及驱动程序类型服务器配置。
- en: The creation of the cluster might take a few minutes. After the creation of
    the cluster, click on **Home**, select a user from its context menu, and create
    a new notebook:![In the Microsoft Azureworkspace, selecting the Home option from
    the left-hand navigation, selecting the user, selecting ‘Create’ from the drop-down
    options, and then clicking on Notebook.](img/B15432_09_21.jpg)
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建集群可能需要几分钟时间。集群创建完成后，点击 **主页**，从其上下文菜单中选择一个用户，并创建一个新的笔记本：![在 Microsoft Azure
    工作区中，从左侧导航中选择“主页”选项，选择用户，从下拉菜单中选择“创建”，然后点击“笔记本”。](img/B15432_09_21.jpg)
- en: 'Figure 9.21: Selecting a new notebook'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.21：选择一个新笔记本
- en: Provide a name to the notebook, as shown next:![In the Create Notebook pane,
    adding the name as CovidAnalysis, Default language as Python, cluster as BookBigDataCluster,
    and then clicking on the Create button.](img/B15432_09_22.jpg)
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为笔记本提供一个名称，如下所示：![在“创建笔记本”面板中，添加名称为 CovidAnalysis，默认语言为 Python，集群为 BookBigDataCluster，然后点击创建按钮。](img/B15432_09_22.jpg)
- en: 'Figure 9.22: Creating a notebook'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.22：创建笔记本
- en: Create a new storage account, as shown next. This will act as storage for the
    raw COVID data in CSV format:![Adding details such as Subscription, Resource group,
    Storage account name, Location, and so on in the ‘Create Storage account’ pane.](img/B15432_09_23.jpg)
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的存储账户，如下所示。这将作为存储原始 COVID 数据的 CSV 格式存储：![在“创建存储账户”面板中，添加订阅、资源组、存储账户名称、位置等信息。](img/B15432_09_23.jpg)
- en: 'Figure 9.23: Creating a new storage account'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.23：创建一个新的存储账户
- en: Create a container for storing the CSV file, as shown next:![Creating a container
    with the name covid data and the Public access level as Private (no anonymous
    access).](img/B15432_09_24.jpg)
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为存储 CSV 文件创建一个容器，如下所示：![创建一个名为 covid data 的容器，公共访问级别设置为私人（无匿名访问）。](img/B15432_09_24.jpg)
- en: 'Figure 9.24: Creating a container'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.24：创建一个容器
- en: Upload the `owid-covid-data.csv` file to this container.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `owid-covid-data.csv` 文件上传到此容器中。
- en: Once you have completed the preceding steps, the next task is to load the data.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 完成上述步骤后，下一步是加载数据。
- en: Loading data
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载数据
- en: 'The second major step is to load the COVID data within the Databricks workspace.
    This can be done in two main ways:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个主要步骤是将 COVID 数据加载到 Databricks 工作区。这可以通过两种主要方式完成：
- en: Mount the Azure storage container in Databricks and then load the files available
    within the mount.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Databricks 中挂载 Azure 存储容器，然后加载挂载中的文件。
- en: Load the data directly from the storage account. This approach has been used
    in the following example.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从存储账户直接加载数据。以下示例使用了这种方法。
- en: 'The following steps should be performed to load and analyze data using Databricks:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤应执行以使用 Databricks 加载和分析数据：
- en: 'The first step is to connect and access the storage account. The key for the
    storage account is needed, which is stored within the Spark configuration. Note
    that the key here is `"fs.azure.account.key.coronadatastorage.blob.core.windows.net"`
    and the value is the associated key:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是连接并访问存储帐户。需要存储帐户的密钥，该密钥存储在Spark配置中。注意，这里的密钥是`"fs.azure.account.key.coronadatastorage.blob.core.windows.net"`，其值是关联的密钥：
- en: '[PRE0]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The key for the Azure storage account can be retrieved by navigating to the
    settings and the `wasbs://{{container}}@{{storage account name}}.blob.core.windows.net/{{filename}}`.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Azure存储帐户的密钥可以通过导航到设置并使用`wasbs://{{container}}@{{storage account name}}.blob.core.windows.net/{{filename}}`来获取。
- en: 'The `read` method of the `SparkSession` object provides methods to read files.
    To read CSV files, the `csv` method should be used along with its required parameters,
    such as the path to the CSV file. There are additional optional parameters that
    can be supplied to customize the reading process of the data files. There are
    multiple types of file formats, such as JSON, **Optimized Row Columnar** (**ORC**),
    and Parquet, and relational databases such as SQL Server and MySQL, NoSQL data
    stores such as Cassandra and MongoDB, and big data platforms such as Apache Hive
    that can all be used within Spark. Let''s take a look at the following command
    to understand the implementation of Spark DataFrames:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`SparkSession`对象的`read`方法提供了读取文件的方法。要读取CSV文件，应该使用`csv`方法，并传递其所需的参数，例如CSV文件的路径。还可以提供额外的可选参数来定制数据文件的读取过程。Spark支持多种文件格式，如JSON、**优化行列格式**（**ORC**）和Parquet，以及关系型数据库如SQL
    Server和MySQL、NoSQL数据存储如Cassandra和MongoDB，以及大数据平台如Apache Hive。让我们通过以下命令了解Spark
    DataFrame的实现：'
- en: '[PRE1]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using this command creates a new object of the DataFrame type in Spark. Spark
    provides **Resilient Distributed Dataset** (**RDD**) objects to manipulate and
    work with data. RDDs are low-level objects and any code written to work with them
    might not be optimized. DataFrames are higher-level constructs over RDDs and provide
    optimization to access and work with them RDDs. It is better to work with DataFrames
    than RDDs.
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用此命令会在Spark中创建一个新的DataFrame类型对象。Spark提供**弹性分布式数据集**（**RDD**）对象来操作和处理数据。RDD是低级对象，任何针对它们编写的代码可能不会经过优化。DataFrame是RDD之上的高级构造，提供了优化访问和处理RDD的功能。与RDD一起工作时，最好使用DataFrame。
- en: DataFrames provide data in row-column format, which makes it easier to visualize
    and work with data. Spark DataFrames are similar to pandas DataFrames, with the
    difference being that they are different implementations.
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DataFrame以行列格式提供数据，这使得可视化和处理数据变得更容易。Spark的DataFrame类似于pandas的DataFrame，区别在于它们是不同的实现。
- en: 'The following command shows the data in a DataFrame. It shows all the rows
    and columns available within the DataFrame:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下命令显示了DataFrame中的数据，展示了DataFrame中所有可用的行和列：
- en: '[PRE2]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You should get a similar output to what you can see in *Figure 9.25*:'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到类似于*图 9.25*中所看到的输出：
- en: '![The output of the coviddata.show() command showing the raw data in the DataFrame.](img/B15432_09_25.jpg)'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![coviddata.show()命令的输出，显示DataFrame中的原始数据。](img/B15432_09_25.jpg)'
- en: 'Figure 9.25: The raw data in a DataFrame'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.25：DataFrame中的原始数据
- en: 'The schema of the loaded data is inferred by Spark and can be checked using
    the following command:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据的模式由Spark推断，可以使用以下命令检查：
- en: '[PRE3]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This should give you a similar output to this:'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这应该会给你类似于以下的输出：
- en: '![The output of the coviddata.printSchema() command displaying the schema of
    the DataFrame for each column.](img/B15432_09_26.jpg)'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![coviddata.printSchema()命令的输出，显示DataFrame每列的模式。](img/B15432_09_26.jpg)'
- en: 'Figure 9.26: Getting the schema of the DataFrame for each column'
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.26：获取DataFrame每列的模式
- en: 'To count the number of rows within the CSV file, the following command can
    be used, and its output shows that there are 19,288 rows in the file:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要计算CSV文件中的行数，可以使用以下命令，输出显示文件中有19,288行：
- en: '[PRE4]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![The output of the coviddata.count() command showing there are 19,288 rows
    in the file.](img/B15432_09_27.jpg)'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![coviddata.count()命令的输出，显示文件中有19,288行。](img/B15432_09_27.jpg)'
- en: 'Figure 9.27: Finding the count of records in a DataFrame'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.27：查找DataFrame中的记录数量
- en: 'The original DataFrame has more than 30 columns. We can also select a subset
    of the available columns and work with them directly, as shown next:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 原始的DataFrame有超过30列。我们也可以选择其中的一部分列并直接使用，如下所示：
- en: '[PRE5]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output of the code will be as shown in *Figure 9.28*:'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码的输出将如下所示，正如*图 9.28*所示：
- en: '![Displaying selected columns from the coviddataDataFrame.](img/B15432_09_28.jpg)'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![显示从 coviddataDataFrame 中选择的列。](img/B15432_09_28.jpg)'
- en: 'Figure 9.28: Selecting a few columns from the overall columns'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.28：从所有列中选择几个列
- en: 'It is also possible to filter data using the `filter` method, as shown next:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 也可以使用 `filter` 方法过滤数据，如下所示：
- en: '[PRE6]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'It is also possible to add multiple conditions together using the AND (`&`)
    or OR (`|`) operators:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还可以使用 AND（`&`）或 OR（`|`）运算符将多个条件组合在一起：
- en: '[PRE7]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To find out the number of rows and other statistical details, such as the mean,
    maximum, minimum, and standard deviation, the `describe` method can be used:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要找出行数及其他统计细节，如均值、最大值、最小值和标准差，可以使用 `describe` 方法：
- en: '[PRE8]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Upon using the preceding command, you''ll get a similar output to this:'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用前面的命令后，你将获得类似这样的输出：
- en: '![The output of the CovidDataSmallSet.describe().show() command displaying
    the statistics of each column.](img/B15432_09_29.jpg)'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CovidDataSmallSet.describe().show() 命令的输出，显示每列的统计信息。](img/B15432_09_29.jpg)'
- en: 'Figure 9.29: Showing each column''s statistics using the describe method'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.29：使用 describe 方法显示每个列的统计信息
- en: 'It is also possible to find out the percentage of null or empty data within
    specified columns. A couple of examples are shown next:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还可以找出指定列中空值或缺失数据的百分比。以下是几个示例：
- en: '[PRE9]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The output shows `5.998548320199087`, which means 95% of the data is null. We
    should remove such columns from data analysis. Similarly, running the same command
    on the `total_tests_per_thousand` column returns `73.62090418913314`, which is
    much better than the previous column.
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出显示 `5.998548320199087`，这意味着 95% 的数据是空值。我们应该从数据分析中删除这些列。同样，在 `total_tests_per_thousand`
    列上运行相同命令返回 `73.62090418913314`，这比前一个列的结果要好得多。
- en: 'To drop some of the columns from the DataFrame, the next command can be used:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要从 DataFrame 中删除某些列，可以使用以下命令：
- en: '[PRE10]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'At times, you will need to have an aggregation of data. In such scenarios,
    you can perform the grouping of data, as shown here:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有时，您需要对数据进行聚合。在这种情况下，可以执行数据分组，如下所示：
- en: '[PRE11]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This will display the data from the `groupBy` statement:'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将显示来自 `groupBy` 语句的数据：
- en: '![The output of the groupby statement displaying the location and the max(date)
    column.](img/B15432_09_30.jpg)'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![groupby 语句的输出，显示 location 和 max(date) 列。](img/B15432_09_30.jpg)'
- en: 'Figure 9.30: Data from the groupby statement'
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.30：来自 groupby 语句的数据
- en: 'As you can see in the `max (date)` column, the dates are mostly the same for
    all the countries, we can use this value to filter the records and get a single
    row for each country representing the maximum date:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如你在 `max (date)` 列中看到的，所有国家的日期大多相同，我们可以使用这个值来过滤记录，并获取每个国家的最大日期对应的一行数据：
- en: '[PRE12]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If we take a count of records for the new DataFrame, we get `209`.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们对新 DataFrame 中的记录进行计数，结果是 `209`。
- en: 'We can save the new DataFrame into another CSV file, which may be needed by
    other data processors:'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以将新的 DataFrame 保存到另一个 CSV 文件中，其他数据处理器可能需要该文件：
- en: '[PRE13]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can check the newly created file with the following command:'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令检查新创建的文件：
- en: '[PRE14]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The mounted path will be displayed as shown in *Figure 9.31*:'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 挂载路径将显示如下图所示的 *图 9.31*：
- en: '![The mounted path within the Spark nodes displayed as dbfs:/mnt/coronadatastorage/uniquecountry.csv/.](img/B15432_09_31.jpg)'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Spark 节点中的挂载路径，显示为 dbfs:/mnt/coronadatastorage/uniquecountry.csv/.](img/B15432_09_31.jpg)'
- en: 'Figure 9.31: The mounted path within the Spark nodes'
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.31：Spark 节点中的挂载路径
- en: 'It is also possible to add the data into the Databricks catalog using the `createTempView`
    or `createOrReplaceTempView` method within the Databricks catalog. Putting data
    into the catalog makes it available in a given context. To add data into the catalog,
    the `createTempView` or `createOrReplaceTempView` method of the DataFrame can
    be used, providing a new view for the table within the catalog:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还可以使用 `createTempView` 或 `createOrReplaceTempView` 方法将数据添加到 Databricks 目录中。将数据放入目录后，可以在给定的上下文中使用它。要将数据添加到目录中，可以使用
    DataFrame 的 `createTempView` 或 `createOrReplaceTempView` 方法，为目录中的表提供一个新视图：
- en: '[PRE15]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once the table is in the catalog, it is accessible from your SQL session, as
    shown next:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦表格进入目录，就可以从你的 SQL 会话中访问，如下所示：
- en: '[PRE16]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The data from the SQL statement will apear as shown in *Figure 9.32*:'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SQL 语句中的数据将显示为 *图 9.32* 所示：
- en: '![Data from the SQL statement displaying the columns Location, date, and total_cases.](img/B15432_09_32.jpg)'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![SQL 语句中的数据，显示了 Location、date 和 total_cases 列。](img/B15432_09_32.jpg)'
- en: 'Figure 9.32: Data from the SQL statement'
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9.32：来自 SQL 语句的数据
- en: 'It possible to perform an additional SQL query against the table, as shown
    next:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以对表执行额外的 SQL 查询，如下所示：
- en: '[PRE17]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: That was a small glimpse of the possibilities with Databricks. There are many
    more features and services within it that could not be covered within a single
    chapter. Read more about it at [https://azure.microsoft.com/services/databricks](https://azure.microsoft.com/services/databricks).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是 Databricks 可能性的一小部分。它内部还有许多更多的功能和服务，无法在单一章节中全面覆盖。您可以在 [https://azure.microsoft.com/services/databricks](https://azure.microsoft.com/services/databricks)
    上了解更多相关信息。
- en: Summary
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter dealt with the Azure Data Factory service, which is responsible
    for providing ETL services in Azure. Since it is a platform as a service, it provides
    unlimited scalability, high availability, and easy-to-configure pipelines. Its
    integration with Azure DevOps and GitHub is also seamless. We also explored the
    features and benefits of using Azure Data Lake Gen2 Storage to store any kind
    of big data. It is a cost-effective, highly scalable, hierarchical data store
    for handling big data, and is compatible with Azure HDInsight, Databricks, and
    the Hadoop ecosystem.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了 Azure Data Factory 服务，它负责提供 Azure 中的 ETL 服务。作为一个平台即服务，它提供无限的可扩展性、高可用性和易于配置的管道。它与
    Azure DevOps 和 GitHub 的集成也非常流畅。我们还探索了使用 Azure Data Lake Gen2 存储大数据的功能和优势。它是一种成本效益高、具有高度可扩展性、分层的数据存储方式，用于处理大数据，并且与
    Azure HDInsight、Databricks 和 Hadoop 生态系统兼容。
- en: By no means did we have a complete deep dive into all the topics mentioned in
    this chapter. It was more about the possibilities in Azure, especially with Databricks
    and Spark. There are multiple technologies in Azure related to big data, including
    HDInsight, Hadoop, Spark and its related ecosystem, and Databricks, which is a
    Platform as a Service environment for Spark with added functionality. In the next
    chapter, you will learn about the serverless computing capabilities in Azure.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并没有对本章中提到的所有主题进行全面深入的探讨。重点更多的是探索 Azure 中的可能性，特别是 Databricks 和 Spark。Azure
    中涉及大数据的多个技术，包括 HDInsight、Hadoop、Spark 及其相关生态系统，还有 Databricks，它是一个为 Spark 提供附加功能的“平台即服务”环境。在下一章中，您将学习
    Azure 中的无服务器计算能力。
