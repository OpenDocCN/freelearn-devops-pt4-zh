- en: 12\. Azure Big Data eventing solutions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12. Azure 大数据事件解决方案
- en: Events are everywhere! Any activity or task that changes the state of a work
    item generates an event. Due to a lack of infrastructure and the non-availability
    of cheap devices, there previously was not much traction for the **Internet of
    Things** (**IoT**). Historically, organizations used hosted environments from **internet
    service providers** (**ISPs**) that just had monitoring systems on top of them.
    These monitoring systems raised events that were few and far between.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 事件无处不在！任何改变工作项状态的活动或任务都会生成一个事件。由于基础设施不足以及廉价设备的缺乏，**物联网**（**IoT**）此前并未获得太多关注。从历史上看，组织使用的是来自**互联网服务提供商**（**ISP**）的托管环境，这些环境上仅有监控系统。这些监控系统产生的事件少之又少。
- en: However, with the advent of the cloud, things are changing rapidly. With increased
    deployments on the cloud, especially of **Platform as a Service** (**PaaS**) services,
    organizations no longer need much control over the hardware and the platform,
    and now every time there is a change in an environment, an event is raised. With
    the emergence of cloud events, IoT has gained a lot of prominence and events have
    started to take center stage.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着云计算的兴起，事物正在迅速变化。随着云端部署的增加，尤其是**平台即服务**（**PaaS**）服务的普及，组织不再需要过多地控制硬件和平台，每当环境发生变化时，都会触发一个事件。随着云事件的出现，物联网变得越来越重要，事件也开始成为中心。
- en: Another recent phenomenon has been the rapid burst of growth in the availability
    of data. The velocity, variety, and volume of data has spiked, and so has the
    need for solutions for storing and processing data. Multiple solutions and platforms
    have emerged, such as Hadoop, data lakes for storage, data lakes for analytics,
    and machine learning services.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个最近的现象是数据可用性迅速增长。数据的速度、种类和数量急剧增加，存储和处理数据的需求也随之上升。多个解决方案和平台相继出现，例如 Hadoop、用于存储的数据湖、用于分析的数据湖和机器学习服务。
- en: Apart from storage and analytics, there is also a need for services that are
    capable of ingesting millions upon millions of events and messages from various
    sources. There is also a need for services that can work on temporal data, rather
    than working on an entire snapshot of data. For example, event data/IoT data is
    used in applications that make decisions based on real-time or near real-time
    data, such as traffic management systems or systems that monitor temperature.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 除了存储和分析外，还有一种需求是能够从各种来源摄取成千上万事件和消息的服务。还需要能够处理时间数据的服务，而不是处理整个数据快照的服务。例如，事件数据/物联网数据被用于做出基于实时或近实时数据的决策的应用程序，例如交通管理系统或监控温度的系统。
- en: 'Azure provides a plethora of services that help in capturing and analyzing
    real-time data from sensors. In this chapter, we will go through a couple of eventing
    services in Azure, as listed here:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 提供了大量帮助捕获和分析传感器实时数据的服务。在本章中，我们将介绍 Azure 中的几种事件服务，具体如下：
- en: Azure Event Hubs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure Event Hubs
- en: Azure Stream Analytics
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure Stream Analytics
- en: There are other eventing services, such as Azure Event Grid, that are not covered
    in this chapter; however, they are extensively covered in *Chapter 10, Azure Integration
    Services with Azure functions (Durable Functions and Proxy functions)*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他事件服务，例如 Azure Event Grid，本章未涉及；然而，它们在*第10章，使用 Azure 函数的 Azure 集成服务（耐久函数和代理函数）*中有广泛介绍。
- en: Introducing events
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入事件
- en: Events are important constructs in both Azure and Azure application architecture.
    Events are everywhere within the software ecosystem. Generally, any action that
    is taken results in an event that can be trapped, and then further action can
    be taken. To take this discussion forward, it is important to first understand
    the basics of events.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 事件是 Azure 和 Azure 应用架构中的重要构件。事件遍布整个软件生态系统。一般来说，任何采取的行动都会导致一个事件的发生，该事件可以被捕捉，然后采取进一步的行动。为了推动这个讨论，首先需要理解事件的基本概念。
- en: 'Events help in capturing the new state of a target resource. A message is a
    lightweight notification of a condition or a state change. Events are different
    than messages. Messages are related to business functionality, such as sending
    order details to another system. They contain raw data and can be large in size.
    In comparison, events are different; for instance, a virtual machine being stopped
    is an event. *Figure 12.1* demonstrates this transition from the current state
    to the target state:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 事件有助于捕获目标资源的新状态。消息是条件或状态变化的轻量级通知。事件与消息不同。消息与业务功能相关，例如将订单详情发送到另一个系统。它们包含原始数据，并且可能很大。相比之下，事件则不同；例如，虚拟机被停止就是一个事件。*图12.1*演示了从当前状态到目标状态的转变：
- en: '![Change of state of a work item due to an event](img/B15432_12_01.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![由于事件引起的工作项状态变化](img/B15432_12_01.jpg)'
- en: 'Figure 12.1: Transition of a state due to an event'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12.1：由于事件引起的状态转变
- en: Events can be stored in durable storage as historical data and events can also
    be used to find patterns that are emerging on an ongoing basis. Events can be
    thought of as data being streamed constantly. To capture, ingest, and perform
    analysis on a stream of data, special infrastructure components that can read
    a small window of data and provide insights are needed, and that is where the
    Stream Analytics service comes into the picture.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 事件可以作为历史数据存储在持久存储中，事件还可以用来发现持续出现的模式。事件可以被视为不断流动的数据。为了捕获、摄取和分析一连串的数据，需要特定的基础设施组件，这些组件能够读取一小段数据并提供洞察力，这就是Stream
    Analytics服务的作用所在。
- en: Event streaming
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 事件流处理
- en: Processing events as they are ingested and streamed over a time window provides
    real-time insights about data. The time window could 15 minutes or an hour—the
    window is defined by the user and depends on the insights that are to be extracted
    from data. Take credit card swipes, for instance—millions of credit card swipes
    happen every minute, and fraud detection can be done over streamed events for
    a time window of one or two minutes.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据流式传输过程中对事件进行处理，能够提供有关数据的实时洞察。时间窗口可以是15分钟或一小时——窗口由用户定义，并取决于要从数据中提取的洞察。例如，信用卡刷卡，每分钟会有成千上万次刷卡事件，欺诈检测可以在流式事件上进行，时间窗口为一到两分钟。
- en: Event streaming refers to services that can accept data as and when it arises,
    rather than accepting it periodically. For example, event streams should be capable
    of accepting temperature information from devices as and when they send it, rather
    than making the data wait in a queue or a staging environment.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 事件流处理指的是能够随时接受数据的服务，而不是按周期性接收数据。例如，事件流应能够随时接收设备发送的温度信息，而不是让数据在队列或暂存环境中等待。
- en: Event streaming also has the capability of querying data while in transit. This
    is temporal data that is stored for a while, and the queries occur on the moving
    data; therefore, the data is not stationary. This capability is not available
    on other data platforms, which can only query stored data and not temporal data
    that has just been ingested.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 事件流处理还具有在数据传输过程中查询数据的能力。这是暂时存储的数据，查询发生在移动数据上；因此，数据不是静态的。其他数据平台无法实现这一功能，它们只能查询已存储的数据，而无法查询刚刚摄取的临时数据。
- en: Event streaming services should be able to scale easily to accept millions or
    even billions of events. They should be highly available such that sources can
    send events and data to them at any time. Real-time data ingestion and being able
    to work on that data, rather than data that's stored in a different location,
    is the key to event streaming.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 事件流服务应能够轻松扩展，接受数百万甚至数十亿个事件。它们应具有高度可用性，使得源可以随时向其发送事件和数据。实时数据摄取并能够在这些数据上工作，而不是在其他位置存储的数据，是事件流处理的关键。
- en: 'But when we already have so many data platforms with advanced query execution
    capabilities, why do we need event steaming? One of the main advantages of event
    streaming is that it provides real-time insights and information whose usefulness
    is time-dependent. The same information found after a few minutes or hours might
    not be that useful. Let''s consider some scenarios in which working on incoming
    data is quite important. These scenarios can''t be effectively and efficiently
    solved by existing data platforms:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 但当我们已经有了许多具备高级查询执行能力的数据平台时，为什么还需要事件流处理呢？事件流处理的主要优势之一是它提供实时洞察和信息，而这些信息的价值依赖于时间。几分钟或几小时后得到的相同信息可能就不那么有用了。让我们考虑一些处理传入数据非常重要的场景。这些场景是现有数据平台无法有效且高效解决的：
- en: '**Credit card fraud detection**: This should happen as and when a fraudulent
    transaction happens.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信用卡欺诈检测**：应该在发生欺诈交易时及时进行。'
- en: '**Telemetry information from sensors**: In the case of IoT devices sending
    vital information about their environments, the user should be notified as and when an
    anomaly is detected.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**来自传感器的遥测信息**：对于发送环境关键信息的物联网设备，用户应在发现异常时及时收到通知。'
- en: '**Live dashboards**: Event streaming is needed to create dashboards that show
    live information.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时仪表盘**：事件流处理用于创建显示实时信息的仪表盘。'
- en: '**Datacenter environment telemetry**: This will let the user know about any
    intrusions, security breaches, failures of components, and more.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据中心环境遥测**：这将让用户知道是否有入侵、安保漏洞、组件故障等问题发生。'
- en: There are many possibilities for applying event streaming within an enterprise,
    and its importance cannot be stressed enough.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 企业内应用事件流处理的可能性非常多，它的重要性无法过分强调。
- en: Event Hubs
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Event Hubs
- en: Azure Event Hubs is a streaming platform that provides functionality related
    to the ingestion and storage of streaming-related events.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Event Hubs 是一个流处理平台，提供与流式事件的获取和存储相关的功能。
- en: 'It can ingest data from a variety of sources; these sources could be IoT sensors
    or any applications using the Event Hubs **Software Development Kit** (**SDK**).
    It supports multiple protocols for ingesting and storing data. These protocols
    are industry standard, and they include the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以从多种来源获取数据；这些来源可以是物联网传感器或任何使用 Event Hubs **软件开发工具包** (**SDK**) 的应用程序。它支持多种协议来获取和存储数据。这些协议是行业标准，包括以下几种：
- en: '**HTTP**: This is a stateless option and does not require an active session.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HTTP**：这是一种无状态的选项，不需要活动会话。'
- en: '**Advanced Messaging Queuing Protocol** (**AMQP**): This requires an active
    session (that is, an established connection using sockets) and works with **Transport
    Layer Security** (**TLS**) and **Secure Socket Layer** (**SSL**).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级消息队列协议** (**AMQP**)：这需要一个活动会话（即通过套接字建立的连接），并与 **传输层安全性** (**TLS**) 和 **安全套接字层**
    (**SSL**) 一起使用。'
- en: '**Apache Kafka**: This is a distributed streaming platform similar to Stream
    Analytics. However, Stream Analytics is designed to run real-time analytics on
    multiple streams of data from various sources, such as IoT sensors and websites.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Kafka**：这是一种类似于流分析的分布式流处理平台。然而，流分析旨在对来自多个数据源（如物联网传感器和网站）的数据流进行实时分析。'
- en: Event Hubs is an event ingestion service. It can't query a request and output
    query results to another location. That is the responsibility of Stream Analytics,
    which is covered in the next section.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Event Hubs 是一个事件获取服务。它不能查询请求并将查询结果输出到其他位置。这是 Stream Analytics 的责任，后文将介绍。
- en: 'To create an Event Hubs instance from the portal, search for Event Hubs in
    Marketplace and click on **Create**. Select a subscription and an existing resource
    group (or create a new one). Provide a name for the Event Hubs namespace, the
    preferred Azure region to host it in, the pricing tier (Basic or Standard, explained
    later), and the number of throughput units (explained later):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要从门户创建 Event Hubs 实例，请在市场中搜索 Event Hubs 并点击 **创建**。选择一个订阅和现有的资源组（或创建一个新的）。为
    Event Hubs 命名空间提供名称，选择首选的 Azure 区域进行托管，定价层级（后文将解释 Basic 或 Standard），以及吞吐量单元的数量（后文将解释）：
- en: '![Creating an Event Hub namespace in the Azure portal](img/B15432_12_02.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![在 Azure 门户中创建 Event Hub 命名空间](img/B15432_12_02.jpg)'
- en: 'Figure 12.2: Creating an Event Hubs namespace'
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.2：创建 Event Hubs 命名空间
- en: Event Hubs, being a PaaS service, is highly distributed, highly available, and
    highly scalable.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Event Hubs 作为一个 PaaS 服务，具有高度分布式、高可用性和高度可扩展性。
- en: 'Event Hubs comes with the following two SKUs or pricing tiers:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Event Hubs提供以下两种SKU或定价层：
- en: '**Basic**: This comes with one consumer group and can retain messages for 1
    day. It can have a maximum of 100 brokered connections.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础版**：此SKU支持一个消费者组，并且可以保留消息1天。最多支持100个中介连接。'
- en: '**Standard**: This comes with a maximum of 20 consumer groups and can retain
    messages for 1 day with additional storage for 7 days. It can have a maximum of
    1,000 brokered connections. It is also possible to define policies in this SKU.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准版**：此SKU最多支持20个消费者组，能够保留消息1天，并且可以额外存储7天的消息。最多支持1,000个中介连接。还可以在此SKU中定义策略。'
- en: '*Figure 12.3* shows the different SKUs available while creating a new Event
    Hubs namespace. It provides an option to choose an appropriate pricing tier, along
    with other important details:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.3*展示了在创建新Event Hubs命名空间时可用的不同SKU。它提供了选择适当定价层的选项，以及其他重要细节：'
- en: '![Comparing Basic and Standard SKU features](img/B15432_12_03.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![比较基本版和标准版SKU的功能](img/B15432_12_03.jpg)'
- en: 'Figure 12.3: Event Hubs SKUs'
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12.3：Event Hubs SKUs
- en: 'Throughput can also be configured at the namespace level. Namespaces are containers
    that consist of multiple event hubs in the same subscription and region. The throughput
    is calculated as **throughput units** (**TUs**). Each TU provides:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 吞吐量也可以在命名空间级别进行配置。命名空间是容器，由同一订阅和区域中的多个事件中心组成。吞吐量以**吞吐量单元**（**TUs**）来计算。每个TU提供：
- en: Up to 1 MB per second of ingress or a maximum of 1,000 ingress events and management
    operations per second.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每秒最多1 MB的入站流量，或者每秒最多1,000个入站事件和管理操作。
- en: Up to 2 MB per second of egress or a maximum of 4,096 events and management
    operations per second.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每秒最多2 MB的出站流量，或者每秒最多4,096个事件和管理操作。
- en: Up to 84 GB of storage.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最多可存储84 GB的存储空间。
- en: The TUs can range from 1 to 20 and they are billed on an hourly basis.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: TUs的数量可以从1到20不等，并且按小时计费。
- en: It is important to note that the SKU cannot be changed after provisioning an
    Event Hubs namespace. Due consideration and planning should be undertaken before
    selecting an SKU. The planning process should include planning the number of consumer
    groups required and the number of applications interested in reading events from
    the event hub.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，Event Hubs命名空间创建后无法更改SKU。选择SKU之前需要进行充分的考虑和规划。规划过程应包括确定所需的消费者组数量以及感兴趣读取事件的应用程序数量。
- en: Also, the Standard SKU is not available in every region. It should be checked
    for availability at the time of the design and implementation of the event hub.
    The URL for checking region availability is [https://azure.microsoft.com/global-infrastructure/services/?products=event-hubs](https://azure.microsoft.com/global-infrastructure/services/?products=event-hubs).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，并非每个区域都提供标准版SKU。在设计和实现Event Hub时应检查其可用性。检查区域可用性的URL是[https://azure.microsoft.com/global-infrastructure/services/?products=event-hubs](https://azure.microsoft.com/global-infrastructure/services/?products=event-hubs)。
- en: Event Hubs architecture
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Event Hubs架构
- en: 'There are three main components of the Event Hubs architecture: The **Event
    Producers**, the **Event Hub**, and the **Event Consumer**, as shown in the following diagram:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Event Hubs架构有三个主要组成部分：**事件生产者**、**事件中心**和**事件消费者**，如下图所示：
- en: '![A basic Event Hubs architecture](img/B15432_12_04.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![基本的Event Hubs架构](img/B15432_12_04.jpg)'
- en: 'Figure 12.4: Event Hubs architecture'
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12.4：Event Hubs架构
- en: '**Event Producers** generate events and send them to the **Event Hub**. The **Event
    Hub** stores the ingested events and provides that data to the **Event Consumer**.
    The **Event Consumer** is whatever is interested in those events, and it connects
    to the **Event Hub** to fetch the data.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**事件生产者**生成事件并将其发送到**事件中心**。**事件中心**存储接收的事件，并将这些数据提供给**事件消费者**。**事件消费者**是对这些事件感兴趣的任何实体，它连接到**事件中心**以提取数据。'
- en: Event hubs cannot be created without an Event Hubs namespace. The Event Hubs
    namespace acts as a container and can host multiple event hubs. Each Event Hubs
    namespace provides a unique REST-based endpoint that is consumed by clients to
    send data to Event Hubs. This namespace is the same namespace that is needed for
    Service Bus artifacts, such as topics and queues.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 创建Event Hubs时必须先创建Event Hubs命名空间。Event Hubs命名空间作为容器，可以托管多个事件中心。每个Event Hubs命名空间提供一个唯一的基于REST的端点，供客户端用于向Event
    Hubs发送数据。此命名空间与Service Bus组件（如主题和队列）所需的命名空间相同。
- en: 'The connection string of an Event Hubs namespace is composed of its URL, policy
    name, and key. A sample connection string is shown in the following code block:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 事件中心命名空间的连接字符串由其 URL、策略名称和密钥组成。以下代码块展示了一个示例连接字符串：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This connection string can be found in the **Shared Access Signature** (**SAS**)
    menu item of the namespace. There can be multiple policies defined for a namespace,
    each having different levels of access to the namespace. The three levels of access
    are as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个连接字符串可以在命名空间的**共享访问签名**（**SAS**）菜单项中找到。命名空间可以定义多个策略，每个策略具有不同的访问级别。访问级别如下：
- en: '**Manage**: This can manage the event hub from an administrative perspective.
    It also has rights for sending and listening to events.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管理**：此选项可以从管理角度管理事件中心。它还具有发送和监听事件的权限。'
- en: '**Send**: This can write events to Event Hubs.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**发送**：此选项可以将事件写入事件中心。'
- en: '**Listen**: This can read events from Event Hubs.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监听**：此选项可以从事件中心读取事件。'
- en: 'By default, the `RootManageSharedAccessKey` policy is created when creating
    an event hub, as shown in *Figure 12.5*. Policies help in creating granular access control on
    Event Hubs. The key associated with each policy is used by consumers to determine
    their identity; additional policies can also be created with any combination of
    the three previously mentioned access levels:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，在创建事件中心时会创建`RootManageSharedAccessKey`策略，如*图12.5*所示。策略有助于在事件中心上创建细粒度的访问控制。与每个策略相关联的密钥由消费者用来确定其身份；还可以创建附加策略，并使用之前提到的三种访问级别的任何组合：
- en: '![A list of shared access policies in Event Hubs](img/B15432_12_05.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![事件中心中的共享访问策略列表](img/B15432_12_05.jpg)'
- en: 'Figure 12.5: Shared access policies in Event Hubs'
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12.5：事件中心中的共享访问策略
- en: 'Event hubs can be created from the Event Hubs namespace service by performing
    the following actions:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过执行以下操作，从事件中心命名空间服务中创建事件中心：
- en: Click on **Event Hubs** in the left-hand menu and click on **+ Event Hub** in
    the resultant screen:![Creating an Event Hub from the Azure portal](img/B15432_12_06.jpg)
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击左侧菜单中的**事件中心**，然后在结果屏幕中点击**+ 事件中心**：![从 Azure 门户创建事件中心](img/B15432_12_06.jpg)
- en: 'Figure 12.6: Creating an event hub from the Azure portal'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12.6：从 Azure 门户创建事件中心
- en: 'Next, provide values for the **Partition Count** and **Message Retention** fields,
    along with the name of your choice. Then, select **Off** for **Capture**, as demonstrated
    in *Figure 12.7*:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，提供**分区数**和**消息保留**字段的值，以及你选择的名称。然后，选择**关闭**作为**捕获**选项，如*图12.7*所示：
- en: '![Providing the Event Hub parameters](img/B15432_12_07.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![提供事件中心参数](img/B15432_12_07.jpg)'
- en: 'Figure 12.7: Creating a new event hub'
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12.7：创建新的事件中心
- en: 'After the event hub is created, you will see it in the list of event hubs,
    as shown in *Figure 12.8*:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 创建事件中心后，你将在事件中心列表中看到它，如*图12.8*所示：
- en: '![List of created event hubs in Azure](img/B15432_12_08.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![Azure 中创建的事件中心列表](img/B15432_12_08.jpg)'
- en: 'Figure 12.8: List of created event hubs'
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12.8：已创建的事件中心列表
- en: Event Hubs also allows the storage of events to a storage account or data lake
    directly using a feature known as Capture.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 事件中心还允许通过名为“捕获”的功能直接将事件存储到存储账户或数据湖中。
- en: 'Capture helps in the automatic storage of ingested data to either an Azure
    storage account or an Azure data lake. This feature ensures that the ingestion
    and storage of events happens in a single step, rather than transferring data
    into storage being a separate activity:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 捕获功能帮助将摄取的数据自动存储到 Azure 存储账户或 Azure 数据湖中。此功能确保数据的摄取和存储在一个步骤中完成，而不是将数据转移到存储中作为单独的活动：
- en: '![Capture feature options](img/B15432_12_09.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![捕获功能选项](img/B15432_12_09.jpg)'
- en: 'Figure 12.9: Capture feature options'
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12.9：捕获功能选项
- en: Separate policies can be assigned to each event hub by adding a new policy at
    the event hub level.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在事件中心级别添加新策略，将单独的策略分配给每个事件中心。
- en: After creating the policy, the connection string is available from the **Secure
    Access Signature** left-menu item in the Azure portal.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 创建策略后，可以从 Azure 门户中**安全访问签名**左侧菜单项获取连接字符串。
- en: 'Since a namespace can consist of multiple event hubs, the connection string
    for an individual event hub will be similar to the following code block. The difference
    here is in the key value and the addition of `EntityPath` with the name of the
    event hub:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一个命名空间可以包含多个事件中心，单个事件中心的连接字符串将类似于以下代码块。这里的区别在于密钥值和添加了`EntityPath`，它指向事件中心的名称：
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We had to keep the **Capture** option set to **Off** while creating the event
    hub, and it can be switched back on after creating the event hub. It helps to
    save events to Azure Blob storage or an Azure Data Lake storage account automatically.
    The configuration for the size and time interval is shown in *Figure 12.10*:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建事件中心时，我们必须保持**捕获**选项设置为**关闭**，但在事件中心创建后可以重新打开它。它有助于将事件自动保存到 Azure Blob 存储或
    Azure 数据湖存储帐户中。大小和时间间隔的配置如*图 12.10*所示：
- en: '![Selecting size and time intervals for capturing events](img/B15432_12_10.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![选择捕获事件的大小和时间间隔](img/B15432_12_10.jpg)'
- en: 'Figure 12.10: Selecting the size and time interval for capturing events'
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.10：选择捕获事件的大小和时间间隔
- en: We did not cover the concepts of partitions and message retention options while
    creating event hubs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建事件中心时，我们没有涵盖分区和消息保留选项的概念。
- en: Partitioning is an important concept related to the scalability of any data
    store. Events are retained within event hubs for a specific period of time. If
    all events are stored within the same data store, then it becomes extremely difficult
    to scale that data store. Every event producer will connect to the same data store
    and send their events to it. Compare this with a data store that can partition
    the same data into multiple smaller data stores, each being uniquely identified
    with a value.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 分区是与任何数据存储的可扩展性相关的重要概念。事件在事件中心内会保留特定的时间。如果所有事件都存储在同一个数据存储中，那么扩展这个数据存储将变得极其困难。每个事件生产者都会连接到同一个数据存储并将事件发送到其中。与此相比，具有分区能力的数据存储可以将相同的数据分割成多个更小的数据存储，每个存储都有一个唯一的标识值。
- en: The smaller data store is called a **partition**, and the value that defines
    the partition is known as the **partition key**. This partition key is part of
    the event data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 更小的数据存储被称为**分区**，定义该分区的值被称为**分区键**。这个分区键是事件数据的一部分。
- en: Now the event producers can connect to the event hub, and based on the value
    of the partition key, the event hub will store the data in an appropriate partition.
    This will allow the event hub to ingest multiple events at the same time in parallel.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，事件生产者可以连接到事件中心，并根据分区键的值，事件中心会将数据存储在适当的分区中。这将允许事件中心同时并行地接收多个事件。
- en: 'Deciding on the number of partitions is a crucial aspect of the scalability
    of an event hub. *Figure 12.11* shows that ingested data is stored in the appropriate
    partition internally by Event Hubs using the partition key:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 确定分区数量是事件中心可扩展性的一个关键方面。*图 12.11* 显示了事件中心如何使用分区键将接收到的数据存储在适当的分区中：
- en: '![The partitioning concept in Event Hub](img/B15432_12_11.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![事件中心中的分区概念](img/B15432_12_11.jpg)'
- en: 'Figure 12.11: Partitioning in an event hub'
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.11：事件中心中的分区
- en: It is important to understand that one partition might have multiple keys. The
    user decides how many partitions are required, and the event hub internally decides
    the best way to allocate the partition keys between them. Each partition stores
    data in an orderly way using a timestamp, and newer events are appended toward
    the end of the partition.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 需要理解的是，一个分区可能有多个键。用户决定需要多少个分区，事件中心则会内部决定最佳的方式在这些分区之间分配分区键。每个分区都会使用时间戳按顺序存储数据，较新的事件会附加到分区的末尾。
- en: It is important to note that it is not possible to change the number of partitions
    once the event hub is created.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，一旦事件中心创建完成，就无法更改分区的数量。
- en: It is also important to remember that partitions also help in bringing parallelism
    and concurrency for applications reading the events. For example, if there are
    10 partitions, 10 parallel readers can read the events without any degradation
    in performance.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要记住，分区还可以为读取事件的应用程序带来并行性和并发性。例如，如果有 10 个分区，10 个并行读取器可以同时读取事件，而不会出现性能下降。
- en: Message retention refers to the time period for which events should be stored.
    After the expiry of the retention period, the events are discarded.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 消息保留期指的是事件应该存储的时间段。在保留期过后，事件将被丢弃。
- en: Consumer groups
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 消费者组
- en: 'Consumers are applications that read events from an event hub. Consumer groups
    are created for consumers to connect to in order to read the events. There can
    be multiple consumer groups for an event hub, and each consumer group has access
    to all the partitions within an event hub. Each consumer group forms a query on
    the events in events hubs. Applications can use consumer groups and each application
    will get a different view of the event hub events. A default `$default` consumer
    group is created when creating an event hub. It is good practice for one consumer
    to be associated with one consumer group for optimal performance. However, it
    is possible to have five readers on each partition in a consumer group:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者是从事件中心读取事件的应用程序。消费组是为消费者创建的，以便消费者连接到并读取事件。一个事件中心可以有多个消费组，每个消费组都可以访问事件中心中的所有分区。每个消费组都会针对事件中心中的事件形成一个查询。应用程序可以使用消费组，每个应用程序将看到事件中心事件的不同视图。在创建事件中心时，会自动创建一个默认的`$default`消费组。为了优化性能，最好将一个消费者与一个消费组关联。然而，每个消费组中的每个分区最多可以有五个读取器：
- en: '![Event receivers in a consumer group](img/B15432_12_12.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![消费组中的事件接收器](img/B15432_12_12.jpg)'
- en: 'Figure 12.12: Event receivers in a consumer group'
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.12：消费组中的事件接收器
- en: Now that you understand consumer groups, it is time to go deeper into the concept
    of Event Hubs throughput.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了消费组的概念，接下来我们将深入了解 Event Hubs 吞吐量的概念。
- en: Throughput
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 吞吐量
- en: Partitions help with scalability, while throughput helps with capacity per second.
    So, what is capacity in terms of Event Hubs? It is the amount of data that can
    be handled per second.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 分区有助于扩展性，而吞吐量则决定了每秒的容量。那么，在 Event Hubs 中，容量是什么？它是每秒可以处理的数据量。
- en: 'In Event Hubs, a single TU allows the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Event Hubs 中，一个单一的 TU 支持以下内容：
- en: 1 MB of ingestion data per second or 1,000 events per second (whichever happens
    first)
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每秒 1 MB 的数据摄取或每秒 1,000 个事件（以先发生者为准）
- en: 2 MB of egress data per second or 4,096 events per second (whichever happens
    first)
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每秒 2 MB 的数据输出或每秒 4,096 个事件（以先发生者为准）
- en: 'The auto-inflate option helps in increasing the throughput automatically if
    the number of incoming/outgoing events or the incoming/outgoing total size crosses
    a threshold. Instead of throttling, the throughput will scale up and down. The
    configuration of throughput at the time of the creation of the namespace is shown
    in *Figure 12.13*. Again, careful thought should go into deciding the TUs:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 自动扩展选项在当传入/传出事件的数量或传入/传出的总大小超过阈值时，自动增加吞吐量。吞吐量会随需求自动扩展或收缩，而不是进行限速。在命名空间创建时的吞吐量配置如*图
    12.13*所示。再次强调，决定 TUs 时需要慎重考虑：
- en: '![Selecting the throughput units and enabling auto-inflate](img/B15432_12_13.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![选择吞吐量单位并启用自动扩展](img/B15432_12_13.jpg)'
- en: 'Figure 12.13: Selecting the TUs along with auto-inflate'
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.13：选择 TUs 并启用自动扩展
- en: A primer on Stream Analytics
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Stream Analytics 入门
- en: Event Hubs is a highly scalable data streaming platform, so we need another
    service that can process these events as a stream rather than just as stored data.
    Stream Analytics helps in processing and examining a stream of big data, and Stream
    Analytics jobs help to execute the processing of events.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Event Hubs 是一个高度可扩展的数据流平台，因此我们需要另一个可以将这些事件作为流处理的服务，而不仅仅是存储数据。Stream Analytics
    帮助处理和检查大数据流，Stream Analytics 作业则帮助执行事件处理。
- en: Stream Analytics can process millions of events per second and it is quite easy
    to get started with it. Azure Stream Analytics is a PaaS that is completely managed
    by Azure. Customers of Stream Analytics do not have to manage the underlying hardware
    and platform.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Stream Analytics 可以处理每秒数百万个事件，而且入门非常简单。Azure Stream Analytics 是一个完全由 Azure 管理的
    PaaS 服务。Stream Analytics 的客户无需管理底层硬件和平台。
- en: 'Each job comprises multiple inputs, outputs, and a query, which transforms
    the incoming data into new output. The whole architecture of Stream Analytics
    is shown in *Figure 12.14*:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 每个作业包含多个输入、输出和一个查询，该查询将传入数据转换为新的输出。Stream Analytics 的整个架构如*图 12.14*所示：
- en: '![Azure Stream Analytics architecture](img/B15432_12_14.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![Azure Stream Analytics 架构](img/B15432_12_14.jpg)'
- en: 'Figure 12.14: Azure Stream Analytics architecture'
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.14：Azure Stream Analytics 架构
- en: In *Figure 12.14*, the event sources are displayed on the extreme left. These
    are the sources that produce the events. They could be IoT devices, custom applications
    written in any programming language, or events coming from other Azure platforms,
    such as Log Analytics or Application Insights.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 12.14*中，事件源显示在最左侧。这些是生成事件的源。它们可以是物联网设备、用任何编程语言编写的自定义应用程序，或者来自其他 Azure 平台的事件，例如
    Log Analytics 或 Application Insights。
- en: These events must first be ingested into the system, and there are numerous
    Azure services that can help to ingest this data. We've already looked at Event
    Hubs and how they help in ingesting data. There are other services, such as IoT
    Hub, that also help in ingesting device-specific and sensor-specific data. IoT
    Hub and ingestion are covered in detail in *Chapter 11, Designing IoT Solutions*.
    This ingested data undergoes processing as it arrives in a stream, and this processing
    is done using Stream Analytics. The output from Stream Analytics could be fed
    to a presentation platform, such as Power BI, to show real-time data to stakeholders,
    or a storage platform such as Cosmos DB, Data Lake Storage, or Azure Storage,
    from which the data can be read and actioned later by Azure Functions and Service
    Bus queues.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这些事件必须首先被摄取到系统中，Azure 提供了许多服务来帮助摄取这些数据。我们已经查看了 Event Hubs 以及它们如何帮助摄取数据。还有其他服务，如
    IoT Hub，也有助于摄取特定设备和传感器的数据。IoT Hub 和数据摄取的详细信息请参见*第 11 章，设计物联网解决方案*。这些摄取的数据在到达流时会进行处理，处理工作由
    Stream Analytics 完成。Stream Analytics 的输出可以传输到展示平台，如 Power BI，向利益相关者展示实时数据，或传输到存储平台，如
    Cosmos DB、Data Lake Storage 或 Azure 存储，数据可以稍后通过 Azure Functions 和 Service Bus
    队列进行读取和处理。
- en: Stream Analytics helps in gathering insights from real-time ingested data within
    a time window frame and helps in identifying patterns.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Stream Analytics 有助于从实时摄取的数据中在时间窗口框架内获取洞察，并帮助识别模式。
- en: 'It does so through three different tasks:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过三种不同的任务来完成这一过程：
- en: '**Input**: The data should be ingested within the analytics process. The data
    can originate from Event Hubs, IoT Hub, or Azure Blob storage. Multiple separate
    reference inputs using a storage account and SQL Database can be used for lookup
    data within queries.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入**：数据应该在分析过程中被摄取。数据可以来源于 Event Hubs、IoT Hub 或 Azure Blob 存储。可以使用存储账户和 SQL
    数据库的多个独立参考输入来查询数据中的查找数据。'
- en: '**Query**: This is where Stream Analytics does the core job of analyzing the
    ingested data and extracting meaningful insights and patterns. It does so with
    the help of JavaScript user-defined functions, JavaScript user-defined aggregates,
    Azure Machine Learning, and Azure Machine Learning studio.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询**：这是 Stream Analytics 执行核心任务的地方，它分析摄取的数据并提取有意义的洞察和模式。它通过 JavaScript 用户定义函数、JavaScript
    用户定义聚合、Azure 机器学习和 Azure 机器学习工作室来实现。'
- en: '**Output**: The result of the queries can be sent to multiple different types
    of destinations, and prominent among them are Cosmos DB, Power BI, Synapse Analytics,
    Data Lake Storage, and Functions:'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出**：查询结果可以发送到多种不同类型的目标，其中突出的目标包括 Cosmos DB、Power BI、Synapse Analytics、Data
    Lake Storage 和 Functions：'
- en: '![Stream Analytics process](img/B15432_12_15.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![Stream Analytics 过程](img/B15432_12_15.jpg)'
- en: 'Figure 12.15: Stream Analytics process'
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.15：Stream Analytics 过程
- en: Stream Analytics is capable of ingesting millions of events per second and can
    execute queries on top of them.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Stream Analytics 能够每秒摄取数百万个事件，并可以在这些事件上执行查询。
- en: 'Input data is supported in any of the three following formats:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据支持以下三种格式之一：
- en: '**JavaScript Object Notation** (**JSON**): This is a lightweight, plaintext-based
    format that is human readable. It consists of name-value pairs; an example of
    a JSON event follows:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JavaScript 对象表示法**（**JSON**）：这是一种轻量级的基于文本的格式，易于人类阅读。它由名称-值对组成；以下是一个 JSON
    事件示例：'
- en: '[PRE2]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Comma-Separated Values** (**CSV**): These are also plaintext values, which
    are separated by commas. An example of CSV is shown in *Figure 12.16*. The first
    row is the header, containing three fields, followed by two rows of data:'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逗号分隔值**（**CSV**）：这些也是纯文本值，使用逗号分隔。*图 12.16*中展示了一个 CSV 示例。第一行是表头，包含三个字段，后面是两行数据：'
- en: '![Input in the CSV datatype](img/B15432_12_16.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![CSV 数据类型中的输入](img/B15432_12_16.jpg)'
- en: 'Figure 12.16: Plaintext values'
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.16：纯文本值
- en: '**Avro**: This format is similar to JSON; however, it is stored in a binary
    format rather than a text format:'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Avro**：这种格式类似于 JSON；然而，它以二进制格式存储，而不是文本格式：'
- en: '[PRE3]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: However, this does not mean that Stream Analytics can only ingest data using
    these three formats. It can also create custom .NET-based deserializers, using
    which any format of data can be ingested, depending upon the deserializers' implementation.
    The steps you can follow to write a custom deserializer are available at [https://docs.microsoft.com/azure/stream-analytics/custom-deserializer-examples](https://docs.microsoft.com/azure/stream-analytics/custom-deserializer-examples).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不意味着流分析只能使用这三种格式来摄取数据。它还可以创建自定义的 .NET 序列化器，通过这些序列化器，任何格式的数据都可以根据序列化器的实现进行摄取。您可以按照[https://docs.microsoft.com/azure/stream-analytics/custom-deserializer-examples](https://docs.microsoft.com/azure/stream-analytics/custom-deserializer-examples)中的步骤编写自定义序列化器。
- en: Not only can Stream Analytics receive events, but it also provides advanced
    query capability for the data that it receives. The queries can extract important
    insights from the temporal data streams and output them.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 流分析不仅可以接收事件，还提供对接收到数据的高级查询功能。这些查询可以从时间数据流中提取重要见解并输出。
- en: 'As shown in *Figure 12.17*, there is an input dataset and an output dataset;
    the query moves the events from the input to the output. The `INTO` clause refers
    to the output location, and the `FROM` clause refers to the input location. The
    queries are very similar to SQL queries, so the learning curve is not too steep
    for SQL programmers:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 12.17*所示，有一个输入数据集和一个输出数据集；查询将事件从输入移动到输出。`INTO` 子句指向输出位置，`FROM` 子句指向输入位置。查询非常类似于
    SQL 查询，因此 SQL 程序员的学习曲线不会太陡峭：
- en: '![Stream Analytics query for receiving Twitter data](img/B15432_12_17.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![接收 Twitter 数据的流分析查询](img/B15432_12_17.jpg)'
- en: 'Figure 12.17: Stream Analytics query for receiving Twitter data'
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.17：接收 Twitter 数据的流分析查询
- en: Event Hubs provides mechanisms for sending outputs from queries to target destinations.
    At the time of writing, Stream Analytics supports multiple destinations for events
    and query outputs, as shown before.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 事件中心提供了将查询输出发送到目标位置的机制。撰写本文时，流分析支持多种事件和查询输出的目标位置，如前所示。
- en: It is also possible to define custom functions that can be reused within queries.
    There are four options provided to define custom functions.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以定义可以在查询中重复使用的自定义函数。提供了四种选项来定义自定义函数。
- en: Azure Machine Learning
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure 机器学习
- en: JavaScript user-defined functions
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JavaScript 用户定义的函数
- en: JavaScript user-defined aggregates
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JavaScript 用户定义的聚合
- en: Azure Machine Learning studio
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure 机器学习工作室
- en: The hosting environment
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 托管环境
- en: 'Stream Analytics jobs can run on hosts that are running on the cloud, or they
    can run on IoT edge devices. IoT edge devices are devices that are near to IoT
    sensors, rather than on the cloud. *Figure 12.18* shows the **New Stream Analytics
    job** pane:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 流分析作业可以在云端运行的主机上运行，也可以在物联网边缘设备上运行。物联网边缘设备是靠近物联网传感器的设备，而不是在云上。*图 12.18*展示了**新建流分析作业**面板：
- en: '![Creating a new Stream Analytics job](img/B15432_12_18.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![创建一个新的流分析作业](img/B15432_12_18.jpg)'
- en: 'Figure 12.18: Creating a new Stream Analytics job'
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.18：创建一个新的流分析作业
- en: Let's check out streaming units in detail.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细查看流处理单元。
- en: Streaming units
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 流处理单元
- en: From *Figure 12.18*, you can see that the only configuration that is unique to
    Stream Analytics is streaming units. Streaming units refers to the resources (that
    is, CPU and memory) that are assigned for running a Stream Analytics job. The
    minimum and maximum streaming units are 1 and 120, respectively.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 12.18*中可以看到，流分析中唯一的配置是流处理单元。流处理单元指的是为运行流分析作业分配的资源（即 CPU 和内存）。最小和最大流处理单元分别为
    1 和 120。
- en: Streaming units must be pre-allocated according to the amount of data and the
    number of queries executed on that data; otherwise, the job will fail.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理单元必须根据数据量和在该数据上执行的查询数量预先分配；否则，作业将失败。
- en: It is possible to scale streaming units up and down from the Azure portal.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过 Azure 门户向上或向下扩展流处理单元。
- en: A sample application using Event Hubs and Stream Analytics
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用事件中心和流分析的示例应用程序
- en: In this section, we will be creating a sample application comprising multiple
    Azure services, including Azure Logic Apps, Azure Event Hubs, Azure Storage, and
    Azure Stream Analytics.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将创建一个包含多个 Azure 服务的示例应用程序，包含 Azure 逻辑应用、Azure 事件中心、Azure 存储和 Azure 流分析。
- en: In this sample application, we will be reading all tweets containing the word
    "Azure" and storing them in an Azure storage account.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例应用程序中，我们将读取所有包含“Azure”一词的推文，并将其存储在 Azure 存储账户中。
- en: To create this solution, we first need to provision all the necessary resources.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建这个解决方案，我们首先需要配置所有必要的资源。
- en: Provisioning a new resource group
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置一个新的资源组
- en: 'Navigate to the Azure portal, log in using valid credentials, click on **+
    Create a resource**, and search for **Resource group**. Select **Resource group** from
    the search results and create a new resource group. Then, provide a name and choose
    an appropriate location. Note that all resources should be hosted in the same
    resource group and location so that it is easy to delete them:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 导航到 Azure 门户，使用有效凭据登录，点击 **+ 创建资源**，搜索 **资源组**。从搜索结果中选择 **资源组**，然后创建一个新的资源组。接着，提供一个名称并选择一个合适的位置。请注意，所有资源应托管在同一个资源组和位置中，以便于删除它们：
- en: '![Provisioning a new resource group in the Azure portal](img/B15432_12_19.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![在 Azure 门户中配置一个新的资源组](img/B15432_12_19.jpg)'
- en: 'Figure 12.19: Provisioning a new resource group in the Azure portal'
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.19：在 Azure 门户中配置一个新的资源组
- en: Next, we will create an Event Hubs namespace.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个 Event Hubs 命名空间。
- en: Creating an Event Hubs namespace
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建一个 Event Hubs 命名空间
- en: 'Click on **+ Create a resource** and search for **Event Hubs**. Select **Event
    Hubs** from the search results and create a new event hub. Then, provide a name
    and location, and select a subscription based on the resource group that was created
    earlier. Select **Standard** as the pricing tier and also select **Enable Auto-inflate**,
    as shown in *Figure 12.20*:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 点击 **+ 创建资源**，并搜索 **Event Hubs**。从搜索结果中选择 **Event Hubs**，然后创建一个新的事件中心。接着，提供一个名称和位置，并根据之前创建的资源组选择一个订阅。选择
    **Standard** 作为定价层，并且勾选 **启用自动扩展**，如 *图 12.20* 所示：
- en: '![Creating an Event Hubs namespace](img/B15432_12_20.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![创建一个 Event Hubs 命名空间](img/B15432_12_20.jpg)'
- en: 'Figure 12.20: Creating an Event Hubs namespace'
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.20：创建一个 Event Hubs 命名空间
- en: By now, an Event Hubs namespace should have been created. It is a pre-requisite
    to have a namespace before an event hub can be created. The next step is to provision
    an event hub.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，应该已经创建了一个 Event Hubs 命名空间。在创建事件中心之前，必须先有一个命名空间。下一步是配置事件中心。
- en: Creating an event hub
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建一个事件中心
- en: 'From the Event Hubs namespace service, click on **Events Hubs** in the left-hand
    menu, and then click on **+ Event hubs** to create a new event hub. Name it **azuretwitterdata** and
    provide an optimal number of partitions and a **Message Retention** value:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Event Hubs 命名空间服务中，点击左侧菜单中的 **Events Hubs**，然后点击 **+ Event Hubs** 创建一个新的事件中心。命名为
    **azuretwitterdata**，并提供一个最佳的分区数量和 **消息保留** 值：
- en: '![Creating an event hub with the desired credentials](img/B15432_12_21.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![创建具有所需凭据的事件中心](img/B15432_12_21.jpg)'
- en: 'Figure 12.21: Creating the azuretwitterdata event hub'
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.21：创建 azuretwitterdata 事件中心
- en: After this step, you will have an event hub that can be used to send event data,
    which is stored in durable storage such as a data lake or an Azure Storage account,
    to be used by downstream services.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在此步骤之后，您将拥有一个事件中心，用于将事件数据发送到下游服务，这些数据将存储在持久存储中，如数据湖或 Azure 存储帐户中。
- en: Provisioning a logic app
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置一个逻辑应用
- en: 'After the resource group is provisioned, click on **+ Create a resource** and
    search for **Logic Apps**. Select **Logic Apps** from the search results and create
    a new logic app. Then, provide a name and location, and select a subscription
    based on the resource group created earlier. It is good practice to enable **Log
    Analytics**. Logic Apps is covered in more detail in *Chapter 11, Azure Solutions
    using Azure Logic Apps, Event Grid, and Functions*. The logic app is responsible
    for connecting to Twitter using an account and fetching all the tweets with **Azure** in
    them:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在资源组配置完成后，点击 **+ 创建资源**，搜索 **Logic Apps**。从搜索结果中选择 **Logic Apps**，然后创建一个新的逻辑应用。接着，提供一个名称和位置，并根据之前创建的资源组选择一个订阅。启用
    **Log Analytics** 是一个好习惯。Logic Apps 的详细内容请参考 *第 11 章，使用 Azure Logic Apps、Event
    Grid 和 Functions 构建 Azure 解决方案*。该逻辑应用负责使用一个账户连接 Twitter 并获取所有包含 **Azure** 的推文：
- en: '![Creating a logic app with the desired credentials](img/B15432_12_22.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![创建具有所需凭据的逻辑应用](img/B15432_12_22.jpg)'
- en: 'Figure 12.22: Creating a logic app'
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.22：创建一个逻辑应用
- en: 'After the logic app is created, select the **When a new tweet is posted** trigger
    on the design surface, sign in, and then configure it as shown in *Figure 12.23*.
    You will need a valid Twitter account before configuring this trigger:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 创建逻辑应用后，选择设计表面上的 **当发布新推文时** 触发器，登录并按 *图 12.23* 中所示进行配置。配置此触发器之前，您需要一个有效的 Twitter
    账户：
- en: '![Configuring the frequency of incoming tweets](img/B15432_12_23.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![配置推文接收频率](img/B15432_12_23.jpg)'
- en: 'Figure 12.23: Configuring the frequency of incoming tweets'
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.23：配置推文接收频率
- en: 'Next, drop a **Send event** action on the designer surface; this action is responsible for
    sending tweets to the event hub:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在设计器表面上拖动一个**发送事件**动作；该动作负责将推文发送到事件中心：
- en: '![Adding an action to send tweets to the event hub](img/B15432_12_24.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![添加一个动作将推文发送到事件中心](img/B15432_12_24.jpg)'
- en: 'Figure 12.24: Adding an action to send tweets to the event hub'
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.24：添加一个动作将推文发送到事件中心
- en: Select the name of the event hub that was created in an earlier step.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 选择先前步骤中创建的事件中心的名称。
- en: 'The value specified in the content textbox is an expression that has been dynamically
    composed using Logic Apps–provided functions and Twitter data. Clicking on **Add
    dynamic content** provides a dialog through which the expression can be composed:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 内容文本框中指定的值是一个表达式，动态组合了 Logic Apps 提供的函数和 Twitter 数据。点击**添加动态内容**，会弹出一个对话框，通过它可以构建表达式：
- en: '![Configuring Logic Apps activity using dynamic expressions](img/B15432_12_25.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![使用动态表达式配置 Logic Apps 活动](img/B15432_12_25.jpg)'
- en: 'Figure 12.25: Configuring Logic Apps activity using dynamic expressions'
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.25：使用动态表达式配置 Logic Apps 活动
- en: 'The value of the expression is as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 表达式的值如下：
- en: '[PRE4]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the next section, we will provision the storage account.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将配置存储账户。
- en: Provisioning the storage account
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置存储账户
- en: Click on **+ Create a resource** and search for **Storage Account**. Select **Storage
    Account** from the search results and create a new storage account. Then, provide
    a name and location, and select a subscription based on the resource group that
    was created earlier. Finally, select **StorageV2** for **Account Kind**, **Standard** for **Performance**,
    and **Locally-redundant storage** (**LRS**) for the **Replication** field.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**+ 创建资源**并搜索**存储账户**。从搜索结果中选择**存储账户**并创建一个新的存储账户。然后，提供名称和位置，并根据之前创建的资源组选择一个订阅。最后，选择**StorageV2**作为**账户类型**，**Standard**作为**性能**，并选择**本地冗余存储**（**LRS**）作为**复制**字段。
- en: Next, we will create a Blob storage container to store the data coming out of
    Stream Analytics.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个 Blob 存储容器，以存储来自流分析的数据。
- en: Creating a storage container
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建存储容器
- en: 'Stream Analytics will output the data as files, which will be stored within
    a Blob storage container. A container named **twitter** will be created within
    Blob storage, as shown in *Figure 12.26*:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 流分析将数据输出为文件，这些文件将存储在 Blob 存储容器中。一个名为**twitter**的容器将被创建在 Blob 存储中，如*图 12.26*所示：
- en: '![Creating a storage container for Twitter data](img/B15432_12_26.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![为 Twitter 数据创建存储容器](img/B15432_12_26.jpg)'
- en: 'Figure 12.26: Creating a storage container'
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.26：创建存储容器
- en: Let's create a new Stream Analytics job with a hosting environment on the cloud
    and set the streaming units to the default settings.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在云端创建一个新的流分析作业，并将流单位设置为默认配置。
- en: Creating Stream Analytics jobs
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建流分析作业
- en: 'The input for this Stream Analytics job comes from the event hub, and so we
    need to configure this from the **Inputs** menu:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 该流分析作业的输入来自事件中心，因此我们需要从**输入**菜单进行配置：
- en: '![Creating an input Stream Analytics job](img/B15432_12_27.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![创建输入流分析作业](img/B15432_12_27.jpg)'
- en: 'Figure 12.27: Creating an input Stream Analytics job'
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.27：创建一个输入流分析作业
- en: 'The output for the Stream Analytics job is a Blob storage account, so you need
    to configure the output accordingly. Provide a path pattern that is suitable for
    this exercise; for example, **{datetime:ss}** is the path pattern that we are
    using for this exercise:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 流分析作业的输出是一个 Blob 存储账户，因此需要相应地配置输出。提供适合本练习的路径模式；例如，**{datetime:ss}** 是我们在此练习中使用的路径模式：
- en: '![Creating a Blob Storage account as output](img/B15432_12_28.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![创建一个作为输出的 Blob 存储账户](img/B15432_12_28.jpg)'
- en: 'Figure 12.28: Creating a Blob storage account as output'
  id: totrans-203
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.28：创建一个作为输出的 Blob 存储账户
- en: 'The query is quite simple; you are just copying the data from the input to
    the output:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 该查询非常简单；你只是将数据从输入复制到输出：
- en: '![Query for copying Twitter feeds](img/B15432_12_29.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![查询以复制 Twitter 信息流](img/B15432_12_29.jpg)'
- en: 'Figure 12.29: Query for copying Twitter feeds'
  id: totrans-206
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.29：查询以复制 Twitter 信息流
- en: While this example just involves copying data, there can be more complex queries
    for performing transformation before loading data into a destination.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个例子只是涉及数据复制，但也可以有更复杂的查询，用于在将数据加载到目标之前执行转换。
- en: This concludes all the steps for the application; now you should be able to
    run it.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了应用程序的所有步骤；现在你应该能够运行它了。
- en: Running the application
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行应用程序
- en: 'The logic app should be enabled and Stream Analytics should be running. Now,
    run the logic app; it will create a job to run all the activities within it, as
    shown in *Figure 12.30*:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Logic App 应该已经启用，且流分析（Stream Analytics）应该正在运行。现在，运行 Logic App；它会创建一个作业来执行其中的所有活动，如*图
    12.30*所示：
- en: '![Overview of the GetAzureTwitterData application](img/B15432_12_30.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![GetAzureTwitterData 应用概述](img/B15432_12_30.jpg)'
- en: 'Figure 12.30: Overview of the GetAzureTwitterData application'
  id: totrans-212
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.30：GetAzureTwitterData 应用概述
- en: 'The **Storage Account** container should get data, as shown in *Figure 12.31*:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**存储账户**容器应该获取数据，如*图 12.31*所示：'
- en: '![Checking the storage account container data](img/B15432_12_31.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![检查存储账户容器数据](img/B15432_12_31.jpg)'
- en: 'Figure 12.31: Checking the Storage Account container data'
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.31：检查存储账户容器数据
- en: 'As an exercise, you can extend this sample solution and evaluate the sentiment
    of the tweets every three minutes. The Logic Apps workflow for such an exercise would
    be as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个练习，你可以扩展这个示例解决方案，并每三分钟评估一次推文的情感。这样的 Logic Apps 工作流如下所示：
- en: '![Flowchart for analyzing tweet sentiments](img/B15432_12_32.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![分析推文情感的流程图](img/B15432_12_32.jpg)'
- en: 'Figure 12.32: Flowchart for analyzing tweet sentiment'
  id: totrans-218
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.32：分析推文情感的流程图
- en: To detect sentiment, you'll need to use the Text Analytics API, which should be
    configured before being used in Logic Apps.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检测情感，你需要使用文本分析 API，且在 Logic Apps 中使用之前需要进行配置。
- en: Summary
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter focused on topics related to the streaming and storage of events.
    Events have become an important consideration in overall solution architecture.
    We covered important resources, such as Event Hubs and Stream Analytics, and foundational
    concepts, such as consumer groups and throughputs, as well as creating an end-to-end
    solution using them along with Logic Apps. You learned that events are raised
    from multiple sources, and in order to get insights in real time about activities
    and their related events, services such as Event Hubs and Stream Analytics play
    a significant role. In the next chapter, we will learn about integrating Azure
    DevOps and Jenkins and implementing some of the industry's best practices while
    developing solutions.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍了与事件流和存储相关的主题。事件已经成为整体解决方案架构中的一个重要考虑因素。我们介绍了重要的资源，如事件中心（Event Hubs）和流分析（Stream
    Analytics），以及一些基础概念，如消费者组（consumer groups）和吞吐量（throughputs），并通过与 Logic Apps 一起使用它们创建了端到端的解决方案。你了解到，事件来自多个源，并且为了实时获取活动及其相关事件的洞察，事件中心和流分析等服务发挥着重要作用。在下一章，我们将学习如何集成
    Azure DevOps 和 Jenkins，并在开发解决方案时实施一些行业最佳实践。
