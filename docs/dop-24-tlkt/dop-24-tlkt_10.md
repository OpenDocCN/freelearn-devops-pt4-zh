## 第十章：打包 Kubernetes 应用

到目前为止，我们遇到了不少挑战。好消息是，我们成功地解决了其中大部分问题。坏消息是，在某些情况下，我们的解决方案感觉并不完美（政治正确地说就是*糟糕*）。

在我们进行大规模部署有状态应用这一章节时，我们花了一些时间来定义 Jenkins 资源。那是一次很好的练习，可以被视为一种学习经历，但我们仍然需要做一些工作才能让它成为一个真正有用的定义。我们定义 Jenkins 的主要问题是，它仍然没有自动化。我们可以启动一个主节点，但仍然需要手动通过设置向导。一旦设置完成，我们还需要安装一些插件，并且需要更改其配置。在我们走这条路之前，我们可能需要先探索一下是否有其他人已经为我们完成了这项工作。如果我们要找一个 Java 库来帮助我们解决应用中的某个问题，我们可能会寻找一个 Maven 仓库。也许 Kubernetes 应用也有类似的东西。也许有一个由社区维护的仓库，里面包含了常用工具的安装解决方案。我们将把找到这样的地方作为我们的任务。

我们面临的另一个问题是自定义我们的 YAML 文件。至少，每次部署新版本时，我们都需要指定不同的镜像标签。在定义持续部署章节中，我们必须使用`sed`来修改定义，然后通过`kubectl`将其发送到 Kube API。虽然这样能工作，但我相信你会同意，像`sed -e "s@:latest@:1.7@g"`这样的命令并不是很直观。它们看起来和感觉都很笨拙。更复杂的是，镜像标签通常不是从一个部署到另一个部署之间唯一变化的内容。我们可能需要更改 Ingress 控制器的域名或路径，以适应将应用程序部署到不同环境（例如，暂存环境和生产环境）的需求。相同的情况也适用于副本数以及许多定义我们要安装内容的其他因素。使用串联的`sed`命令可能很快变得复杂，而且不太友好。没错，我们可以每次例如发布新版本时修改 YAML 文件。我们也可以为每个我们计划使用的环境创建不同的定义。但是，我们不会那样做。那样只会导致重复和维护上的噩梦。我们已经有了两个 YAML 文件用于`go-demo-3`应用程序（一个用于测试，另一个用于生产）。如果我们继续这样下去，最终可能会有十个、二十个，甚至更多版本的相同定义。我们可能还会被迫在每次代码提交时更改它，以确保标签始终是最新的。那条路不是我们要走的路，它通向悬崖。我们需要的是一个模板机制，允许我们在将定义发送到 Kube API 之前进行修改。

本章我们要尝试解决的最后一个问题是描述我们的应用程序及其他人在将其安装到集群之前可能进行的更改。说实话，这已经是可以做到的。任何人都可以读取我们的 YAML 文件来推断出应用程序的组成。任何人都可以拿着我们的 YAML 文件并修改它，以适应他们自己的需求。在某些情况下，即使是有 Kubernetes 经验的人也可能觉得有挑战性。然而，我们的主要关注点是那些不是 Kubernetes 专家的人员。我们不能指望我们组织中的每个人都花一年的时间学习 Kubernetes，只有这样他们才能部署应用程序。另一方面，我们确实希望为每个人提供这种能力。我们希望赋能每个人。当我们面临每个人都需要使用 Kubernetes，而并非每个人都将成为 Kubernetes 专家的现实时，我们显然需要一种更具描述性、易于自定义且更用户友好的方式来发现和部署应用程序。

我们将在本章中尝试解决这些问题以及一些其他问题。我们将尽力找到一个地方，在这里社区可以为常用应用（例如，Jenkins）提供定义。我们将寻求一种模板机制，使我们能够在安装应用之前进行定制。最后，我们将尝试找到一种更好的方式来记录我们的定义。我们将尽力简化到即使是不懂 Kubernetes 的人也能安全地将应用部署到集群中。我们需要的是一个 Kubernetes 版的包管理器，类似于*apt*、*yum*、*apk*、[Homebrew](https://brew.sh/) 或 [Chocolatey](https://chocolatey.org/)，并结合能够以任何人都能使用的方式来记录我们的包。

我会帮你省去寻找解决方案的麻烦，直接揭示答案。我们将探索 [Helm](https://helm.sh/)，作为让我们部署变得可定制和用户友好的关键。如果幸运的话，它甚至可能是那个能帮助我们避免为常用应用重新发明轮子的解决方案。

在我们继续之前，我们需要一个集群。是时候让我们动手了。

### 创建集群

又到动手操作的时刻了。我们需要回到本地的 [vfarcic/k8s-specs](https://github.com/vfarcic/k8s-specs) 仓库并拉取最新版本。

```
`1` `cd` k8s-specs
`2` 
`3` git pull 
```

``````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` Just as in the previous chapters, we’ll need a cluster if we are to execute hands-on exercises. The rules are still the same. You can continue using the same cluster as before, or you can switch to a different Kubernetes flavor. You can keep using one of the Kubernetes distributions listed below, or be adventurous and try something different. If you go with the latter, please let me know how it went, and I’ll test it myself and incorporate it into the list.    Cluster requirements in this chapter are the same as in the previous. We’ll need at least 3 CPUs and 3 GB RAM if running a single-node cluster, and slightly more if those resources are spread across multiple nodes.    For your convenience, the Gists and the specs we used in the previous chapter are available here as well.    *   [docker4mac-3cpu.sh](https://gist.github.com/bf08bce43a26c7299b6bd365037eb074): **Docker for Mac** with 3 CPUs, 3 GB RAM, and with nginx Ingress. *   [minikube-3cpu.sh](https://gist.github.com/871b5d7742ea6c10469812018c308798): **minikube** with 3 CPUs, 3 GB RAM, and with `ingress`, `storage-provisioner`, and `default-storageclass` addons enabled. *   [kops.sh](https://gist.github.com/2a3e4ee9cb86d4a5a65cd3e4397f48fd): **kops in AWS** with 3 t2.small masters and 2 t2.medium nodes spread in three availability zones, and with nginx Ingress (assumes that the prerequisites are set through Appendix B). *   [minishift-3cpu.sh](https://gist.github.com/2074633688a85ef3f887769b726066df): **minishift** with 3 CPUs, 3 GB RAM, and version 1.16+. *   [gke-2cpu.sh](https://gist.github.com/e3a2be59b0294438707b6b48adeb1a68): **Google Kubernetes Engine (GKE)** with 3 n1-highcpu-2 (2 CPUs, 1.8 GB RAM) nodes (one in each zone), and with nginx Ingress controller running on top of the “standard” one that comes with GKE. We’ll use nginx Ingress for compatibility with other platforms. Feel free to modify the YAML files if you prefer NOT to install nginx Ingress. *   [eks.sh](https://gist.github.com/5496f79a3886be794cc317c6f8dd7083): **Elastic Kubernetes Service (EKS)** with 2 t2.medium nodes, with **nginx Ingress** controller, and with a **default StorageClass**.    With a cluster up-and-running, we can proceed with an introduction to Helm.    ### What Is Helm?    I will not explain about Helm. I won’t even give you the elevator pitch. I’ll only say that it is a project with a big and healthy community, that it is a member of [Cloud Native Computing Foundation (CNCF)](https://www.cncf.io/), and that it has the backing of big guys like Google, Microsoft, and a few others. For everything else, you’ll need to follow the exercises. They’ll lead us towards an understanding of the project, and they will hopefully help us in our goal to refine our continuous deployment pipeline.    The first step is to install it.    ### Installing Helm    Helm is a client/server type of application. We’ll start with a client. Once we have it running, we’ll use it to install the server (Tiller) inside our newly created cluster.    The Helm client is a command line utility responsible for the local development of Charts, managing repositories, and interaction with the Tiller. Tiller server, on the other hand, runs inside a Kubernetes cluster and interacts with Kube API. It listens for incoming requests from the Helm client, combines Charts and configuration values to build a release, installs Charts and tracks subsequent releases, and is in charge of upgrading and uninstalling Charts through interaction with Kube API.    I’m sure that this brief explanation is more confusing than helpful. Worry not. Everything will be explained soon through examples. For now, we’ll focus on installing Helm and Tiller.    If you are a **MacOS user**, please use [Homebrew](https://brew.sh/) to install Helm. The command is as follows.    ``` `1` brew install kubernetes-helm  ```   `````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` If you are a **Windows user**, please use [Chocolatey](https://chocolatey.org/) to install Helm. The command is as follows.    ``` `1` choco install kubernetes-helm  ```   ````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` Finally, if you are neither Windows nor MacOS user, you must be running **Linux**. Please go to the [releases](https://github.com/kubernetes/helm/releases) page, download `tar.gz` file, unpack it, and move the binary to `/usr/local/bin/`.    If you already have Helm installed, please make sure that it is newer than 2.8.2\. That version, and probably a few versions before, was failing on Docker For Mac/Windows.    Once you’re done installing (or upgrading) Helm, please execute `helm help` to verify that it is working.    We are about to install *Tiller*. It’ll run inside our cluster. Just as `kubectl` is a client that communicates with Kube API, `helm` will propagate our wishes to `tiller` which, in turn, will issue requests to Kube API.    It should come as no surprise that Tiller will be yet another Pod in our cluster. As such, you should already know that we’ll need a ServiceAccount that will allow it to establish communication with Kube API. Since we hope to use Helm for all our installation in Kubernetes, we should give that ServiceAccount very generous permissions across the whole cluster.    Let’s take a look at the definition of a ServiceAccount we’ll create for Tiller.    ``` `1` cat helm/tiller-rbac.yml  ```   ```````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` The output is as follows.    ```  `1` `apiVersion``:` `v1`  `2` `kind``:` `ServiceAccount`  `3` `metadata``:`  `4`  `name``:` `tiller`  `5`  `namespace``:` `kube-system`  `6`   `7` `---`  `8`   `9` `apiVersion``:` `rbac.authorization.k8s.io/v1beta1` `10` `kind``:` `ClusterRoleBinding` `11` `metadata``:` `12 `  `name``:` `tiller` `13` `roleRef``:` `14 `  `apiGroup``:` `rbac.authorization.k8s.io` `15 `  `kind``:` `ClusterRole` `16 `  `name``:` `cluster-admin` `17` `subjects``:` `18 `  `-` `kind``:` `ServiceAccount` `19 `    `name``:` `tiller` `20 `    `namespace``:` `kube-system`  ```   ``````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` Since by now you are an expert in ServiceAccounts, there should be no need for a detailed explanation of the definition. We’re creating a ServiceAccount called `tiller` in the `kube-system` Namespace, and we are assigning it ClusterRole `cluster-admin`. In other words, the account will be able to execute any operation anywhere inside the cluster.    You might be thinking that having such broad permissions might seem dangerous, and you would be right. Only a handful of people should have the user permissions to operate inside `kube-system` Namespace. On the other hand, we can expect much wider circle of people being able to use Helm. We’ll solve that problem later in one of the next chapters. For now, we’ll focus only on how Helm works, and get back to the permissions issue later.    Let’s create the ServiceAccount.    ``` `1` kubectl create `\` `2 `    -f helm/tiller-rbac.yml `\` `3 `    --record --save-config  ```   `````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` We can see from the output that both the ServiceAccount and the ClusterRoleBinding were created.    Now that we have the ServiceAccount that gives Helm full permissions to manage any Kubernetes resource, we can proceed and install Tiller.    ``` `1` helm init --service-account tiller `2`  `3` kubectl -n kube-system `\` `4 `    rollout status deploy tiller-deploy  ```   ````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` We used `helm init` to create the server component called `tiller`. Since our cluster uses RBAC and all the processes require authentication and permissions to communicate with Kube API, we added `--service-account tiller` argument. It’ll attach the ServiceAccount to the `tiller` Pod.    The latter command waits until the Deployment is rolled out.    We could have specified `--tiller-namespace` argument to deploy it to a specific Namespace. That ability will come in handy in one of the next chapters. For now, we omitted that argument, so Tiller was installed in the `kube-system` Namespace by default. To be on the safe side, we’ll list the Pods to confirm that it is indeed running.    ``` `1` kubectl -n kube-system get pods  ```   ```````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` The output, limited to the relevant parts, is as follows.    ``` `1` NAME              READY STATUS  RESTARTS AGE `2` ... `3` tiller-deploy-... 1/1   Running 0        59s  ```   ``````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` Helm already has a single repository pre-configured. For those of you who just installed Helm for the first time, the repository is up-to-date. On the other hand, if you happen to have Helm from before, you might want to update the repository references by executing the command that follows.    ``` `1` helm repo update  ```   `````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` The only thing left is to search for our favorite application hoping that it is available in the Helm repository.    ``` `1` helm search  ```   ````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` The output, limited to the last few entries, is as follows.    ``` `1` ... `2` stable/weave-scope 0.9.2 1.6.5 A Helm chart for the Weave Scope cluster visual... `3` stable/wordpress   1.0.7 4.9.6 Web publishing platform for building blogs and ... `4` stable/zeppelin    1.0.1 0.7.2 Web-based notebook that enables data-driven, in... `5` stable/zetcd       0.1.9 0.0.3 CoreOS zetcd Helm chart for Kubernetes  ```   ```````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` We can see that the default repository already contains quite a few commonly used applications. It is the repository that contains the official Kubernetes Charts which are carefully curated and well maintained. Later on, in one of the next chapters, we’ll add more repositories to our local Helm installation. For now, we just need Jenkins, which happens to be one of the official Charts.    I already mentioned Charts a few times. You’ll find out what they are soon. For now, all you should know is that a Chart defines everything an application needs to run in a Kubernetes cluster.    ### Installing Helm Charts    The first thing we’ll do is to confirm that Jenkins indeed exists in the official Helm repository. We could do that by executing `helm search` (again) and going through all the available Charts. However, the list is pretty big and growing by the day. We’ll filter the search to narrow down the output.    ``` `1` helm search jenkins  ```   ``````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` The output is as follows.    ``` `1` NAME           CHART VERSION APP VERSION DESCRIPTION                                \ `2 `        `3` stable/jenkins 0.16.1        2.107       Open source continuous integration server. \ `4` It s...  ```   `````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` We can see that the repository contains `stable/jenkins` chart based on Jenkins version 2.107.    We’ll install Jenkins with the default values first. If that works as expected, we’ll try to adapt it to our needs later on.    Now that we know (through `search`) that the name of the Chart is `stable/jenkins`, all we need to do is execute `helm install`.    ``` `1` helm install stable/jenkins `\` `2 `    --name jenkins `\` `3 `    --namespace jenkins  ```   ````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` We instructed Helm to install `stable/jenkins` with the name `jenkins`, and inside the Namespace also called `jenkins`.    The output is as follows.    ```  `1` `NAME``:`   `jenkins`  `2` `LAST` `DEPLOYED``:` `Sun` `May` `...`  `3` `NAMESPACE``:` `jenkins`  `4` `STATUS``:` `DEPLOYED`  `5`   `6` `RESOURCES``:`  `7` `==>` `v1``/``Service`  `8` `NAME`          `TYPE`         `CLUSTER``-``IP`     `EXTERNAL``-``IP` `PORT``(``S``)`        `AGE`  `9` `jenkins``-``agent` `ClusterIP`    `10.111``.``123.174` `<``none``>`      `50000``/``TCP`      `1``s` `10` `jenkins`       `LoadBalancer` `10.110``.``48.57`   `localhost`   `8080``:``31294``/``TCP` `0``s` `11`  `12` `==>` `v1beta1``/``Deployment` `13` `NAME`    `DESIRED` `CURRENT` `UP``-``TO``-``DATE` `AVAILABLE` `AGE` `14` `jenkins` `1`       `1`       `1`          `0`         `0``s` `15`  `16` `==>` `v1``/``Pod``(``related``)` `17` `NAME`        `READY` `STATUS`   `RESTARTS` `AGE` `18` `jenkins``-...` `0``/1   Init:0/``1` `0`        `0``s` `19`  `20` `==>` `v1``/``Secret` `21` `NAME`    `TYPE`   `DATA` `AGE` `22` `jenkins` `Opaque` `2`    `1``s` `23`  `24` `==>` `v1``/``ConfigMap` `25` `NAME`          `DATA` `AGE` `26` `jenkins`       `4`    `1``s` `27` `jenkins``-``tests` `1`    `1``s` `28`  `29` `==>` `v1``/``PersistentVolumeClaim` `30` `NAME`    `STATUS` `VOLUME`  `CAPACITY` `ACCESS` `MODES` `STORAGECLASS` `AGE` `31` `jenkins` `Bound`  `pvc``-...` `8``Gi`      `RWO`          `gp2`          `1``s` `32`  `33`  `34` `NOTES``:` `35` `1``.` `Get` `your` `'admin'` `user` `password` `by` `running``:` `36 `  `printf` `$``(``kubectl` `get` `secret` `--``namespace` `jenkins` `jenkins` `-``o` `jsonpath``=``"{.data.jenkin\` `37` `s-admin-password}"` `|` `base64` `--``decode``);``echo` `38` `2``.` `Get` `the` `Jenkins` `URL` `to` `visit` `by` `running` `these` `commands` `in` `the` `same` `shell``:` `39 `  `NOTE``:` `It` `may` `take` `a` `few` `minutes` `for` `the` `LoadBalancer` `IP` `to` `be` `available``.` `40 `        `You` `can` `watch` `the` `status` `of` `by` `running` `'kubectl get svc --namespace jenkins \` `41` `-w jenkins'` `42 `  `export` `SERVICE_IP``=``$``(``kubectl` `get` `svc` `--``namespace` `jenkins` `jenkins` `--``template` `"{{ ran\` `43` `ge (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}"``)` `44 `  `echo` `http``://``$SERVICE_IP``:``8080``/``login` `45`  `46` `3``.` `Login` `with` `the` `password` `from` `step` `1` `and` `the` `username``:` `admin` `47`  `48` `For` `more` `information` `on` `running` `Jenkins` `on` `Kubernetes``,` `visit``:` `49` `https``://``cloud``.``google``.``com``/solutions/``jenkins``-``on``-``container``-``engine`  ```   ```````````````````````````````````````````````````````````````````````````````````````````````````````````````````` At the top of the output, we can see some general information like the name we gave to the installed Chart (`jenkins`), when it was deployed, what the Namespace is, and the status.    Below the general information is the list of the installed resources. We can see that the Chart installed two services; one for the master and the other for the agents. Below is the Deployment and the Pod. It also created a Secret that holds the administrative username and password. We’ll use it soon. Further on, we can see that it created two ConfigMaps. One (`jenkins`) holds all the configurations Jenkins might need. Later on, when we customize it, the data in this ConfigMap will reflect those changes. The second ConfigMap (`jenkins-tests`) is, at the moment, used only to provide a command used for executing liveness and readiness probes. Finally, we can see that a PersistentVolumeClass was created as well, thus making our Jenkins fault tolerant without losing its state.    Don’t worry if you feel overwhelmed. We’ll do a couple of iterations of the Jenkins installation process, and that will give us plenty of opportunities to explore this Chart in more details. If you are impatient, please `describe` any of those resources to get more insight into what’s installed.    One thing worthwhile commenting right away is the type of the `jenkins` Service. It is, by default, set to `LoadBalancer`. We did not explore that type in [The DevOps 2.3 Toolkit: Kubernetes](https://amzn.to/2GvzDjy), primarily because the book is, for the most part, based on minikube.    On cloud providers which support external load balancers, setting the type field to `LoadBalancer` will provision an external load balancer for the Service. The actual creation of the load balancer happens asynchronously, and information about the provisioned balancer is published in the Serviceâ€™s `status.loadBalancer` field.    When a Service is of the `LoadBalancer` type, it publishes a random port just as if it is the `NodePort` type. The additional feature is that it also communicates that change to the external load balancer (LB) which, in turn, should open a port as well. In most cases, the port opened in the external LB will be the same as the Service’s `TargetPort`. For example, if the `TargetPort` of a Service is `8080` and the published port is `32456`, the external LB will be configured to accept traffic on the port `8080`, and it will forward traffic to one of the healthy nodes on the port `32456`. From there on, requests will be picked up by the Service and the standard process of forwarding it further towards the replicas will be initiated. From user’s perspective, it seems as if the published port is the same as the `TargetPort`.    The problem is that not all load balancers and hosting vendors support the `LoadBalancer` type, so we’ll have to change it to `NodePort` in some of the cases. Those changes will be outlined as notes specific to the Kubernetes flavor.    Going back to the Helm output…    At the bottom of the output, we can see the post-installation instructions provided by the authors of the Chart. In our case, those instructions tell us how to retrieve the administrative password from the Secret, how to open Jenkins in a browser, and how to log in.    Next, we’ll wait until `jenkins` Deployment is rolled out.    ``` `1` kubectl -n jenkins `\` `2 `    rollout status deploy jenkins  ```   ``````````````````````````````````````````````````````````````````````````````````````````````````````````````````` We are almost ready to open Jenkins in a browser. But, before we do that, we need to retrieve the hostname (or IP) through which we can access our first Helm install.    ``` `1` `ADDR``=``$(`kubectl -n jenkins `\` `2 `    get svc jenkins `\` `3 `    -o `jsonpath``=``"{.status.loadBalancer.ingress[0].hostname}"``)`:8080  ```   `````````````````````````````````````````````````````````````````````````````````````````````````````````````````` To be on the safe side, we’ll `echo` the address we retrieved and confirm that it looks valid.    ``` `1` `echo` `$ADDR`  ```   ````````````````````````````````````````````````````````````````````````````````````````````````````````````````` The format of the output will differ from one Kubernetes flavor to another. In case of AWS with kops, it should be similar to the one that follows.    ``` `1` ...us-east-2.elb.amazonaws.com  ```   ```````````````````````````````````````````````````````````````````````````````````````````````````````````````` Now we can finally open Jenkins. We won’t do much with it. Our goal, for now, is only to confirm that it is up-and-running.    ``` `1` open `"http://``$ADDR``"`  ```   ``````````````````````````````````````````````````````````````````````````````````````````````````````````````` You should be presented with the login screen. There is no setup wizard indicating that this Helm chart already configured Jenkins with some sensible default values. That means that, among other things, the Chart created a user with a password during the automatic setup. We need to discover it.    Fortunately, we already saw from the `helm install` output that we should retrieve the password by retrieving the `jenkins-admin-password` entry from the `jenkins` secret. If you need to refresh your memory, please scroll back to the output, or ignore it all together and execute the command that follows.    ``` `1` kubectl -n jenkins `\` `2 `     get secret jenkins `\` `3 `    -o `jsonpath``=``"{.data.jenkins-admin-password}"` `\` `4 `    `|` base64 --decode`;` `echo`  ```   `````````````````````````````````````````````````````````````````````````````````````````````````````````````` The output should be a random set of characters similar to the one that follows.    ``` `1` shP7Fcsb9g  ```   ````````````````````````````````````````````````````````````````````````````````````````````````````````````` Please copy the output and return to Jenkins` login screen in your browser. Type *admin* into the *User* field, paste the copied output into the *Password* field and click the *log in* button.    Mission accomplished. Jenkins is up-and-running without us spending any time writing YAML file with all the resources. It was set up automatically with the administrative user and probably quite a few other goodies. We’ll get to them later. For now, we’ll “play” with a few other `helm` commands that might come in handy.    If you are ever unsure about the details behind one of the Helm Charts, you can execute `helm inspect`.    ``` `1` helm inspect stable/jenkins  ```   ```````````````````````````````````````````````````````````````````````````````````````````````````````````` The output of the `inspect` command is too big to be presented in a book. It contains all the information you might need before installing an application (in this case Jenkins).    If you prefer to go through the available Charts visually, you might want to visit [Kubeapps](https://kubeapps.com/) project hosted by [bitnami](https://bitnami.com/). Click on the *Explore Apps* button, and you’ll be sent to the hub with the list of all the official Charts. If you search for Jenkins, you’ll end up on the [page with the Chart’s details](https://hub.kubeapps.com/charts/stable/jenkins). You’ll notice that the info in that page is the same as the output of the `inspect` command.    We won’t go back to [Kubeapps](https://kubeapps.com/) since I prefer command line over UIs. A firm grip on the command line helps a lot when it comes to automation, which happens to be the goal of this book.    With time, the number of the Charts running in your cluster will increase, and you might be in need to list them. You can do that with the `ls` command.    ``` `1` helm ls  ```   ``````````````````````````````````````````````````````````````````````````````````````````````````````````` The output is as follows.    ``` `1` NAME    REVISION UPDATED     STATUS   CHART          NAMESPACE `2` jenkins 1        Thu May ... DEPLOYED jenkins-0.16.1 jenkins  ```   `````````````````````````````````````````````````````````````````````````````````````````````````````````` There is not much to look at right now since we have only one Chart. Just remember that the command exists. It’ll come in handy later on.    If you need to see the details behind one of the installed Charts, please use the `status` command.    ``` `1` helm status jenkins  ```   ````````````````````````````````````````````````````````````````````````````````````````````````````````` The output should be very similar to the one you saw when we installed the Chart. The only difference is that this time all the Pods are running.    Tiller obviously stores the information about the installed Charts somewhere. Unlike most other applications that tend to save their state on disk, or replicate data across multiple instances, tiller uses Kubernetes ConfgMaps to preserve its state.    Let’s take a look at the ConfigMaps in the `kube-system` Namespace where tiller is running.    ``` `1` kubectl -n kube-system get cm  ```   ```````````````````````````````````````````````````````````````````````````````````````````````````````` The output, limited to the relevant parts, is as follows.    ``` `1` NAME       DATA AGE `2` ... `3` jenkins.v1 1    25m `4` ...  ```   ``````````````````````````````````````````````````````````````````````````````````````````````````````` We can see that there is a config named `jenkins.v1`. We did not explore revisions just yet. For now, only assume that each new installation of a Chart is version 1.    Let’s take a look at the contents of the ConfigMap.    ``` `1` kubectl -n kube-system `\` `2 `    describe cm jenkins.v1  ```   `````````````````````````````````````````````````````````````````````````````````````````````````````` The output is as follows.    ```  `1` `Name``:`        `jenkins``.``v1`  `2` `Namespace``:`   `kube``-``system`  `3` `Labels``:`      `MODIFIED_AT``=``1527424681`  `4`             `NAME``=``jenkins`  `5`             `OWNER``=``TILLER`  `6`             `STATUS``=``DEPLOYED`  `7`             `VERSION``=``1`  `8` `Annotations``:` `<``none``>`  `9`  `10` `Data` `11` `====` `12` `release``:` `13` `----` `14` `[``ENCRYPTED` `RELEASE` `INFO``]` `15` `Events``:`  `<``none``>`  ```   ````````````````````````````````````````````````````````````````````````````````````````````````````` I replaced the content of the release Data with `[ENCRYPTED RELEASE INFO]` since it is too big to be presented in the book. The release contains all the info tiller used to create the first `jenkins` release. It is encrypted as a security precaution.    We’re finished exploring our Jenkins installation, so our next step is to remove it.    ``` `1` helm delete jenkins  ```   ```````````````````````````````````````````````````````````````````````````````````````````````````` The output shows that the `release "jenkins"` was `deleted`.    Since this is the first time we deleted a Helm Chart, we might just as well confirm that all the resources were indeed removed.    ``` `1` kubectl -n jenkins get all  ```   ``````````````````````````````````````````````````````````````````````````````````````````````````` The output is as follows.    ``` `1` NAME           READY STATUS      RESTARTS AGE `2` po/jenkins-... 0/1   Terminating 0        5m  ```   `````````````````````````````````````````````````````````````````````````````````````````````````` Everything is gone except the Pod that is still `terminating`. Soon it will disappear as well, and there will be no trace of Jenkins anywhere in the cluster. At least, that’s what we’re hoping for.    Let’s check the status of the `jenkins` Chart.    ``` `1` helm status jenkins  ```   ````````````````````````````````````````````````````````````````````````````````````````````````` The relevant parts of the output are as follows.    ``` `1` LAST DEPLOYED: Thu May 24 11:46:38 2018 `2` NAMESPACE: jenkins `3` STATUS: DELETED `4`  `5` ...  ```   ```````````````````````````````````````````````````````````````````````````````````````````````` If you expected an empty output or an error stating that `jenkins` does not exist, you were wrong. The Chart is still in the system, only this time its status is `DELETED`. You’ll notice that all the resources are gone though.    When we execute `helm delete [THE_NAME_OF_A_CHART]`, we are only removing the Kubernetes resources. The Chart is still in the system. We could, for example, revert the `delete` action and return to the previous state with Jenkins up-and-running again.    If you want to delete not only the Kubernetes resources created by the Chart but also the Chart itself, please add `--purge` argument.    ``` `1` helm delete jenkins --purge  ```   ``````````````````````````````````````````````````````````````````````````````````````````````` The output is still the same as before. It states that the `release "jenkins"` was `deleted`.    Let’s check the status now after we purged the system.    ``` `1` helm status jenkins  ```   `````````````````````````````````````````````````````````````````````````````````````````````` The output is as follows.    ``` `1` `Error``:` `getting` `deployed` `release` `"jenkins"``:` `release``:` `"jenkins"` `not` `found`  ```   ````````````````````````````````````````````````````````````````````````````````````````````` This time, everything was removed, and `helm` cannot find the `jenkins` Chart anymore.    ### Customizing Helm Installations    We’ll almost never install a Chart as we did. Even though the default values do often make a lot of sense, there is always something we need to tweak to make an application behave as we expect.    What if we do not want the Jenkins tag predefined in the Chart? What if for some reason we want to deploy Jenkins `2.112-alpine`? There must be a sensible way to change the tag of the `stable/jenkins` Chart.    Helm allows us to modify installation through variables. All we need to do is to find out which variables are available.    Besides visiting project’s documentation, we can retrieve the available values through the command that follows.    ``` `1` helm inspect values stable/jenkins  ```   ```````````````````````````````````````````````````````````````````````````````````````````` The output, limited to the relevant parts, is as follows.    ``` `1` ... `2` Master: `3 `  Name: jenkins-master `4 `  Image: "jenkins/jenkins" `5 `  ImageTag: "lts" `6 `  ...  ```   ``````````````````````````````````````````````````````````````````````````````````````````` We can see that within the `Master` section there is a variable `ImageTag`. The name of the variable should be, in this case, sufficiently self-explanatory. If we need more information, we can always inspect the Chart.    ``` `1` helm inspect stable/jenkins  ```   `````````````````````````````````````````````````````````````````````````````````````````` I encourage you to read the whole output at some later moment. For now, we care only about the `ImageTag`.    The output, limited to the relevant parts, is as follows.    ``` `1` ... `2` | Parameter         | Description      | Default | `3` | ----------------- | ---------------- | ------- | `4` ... `5` | `Master.ImageTag` | Master image tag | `lts`   | `6` ...  ```   ````````````````````````````````````````````````````````````````````````````````````````` That did not provide much more info. Still, we do not really need more than that. We can assume that `Master.ImageTag` will allow us to replace the default value `lts` with `2.112-alpine`.    If we go through the documentation, we’ll discover that one of the ways to overwrite the default values is through the `--set` argument. Let’s give it a try.    ``` `1` helm install stable/jenkins `\` `2 `    --name jenkins `\` `3 `    --namespace jenkins `\` `4 `    --set Master.ImageTag`=``2`.112-alpine  ```   ```````````````````````````````````````````````````````````````````````````````````````` The output of the `helm install` command is almost the same as when we executed it the first time, so there’s probably no need to go through it again. Instead, we’ll wait until `jenkins` rolls out.    ``` `1` kubectl -n jenkins `\` `2 `    rollout status deployment jenkins  ```   ``````````````````````````````````````````````````````````````````````````````````````` Now that the Deployment rolled out, we are almost ready to test whether the change of the variable had any effect. First, we need to get the Jenkins address. We’ll retrieve it in the same way as before, so there’s no need to lengthy explanation.    ``` `1` `ADDR``=``$(`kubectl -n jenkins `\` `2 `    get svc jenkins `\` `3 `    -o `jsonpath``=``"{.status.loadBalancer.ingress[0].hostname}"``)`:8080  ```   `````````````````````````````````````````````````````````````````````````````````````` As a precaution, please output the `ADDR` variable and check whether the address looks correct.    ``` `1` `echo` `$ADDR`  ```   ````````````````````````````````````````````````````````````````````````````````````` Now we can open Jenkins UI.    ``` `1` open `"http://``$ADDR``"`  ```   ```````````````````````````````````````````````````````````````````````````````````` This time there is no need even to log in. All we need to do is to check whether changing the tag worked. Please observe the version in the bottom-right corner of the screen. If should be *Jenkins ver. 2.112*.    Let’s imagine that some time passed and we decided to upgrade our Jenkins from *2.112* to *2.116*. We go through the documentation and discover that there is the `upgrade` command we can leverage.    ``` `1` helm upgrade jenkins stable/jenkins `\` `2 `    --set Master.ImageTag`=``2`.116-alpine `\` `3 `    --reuse-values  ```   ``````````````````````````````````````````````````````````````````````````````````` This time we did not specify the Namespace, but we did set the `--reuse-values` argument. With it, the upgrade will maintain all the values used the last time we installed or upgraded the Chart. The result is an upgrade of the Kubernetes resources so that they comply with our desire to change the tag, and leave everything else intact.    The output of the `upgrade` command, limited to the first few lines, is as follows.    ``` `1` Release "jenkins" has been upgraded. Happy Helming! `2` LAST DEPLOYED: Thu May 24 12:51:03 2018 `3` NAMESPACE: jenkins `4` STATUS: DEPLOYED `5` ...  ```   `````````````````````````````````````````````````````````````````````````````````` We can see that the release was upgraded.    To be on the safe side, we’ll describe the `jenkins` Deployment and confirm that the image is indeed `2.116-alpine`.    ``` `1` kubectl -n jenkins `\` `2 `    describe deployment jenkins  ```   ````````````````````````````````````````````````````````````````````````````````` The output, limited to the relevant parts, is as follows.    ``` `1` `Name``:`              `jenkins` `2` `Namespace``:`         `jenkins` `3` `...` `4` `Pod` `Template``:` `5 `  `...` `6 `  `Containers``:` `7 `   `jenkins``:` `8 `    `Image``:` `jenkins``/``jenkins``:``2.116``-``alpine` `9 `    `...`  ```   ```````````````````````````````````````````````````````````````````````````````` The image was indeed updated to the tag `2.116-alpine`.    To satisfy my paranoid nature, we’ll also open Jenkins UI and confirm the version there. But, before we do that, we need to wait until the update rolls out.    ``` `1` kubectl -n jenkins `\` `2 `    rollout status deployment jenkins  ```   ``````````````````````````````````````````````````````````````````````````````` Now we can open Jenkins UI.    ``` `1` open `"http://``$ADDR``"`  ```   `````````````````````````````````````````````````````````````````````````````` Please note the version in the bottom-right corner of the screen. It should say *Jenkins ver. 2.116*.    ### Rolling Back Helm Revisions    No matter how we deploy our applications and no matter how much we trust our validations, the truth is that sooner or later we’ll have to roll back. That is especially true with third-party applications. While we could roll forward faulty applications we developed, the same is often not an option with those that are not in our control. If there is a problem and we cannot fix it fast, the only alternative is to roll back.    Fortunately, Helm provides a mechanism to roll back. Before we try it out, let’s take a look at the list of the Charts we installed so far.    ``` `1` helm list  ```   ````````````````````````````````````````````````````````````````````````````` The output is as follows.    ``` `1` NAME    REVISION UPDATED     STATUS   CHART          NAMESPACE `2` jenkins 2        Thu May ... DEPLOYED jenkins-0.16.1 jenkins  ```   ```````````````````````````````````````````````````````````````````````````` As expected, we have only one Chart running in our cluster. The critical piece of information is that it is the second revision. First, we installed the Chart with Jenkins version 2.112, and then we upgraded it to 2.116.    We can roll back to the previous version (`2.112`) by executing `helm rollback jenkins 1`. That would roll back from the revision `2` to whatever was defined as the revision `1`. However, in most cases that is unpractical. Most of our rollbacks are likely to be executed through our CD or CDP processes. In those cases, it might be too complicated for us to find out what was the previous release number.    Luckily, there is an undocumented feature that allows us to roll back to the previous version without explicitly setting up the revision number. By the time you read this, the feature might become documented. I was about to start working on it and submit a pull request. Luckily, while going through the code, I saw that it’s already there.    Please execute the command that follows.    ``` `1` helm rollback jenkins `0`  ```   ``````````````````````````````````````````````````````````````````````````` By specifying `0` as the revision number, Helm will roll back to the previous version. It’s as easy as that.    We got the visual confirmation in the form of the “`Rollback was a success! Happy Helming!`” message.    Let’s take a look at the current situation.    ``` `1` helm list  ```   `````````````````````````````````````````````````````````````````````````` The output is as follows.    ``` `1` NAME   	REVISION UPDATED     STATUS   CHART          NAMESPACE `2` jenkins	3        Thu May ... DEPLOYED jenkins-0.16.1 jenkins  ```   ````````````````````````````````````````````````````````````````````````` We can see that even though we issued a rollback, Helm created a new revision `3`. There’s no need to panic. Every change is a new revision, even when a change means re-applying definition from one of the previous releases.    To be on the safe side, we’ll go back to Jenkins UI and confirm that we are using version `2.112` again.    ``` `1` kubectl -n jenkins `\` `2 `    rollout status deployment jenkins `3`  `4` open `"http://``$ADDR``"`  ```   ```````````````````````````````````````````````````````````````````````` We waited until Jenkins rolled out, and opened it in our favorite browser. If we look at the version information located in the bottom-right corner of the screen, we are bound to discover that it is *Jenkins ver. 2.112* once again.    We are about to start over one more time, so our next step it to purge Jenkins.    ``` `1` helm delete jenkins --purge  ```   ``````````````````````````````````````````````````````````````````````` ### Using YAML Values To Customize Helm Installations    We managed to customize Jenkins by setting `ImageTag`. What if we’d like to set CPU and memory. We should also add Ingress, and that would require a few annotations. If we add Ingress, we might want to change the Service type to ClusterIP and set HostName to our domain. We should also make sure that RBAC is used. Finally, the plugins that come with the Chart are probably not all the plugins we need.    Applying all those changes through `--set` arguments would end up as a very long command and would constitute an undocumented installation. We’ll have to change the tactic and switch to `--values`. But before we do all that, we need to generate a domain we’ll use with our cluster.    We’ll use [nip.io](http://nip.io) to generate valid domains. The service provides a wildcard DNS for any IP address. It extracts IP from the nip.io subdomain and sends it back in the response. For example, if we generate 192.168.99.100.nip.io, it’ll be resolved to 192.168.99.100\. We can even add sub-sub domains like something.192.168.99.100.nip.io, and it would still be resolved to 192.168.99.100\. It’s a simple and awesome service that quickly became an indispensable part of my toolbox.    The service will be handy with Ingress since it will allow us to generate separate domains for each application, instead of resorting to paths which, as you will see, are unsupported by many Charts. If our cluster is accessible through *192.168.99.100*, we can have *jenkins.192.168.99.100.nip.io* and *go-demo-3.192.168.99.100.nip.io*.    We could use [xip.ip](http://xip.io) instead. For the end-users, there is no significant difference between the two. The main reason why we’ll use nip.io instead of xip.io is integration with some of the tool. Minishift, for example, comes with Routes pre-configured to use nip.io.    First things first… We need to find out the IP of our cluster, or the external LB if it is available. The commands that follow will differ from one cluster type to another.    If your cluster is running in **AWS** and was created with **kops**, we’ll need to retrieve the hostname from the Ingress Service, and extract the IP from it. Please execute the commands that follow.    ``` `1` `LB_HOST``=``$(`kubectl -n kube-ingress `\` `2 `    get svc ingress-nginx `\` `3 `    -o `jsonpath``=``"{.status.loadBalancer.ingress[0].hostname}"``)` `4`  `5` `LB_IP``=``"``$(`dig +short `$LB_HOST` `\` `6 `    `|` tail -n `1``)``"`  ```   `````````````````````````````````````````````````````````````````````` If your cluster is running in **AWS** and was created as **EKS**, we’ll need to retrieve the hostname from the Ingress Service, and extract the IP from it. Please execute the commands that follow.    ``` `1` `LB_HOST``=``$(`kubectl -n ingress-nginx `\` `2 `    get svc ingress-nginx `\` `3 `    -o `jsonpath``=``"{.status.loadBalancer.ingress[0].hostname}"``)` `4`  `5` `LB_IP``=``"``$(`dig +short `$LB_HOST` `\` `6 `    `|` tail -n `1``)``"`  ```   ````````````````````````````````````````````````````````````````````` If your cluster is running in **Docker For Mac/Windows**, the IP is `127.0.0.1` and all you have to do is assign it to the environment variable `LB_IP`. Please execute the command that follows.    ``` `1` `LB_IP``=``"127.0.0.1"`  ```   ```````````````````````````````````````````````````````````````````` If your cluster is running in **minikube**, the IP can be retrieved using `minikube ip` command. Please execute the command that follows.    ``` `1` `LB_IP``=``"``$(`minikube ip`)``"`  ```   ``````````````````````````````````````````````````````````````````` If your cluster is running in **GKE**, the IP can be retrieved from the Ingress Service. Please execute the command that follows.    ``` `1` `LB_IP``=``$(`kubectl -n ingress-nginx `\` `2 `    get svc ingress-nginx `\` `3 `    -o `jsonpath``=``"{.status.loadBalancer.ingress[0].ip}"``)`  ```   `````````````````````````````````````````````````````````````````` Next, we’ll output the retrieved IP to confirm that the commands worked, and generate a sub-domain `jenkins`.    ``` `1` `echo` `$LB_IP` `2`  `3` `HOST``=``"jenkins.``$LB_IP``.nip.io"` `4`  `5` `echo` `$HOST`  ```   ````````````````````````````````````````````````````````````````` The output of the second `echo` command should be similar to the one that follows.    ``` `1` jenkins.192.168.99.100.nip.io  ```   ```````````````````````````````````````````````````````````````` *nip.io* will resolve that address to `192.168.99.100`, and we’ll have a unique domain for our Jenkins installation. That way we can stop using different paths to distinguish applications in Ingress config. Domains work much better. Many Helm charts do not even have the option to configure unique request paths and assume that Ingress will be configured with a unique domain.    Now that we have a valid `jenkins.*` domain, we can try to figure out how to apply all the changes we discussed.    We already learned that we can inspect all the available values using `helm inspect` command. Let’s take another look.    ``` `1` helm inspect values stable/jenkins  ```   ``````````````````````````````````````````````````````````````` The output, limited to the relevant parts, is as follows.    ```  `1` `Master``:`  `2`  `Name``:` `jenkins-master`  `3`  `Image``:` `"jenkins/jenkins"`  `4`  `ImageTag``:` `"lts"`  `5`  `...`  `6`  `Cpu``:` `"200m"`  `7`  `Memory``:` `"256Mi"`  `8`  `...`  `9`  `ServiceType``:` `LoadBalancer` `10 `  `# Master Service annotations` `11 `  `ServiceAnnotations``:` `{}` `12 `  `...` `13 `  `# HostName: jenkins.cluster.local` `14 `  `...` `15 `  `InstallPlugins``:` `16 `    `-` `kubernetes:1.1` `17 `    `-` `workflow-aggregator:2.5` `18 `    `-` `workflow-job:2.15` `19 `    `-` `credentials-binding:1.13` `20 `    `-` `git:3.6.4` `21 `  `...` `22 `  `Ingress``:` `23 `    `ApiVersion``:` `extensions/v1beta1` `24 `    `Annotations``:` `25 `    `...` `26` `...` `27` `rbac``:` `28 `  `install``:` `false` `29 `  `...`  ```   `````````````````````````````````````````````````````````````` Everything we need to accomplish our new requirements is available through the values. Some of them are already filled with defaults, while others are commented. When we look at all those values, it becomes clear that it would be unpractical to try to re-define them all through `--set` arguments. We’ll use `--values` instead. It will allow us to specify the values in a file.    I already prepared a YAML file with the values that will fulfill our requirements, so let’s take a quick look at them.    ``` `1` cat helm/jenkins-values.yml  ```   ````````````````````````````````````````````````````````````` The output is as follows.    ```  `1` `Master``:`  `2`  `ImageTag``:` `"2.116-alpine"`  `3`  `Cpu``:` `"500m"`  `4`  `Memory``:` `"500Mi"`  `5`  `ServiceType``:` `ClusterIP`  `6`  `ServiceAnnotations``:`  `7`    `service.beta.kubernetes.io/aws-load-balancer-backend-protocol``:` `http`  `8`  `InstallPlugins``:`  `9`    `-` `blueocean:1.5.0` `10 `    `-` `credentials:2.1.16` `11 `    `-` `ec2:1.39` `12 `    `-` `git:3.8.0` `13 `    `-` `git-client:2.7.1` `14 `    `-` `github:1.29.0` `15 `    `-` `kubernetes:1.5.2` `16 `    `-` `pipeline-utility-steps:2.0.2` `17 `    `-` `script-security:1.43` `18 `    `-` `slack:2.3` `19 `    `-` `thinBackup:1.9` `20 `    `-` `workflow-aggregator:2.5` `21 `  `Ingress``:` `22 `    `Annotations``:` `23 `      `nginx.ingress.kubernetes.io/ssl-redirect``:` `"false"` `24 `      `nginx.ingress.kubernetes.io/proxy-body-size``:` `50m` `25 `      `nginx.ingress.kubernetes.io/proxy-request-buffering``:` `"off"` `26 `      `ingress.kubernetes.io/ssl-redirect``:` `"false"` `27 `      `ingress.kubernetes.io/proxy-body-size``:` `50m` `28 `      `ingress.kubernetes.io/proxy-request-buffering``:` `"off"` `29 `  `HostName``:` `jenkins.acme.com` `30` `rbac``:` `31 `  `install``:` `true`  ```   ```````````````````````````````````````````````````````````` As you can see, the variables in that file follow the same format as those we output through the `helm inspect values` command. The only difference is in values, and the fact that `helm/jenkins-values.yml` contains only those that we are planning to change.    We defined that the `ImageTag` should be fixed to `2.116-alpine`.    We specified that our Jenkins master will need half a CPU and 500 MB RAM. The default values of 0.2 CPU and 256 MB RAM are probably not enough. What we set is also low, but since we’re not going to run any serious load (at least not yet), what we re-defined should be enough.    The service was changed to `ClusterIP` to better accommodate Ingress resource we’re defining further down.    If you are not using AWS, you can ignore `ServiceAnnotations`. They’re telling ELB to use HTTP protocol.    Further down, we are defining the plugins we’ll use throughout the book. Their usefulness will become evident in the next chapters.    The values in the `Ingress` section are defining the annotations that tell Ingress not to redirect HTTP requests to HTTPS (we don’t have SSL certificates), as well as a few other less important options. We set both the old style (`ingress.kubernetes.io`) and the new style (`nginx.ingress.kubernetes.io`) of defining NGINX Ingress. That way it’ll work no matter which Ingress version you’re using. The `HostName` is set to a value that apparently does not exist. I could not know in advance what will be your hostname, so we’ll overwrite it later on.    Finally, we set `rbac.install` to `true` so that the Chart knows that it should set the proper permissions.    Having all those variables defined at once might be a bit overwhelming. You might want to go through the [Jenkins Chart documentation](https://hub.kubeapps.com/charts/stable/jenkins) for more info. In some cases, documentation alone is not enough, and I often end up going through the files that form the chart. You’ll get a grip on them with time. For now, the important thing to observe is that we can re-define any number of variables through a YAML file.    Let’s install the Chart with those variables.    ``` `1` helm install stable/jenkins `\` `2 `    --name jenkins `\` `3 `    --namespace jenkins `\` `4 `    --values helm/jenkins-values.yml `\` `5 `    --set Master.HostName`=``$HOST`  ```   ``````````````````````````````````````````````````````````` We used the `--values` argument to pass the contents of the `helm/jenkins-values.yml`. Since we had to overwrite the `HostName`, we used `--set`. If the same value is defined through `--values` and `--set`, the latter always takes precedence.    Next, we’ll wait for `jenkins` Deployment to roll out and open its UI in a browser.    ``` `1` kubectl -n jenkins `\` `2 `    rollout status deployment jenkins `3`  `4` open `"http://``$HOST``"`  ```   `````````````````````````````````````````````````````````` The fact that we opened Jenkins through a domain defined as Ingress (or Route in case of OpenShift) tells us that the values were indeed used. We can double check those currently defined for the installed Chart with the command that follows.    ``` `1` helm get values jenkins  ```   ````````````````````````````````````````````````````````` The output is as follows.    ```  `1` `Master``:`  `2`  `Cpu``:` `500m`  `3`  `HostName``:` `jenkins.18.220.212.56.nip.io`  `4`  `ImageTag``:` `2.116-alpine`  `5`  `Ingress``:`  `6`    `Annotations``:`  `7`      `ingress.kubernetes.io/proxy-body-size``:` `50m`  `8`      `ingress.kubernetes.io/proxy-request-buffering``:` `"off"`  `9`      `ingress.kubernetes.io/ssl-redirect``:` `"false"` `10 `      `nginx.ingress.kubernetes.io/proxy-body-size``:` `50m` `11 `      `nginx.ingress.kubernetes.io/proxy-request-buffering``:` `"off"` `12 `      `nginx.ingress.kubernetes.io/ssl-redirect``:` `"false"` `13 `  `InstallPlugins``:` `14 `  `-` `blueocean:1.5.0` `15 `  `-` `credentials:2.1.16` `16 `  `-` `ec2:1.39` `17 `  `-` `git:3.8.0` `18 `  `-` `git-client:2.7.1` `19 `  `-` `github:1.29.0` `20 `  `-` `kubernetes:1.5.2` `21 `  `-` `pipeline-utility-steps:2.0.2` `22 `  `-` `script-security:1.43` `23 `  `-` `slack:2.3` `24 `  `-` `thinBackup:1.9` `25 `  `-` `workflow-aggregator:2.5` `26 `  `Memory``:` `500Mi` `27 `  `ServiceAnnotations``:` `28 `    `service.beta.kubernetes.io/aws-load-balancer-backend-protocol``:` `http` `29 `  `ServiceType``:` `ClusterIP` `30` `rbac``:` `31 `  `install``:` `true`  ```   ```````````````````````````````````````````````````````` Even though the order is slightly different, we can easily confirm that the values are the same as those we defined in `helm/jenkins-values.yml`. The exception is the `HostName` which was overwritten through the `--set` argument.    Now that we explored how to use Helm to deploy publicly available Charts, we’ll turn our attention towards development. Can we leverage the power behind Charts for our applications?    Before we proceed, please delete the Chart we installed as well as the `jenkins` Namespace.    ``` `1` helm delete jenkins --purge `2`  `3` kubectl delete ns jenkins  ```   ``````````````````````````````````````````````````````` ### Creating Helm Charts    Our next goal is to create a Chart for the *go-demo-3* application. We’ll use the fork you created in the previous chapter.    First, we’ll move into the fork’s directory.    ``` `1` `cd` ../go-demo-3  ```   `````````````````````````````````````````````````````` To be on the safe side, we’ll push the changes you might have made in the previous chapter and then we’ll sync your fork with the upstream repository. That way we’ll guarantee that you have all the changes I might have made.    You probably already know how to push your changes and how to sync with the upstream repository. In case you don’t, the commands are as follows.    ```  `1` git add .  `2`   `3` git commit -m `\`  `4`    `"Defining Continuous Deployment chapter"`  `5`   `6` git push  `7`   `8` git remote add upstream `\`  `9`    https://github.com/vfarcic/go-demo-3.git `10`  `11` git fetch upstream `12`  `13` git checkout master `14`  `15` git merge upstream/master  ```   ````````````````````````````````````````````````````` We pushed the changes we made in the previous chapter, we fetched the upstream repository *vfarcic/go-demo-3*, and we merged the latest code from it. Now we are ready to create our first Chart.    Even though we could create a Chart from scratch by creating a specific folder structure and the required files, we’ll take a shortcut and create a sample Chart that can be modified later to suit our needs.    We won’t start with a Chart for the *go-demo-3* application. Instead, we’ll create a creatively named Chart *my-app* that we’ll use to get a basic understanding of the commands we can use to create and manage our Charts. Once we’re familiar with the process, we’ll switch to *go-demo-3*.    Here we go.    ``` `1` helm create my-app `2`  `3` ls -1 my-app  ```   ```````````````````````````````````````````````````` The first command created a Chart named *my-app*, and the second listed the files and the directories that form the new Chart.    The output of the latter command is as follows.    ``` `1` Chart.yaml `2` charts `3` templates `4` values.yaml  ```   ``````````````````````````````````````````````````` We will not go into the details behind each of those files and directories just yet. For now, just note that a Chart consists of files and directories that follow certain naming conventions.    If our Chart has dependencies, we could download them with the `dependency update` command.    ``` `1` helm dependency update my-app  ```   `````````````````````````````````````````````````` The output shows that `no requirements` were `found in .../go-demo-3/my-app/charts`. That makes sense because we did not yet declare any dependencies. For now, just remember that they can be downloaded or updated.    Once we’re done with defining the Chart of an application, we can package it.    ``` `1` helm package my-app  ```   ````````````````````````````````````````````````` We can see from the output that Helm `successfully packaged chart and saved it to: .../go-demo-3/my-app-0.1.0.tgz`. We do not yet have a repository for our Charts. We’ll work on that in the next chapter.    If we are unsure whether we made a mistake in our Chart, we can validate it by executing `lint` command.    ``` `1` helm lint my-app  ```   ```````````````````````````````````````````````` The output is as follows.    ``` `1` ==> Linting my-app `2` [INFO] Chart.yaml: icon is recommended `3`  `4` 1 chart(s) linted, no failures  ```   ``````````````````````````````````````````````` We can see that our Chart contains no failures, at least not those based on syntax. That should come as no surprise since we did not even modify the sample Chart Helm created for us.    Charts can be installed using a Chart repository (e.g., `stable/jenkins`), a local Chart archive (e.g., `my-app-0.1.0.tgz`), an unpacked Chart directory (e.g., `my-app`), or a full URL (e.g., `https://acme.com/charts/my-app-0.1.0.tgz`). So far we used Chart repository to install Jenkins. We’ll switch to the local archive option to install `my-app`.    ``` `1` helm install ./my-app-0.1.0.tgz `\` `2 `    --name my-app  ```   `````````````````````````````````````````````` The output is as follows.    ```  `1` `NAME``:`   `my``-``app`  `2` `LAST` `DEPLOYED``:` `Thu` `May` `24` `13``:``43``:``17` `2018`  `3` `NAMESPACE``:` `default`  `4` `STATUS``:` `DEPLOYED`  `5`   `6` `RESOURCES``:`  `7` `==>` `v1``/``Service`  `8` `NAME`   `TYPE`      `CLUSTER``-``IP`     `EXTERNAL``-``IP` `PORT``(``S``)` `AGE`  `9` `my``-``app` `ClusterIP` `100.65``.``227.236` `<``none``>`      `80``/``TCP`  `1``s` `10`  `11` `==>` `v1beta2``/``Deployment` `12` `NAME`   `DESIRED` `CURRENT` `UP``-``TO``-``DATE` `AVAILABLE` `AGE` `13` `my``-``app` `1`       `1`       `1`          `0`         `1``s` `14`  `15` `==>` `v1``/``Pod``(``related``)` `16` `NAME`                    `READY` `STATUS`            `RESTARTS` `AGE` `17` `my``-``app``-``7``f4d66bf86``-``dns28` `0``/``1`   `ContainerCreating` `0`        `1``s` `18`  `19`  `20` `NOTES``:` `21` `1``.` `Get` `the` `application` `URL` `by` `running` `these` `commands``:` `22 `  `export` `POD_NAME``=``$``(``kubectl` `get` `pods` `--``namespace` `default` `-``l` `"app=my-app,release=my-a\` `23` `pp"` `-``o` `jsonpath``=``"{.items[0].metadata.name}"``)` `24 `  `echo` `"Visit http://127.0.0.1:8080 to use your application"` `25 `  `kubectl` `port``-``forward` `$POD_NAME` `8080``:``80`  ```   ````````````````````````````````````````````` The sample application is a straightforward one with a Service and a Deployment. There’s not much to say about it. We used it only to explore the basic commands for creating and managing Charts. We’ll delete everything we did and start over with a more serious example.    ``` `1` helm delete my-app --purge `2`  `3` rm -rf my-app `4`  `5` rm -rf my-app-0.1.0.tgz  ```   ```````````````````````````````````````````` We deleted the Chart from the cluster, as well as the local directory and the archive we created earlier. The time has come to apply the knowledge we obtained and explore the format of the files that constitute a Chart. We’ll switch to the *go-demo-3* application next.    ### Exploring Files That Constitute A Chart    I prepared a Chart that defines the *go-demo-3* application. We’ll use it to get familiar with writing Charts. Even if we choose to use Helm only for third-party applications, familiarity with Chart files is a must since we might have to look at them to better understand the application we want to install.    The files are located in `helm/go-demo-3` directory inside the repository. Let’s take a look at what we have.    ``` `1` ls -1 helm/go-demo-3  ```   ``````````````````````````````````````````` The output is as follows.    ``` `1` Chart.yaml `2` LICENSE `3` README.md `4` templates `5` values.yaml  ```   `````````````````````````````````````````` A chart is organized as a collection of files inside a directory. The directory name is the name of the Chart (without versioning information). So, a Chart that describes *go-demo-3* is stored in the directory with the same name.    The first file we’ll explore is *Chart.yml*. It is a mandatory file with a combination of compulsory and optional fields.    Let’s take a closer look.    ``` `1` cat helm/go-demo-3/Chart.yaml  ```   ````````````````````````````````````````` The output is as follows.    ```  `1` `name``:` `go-demo-3`  `2` `version``:` `0.0.1`  `3` `apiVersion``:` `v1`  `4` `description``:` `A silly demo based on API written in Go and MongoDB`  `5` `keywords``:`  `6` `-` `api`  `7` `-` `backend`  `8` `-` `go`  `9` `-` `database` `10` `-` `mongodb` `11` `home``:` `http://www.devopstoolkitseries.com/` `12` `sources``:` `13` `-` `https://github.com/vfarcic/go-demo-3` `14` `maintainers``:` `15` `-` `name``:` `Viktor Farcic` `16 `  `email``:` `viktor@farcic.com`  ```   ```````````````````````````````````````` The `name`, `version`, and `apiVersion` are mandatory fields. All the others are optional.    Even though most of the fields should be self-explanatory, we’ll go through each of them just in case.    The `name` is the name of the Chart, and the `version` is the version. That’s obvious, isn’t it? The critical thing to note is that versions must follow [SemVer 2](http://semver.org/) standard. The full identification of a Chart package in a repository is always a combination of a name and a version. If we package this Chart, its name would be *go-demo-3-0.0.1.tgz*. The `apiVersion` is the version of the Helm API and, at this moment, the only supported value is `v1`.    The rest of the fields are mostly informational. You should be able to understand their meaning so I won’t bother you with lengthy explanations.    The next in line is the LICENSE file.    ``` `1` cat helm/go-demo-3/LICENSE  ```   ``````````````````````````````````````` The first few lines of the output are as follows.    ``` `1` The MIT License (MIT) `2`  `3` Copyright (c) 2018 Viktor Farcic `4`  `5` Permission is hereby granted, free ...  ```   `````````````````````````````````````` The *go-demo-3* application is licensed as MIT. It’s up to you to decide which license you’ll use, if any.    README.md is used to describe the application.    ``` `1` cat helm/go-demo-3/README.md  ```   ````````````````````````````````````` The output is as follows.    ``` `1` This is just a silly demo.  ```   ```````````````````````````````````` I was too lazy to write a proper description. You shouldn’t be. As a rule of thumb, README.md should contain a description of the application, a list of the pre-requisites and the requirements, a description of the options available through values.yaml, and anything else you might deem important. As the extension suggests, it should be written in Markdown format.    Now we are getting to the critical part.    The values that can be used to customize the installation are defined in `values.yaml`.    ``` `1` cat helm/go-demo-3/values.yaml  ```   ``````````````````````````````````` The output is as follows.    ```  `1` `replicaCount``:` `3`  `2` `dbReplicaCount``:` `3`  `3` `image``:`  `4`  `tag``:` `latest`  `5`  `dbTag``:` `3.3`  `6` `ingress``:`  `7`  `enabled``:` `true`  `8`  `host``:` `acme.com`  `9` `service``:` `10 `  `# Change to NodePort if ingress.enable=false` `11 `  `type``:` `ClusterIP` `12` `rbac``:` `13 `  `enabled``:` `true` `14` `resources``:` `15 `  `limits``:` `16 `   `cpu``:` `0.2` `17 `   `memory``:` `20Mi` `18 `  `requests``:` `19 `   `cpu``:` `0.1` `20 `   `memory``:` `10Mi` `21` `dbResources``:` `22 `  `limits``:` `23 `    `memory``:` `"200Mi"` `24 `    `cpu``:` `0.2` `25 `  `requests``:` `26 `    `memory``:` `"100Mi"` `27 `    `cpu``:` `0.1` `28` `dbPersistence``:` `29 `  `## If defined, storageClassName: <storageClass>` `30 `  `## If set to "-", storageClassName: "", which disables dynamic provisioning` `31 `  `## If undefined (the default) or set to null, no storageClassName spec is` `32 `  `##   set, choosing the default provisioner.  (gp2 on AWS, standard on` `33 `  `##   GKE, AWS & OpenStack)` `34 `  `##` `35 `  `# storageClass: "-"` `36 `  `accessMode``:` `ReadWriteOnce` `37 `  `size``:` `2Gi`  ```   `````````````````````````````````` As you can see, all the things that may vary from one *go-demo-3* installation to another are defined here. We can set how many replicas should be deployed for both the API and the DB. Tags of both can be changed as well. We can disable Ingress and change the host. We can change the type of the Service or disable RBAC. The resources are split into two groups, so that the API and the DB can be controlled separately. Finally, we can change database persistence by specifying the `storageClass`, the `accessMode`, or the `size`.    I should have described those values in more detail in `README.md`, but, as I already admitted, I was too lazy to do that. The alternative explanation of the lack of proper README is that we’ll go through the YAML files where those values are used, and everything will become much more apparent.    The important thing to note is that the values defined in that file are defaults that are used only if we do not overwrite them during the installation through `--set` or `--values` arguments.    The files that define all the resources are in the `templates` directory.    ``` `1` ls -1 helm/go-demo-3/templates/  ```   ````````````````````````````````` The output is as follows.    ``` `1` NOTES.txt `2` _helpers.tpl `3` deployment.yaml `4` ing.yaml `5` rbac.yaml `6` sts.yaml `7` svc.yaml  ```   ```````````````````````````````` The templates are written in [Go template language](https://golang.org/pkg/text/template/) extended with add-on functions from [Sprig library](https://github.com/Masterminds/sprig) and a few others specific to Helm. Don’t worry if you are new to Go. You will not need to learn it. For most use-cases, a few templating rules are more than enough for most of the use-cases. With time, you might decide to “go crazy” and learn everything templating offers. That time is not today.    When Helm renders the Chart, it’ll pass all the files in the `templates` directory through its templating engine.    Let’s take a look at the `NOTES.txt` file.    ``` `1` cat helm/go-demo-3/templates/NOTES.txt  ```   ``````````````````````````````` The output is as follows.    ```  `1` `1\. Wait until the applicaiton is rolled out:`  `2``kubectl -n` `{{` `.Release.Namespace` `}}` `rollout status deployment` `{{` `template` `"helm.fu\`  `3` `llname"` `.` `}}` `````` `4`   `5` `2\. Test the application by running these commands:`  `6` `{{``-` `if` `.Values.ingress.enabled` `}}` ````` `7``  curl http://``{{` `.Values.ingress.host` `}}``/demo/hello`  `8` `{{``-` `else` `if` `contains` `"NodePort"` `.Values.service.type` `}}` ```` `9``export PORT=$(kubectl -n` `{{` `.Release.Namespace` `}}` `get svc` `{{` `template` `"helm.fullna\` `10` `me"` `.` `}}` `-o jsonpath="{.spec.ports[0].nodePort}")` `11`  `12 ``  # If you are running Docker for Mac/Windows` `13 ``  export ADDR=localhost` `14`  `15 ``  # If you are running minikube` `16 ``  export ADDR=$(minikube ip)` `17`  `18 ``  # If you are running anything else` `19 ``export ADDR=$(kubectl -n` `{{` `.Release.Namespace` `}}` `get nodes -o jsonpath="{.items[0\` `20` `].status.addresses[0].address}")` `21`  `22 ``  curl http://$NODE_IP:$PORT/demo/hello` `23` `{{``-` `else` `}}` ``` `24 ``  If the application is running in OpenShift, please create a Route to enable access.` `25`  `26 ``  For everyone else, you set ingress.enabled=false and service.type is not set to No\` `27` `dePort. The application cannot be accessed from outside the cluster.` `28` `{{``-` `end` `}}` ``` ```` ````` `````` ```   `````````````````````````````` ````````````````````````````` ```````````````````````````` The content of the NOTES.txt file will be printed after the installation or upgrade. You already saw a similar one in action when we installed Jenkins. The instructions we received how to open it and how to retrieve the password came from the NOTES.txt file stored in Jenkins Chart.    That file is our first direct contact with Helm templating. You’ll notice that parts of it are inside `if/else` blocks. If we take a look at the second bullet, we can deduce that one set of instructions will be printed if `ingress` is `enabled`, another if the `type` of the Service is `NodePort`, and yet another if neither of the first two conditions is met.    Template snippets are always inside double curly braces (e.g., `{{` and `}}`). Inside them can be (often simple) logic like an `if` statement, as well as predefined and custom made function. An example of a custom made function is `{{ template "helm.fullname" . }}`. It is defined in `_helpers.tpl` file which we’ll explore soon.    Variables always start with a dot (`.`). Those coming from the `values.yaml` file are always prefixed with `.Values`. An example is `.Values.ingress.host` that defines the `host` that will be configured in our Ingress resource.    Helm also provides a set of pre-defined variables prefixed with `.Release`, `.Chart`, `.Files`, and `.Capabilities`. As an example, near the top of the NOTES.txt file is `{{ .Release.Namespace }}` snippet that will get converted to the Namespace into which we decided to install our Chart.    The full list of the pre-defined values is as follows (a copy of the official documentation).    *   `Release.Name`: The name of the release (not the Chart) *   `Release.Time`: The time the chart release was last updated. This will match the Last Released time on a Release object. *   `Release.Namespace`: The Namespace the Chart was released to. *   `Release.Service`: The service that conducted the release. Usually this is Tiller. *   `Release.IsUpgrade`: This is set to `true` if the current operation is an upgrade or rollback. *   `Release.IsInstall`: This is set to `true` if the current operation is an install. *   `Release.Revision`: The revision number. It begins at 1, and increments with each helm upgrade. *   `Chart`: The contents of the Chart.yaml. Thus, the Chart version is obtainable as Chart.Version and the maintainers are in Chart.Maintainers. *   `Files`: A map-like object containing all non-special files in the Chart. This will not give you access to templates, but will give you access to additional files that are present (unless they are excluded using .helmignore). Files can be accessed using `{{index .Files "file.name"}}` or using the `{{.Files.Get name}}` or `{{.Files.GetString name}}` functions. You can also access the contents of the file as `[]byte` using `{{.Files.GetBytes}}` *   `Capabilities`: A map-like object that contains information about the versions of Kubernetes (`{{.Capabilities.KubeVersion}}`, Tiller (`{{.Capabilities.TillerVersion}}`, and the supported Kubernetes API versions (`{{.Capabilities.APIVersions.Has "batch/v1"}}`)    You’ll also notice that our `if`, `else if`, `else`, and `end` statements start with a dash (`-`). That’s the Go template way of specifying that we want all empty space before the statement (when `-` is on the left) or after the statement (when `-` is on the right) to be removed.    There’s much more to Go templating that what we just explored. I’ll comment on other use-cases as they come. For now, this should be enough to get you going. You are free to consult [template package documentation](https://golang.org/pkg/text/template/) for more info. For now, the critical thing to note is that we have the `NOTES.txt` file that will provide useful post-installation information to those who will use our Chart.    I mentioned `_helpers.tpl` as the source of custom functions and variables. Let’s take a look at it.    ``` `1` cat helm/go-demo-3/templates/_helpers.tpl  ```   ``````````````````````````` The output is as follows.    ```  `1` `{{``/*` `vim``:` `set` `filetype``=``mustache``:` `*/``}}` ``````````````` `2` `{{``/*`  `3` `Expand` `the` `name` `of` `the` `chart``.`  `4` `*/``}}` `````````````` `5` `{{``-` `define` `"helm.name"` -`}}` ````````````` `6` `{{``-` `default` `.Chart.Name` `.Values.nameOverride` `|` `trunc` `63` `|` `trimSuffix` `"-"` -`}}` ```````````` `7` `{{``-` `end` -`}}` ``````````` `8`   `9` `{{``/*` `10` `Create` `a` `default` `fully` `qualified` `app` `name``.` `11` `We` `truncate` `at` `63` `chars` `because` `some` `Kubernetes` `name` `fields` `are` `limited` `to` `this` `(``by` `\` `12` `the` `DNS` `naming` `spec``)``.` `13` `If` `release` `name` `contains` `chart` `name` `it` `will` `be` `used` `as` `a` `full` `name``.` `14` `*/``}}` `````````` `15` `{{``-` `define` `"helm.fullname"` -`}}` ````````` `16` `{{``-` `$``name` `:=` `default` `.Chart.Name` `.Values.nameOverride` -`}}` ```````` `17` `{{``-` `if` `contains` `$``name` `.Release.Name` -`}}` ``````` `18` `{{``-` `.Release.Name` `|` `trunc` `63` `|` `trimSuffix` `"-"` -`}}` `````` `19` `{{``-` `else` -`}}` ````` `20` `{{``-` `printf` `"%s-%s"` `.Release.Name` `$``name` `|` `trunc` `63` `|` `trimSuffix` `"-"` -`}}` ```` `21` `{{``-` `end` -`}}` ``` `22` `{{``-` `end` -`}}` ``` ```` ````` `````` ``````` ```````` ````````` `````````` ``````````` ```````````` ````````````` `````````````` ``````````````` ```   `````````````````````````` ````````````````````````` ```````````````````````` That file is the exact copy of the `_helpers.tpl` file that was created with the `helm create` command that generated a sample Chart. You can extend it with your own functions. I didn’t. I kept it as-is. It consists of two functions with comments that describe them. The first (`helm.name`) returns the name of the chart trimmed to 63 characters which is the limitation for the size of some of the Kubernetes fields. The second function (`helm.fullname`) returns fully qualified name of the application. If you go back to the NOTES.txt file, you’ll notice that we are using `helm.fullname` in a few occasions. Later on, you’ll see that we’ll use it in quite a few other places.    Now that NOTES.txt and _helpers.tpl are out of the way, we can take a look at the first template that defines one of the Kubernetes resources.    ``` `1` cat helm/go-demo-3/templates/deployment.yaml  ```   ``````````````````````` The output is as follows.    ```  `1` `apiVersion``:` `apps/v1beta2`  `2` `kind``:` `Deployment`  `3` `metadata``:`  `4`  `name``:` `{{` `template "helm.fullname" .` `}}`  `5`  `labels``:`  `6`    `app``:` `{{` `template "helm.name" .` `}}`  `7`    `chart``:` `{{` `.Chart.Name` `}}``-{{ .Chart.Version | replace "+" "_" }}`  `8`    `release``:` `{{` `.Release.Name` `}}`  `9`    `heritage``:` `{{` `.Release.Service` `}}` `10` `spec``:` `11 `  `replicas``:` `{{` `.Values.replicaCount` `}}` `12 `  `selector``:` `13 `    `matchLabels``:` `14 `      `app``:` `{{` `template "helm.name" .` `}}` `15 `      `release``:` `{{` `.Release.Name` `}}` `16 `  `template``:` `17 `    `metadata``:` `18 `      `labels``:` `19 `        `app``:` `{{` `template "helm.name" .` `}}` `20 `        `release``:` `{{` `.Release.Name` `}}` `21 `    `spec``:` `22 `      `containers``:` `23 `      `-` `name``:` `api` `24 `        `image``:` `"vfarcic/go-demo-3:{{``.Values.image.tag``}}"` `25 `        `env``:` `26 `        `-` `name``:` `DB` `27 `          `value``:` `{{` `template "helm.fullname" .` `}}``-db` `28 `        `readinessProbe``:` `29 `          `httpGet``:` `30 `            `path``:` `/demo/hello` `31 `            `port``:` `8080` `32 `          `periodSeconds``:` `1` `33 `        `livenessProbe``:` `34 `          `httpGet``:` `35 `            `path``:` `/demo/hello` `36 `            `port``:` `8080` `37 `        `resources``:` `38` `{{` `toYaml .Values.resources | indent 10` `}}`  ```   `````````````````````` That file defines the Deployment of the *go-demo-3* API. The first thing I did was to copy the definition from the YAML file we used in the previous chapters. Afterwards, I replaced parts of it with functions and variables. The `name`, for example, is now `{{ template "helm.fullname" . }}`, which guarantees that this Deployment will have a unique name. The rest of the file follows the same logic. Some things are using pre-defined values like `{{ .Chart.Name }}` and `{{ .Release.Name }}`, while others are using those from the `values.yaml`. An example of the latter is `{{ .Values.replicaCount }}`.    The last line contains a syntax we haven’t seen before. `{{ toYaml .Values.resources | indent 10 }}` will take all the entries from the `resources` field in the `values.yaml`, and convert them to YAML format. Since the final YAML needs to be correctly indented, we piped the output to `indent 10`. Since the `resources:` section of `deployment.yaml` is indented by eight spaces, indenting the entries from `resources` in `values.yaml` by ten will put them just two spaces inside it.    Let’s take a look at one more template.    ``` `1` cat helm/go-demo-3/templates/ing.yaml  ```   ````````````````````` The output is as follows.    ```  `1` `{{``- if .Values.ingress.enabled -``}}`  `2` `{{``- $serviceName` `:``= include "helm.fullname" . -``}}`  `3` `apiVersion``:` `extensions/v1beta1`  `4` `kind``:` `Ingress`  `5` `metadata``:`  `6`  `name``:` `{{` `template "helm.fullname" .` `}}`  `7`  `labels``:`  `8`    `app``:` `{{` `template "helm.name" .` `}}`  `9`    `chart``:` `{{` `.Chart.Name` `}}``-{{ .Chart.Version | replace "+" "_" }}` `10 `    `release``:` `{{` `.Release.Name` `}}` `11 `    `heritage``:` `{{` `.Release.Service` `}}` `12 `  `annotations``:` `13 `    `ingress.kubernetes.io/ssl-redirect``:` `"false"` `14 `    `nginx.ingress.kubernetes.io/ssl-redirect``:` `"false"` `15` `spec``:` `16 `  `rules``:` `17 `  `-` `http``:` `18 `      `paths``:` `19 `      `-` `backend``:` `20 `          `serviceName``:` `{{` `$serviceName` `}}` `21 `          `servicePort``:` `8080` `22 `    `host``:` `{{` `.Values.ingress.host` `}}` `23` `{{``- end -``}}`  ```   ```````````````````` That YAML defines the Ingress resource that makes the API Deployment accessible through its Service. Most of the values are the same as in the Deployment. There’s only one difference worthwhile commenting.    The whole YAML is enveloped in the `{{- if .Values.ingress.enabled -}}` statement. The resource will be installed only if `ingress.enabled` value is set to `true`. Since that is already the default value in `values.yaml`, we’ll have to explicitly disable it if we do not want Ingress.    Feel free to explore the rest of the templates. They are following the same logic as the two we just described.    There’s one potentially significant file we did not define. We have not created `requirements.yaml` for *go-demo-3*. We did not need any. We will use it though in one of the next chapters, so I’ll save the explanation for later.    Now that we went through the files that constitute the *go-demo-3* Chart, we should `lint` it to confirm that the format does not contain any apparent issues.    ``` `1` helm lint helm/go-demo-3  ```   ``````````````````` The output is as follows.    ``` `1` ==> Linting helm/go-demo-3 `2` [INFO] Chart.yaml: icon is recommended `3`  `4` 1 chart(s) linted, no failures  ```   `````````````````` If we ignore the complaint that the icon is not defined, our Chart seems to be defined correctly, and we can create a package.    ``` `1` helm package helm/go-demo-3 -d helm  ```   ````````````````` The output is as follows.    ``` `1` Successfully packaged chart and saved it to: helm/go-demo-3-0.0.1.tgz  ```   ```````````````` The `-d` argument is new. It specified that we want to create a package in `helm` directory.    We will not use the package just yet. For now, I wanted to make sure that you remember that we can create it.    ### Upgrading Charts    We are about to install the *go-demo-3* Chart. You should already be familiar with the commands, so you can consider this as an exercise that aims to solidify what you already learned. There will be one difference when compared to the commands we executed earlier. It’ll prove to be a simple, and yet an important one for our continuous deployment processes.    We’ll start by inspecting the values.    ``` `1` helm inspect values helm/go-demo-3  ```   ``````````````` The output is as follows.    ```  `1` `replicaCount``:` `3`  `2` `dbReplicaCount``:` `3`  `3` `image``:`  `4`  `tag``:` `latest`  `5`  `dbTag``:` `3.3`  `6` `ingress``:`  `7`  `enabled``:` `true`  `8`  `host``:` `acme.com`  `9` `route``:` `10 `  `enabled``:` `true` `11` `service``:` `12 `  `# Change to NodePort if ingress.enable=false` `13 `  `type``:` `ClusterIP` `14` `rbac``:` `15 `  `enabled``:` `true` `16` `resources``:` `17 `  `limits``:` `18 `   `cpu``:` `0.2` `19 `   `memory``:` `20Mi` `20 `  `requests``:` `21 `   `cpu``:` `0.1` `22 `   `memory``:` `10Mi` `23` `dbResources``:` `24 `  `limits``:` `25 `    `memory``:` `"200Mi"` `26 `    `cpu``:` `0.2` `27 `  `requests``:` `28 `    `memory``:` `"100Mi"` `29 `    `cpu``:` `0.1` `30` `dbPersistence``:` `31 `  `## If defined, storageClassName: <storageClass>` `32 `  `## If set to "-", storageClassName: "", which disables dynamic provisioning` `33 `  `## If undefined (the default) or set to null, no storageClassName spec is` `34 `  `##   set, choosing the default provisioner.  (gp2 on AWS, standard on` `35 `  `##   GKE, AWS & OpenStack)` `36 `  `##` `37 `  `# storageClass: "-"` `38 `  `accessMode``:` `ReadWriteOnce` `39 `  `size``:` `2Gi`  ```   `````````````` We are almost ready to install the application. The only thing we’re missing is the host we’ll use for the application.    You’ll find two commands below. Please execute only one of those depending on your Kubernetes flavor.    If you are **NOT** using **minishift**, please execute the command that follows.    ``` `1` `HOST``=``"go-demo-3.``$LB_IP``.nip.io"`  ```   ````````````` If you are using minishift, you can retrieve the host with the command that follows.    ``` `1` `HOST``=``"go-demo-3-go-demo-3.``$(`minishift ip`)``.nip.io"`  ```   ```````````` No matter how you retrieved the host, we’ll output it so that we can confirm that it looks OK.    ``` `1` `echo` `$HOST`  ```   ``````````` In my case, the output is as follows.    ``` `1` go-demo-3.192.168.99.100.nip.io  ```   `````````` Now we are finally ready to install the Chart. However, we won’t use `helm install` as before. We’ll use `upgrade` instead.    ``` `1` helm upgrade -i `\` `2 `    go-demo-3 helm/go-demo-3 `\` `3 `    --namespace go-demo-3 `\` `4 `    --set image.tag`=``1`.0 `\` `5 `    --set ingress.host`=``$HOST` `\` `6 `    --reuse-values  ```   ````````` The reason we are using `helm upgrade` this time lies in the fact that we are practicing the commands we hope to use inside our CDP processes. Given that we want to use the same process no matter whether it’s the first release (install) or those that follow (upgrade). It would be silly to have `if/else` statements that would determine whether it is the first release and thus execute the install, or to go with an upgrade. We are going with a much simpler solution. We will always upgrade the Chart. The trick is in the `-i` argument that can be translated to “install unless a release by the same name doesn’t already exist.”    The next two arguments are the name of the Chart (`go-demo-3`) and the path to the Chart (`helm/go-demo-3`). By using the path to the directory with the Chart, we are experiencing yet another way to supply the Chart files. In the next chapter will switch to using `tgz` packages.    The rest of the arguments are making sure that the correct tag is used (`1.0`), that Ingress is using the desired host, and that the values that might have been used in the previous upgrades are still the same (`--reuse-values`).    If this command is used in the continuous deployment processes, we would need to set the tag explicitly through the `--set` argument to ensure that the correct image is used. The host, on the other hand, is static and unlikely to change often (if ever). We would be better of defining it in `values.yaml`. However, since I could not predict what will be your host, we had to define it as the `--set` argument.    Please note that minishift does not support Ingress (at least not by default). So, it was created, but it has no effect. I thought that it is a better option than to use different commands for OpenShift than for the rest of the flavors. If minishift is your choice, feel free to add `--set ingress.enable=false` to the previous command.    The output of the `upgrade` is the same as if we executed `install` (resources are removed for brevity).    ```  `1` `NAME``:`   `go``-``demo``-``3`  `2` `LAST` `DEPLOYED``:` `Fri` `May` `25` `14``:``40``:``31` `2018`  `3` `NAMESPACE``:` `go``-``demo``-``3`  `4` `STATUS``:` `DEPLOYED`  `5`   `6` `...`  `7`   `8` `NOTES``:`  `9` `1``.` `Wait` `until` `the` `application` `is` `rolled` `out``:` `10 `  `kubectl` `-``n` `go``-``demo``-``3` `rollout` `status` `deployment` `go``-``demo``-``3` `11`  `12` `2``.` `Test` `the` `application` `by` `running` `these` `commands``:` `13 `  `curl` `http``://``go``-``demo``-``3.18``.``222.53``.``124``.``nip``.``io``/demo/``hello`  ```   ```````` We’ll wait until the Deployment rolls out before proceeding.    ``` `1` kubectl -n go-demo-3 `\` `2 `    rollout status deployment go-demo-3  ```   ``````` The output is as follows.    ``` `1` Waiting for rollout to finish: 0 of 3 updated replicas are available... `2` Waiting for rollout to finish: 1 of 3 updated replicas are available... `3` Waiting for rollout to finish: 2 of 3 updated replicas are available... `4` deployment "go-demo-3" successfully rolled out  ```   `````` Now we can confirm that the application is indeed working by sending a `curl` request.    ``` `1` curl http://`$HOST`/demo/hello  ```   ````` The output should display the familiar `hello, world!` message, thus confirming that the application is indeed running and that it is accessible through the host we defined in Ingress (or Route in case of minishift).    Let’s imagine that some time passed since we installed the first release, that someone pushed a change to the master branch, that we already run all our tests, that we built a new image, and that we pushed it to Docker Hub. In that hypothetical situation, our next step would be to execute another `helm upgrade`.    ``` `1` helm upgrade -i `\` `2 `    go-demo-3 helm/go-demo-3 `\` `3 `    --namespace go-demo-3 `\` `4 `    --set image.tag`=``2`.0 `\` `5 `    --reuse-values  ```   ```` When compared with the previous command, the difference is in the tag. This time we set it to `2.0`. We also removed `--set ingress.host=$HOST` argument. Since we have `--reuse-values`, all those used in the previous release will be maintained.    There’s probably no need to further validations (e.g., wait for it to roll out and send a `curl` request). All that’s left is to remove the Chart and delete the Namespace. We’re done with the hands-on exercises.    ``` `1` helm delete go-demo-3 --purge `2`  `3` kubectl delete ns go-demo-3  ```   `### Helm vs. OpenShift Templates    I could give you a lengthy comparison between Helm and OpenShift templates. I won’t do that. The reason is simple. Helm is the de-facto standard for installing applications. It’s the most widely used, and its adoption is going through the roof. Among the similar tools, it has the biggest community, it has the most applications available, and it is becoming adopted by more software vendors than any other solution. The exception is RedHat. They created OpenShift templates long before Helm came into being. Helm borrowed many of its concepts, improved them, and added a few additional features. When we add to that the fact that OpenShift templates work only on OpenShift, the decision which one to use is pretty straightforward. Helm wins, unless you chose OpenShift as your Kubernetes flavor. In that case, the choice is harder to make. On the one hand, Routes and a few other OpenShift-specific types of resources cannot be defined (easily) in Helm. On the other hand, it is likely that OpenShift will switch to Helm at some moment. So, you might just as well jump into Helm right away.    I must give a big thumbs up to RedHat for paving the way towards some of the Kubernetes resources that are in use today. They created Routes when Ingress did not exist. They developed OpenShift templates before Helm was created. Both Ingress and Helm were heavily influenced by their counterparts in OpenShift. There are quite a few other similar examples.    The problem is that RedHat does not want to let go of the things they pioneered. They stick with Routes, even though Ingress become standard. If Routes provide more features than, let’s say, nginx Ingress controller, they could still maintain them as OpenShift Ingress (or whatever would be the name). Routes are not the only example. They continue forcing OpenShift templates, even though it’s clear that Helm is the de-facto standard. By not switching to the standards that they pioneered, they are making their platform incompatible with others. In the previous chapters, we experienced the pain Routes cause when trying to define YAML files that should work on all other Kubernetes flavors. Now we experienced the same problem with Helm.    If you chose OpenShift, it’s up to you to decide whether to use Helm or OpenShift templates. Both choices have pros and cons. Personally, one of the things that attract me the most with Kubernetes is the promise that our applications can run on any hosting solution and on any Kubernetes flavor. RedHat is breaking that promise. It’s not that I don’t expect different solutions to come up with new things that distinguish them from the competition. I do. OpenShift has quite a few of those. But, it also has features that have equally good or better equivalents that are part of Kubernetes core or widely accepted by the community. Helm is one of those that are better than their counterpart in OpenShift.    We’ll continue using Helm throughout the rest of the book. If you do choose to stick with OpenShift templates, you’ll have to do a few modifications to the examples. The good news is that those changes should be relatively easy to make. I believe that you won’t have a problem adapting.    ### What Now?    We have a couple of problems left to solve. We did not yet figure out how to store the Helm charts in a way that they can be easily retrieved and used by others. We’ll tackle that issue in the next chapter.    I suggest you take a rest. You deserve it. If you do feel that way, please destroy the cluster. Otherwise, jump to the next chapter right away. The choice is yours.` ```` ````` `````` ``````` ```````` ````````` `````````` ``````````` ```````````` ````````````` `````````````` ``````````````` ```````````````` ````````````````` `````````````````` ``````````````````` ```````````````````` ````````````````````` `````````````````````` ``````````````````````` ```````````````````````` ````````````````````````` `````````````````````````` ``````````````````````````` ```````````````````````````` ````````````````````````````` `````````````````````````````` ``````````````````````````````` ```````````````````````````````` ````````````````````````````````` `````````````````````````````````` ``````````````````````````````````` ```````````````````````````````````` ````````````````````````````````````` `````````````````````````````````````` ``````````````````````````````````````` ```````````````````````````````````````` ````````````````````````````````````````` `````````````````````````````````````````` ``````````````````````````````````````````` ```````````````````````````````````````````` ````````````````````````````````````````````` `````````````````````````````````````````````` ``````````````````````````````````````````````` ```````````````````````````````````````````````` ````````````````````````````````````````````````` `````````````````````````````````````````````````` ``````````````````````````````````````````````````` ```````````````````````````````````````````````````` ````````````````````````````````````````````````````` `````````````````````````````````````````````````````` ``````````````````````````````````````````````````````` ```````````````````````````````````````````````````````` ````````````````````````````````````````````````````````` `````````````````````````````````````````````````````````` ``````````````````````````````````````````````````````````` ```````````````````````````````````````````````````````````` ````````````````````````````````````````````````````````````` `````````````````````````````````````````````````````````````` ``````````````````````````````````````````````````````````````` ```````````````````````````````````````````````````````````````` ````````````````````````````````````````````````````````````````` `````````````````````````````````````````````````````````````````` ``````````````````````````````````````````````````````````````````` ```````````````````````````````````````````````````````````````````` ````````````````````````````````````````````````````````````````````` `````````````````````````````````````````````````````````````````````` ``````````````````````````````````````````````````````````````````````` ```````````````````````````````````````````````````````````````````````` ````````````````````````````````````````````````````````````````````````` `````````````````````````````````````````````````````````````````````````` ``````````````````````````````````````````````````````````````````````````` ```````````````````````````````````````````````````````````````````````````` ````````````````````````````````````````````````````````````````````````````` `````````````````````````````````````````````````````````````````````````````` ``````````````````````````````````````````````````````````````````````````````` ```````````````````````````````````````````````````````````````````````````````` ````````````````````````````````````````````````````````````````````````````````` `````````````````````````````````````````````````````````````````````````````````` ``````````````````````````````````````````````````````````````````````````````````` ```````````````````````````````````````````````````````````````````````````````````` ````````````````````````````````````````````````````````````````````````````````````` `````````````````````````````````````````````````````````````````````````````````````` ``````````````````````````````````````````````````````````````````````````````````````` ```````````````````````````````````````````````````````````````````````````````````````` ````````````````````````````````````````````````````````````````````````````````````````` `````````````````````````````````````````````````````````````````````````````````````````` ``````````````````````````````````````````````````````````````````````````````````````````` ```````````````````````````````````````````````````````````````````````````````````````````` ````````````````````````````````````````````````````````````````````````````````````````````` `````````````````````````````````````````````````````````````````````````````````````````````` ``````````````````````````````````````````````````````````````````````````````````````````````` ```````````````````````````````````````````````````````````````````````````````````````````````` ````````````````````````````````````````````````````````````````````````````````````````````````` `````````````````````````````````````````````````````````````````````````````````````````````````` ``````````````````````````````````````````````````````````````````````````````````````````````````` ```````````````````````````````````````````````````````````````````````````````````````````````````` ````````````````````````````````````````````````````````````````````````````````````````````````````` `````````````````````````````````````````````````````````````````````````````````````````````````````` ``````````````````````````````````````````````````````````````````````````````````````````````````````` ```````````````````````````````````````````````````````````````````````````````````````````````````````` ````````````````````````````````````````````````````````````````````````````````````````````````````````` `````````````````````````````````````````````````````````````````````````````````````````````````````````` ``````````````````````````````````````````````````````````````````````````````````````````````````````````` ```````````````````````````````````````````````````````````````````````````````````````````````````````````` ````````````````````````````````````````````````````````````````````````````````````````````````````````````` `````````````````````````````````````````````````````````````````````````````````````````````````````````````` ``````````````````````````````````````````````````````````````````````````````````````````````````````````````` ```````````````````````````````````````````````````````````````````````````````````````````````````````````````` ````````````````````````````````````````````````````````````````````````````````````````````````````````````````` `````````````````````````````````````````````````````````````````````````````````````````````````````````````````` ``````````````````````````````````````````````````````````````````````````````````````````````````````````````````` ```````````````````````````````````````````````````````````````````````````````````````````````````````````````````` ````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` `````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` ``````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` ```````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` ````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` `````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` ``````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` ```````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` ````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` `````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` ``````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` ```````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` ````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` `````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` ```````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````
