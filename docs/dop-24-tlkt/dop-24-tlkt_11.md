## 分发 Kubernetes 应用程序

我们已经将镜像存储在[Docker Hub](https://hub.docker.com/)中。我们本可以选择其他容器注册表，但由于 Docker Hub 非常方便，我们将继续在本书中使用它。尽管这可能不是最好的选择，但如果我们将关于容器镜像仓库的讨论放到一边，就可以集中精力讨论 YAML 文件，或者更具体地说，Helm Charts。

此时，你可能会认为在笔记本电脑上运行位于本地的 Charts 是一种非常好的方式。你只需检查某个应用程序的代码，期望该 Chart 已经存在，然后执行像`helm upgrade -i go-demo-3 helm/go-demo-3`这样的命令。你是对的，这是安装或升级你正在开发的应用程序的最简单方式。然而，你安装的不仅仅是你自己的应用程序。

如果你是开发者，你几乎肯定希望在笔记本电脑上运行多个应用程序。如果你需要检查你的应用程序是否与同事开发的应用程序集成，你也需要运行他们的应用程序。你可以继续检查他们的代码并安装本地的 Charts。但这开始变得繁琐。你需要知道他们使用的仓库，并检查比你真正需要更多的代码。难道不更好地像安装公开的第三方应用程序一样安装同事的应用程序吗？如果你能执行类似`helm search my-company-repo/`的命令，获取你所在组织内所有应用程序的列表，并安装你需要的那些，岂不是更方便？我们已经在使用相同的方法来处理容器镜像（例如`docker image pull`）、Linux 包（`apt install vim`）以及许多其他包和分发版了。为什么不把 Helm Charts 也做成这样呢？为什么我们只限制从第三方获取应用程序定义的能力？我们应该能够以相同的方式分发我们的应用程序。

Helm Charts 仍然非常年轻。该项目刚刚起步，选择的仓库也不多。今天（2018 年 6 月），[ChartMuseum](https://github.com/kubernetes-helm/chartmuseum) 是为数不多的选择之一，甚至可能是唯一的选择。因此，选择正确的解决方案非常直接。当选择不多时，选择过程就变得简单。

在本章中，我们将探讨 Helm 仓库以及如何利用它们在组织内分发我们的 Charts，或者如果我们从事向更广泛的公众提供软件的业务，如何将它们发布给更广泛的受众。

和往常一样，我们需要从某个地方开始，那就是 Kubernetes 集群。

### 创建集群并获取其 IP

你知道该怎么做。创建一个新集群，或者重用你为练习而专门设置的集群。

首先，我们将前往本地的*vfarcic/k8s-specs* 仓库，并确保我们拥有最新的版本。谁知道呢？自从你读完上一章后，我可能已经做了一些修改。

```
`1` `cd` k8s-specs
`2` 
`3` git pull 
```

```````````````````````````````````````````````````````````` The requirements for the cluster are now slightly different. We’ll need **Helm server** (**tiller**). On top of that, if you are a **minishift** user, you’ll need a cluster with 4GB RAM.    For your convenience, the new Gists and the specs are available.    *   [docker4mac-helm.sh](https://gist.github.com/7e6b068c6d3d56fc53416ac2cd4086e3): **Docker for Mac** with 3 CPUs, 3 GB RAM, with **nginx Ingress**, and with **tiller**. *   [minikube-helm.sh](https://gist.github.com/728246d8be349ffb52770f72c39e9564): **minikube** with 3 CPUs, 3 GB RAM, with `ingress`, `storage-provisioner`, and `default-storageclass` addons enabled, and with **tiller**. *   [kops-helm.sh](https://gist.github.com/6c1ebd59305fba9eb0102a5a8cea863b): **kops in AWS** with 3 t2.small masters and 2 t2.medium nodes spread in three availability zones, with **nginx Ingress**, and with **tiller**. The Gist assumes that the prerequisites are set through Appendix B. *   [minishift-helm.sh](https://gist.github.com/945ab1e68afa9e49f85cec3bc6df4959): **minishift** with 3 CPUs, 3 GB RAM, with version 1.16+, and with **tiller**. *   [gke-helm.sh](https://gist.github.com/1593ed36c4b768a462b1a32d5400649b): **Google Kubernetes Engine (GKE)** with 3 n1-highcpu-2 (2 CPUs, 1.8 GB RAM) nodes (one in each zone), and with **nginx Ingress** controller running on top of the “standard” one that comes with GKE, and with **tiller**. We’ll use nginx Ingress for compatibility with other platforms. Feel free to modify the YAML files and Helm Charts if you prefer NOT to install nginx Ingress. *   [eks-helm.sh](https://gist.github.com/6de44c440c0d0facb20b743c079bd12f): **Elastic Kubernetes Service (EKS)** with 2 t2.medium nodes, with **nginx Ingress** controller, with a **default StorageClass**, and with **tiller**.    Besides creating a cluster, we’ll need an IP through which we can access it. The instructions that follow differ from one Kubernetes flavor to another. Please make sure you execute those matching your cluster.    If your cluster is running in **AWS** and if it was created with **kops**, we’ll retrieve the IP by digging the hostname of the Elastic Load Balancer (ELB). Please execute the commands that follow.    ``` `1` `LB_HOST``=``$(`kubectl -n kube-ingress `\` `2 `    get svc ingress-nginx `\` `3 `    -o `jsonpath``=``"{.status.loadBalancer.ingress[0].hostname}"``)` `4`  `5` `LB_IP``=``"``$(`dig +short `$LB_HOST` `\` `6 `    `|` tail -n `1``)``"`  ```   ``````````````````````````````````````````````````````````` If your cluster is running in **AWS** and if it was created as **EKS**, we’ll retrieve the IP by digging the hostname of the Elastic Load Balancer (ELB). Please execute the commands that follow.    ``` `1` `LB_HOST``=``$(`kubectl -n ingress-nginx `\` `2 `    get svc ingress-nginx `\` `3 `    -o `jsonpath``=``"{.status.loadBalancer.ingress[0].hostname}"``)` `4`  `5` `LB_IP``=``"``$(`dig +short `$LB_HOST` `\` `6 `    `|` tail -n `1``)``"`  ```   `````````````````````````````````````````````````````````` If you’re using **Docker For Mac or Windows**, the cluster is accessible through localhost. Since we need an IP, we’ll use `127.0.0.1` instead. Please execute the command that follows.    ``` `1` `LB_IP``=``"127.0.0.1"`  ```   ````````````````````````````````````````````````````````` **Minikube** users can retrieve the IP through `minikube ip`. If you are one of them, please execute the command that follows.    ``` `1` `LB_IP``=``$(`minikube ip`)`  ```   ```````````````````````````````````````````````````````` Retrieving IP from **minishift** is similar to minikube. If that’s your Kubernetes flavor, please execute the command that follows.    ``` `1` `LB_IP``=``$(`minishift ip`)`  ```   ``````````````````````````````````````````````````````` Finally, if you are using **GKE**, the IP we’re looking for is available through the `ingress-nginx` service. Please execute the command that follows.    ``` `1` `LB_IP``=``$(`kubectl -n ingress-nginx `\` `2 `    get svc ingress-nginx `\` `3 `    -o `jsonpath``=``"{.status.loadBalancer.ingress[0].ip}"``)`  ```   `````````````````````````````````````````````````````` No matter how you retrieved the IP of your cluster, we’ll validate it by echoing the `LB_IP` variable.    ``` `1` `echo` `$LB_IP`  ```   ````````````````````````````````````````````````````` The output will differ from one case to another. In my case, it is `52.15.140.221`.    There’s only one more thing left before we jump into Chart repositories. We’ll merge your fork of the *go-demo-3* code repository with the origin and thus ensure that you are up-to-date with changes I might have made in the meantime.    First, we’ll move into the fork’s directory.    ``` `1` `cd` ../go-demo-3  ```   ```````````````````````````````````````````````````` To be on the safe side, we’ll push the changes you might have made in the previous chapter, and then we’ll sync your fork with the upstream repository. That way, we’ll guarantee that you have all the changes I might have made.    You probably already know how to push your changes and how to sync with the upstream repository. In case you don’t, the commands are as follows.    ```  `1` git add .  `2`   `3` git commit -m `\`  `4`    `"Packaging Kubernetes Applications chapter"`  `5`   `6` git push  `7`   `8` git remote add upstream `\`  `9`    https://github.com/vfarcic/go-demo-3.git `10`  `11` git fetch upstream `12`  `13` git checkout master `14`  `15` git merge upstream/master  ```   ``````````````````````````````````````````````````` We pushed the changes we made in the previous chapter, we fetched the upstream repository *vfarcic/go-demo-3*, and we merged the latest code from it. The only thing left is to go back to the `k8s-specs` directory.    ``` `1` `cd` ../k8s-specs  ```   `````````````````````````````````````````````````` Now we are ready to explore Helm repositories with *ChartMuseum*.    ### Using ChartMuseum    Just as [Docker Registry](https://docs.docker.com/registry/) is a place where we can publish our container images and make them accessible to others, we can use Chart repository to accomplish similar goals with our Charts.    A Chart repository is a location where packaged Charts can be stored and retrieved. We’ll use [ChartMuseum](https://github.com/kubernetes-helm/chartmuseum) for that. There aren’t many other solutions to choose. We can say that we picked it because there were no alternatives. That will change soon. I’m sure that Helm Charts will become integrated into general purpose repositories. At the time of this writing (June 2018), Charts are already supported by JFrog’s [Artifactory](https://www.jfrog.com/confluence/display/RTF/Helm+Chart+Repositories). You could easily build one yourself if you’re adventurous. All you’d need is a way to store `index.yaml` file that contains all the Charts and an API that could be used to push and retrieve packages. Anything else would be a bonus, not a requirement.    That’s it. That’s all the explanation you need, except a note that we’ll go with the easiest solution. We won’t build a Charts repository ourselves, nor we are going to pay for Artifactory. We’ll use ChartMuseum.    ChartMuseum is already available in the official Helm repository. We’ll add it to your Helm installation just in case you removed it accidentally.    ``` `1` helm repo add stable `\` `2 `    https://kubernetes-charts.storage.googleapis.com  ```   ````````````````````````````````````````````````` You should see the output claiming that `"stable" has been added to your repositories`.    Next, we’ll take a quick look at the values available in `chartmuseum`.    ``` `1` helm inspect values stable/chartmuseum  ```   ```````````````````````````````````````````````` The output, limited to the relevant parts, is as follows.    ```  `1` `...`  `2` `image``:`  `3`  `repository``:` `chartmuseum/chartmuseum`  `4`  `tag``:` `v0.7.0`  `5`  `pullPolicy``:` `IfNotPresent`  `6` `env``:`  `7`  `open``:`  `8`    `...`  `9`    `DISABLE_API``:` `true` `10 `    `...` `11 `  `secret``:` `12 `    `# username for basic http authentication` `13 `    `BASIC_AUTH_USER``:` `14 `    `# password for basic http authentication` `15 `    `BASIC_AUTH_PASS``:` `16 `    `...` `17` `resources``:` `{}` `18` `#  limits:` `19` `#    cpu: 100m` `20` `#    memory: 128Mi` `21` `#  requests:` `22` `#    cpu: 80m` `23` `#    memory: 64Mi` `24` `...` `25` `persistence``:` `26 `  `enabled``:` `false` `27 `  `...` `28` `## Ingress for load balancer` `29` `ingress``:` `30 `  `enabled``:` `false` `31` `...` `32` `#   annotations:` `33` `#     kubernetes.io/ingress.class: nginx` `34` `#     kubernetes.io/tls-acme: "true"` `35`  `36` `## Chartmuseum Ingress hostnames` `37` `## Must be provided if Ingress is enabled` `38` `##` `39` `#   hosts:` `40` `#     chartmuseum.domain.com:` `41` `#         - /charts` `42` `#         - /index.yaml` `43` `...`  ```   ``````````````````````````````````````````````` We can, and we will change the image tag. We’ll try to make that our practice with all installations. We’ll always use a specific tag, and leave latest for developers and others who might not be concerned with stability of the system.    By default, access to the API is disabled through the `DISABLE_API: true` entry. We’ll have to enable it if we are to interact with the API. We can see that there are, among others, `BASIC_AUTH_USER` and `BASIC_AUTH_PASS` secrets which we can use if we’d like to provide a basic HTTP authentication.    i> Please visit [ChartMuseum API](https://github.com/helm/chartmuseum#api) documentation if you’re interested in more details.    Further down are the commented resources. We’ll have to define them ourselves.    We’ll need to persist the state of the application and make it accessible through Ingress. Both can be accomplished by changing related `enabled` entries to `true` and, in case of Ingress, by adding a few annotations and a host.    Now that we went through the values we’re interested in, we can proceed with the practical parts. We’ll need to define the address (domain) we’ll use for ChartMuseum.    We already have the IP of the cluster (hopefully the IP of the external LB), and we can use it to create a `nip.io` domain, just as we did in the previous chapter.    ``` `1` `CM_ADDR``=``"cm.``$LB_IP``.nip.io"`  ```   `````````````````````````````````````````````` To be on the safe side, we’ll `echo` the value stored in `CM_ADDR`, and check whether it looks OK.    ``` `1` `echo` `$CM_ADDR`  ```   ````````````````````````````````````````````` In my case, the output is `cm.18.221.122.90.nip.io`.    If you go back to the values output, you’ll notice that the Chart requires host to be defined as a key/value pairs. The problem is that “special” characters cannot be used as part of keys. In the case of our address, we need to escape all the dots. We’ll use a bit of `sed` magic for that.    ``` `1` `CM_ADDR_ESC``=``$(``echo` `$CM_ADDR` `\` `2 `    `|` sed -e `"s@\.@\\\.@g"``)` `3`  `4` `echo` `$CM_ADDR_ESC`  ```   ```````````````````````````````````````````` We echoed the address, and we sent the output to the `sed` command that replaced every `.` character with `\.`. The output of the latter command should be similar to the one that follows.    ``` `1` cm\.18\.221\.122\.90\.nip\.io  ```   ``````````````````````````````````````````` I already prepared a file with all the values we’ll want to customize. Let’s take a quick look at it.    ``` `1` cat helm/chartmuseum-values.yml  ```   `````````````````````````````````````````` The output is as follows.    ```  `1` `image``:`  `2`  `tag``:` `v0.7.0`  `3` `env``:`  `4`  `open``:`  `5`    `DISABLE_API``:` `false`  `6` `resources``:`  `7`  `limits``:`  `8`    `cpu``:` `100m`  `9`    `memory``:` `128Mi` `10 `  `requests``:` `11 `    `cpu``:` `80m` `12 `    `memory``:` `64Mi` `13` `persistence``:` `14 `  `enabled``:` `true` `15` `ingress``:` `16 `  `enabled``:` `true` `17 `  `annotations``:` `18 `    `kubernetes.io/ingress.class``:` `"nginx"` `19 `    `ingress.kubernetes.io/ssl-redirect``:` `"false"` `20 `    `nginx.ingress.kubernetes.io/ssl-redirect``:` `"false"` `21 `  `hosts``:` `22 `    `cm.127.0.0.1.nip.io``:` `23 `    `-` `/`  ```   ````````````````````````````````````````` This is becoming monotonous, and that’s OK. It should be that way. Installations should be boring and follow the same pattern. We found that pattern in Helm.    The *chartmuseum-values.yml* file defines the values we discussed. It sets the `tag` we’ll use, and it enables the API. It defines the `resources`, and you already know that the values we’re using should be taken with a lot of skepticism. In the “real” production, the amount of memory and CPU your applications require will differ significantly from what we can observe in our examples. So we should always monitor our applications real usage patterns, and fine-tune the configuration instead of guessing.    We enabled `persistence`, and we’ll use the default StorageClass, since we did not specify any explicitly.    Ingress section defines the same annotations as those we used with the other Helm installations. It also defines a single host that will handle requests from all paths (`/`). Think of it as a reminder only. We cannot rely on the host in the *chartmuseum-values.yml* file since it likely differs from the `nip.io` address you defined. I could not predict which one will be in your case. So, we’ll overwrite that value with a `--set` argument.    Let’s install the Chart.    ``` `1` helm install stable/chartmuseum `\` `2 `    --namespace charts `\` `3 `    --name cm `\` `4 `    --values helm/chartmuseum-values.yml `\` `5 `    --set ingress.hosts.`"``$CM_ADDR_ESC``"``={``"/"``}` `\` `6 `    --set env.secret.BASIC_AUTH_USER`=`admin `\` `7 `    --set env.secret.BASIC_AUTH_PASS`=`admin  ```   ```````````````````````````````````````` The Chart is installed. Instead of waiting in silence for all the Pods to start running, we’ll briefly discuss security.    We defined the username and the password through `--set` arguments. They shouldn’t be stored in `helm/chartmuseum-values.yml` since that would defy the purpose of secrecy.    Personally, I believe that there’s no reason to hide the Charts. They do not (should not) contain anything confidential. The applications are stored in a container registry. Even if someone decides to use our Charts, that person would not be able to deploy our images, if our registry is configured to require authentication.    If that is not enough, and we do want to protect our Charts besides protecting images, we should ask yourself who should not be allowed to access them. If we want to prevent only outsiders from accessing our Charts, the fix is easy. We can put our cluster inside a VPN and make the domain accessible only to internal users. On the other hand, if we want to prevent even internal users from accessing our Charts, we can add basic HTTP authentication. We already saw the `secret` section when we inspected the values. You could set `env.secret.BASIC_AUTH_USER` and `env.secret.BASIC_AUTH_PASS` to enable basic authentication. That’s what we did in our example.    If none of those methods is secure enough, we can implement the best security measure of all. We can disable access to all humans by removing Ingress and changing the Service type to `ClusterIP`. That would result in only processes running in Pods being able to access the Charts. A good example would be to allow Jenkins to push and pull the Charts, and no one else. Even though that approach is more secure, it does not provide access to the Charts to people who might need it. Humans are true users of ChartMuseum. For scripts, it is easy to know which repository contains the definitions they need and to clone the code, even if that is only for the purpose of retrieving Charts. Humans need a way to search for Charts, to inspect them, and to run them on their laptops or servers.    We opted to a middle solution. We set up basic authentication which is better than no authentication, but still less secure than allowing only those within a VPN to access Charts or disabling human access altogether.    By now, the resources we installed should be up-and-running. We’ll confirm that just to be on the safe side.    ``` `1` kubectl -n charts `\` `2 `    rollout status deploy `\` `3 `    cm-chartmuseum  ```   ``````````````````````````````````````` The output should show that the `deployment "cm-chartmuseum"` was `successfully rolled out`.    Next, we’ll check whether the application is healthy.    ``` `1` curl `"http://``$CM_ADDR``/health"`  ```   `````````````````````````````````````` The output is as follows.    ``` `1` `{``"healthy"``:``true``}`  ```   ````````````````````````````````````` Now we can open ChartMuseum in browser.    ``` `1` open `"http://``$CM_ADDR``"`  ```   ```````````````````````````````````` You will be asked for a username and a password. Please use *admin* for both and click the *Sign in* button.  ![Figure 5-1: ChartMuseum's welcome screen](img/00014.jpeg)  Figure 5-1: ChartMuseum’s welcome screen    As you can see, there’s not much of the UI to look. We are supposed to interact with ChartMuseum through its API. If we need to visualize our Charts, we’ll need to look for a different solution.    Let’s see the index.    ``` `1` curl `"http://``$CM_ADDR``/index.yaml"`  ```   ``````````````````````````````````` Since we did not specify the username and the password, we got `{"error":"unauthorized"}` as the output. We’ll need to authenticate every time we want to interact with ChartMuseum API.    Let’s try again but, this time, with the authentication info.    ``` `1` curl -u admin:admin `\` `2 `    `"http://``$CM_ADDR``/index.yaml"`  ```   `````````````````````````````````` The output is as follows.    ``` `1` `apiVersion``:` `v1` `2` `entries``:` `{}` `3` `generated``:` `"2018-06-02T21:38:30Z"`  ```   ````````````````````````````````` It should come as no surprise that we have no entries to the museum. We did not yet push a Chart. Before we do any pushing, we should add a new repository to our Helm client.    ``` `1` helm repo add chartmuseum `\` `2 `    http://`$CM_ADDR` `\` `3 `    --username admin `\` `4 `    --password admin  ```   ```````````````````````````````` The output states that `"chartmuseum" has been added to your repositories`. From now on, all the Charts we store in our ChartMuseum installation will be available through our Helm client.    The only thing left is to start pushing Charts to ChartMuseum. We could do that by sending `curl` requests. However, there is a better way, so we’ll skip HTTP requests and install a Helm plugin instead.    ``` `1` helm plugin install `\` `2 `    https://github.com/chartmuseum/helm-push  ```   ``````````````````````````````` This plugin added a new command `helm push`. Let’s give it a spin.    ``` `1` helm push `\` `2 `    ../go-demo-3/helm/go-demo-3/ `\` `3 `    chartmuseum `\` `4 `    --username admin `\` `5 `    --password admin  ```   `````````````````````````````` The output is as follows.    ``` `1` Pushing go-demo-3-0.0.1.tgz to chartmuseum... `2` Done.  ```   ````````````````````````````` We pushed a Chart located in the `../go-demo-3/helm/go-demo-3/` directory into a repository `chartmuseum`. We can confirm that the push was indeed successful by retrieving `index.yaml` file from the repository.    ``` `1` curl `"http://``$CM_ADDR``/index.yaml"` `\` `2 `    -u admin:admin  ```   ```````````````````````````` The output is as follows.    ```  `1` `apiVersion``:` `v1`  `2` `entries``:`  `3`  `go-demo-3``:`  `4`  `-` `apiVersion``:` `v1`  `5`    `created``:` `"2018-06-02T21:39:21Z"`  `6`    `description``:` `A silly demo based on API written in Go and MongoDB`  `7`    `digest``:` `d8443c78485e80644ff9bfddcf32cc9f270864fb50b75377dbe813b280708519`  `8`    `home``:` `http://www.devopstoolkitseries.com/`  `9`    `keywords``:` `10 `    `-` `api` `11 `    `-` `backend` `12 `    `-` `go` `13 `    `-` `database` `14 `    `-` `mongodb` `15 `    `maintainers``:` `16 `    `-` `email``:` `viktor@farcic.com` `17 `      `name``:` `Viktor Farcic` `18 `    `name``:` `go-demo-3` `19 `    `sources``:` `20 `    `-` `https://github.com/vfarcic/go-demo-3` `21 `    `urls``:` `22 `    `-` `charts/go-demo-3-0.0.1.tgz` `23 `    `version``:` `0.0.1` `24` `generated``:` `"2018-06-02T21:39:28Z"`  ```   ``````````````````````````` We can see that the `go-demo-3` Chart is now in the repository. Most of the information comes from the `Chart.yaml` file we explored in the previous chapter.    Finally, we should validate that our local Helm client indeed sees the new Chart.    ``` `1` helm search chartmuseum/  ```   `````````````````````````` The output is probably disappointing. It states that `no results` were `found`. The problem is that even though the Chart is stored in the ChartMuseum repository, we did not update the repository information stored locally in the Helm client. So, let’s update it first.    ``` `1` helm repo update  ```   ````````````````````````` The output is as follows.    ``` `1` Hang tight while we grab the latest from your chart repositories... `2` ...Skip local chart repository `3` ...Successfully got an update from the "chartmuseum" chart repository `4` ...Successfully got an update from the "stable" chart repository `5` Update Complete. Happy Helming!  ```   ```````````````````````` If you added more repositories to your Helm client, you might see a bigger output. Those additional repositories do not matter in this context. What does matter is that the `chartmuseum` was updated and that we can try to search it again.    ``` `1` helm search chartmuseum/  ```   ``````````````````````` This time, the output is not empty.    ``` `1` NAME                 	CHART VERSION	APP VERSION	DESCRIPTION                         \ `2 `               `3` chartmuseum/go-demo-3	0.0.1        	           	A silly demo based on API written in\ `4 ` Go and Mon...  ```   `````````````````````` Our Chart is now available in ChartMuseum, and we can access it with our Helm client. Let’s inspect the Chart.    ``` `1` helm inspect chartmuseum/go-demo-3  ```   ````````````````````` We won’t go through the output since it is the same as the one we explored in the previous chapter. The only difference is that this time it is not retrieved from the Chart stored locally, but from ChartMuseum running inside our cluster. From now on, anyone with the access to that repository can deploy the *go-demo-3* application.    To be on the safe side, and fully confident in the solution, we’ll deploy the Chart before announcing to everyone that they can use the new repository to install applications.    Just as with the other applications, we’ll start by defining a domain we’ll use for *go-demo-3*.    ``` `1` `GD3_ADDR``=``"go-demo-3.``$LB_IP``.nip.io"`  ```   ```````````````````` Next, we’ll output the address as a way to confirm that it looks OK.    ``` `1` `echo` `$GD3_ADDR`  ```   ``````````````````` The output should be similar to `go-demo-3.18.221.122.90.nip.io`.    Now we can finally install *go-demo-3* Chart stored in ChartMuseum running inside our cluster. We’ll continue using `upgrade` with `-i` since that is more friendly to our yet-to-be-defined continuous deployment process.    ``` `1` helm upgrade -i go-demo-3 `\` `2 `    chartmuseum/go-demo-3 `\` `3 `    --namespace go-demo-3 `\` `4 `    --set image.tag`=``1`.0 `\` `5 `    --set ingress.host`=``$GD3_ADDR` `\` `6 `    --reuse-values  ```   `````````````````` We can see from the first line of the output that the `release "go-demo-3" does not exist`, so Helm decided to install it, instead of doing the upgrade. The rest of the output is the same as the one you saw in the previous chapter. It contains the list of the resources created from the Chart as well as the post-installation instructions.    Next, we’ll wait until the application is rolled out and confirm that we can access it.    ``` `1` kubectl -n go-demo-3 `\` `2 `    rollout status deploy go-demo-3 `3`  `4` curl `"http://``$GD3_ADDR``/demo/hello"`  ```   ````````````````` The latter command output the familiar `hello, world!` message thus confirming that the application is up-and-running.    The only thing left to learn is how to remove Charts from ChartMuseum. But, before we do that, we’ll delete *go-demo-3* from the cluster. We don’t need it anymore.    ``` `1` helm delete go-demo-3 --purge  ```   ```````````````` Unfortunately, there is no Helm plugin that will allow us to delete a Chart from a repository, so we’ll accomplish our mission using `curl`.    ``` `1` curl -XDELETE `\` `2 `    `"http://``$CM_ADDR``/api/charts/go-demo-3/0.0.1"` `\` `3 `    -u admin:admin  ```   ``````````````` The output is as follows.    ``` `1` `{``"deleted"``:``true``}`  ```   `````````````` The chart is deleted from the repository.    Now you know everything there is to know about ChartMuseum. OK, maybe you don’t know everything you should know, but you do know the basics that will allow you to explore it further.    Now that you know how to push and pull Charts to and from ChartMuseum, you might still be wondering if there is an UI that will allow us to visualize Charts. Read on.    ### Using Monocular    I don’t think that UIs are useful. We tend to focus on the features they provide, and that distracts us from command line and code. We often get so immersed into filling fields and clicking buttons, that we often forget that the key to automation is to master CLIs and to write code that lives in a code repository. I think that UIs do more damage than good to software engineers.    That being said, I am fully aware that not everyone shares my views. Some like UIs and prefer pretty colors over black and white terminal screens. For those, I will guide you how to get a UI that will utilize Helm repositories and allow you to do some of the things we did through CLI by clicking buttons. We’ll explore [Monocular](https://github.com/kubernetes-helm/monocular).    Monocular is web-based UI for managing Kubernetes applications packaged as Helm Charts. It allows us to search and discover available charts from multiple repositories, and install them in our clusters with one click.    Monocular can be installed with Helm. It is available through a Chart residing in its own [repository](https://kubernetes-helm.github.io/monocular). So, our first step is to add the repository to our Helm client.    ``` `1` helm repo add monocular `\` `2 `    https://kubernetes-helm.github.io/monocular  ```   ````````````` Let’s take a look at the available values.    ``` `1` helm inspect values monocular/monocular  ```   ```````````` The output, limited to the values we’re interested in, is as follows.    ```  `1` `api``:`  `2`  `...`  `3`  `image``:`  `4`    `repository``:` `bitnami/monocular-api`  `5`    `tag``:` `v0.7.2`  `6`    `...`  `7`  `resources``:`  `8`    `limits``:`  `9`      `cpu``:` `100m` `10 `      `memory``:` `128Mi` `11 `    `requests``:` `12 `      `cpu``:` `100m` `13 `      `memory``:` `128Mi` `14 `  `...` `15` `ui``:` `16 `  `...` `17 `  `image``:` `18 `    `repository``:` `bitnami/monocular-ui` `19 `    `tag``:` `v0.7.2` `20 `    `...` `21` `ingress``:` `22 `  `enabled``:` `true` `23 `  `hosts``:` `24 `  `# Wildcard` `25 `  `-` `26 `  `# - monocular.local` `27`  `28 `  `## Ingress annotations` `29 `  `##` `30 `  `annotations``:` `31 `    `## Nginx ingress controller (default)` `32 `    `nginx.ingress.kubernetes.io/rewrite-target``:` `/` `33 `    `kubernetes.io/ingress.class``:` `nginx` `34 `    `...`  ```   ``````````` Just as with the other Charts, we’ll use a fixed version of the images by customizing `image.tag` values in both the `api` and the `ui` sections.    We’ll need to increase the resources since those specified by default are too low.    Ingress is already enabled, but we’ll have to specify the host. Also, we’ll add the “old” style annotations so that older versions of nginx Ingress are supported as well.    Those changes are already available in the `monocular-values.yml` file, so let’s take a quick look at it.    ``` `1` cat helm/monocular-values.yml  ```   `````````` The output is as follows.    ```  `1` `api``:`  `2`  `image``:`  `3`    `tag``:` `v0.7.0`  `4`  `resources``:`  `5`    `limits``:`  `6`      `cpu``:` `500m`  `7`      `memory``:` `1Gi`  `8`    `requests``:`  `9`      `cpu``:` `200m` `10 `      `memory``:` `512Mi` `11` `ui``:` `12 `  `image``:` `13 `    `tag``:` `v0.7.0` `14` `ingress``:` `15 `  `annotations``:` `16 `    `kubernetes.io/ingress.class``:` `"nginx"` `17 `    `ingress.kubernetes.io/rewrite-target``:` `/` `18 `    `nginx.ingress.kubernetes.io/rewrite-target``:` `/` `19 `    `ingress.kubernetes.io/ssl-redirect``:` `"false"` `20 `    `nginx.ingress.kubernetes.io/ssl-redirect``:` `"false"`  ```   ````````` Before we proceed, we’ll have to generate a valid hostname that we’ll use with Monocular Ingress resource.    ``` `1` `MONOCULAR_ADDR``=``"monocular.``$LB_IP``.nip.io"` `2`  `3` `echo` `$MONOCULAR_ADDR`  ```   ```````` The output of the latter command should be similar to the one that follows.    ``` `1` monocular.18.221.122.90.nip.io  ```   ``````` Now we are ready to install Monocular Chart.    ``` `1` helm install monocular/monocular `\` `2 `    --namespace charts `\` `3 `    --name monocular `\` `4 `    --values helm/monocular-values.yml `\` `5 `    --set ingress.hosts`={``$MONOCULAR_ADDR``}`  ```   `````` The output follows the same pattern as the other charts. It shows the status at the top, followed with the resources it created. At the bottom are short instructions for the post-installation steps.    We should wait until the application rolls out before giving a spin to its UI.    ``` `1` kubectl -n charts `\` `2 `    rollout status `\` `3 `    deploy monocular-monocular-api  ```   ````` It will take a while until the API rolls out and the `monocular-api` Pods might fail a few times. Be patient.    Now we can open Monocular in a browser.    ``` `1` open `"http://``$MONOCULAR_ADDR``"`  ```  ```` ![Figure 5-2: Monocular's home screen](img/00015.jpeg)  Figure 5-2: Monocular’s home screen    If we click on the *Charts* link in top-right corner of the screen, we’ll see all the charts available in the two default repositories (*stable* and *incubator*). We can use the link on the left-hand menu to filter them by a repository and to change the ordering. We can also use the *Search charts…* field to filter Charts.    The *Repositories* screen can be used to list those that are currently configured, as well as to add new ones.    The *Deployments* screen list all the Helm installations. At the moment, we have *cm* (ChartMuseum) and *monocular* running through Helm Charts. Additionally, there is the *New deployment* button that we can use to install a new Chart. Please click it and observe that you are taken back to the *Charts* screen. We are about to install Jenkins.    Type *jenkins* in the *Search charts…* field. The list of the Charts will be filtered, and we’ll see only Jenkins. Click on the only Chart.    On the left-hand side of the screen is the same information we can see by executing `helm inspect stable/jenkins` command. On the right-hand side, there is the *Install* section which we can use for *one click installation* or to copy *Helm CLI* commands.    Please remain in the *One Click Installation* tab and click the *Install jenkins v…* button. You will be presented with a popup with a field to specify a Namespace where the Chart will be installed. Please type *jenkins* and click the *Deploy* button.    We were redirected to the screen with the same information we’d get if we executed `helm install stable/jenkins --namespace jenkins`.    Even though using Monocular might seem tempting at the begging, it has a few serious drawbacks. We’ll discuss them soon. For now, please click the red *Delete deployment* button to remove Jenkins.    The major problem with Monocular is that it does not allow us to specify values during Charts installation. There will hardly ever be the case when we’ll install Charts without any custom values. That inability alone should be enough to discard Monocular for any serious usage. On top of that, it does not provide the option to upgrade Charts.    Today (June 2018) Monocular project is still too young to be taken seriously. That will probably change as the project matures. For now, my recommendation is not to use it. That might provoke an adverse reaction. You might feel that I wasted your time by showing you a tool that is not useful. However, I thought that you should know that the UI exists and that it is the only free option we have today. I’ll leave that decision to you. You know what it does and how to install it.    ### What Now?    We will continue using ChartMuseum throughout the rest of the book, and I will leave it to you to decide whether Monocular is useful or a waste of computing resources.    We could have set up a container registry, but we didn’t. There are too many tools in the market ranging from free solutions like [Docker Registry](https://docs.docker.com/registry/) all the way until enterprise products like [Docker Trusted Registry](https://docs.docker.com/ee/dtr/) and JFrog’ [Artifactory](https://www.jfrog.com/confluence/display/RTF/Docker+Registry). The problem is that Docker Registry (free version) is very insecure. It provides only a very basic authentication. Still, the price is right (it’s free). On the other hand, you might opt for one of the commercial solutions and leverage the additional features they provide. Never the less, I felt that for our use-case it is the best if we stick with [Docker Hub](https://hub.docker.com/). Almost everyone has an account there, and it is an excellent choice for the examples we’re having. Once you translate the knowledge from here to your “real” processes, you should have no problem switching to any other container registry if you choose to do so. By now, you should have all the skills required to run a registry in your cluster.    All in all, we’ll continue using Docker Hub for storing container images, and we’ll run ChartMuseum in our cluster and use it to distribute Helm Charts.    All that’s left is for us to remove the Charts we installed. We’ll delete them all at once. Alternatively, you can delete the whole cluster if you do plan to make a break. In any case, the next chapter will start from scratch.    ``` `1` helm delete `$(`helm ls -q`)` --purge `2`  `3` kubectl delete ns `\` `4 `    charts go-demo-3 jenkins  ``` ```` ````` `````` ``````` ```````` ````````` `````````` ``````````` ```````````` ````````````` `````````````` ``````````````` ```````````````` ````````````````` `````````````````` ``````````````````` ```````````````````` ````````````````````` `````````````````````` ``````````````````````` ```````````````````````` ````````````````````````` `````````````````````````` ``````````````````````````` ```````````````````````````` ````````````````````````````` `````````````````````````````` ``````````````````````````````` ```````````````````````````````` ````````````````````````````````` `````````````````````````````````` ``````````````````````````````````` ```````````````````````````````````` ````````````````````````````````````` `````````````````````````````````````` ``````````````````````````````````````` ```````````````````````````````````````` ````````````````````````````````````````` `````````````````````````````````````````` ``````````````````````````````````````````` ```````````````````````````````````````````` ````````````````````````````````````````````` `````````````````````````````````````````````` ``````````````````````````````````````````````` ```````````````````````````````````````````````` ````````````````````````````````````````````````` `````````````````````````````````````````````````` ``````````````````````````````````````````````````` ```````````````````````````````````````````````````` ````````````````````````````````````````````````````` `````````````````````````````````````````````````````` ``````````````````````````````````````````````````````` ```````````````````````````````````````````````````````` ````````````````````````````````````````````````````````` `````````````````````````````````````````````````````````` ``````````````````````````````````````````````````````````` ````````````````````````````````````````````````````````````
