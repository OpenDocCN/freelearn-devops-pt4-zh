- en: Test Design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having had a look at the testability framework, the Test Tool, and the standard
    tests and libraries, I have shown you what is available in the platform and the
    application that will allow you to create and execute automated tests. And that's
    what we are going to do in this part of this book. But let's step back and not
    just bluntly dive into creating code. Firstly, I would like to introduce a couple
    of concepts and design patterns that will allow you to conceive your tests more
    effectively and efficiently. At the same time, these concepts will make your tests
    not just a technical exercise and will help you to get your entire team involved.
  prefs: []
  type: TYPE_NORMAL
- en: I am surely not going to bother you with formal test documentation and the top-down
    approach with eight defined stages, from test plan, through test design/case specification
    to test summary report. That's way out of the scope of this book and, not in the
    least, beyond the daily practices of most Dynamics 365 Business Central implementations.
    Nevertheless, a couple of thoughts on design spent upfront will give you leverage
    in your work.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: No design, no test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test case design patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test data design patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer wish as test design
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If I have been overloading you with information and you want to get your hands
    on coding, you might want to jump to the next chapters. There, we will be exercising
    all of the things discussed so far and later in this chapter. However, if you
    find out you are missing background details, come back to this chapter and get
    yourself informed.
  prefs: []
  type: TYPE_NORMAL
- en: No design, no test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Goal: Understand why tests should be conceived before they are coded and
    executed.**'
  prefs: []
  type: TYPE_NORMAL
- en: I guess I am not far off saying that most of the application testing done in
    our world falls under the term of exploratory testing. Manual testing is done
    by experienced resources that know the application under test and have a good
    understanding and feeling of how to *break the thing*. But this with no explicit
    design and no reproducible, shareable, and reusable scripts. Now, in this world,
    we typically don't want developers to test their own code as they, consciously
    or unconsciously, know how to use the software and evade issues. Their mindset
    is *how to make it* (work), not *how to break it*.
  prefs: []
  type: TYPE_NORMAL
- en: But with automated tests, it will be developers that will code them. And more
    often than not, it will be the same developer that did the application coding.
    So, they need a design of what tests to code. Tests that will cover a broad set
    of scenarios. Sunny and rainy ones. Headless and UI tests. And what about unit
    and functional tests?
  prefs: []
  type: TYPE_NORMAL
- en: In my humble opinion, the test design, like any other deliverable, is owned
    by the team. It is a joint effort to agree upon the test design. It's an agreement
    between the product owner, tester, developer, functional consultant, and key user.
    And if there is *no design*, there will be *no test*. A test design is an object
    to help the team to discuss their test effort, to reveal the holes in their thoughts,
    and to let it mature while working on it. And as will be discussed as follows,
    it is a possible way of putting your requirements down that allows your team to
    efficiently get from requirements to test and application code.
  prefs: []
  type: TYPE_NORMAL
- en: 'A fully-fledged test design would describe the various kinds of tests that
    should be executed such as performance, application, and security; the conditions
    under which they must be performed; and the criteria that make them successful.
    Our test design will only address application tests, as that is the focus of this
    book: how to create application test automation. Once our test design holds a
    complete set of test cases, they need to be detailed out and that''s what the
    next section is about.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to learn more about formal test documentation, this Wikipedia article
    could be a first stepping stone: [https://en.wikipedia.org/wiki/Software_test_documentation](https://en.wikipedia.org/wiki/Software_test_documentation).'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding test case design patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Goal: Learn the basic patterns for designing tests.**'
  prefs: []
  type: TYPE_NORMAL
- en: If you have been testing software, you might know that each test has a similar
    overall structure. Before you can perform the action under test, for example,
    the posting of a document, first, the data needs to be *set up*. Then, the action
    will be *exercised*. And finally, the result of the action has to be *verified*.
    In some cases, a fourth phase applies, a so-called *teardown*, which is used to
    revert the system under test to its previous state.
  prefs: []
  type: TYPE_NORMAL
- en: 'The four phasesof a test case design pattern are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verify
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Teardown
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a short and clear description of the four-phase design pattern, please
    refer to the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://robots.thoughtbot.com/four-phase-test](http://robots.thoughtbot.com/four-phase-test)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This design pattern was typically the pattern used by Microsoft in the early
    years of C/SIDE test coding. Like the following test function example, taken from
    codeunit 137295 - `SCM Inventory Misc. III`, you will encounter it in a vast number
    of older test codeunits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Acceptance Test-Driven Development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Nowadays, Microsoft uses the **Acceptance Test-Driven Development** (**ATDD**)design
    pattern. This is a more complete structure and closer to the customer as tests
    are described from the user''s perspective. The pattern is defined by the following
    so-called tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '`FEATURE`: Defines what feature(s) the test or collection of test cases is
    testing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SCENARIO`: Defines for a single test the scenario being tested'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GIVEN`: Defines what data setup is needed; a test case can have multiple `GIVEN`
    tags when data setup is more complex'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WHEN`: Defines the action under test; each test case should have only one
    `WHEN` tag'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`THEN`: Defines the result of the action, or more specifically the verification
    of the result; if multiple results apply, multiple `THEN` tags will be needed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following test example, taken from test codeunit 134141 - `ERM Bank Reconciliation`
    displays an ATDD design pattern-based test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Before any test coding is done, the test case design should already have been
    conceived. In the case of the preceding example, this is what would have been
    handed off to the developer before writing the test code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A note on test verification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In my workshops, it is mainly developers that participate. For them, when working
    on test automation, one of the hurdles to take is the verification part. It makes
    perfect sense to all of them that data setup, the `GIVEN` part, has to be accounted
    for, not to mention the action under test in the `WHEN` part. The `THEN` part,
    however, is something easily neglected, especially if their assignment is to come
    up with the `GIVEN`-`WHEN`-`THEN` design themselves. Some might ask: why should
    I need verification if the code executes successfully?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because you need to check if:'
  prefs: []
  type: TYPE_NORMAL
- en: The data created is the right data, that is, the expected data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The error thrown, in case of a positive-negative test, is the expected error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The confirm handled is indeed the expected confirm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sufficient verification will make sure your tests will stand the test of time.
    You might want to put the next phrase as a poster on the wall:'
  prefs: []
  type: TYPE_NORMAL
- en: A test without verification is no test at all!
  prefs: []
  type: TYPE_NORMAL
- en: You may add a bunch of exclamation marks.
  prefs: []
  type: TYPE_NORMAL
- en: You might have noticed that the ATDD design pattern has no equivalent of the
    teardown phase in the four-phase test design patterns. As mentioned, ATDD is user-oriented
    and the teardown is a more technical exercise. But of course, if needed, a teardown
    section should be coded at the end of a test.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on ATDD you can go to the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Acceptance_test%E2%80%93driven_development](https://en.wikipedia.org/wiki/Acceptance_test%E2%80%93driven_development)
    or [https://docs.microsoft.com/en-us/dynamics365/business-central/dev-itpro/developer/devenv-extension-advanced-example-test#describing-your-tests](https://docs.microsoft.com/en-us/dynamics365/business-central/dev-itpro/developer/devenv-extension-advanced-example-test#describing-your-tests).'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding test data setup design patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Goal: Learn the basic patterns for setting up test data.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you carry out your manual tests, you know that most of the time is consumed
    by setting up the right data. Being a real IT pro, you will think up ways of doing
    this as efficiently as possible. You might have thought of making sure of the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: A basic setup of data is available, which will be the foundation for all of
    the tests you are going to execute
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each feature under test, additional test data exists upfront
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test specific data will be created on the fly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This way you created yourself a number of patterns that help you efficiently
    get your test data setup done. These are what we call **test data setup** design
    patterns, or **fixture** or **test fixture** design patterns, and each has its
    own name:'
  prefs: []
  type: TYPE_NORMAL
- en: The first one is what we call a **prebuilt fixture**. This is test data that
    is created before any of the tests are run. In the context of Dynamics 365 Business
    Central, this will be a prepared database, such as `CRONUS`, the demo company
    that Microsoft provides.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second pattern is known as **shared fixture**, or lazy setup. This concerns
    the setup of data shared by a group of tests. In our Dynamics 365 Business Central
    context, this concerns generic master data, supplemental data, and setup data,
    such as customer and currency data and a rounding precision, all needed to run
    a group of tests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third and last pattern is **fresh fixture**, or fresh setup. This entails
    data particularly needed for a single test, such as an empty location, a specific
    sales price, or a document to be posted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When automating tests, we will make use of these patterns for the following
    reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Efficient test execution**: Even though an automated test seems to run at
    the speed of light, building up a test collateral over the years will increase
    the total execution time, which might easily run into hours; the shorter the automated
    test run will be, the more it will be used'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Effective data setup**: When designing test cases, it is straight away clear
    what data will be needed and at what stage; this will speed up the coding of the
    tests'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Read more on fixture patterns here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://xunitpatterns.com/Fixture Setup Patterns.html](http://xunitpatterns.com/Fixture%20Setup%20Patterns.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Note that there is much more to formalize in the test data setup. In our test
    coding in the next chapters, we will utilize a couple more of the patterns mentioned.
  prefs: []
  type: TYPE_NORMAL
- en: Test fixture, data agnostics, and prebuilt fixture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As stated in the introductory chapter, automated tests are *reproducible*,
    *fast*, and *objective*. They are reproducible in their execution as the code
    is always the same. But this does not guarantee whether the outcome is reproducible.
    If each time a test is run, the input to the test, that is, the data setup, is
    different, then presumably the output of the test will also be different. The
    following three things help to ensure your tests are reproducible:'
  prefs: []
  type: TYPE_NORMAL
- en: Make a test run on the same fixture
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make a test follow the same code execution path
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make a test verify the outcome based on the same and sufficient set of criteria
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To have full control on the fixture, it is highly preferable to let your automated
    tests create the data they need anew, with each run. In other words, do not rely
    on the data present in the system before tests are run. Automated tests should
    be agnostic of any data residing in the system under test. Consequently, running
    your tests in Dynamics 365 Business Central should not rely on the data present
    in the database, be it `CRONUS`, your own demo data, or customer-specific data.
    Yes, you might need customer specific data to reproduce a reported issue, but
    once fixed and the test automation is updated, it should be able to run data agnostically.
    Because of this, we will not use the prebuilt fixture pattern in any of our tests.
  prefs: []
  type: TYPE_NORMAL
- en: If you've ever run the standard tests, you might have noticed that quite a number
    of them are not data agnostic. They highly rely on the data present in `CRONUS`.
    You might also have noticed that this applies to the older tests. At present, standard
    tests strive to be data agnostic.
  prefs: []
  type: TYPE_NORMAL
- en: Test fixture and test isolation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To start each set of tests with the same fixture, we will take advantage of
    the test isolation feature of the test runner codeunit, as discussed in [Chapter
    2](1e679d13-4037-48e2-b8ae-f550f507f8c9.xhtml), *The Testability Framework*, in
    the *Pillar 4 - test runner and test isolation* section. Using the test isolation
    value codeunit of the standard test runner and putting a coherent set of tests
    in one test codeunit, a generic *teardown* for the whole test codeunit is set.
    It will assure that, at the termination of each test codeunit, the fixture is
    reverted to its initial state. If the test runner utilizes the *Function* test
    isolation, it would add a generic teardown to each test function.
  prefs: []
  type: TYPE_NORMAL
- en: Shared fixture implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You might have observed in the two Microsoft test functions used as examples
    for the four-phase and ATDD patterns that each test starts with a call to a function
    named `Initialize`, right after the scenario description. `Initialize` contains
    the standard implementation of the shared fixture pattern (next to a generic fresh
    fixture pattern we will elaborate on) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'When calling `Initialize` at the start of each test function in the same test
    codeunit, the lazy setup part will just be executed once, since only with the
    first call to `Initialize` will the Boolean variable be `false`. Note that `Initialize`
    also incorporates three hooks, that is event publishers, to allow extending `Initialize`
    by linking subscriber functions to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '`OnTestInitialize`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OnBeforeTestSuiteInitialize`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OnAfterTestSuiteInitialize`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [Chapter 9](1e679d13-4037-48e2-b8ae-f550f507f8c9.xhtml), *Getting Business
    Central Standard Tests Working on Your Code*, we will specifically make use of
    these publishers.
  prefs: []
  type: TYPE_NORMAL
- en: The lazy setup part of `Initialize` is what xUnit patterns call SuiteFixture
    setup.
  prefs: []
  type: TYPE_NORMAL
- en: Fresh fixture implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A fresh fixture can be (partly) set up in a generic way as per the implementation
    in the `Initialize` function as discussed previously. This is for data that needs
    to be created or cleaned out at the start of each test. A fresh setup specifically
    needed for one test only is what is to be found inline in the test function that
    implements the test, defined by the `GIVEN` tags.
  prefs: []
  type: TYPE_NORMAL
- en: The generic fresh setup part of `Initialize` is what xUnit patterns call Implicit
    Setup. The test specific fresh setup is called inline setup.
  prefs: []
  type: TYPE_NORMAL
- en: Using customer wish as test design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Goal: Learn why to describe requirements in the form of a test design.**'
  prefs: []
  type: TYPE_NORMAL
- en: A past ideal of development was a staged one, where each phase would finish
    before the next started. Just like a waterfall, with water flowing from one level
    to the other. From requirements gathering to analyses, to design, coding, testing,
    and finally, operation and maintenance, each phase would have its deadlines, and
    documented deliverables handed off to the next phase. One of the major drawbacks
    of this system is its responsiveness to changing insights, resulting in changing
    requirements. Another is the significant overhead of documents produced. In the
    recent decade or two, agile methodologies have become a general practice for tackling
    these drawbacks.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing a test design, an extra document, to your development practice is
    maybe not what you have been waiting for, even though I can guarantee that your
    development practices will get leveraged. But what if your test design could be
    a kind of unified document? Being the input for each discipline in your project?
    The same truth shared at each level? *What if you could kill five birds with one
    stone*? If you could write your requirements in one format as input for all implementation
    tasks?
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s a common practice to define your requirements as user stories or use
    cases. But I personally think a major omission of both is that they tend to only
    define the sunny path and have no explicit description of the rainy scenarios.
    How should your feature behave under non-typical input? How should it error out?
    As mentioned before, this is where a tester''s mind deviates from a developer''s
    mind: how to make it break versus how to make it work. This would definitely be
    part of a test design. So, why not promote the test design to become the requirements.
    Or inside-out: write your requirements like a test design using the ATDD pattern.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what we are trying at my main employer now. This is what I am advocating
    in my workshops and what implementation partners are picking up. Break down each
    wish and each feature into a list of tests like the following, and make this our
    primary vehicle of communication:'
  prefs: []
  type: TYPE_NORMAL
- en: Detailing of your **customer wish**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementation of your **application code**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Structured execution of your **manual tests**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Coding of your **test automation**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Up-to-date **documentation** of your solution
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Through this, your test automation will be a logical result of previous work.
    New insights, resulting in requirement updates, will be reflected in this list
    and accordingly in your test automation. Whereas your current requirement documentation
    might not always be in sync with the latest version of the implementation, they
    will be, when promoting your test design to requirements, as your automated tests
    will have to reflect the latest version of your app code. In this manner, your
    test automation is your up-to-date documentation. Killing five birds with one
    stone, indeed.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we will be doing in the next chapters, we will specify our requirements
    as a test design, initially at the feature and scenario level, using the `FEATURE`
    and `SCENARIO` tags. Then, this will be followed by a detailed specification using
    the `GIVEN`, `WHEN`, and `THEN` tags. Have a peek preview of how this looks in
    the following example, being one scenario for the `LookupValue` extensions that
    we are going to work on in the next chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The full ATDD test design is stored as an Excel sheet `LookupValue` on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Test automation will profit from a structured approach and, for this, we introduced
    a set of concepts and test patterns, such as the ATDD pattern and test fixture
    patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Now, in the next chapter, [Chapter 5](56634efe-664c-421a-9582-b2a6ae69722a.xhtml), *From
    Customer Wish to Test Automation – The Basics*, we will utilize these patterns
    to finally implement test code.
  prefs: []
  type: TYPE_NORMAL
