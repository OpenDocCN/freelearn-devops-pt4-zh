<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer026">
			<h1 id="_idParaDest-56"><a id="_idTextAnchor056"/>Chapter 2: Fundamental AWS Services</h1>
			<p>Now that we have an understanding of the service principles and pillars that make up the best practices when using <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>), it's time to look at some of the fundamental services that are used throughout the environments and accounts you will be working in. The fundamental services that we are referring to are compute services such as <strong class="bold">Elastic Cloud Compute</strong> (<strong class="bold">EC2</strong>), the global <strong class="bold">Domain Name System</strong> (<strong class="bold">DNS</strong>) service of <strong class="bold">Route 53</strong>, database services such as <strong class="bold">RDS</strong> and <strong class="bold">Aurora</strong>, and the advisory service of Trusted Advisor. This may seem like a review of services you are already familiar with if you have taken either the cloud practitioner, SysOps, or developer exams. However, since there is no longer a requirement to pass any of the lower associate exams before attempting (and passing) the DevOps professional test, it's not a bad idea to level-set on some of the basic services.</p>
			<p>This is not meant to be an exhaustive look at these services. The services mentioned will be brought into the context of the DevOps exam, so skipping over this chapter is not advised. However, if you feel that you have a strong grasp of the topics mentioned, then verify this by checking the review questions, along with the reference material. Likewise, you can review any topics that you feel that you already have a strong grasp of. </p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Setting up and accessing your AWS account </li>
				<li>Virtual Private Cloud networking and Route 53 networking </li>
				<li>Cloud databases </li>
				<li>Message and queueing systems</li>
				<li>Trusted Advisor </li>
			</ul>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor057"/>Technical requirements</h1>
			<p>You will need an AWS account to access the Management Console and CLI, which will be mentioned in the initial part of this chapter. If you need assistance with creating your account, then the <a href="https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/">https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/</a> page will walk you through the steps of creating a new account. Basic knowledge of how to use a terminal to complete the shell commands is also required.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">In this book, we will not be going over the geography, regions, Availability Zones, or edge locations of AWS. These are basic concepts that you should have a firm grasp of before attempting the DevOps exam.</p>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor058"/>Setting up and accessing your AWS account </h1>
			<p>At this point, you most likely have an AWS account<a id="_idIndexMarker078"/> to work with; nevertheless, you may only have access<a id="_idIndexMarker079"/> through your workplace, where some of the permissions are restricted, so you would not be able to practice all of the skills that you need to feel confident in passing the DevOps professional exam.</p>
			<p>If you don't already have a personal account for testing things, then this would be the perfect time to set one up. Even if you do have an account, you may want to take the time to set up a new account just for the exam to ensure that you are allowed to take advantage of the free tier (allocated for your first year in AWS) on a number of the provided services. </p>
			<p>If you do already have an account, making the switch to <strong class="bold">AWS Organizations</strong>, especially using Control Tower, is an excellent<a id="_idIndexMarker080"/> exercise if you wish to create service control policies, organizational units, <strong class="bold">Single Sign-On</strong> (<strong class="bold">SSO</strong>), and cross-account roles. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Control Tower takes a minimum of three distinct email accounts to get started for the three separate accounts that are created. </p>
			<p>Once you have created<a id="_idIndexMarker081"/> your master account, you can send an invitation to the previous account<a id="_idIndexMarker082"/> that you created and allow it to join the organization. </p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor059"/>Accessing the AWS Management Console</h2>
			<p>The AWS Management Console<a id="_idIndexMarker083"/> is the front door (GUI) to accessing your AWS account.</p>
			<p>Start by opening any<a id="_idIndexMarker084"/> web browser and going to <a href="https://aws.amazon.com/">https://aws.amazon.com/</a>. </p>
			<p>On the start page, look for the <strong class="bold">My Account</strong> menu item. Hovering over this menu should give you the option of choosing <strong class="bold">AWS Management Console</strong>:</p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="Images/Figure_2.1_B17405.jpg" alt="Figure 2.1 – Accessing the AWS Management Console&#13;&#10;" width="786" height="288"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1 – Accessing the AWS Management Console</p>
			<p>When following the link to log into AWS, so long as you are not going through an SSO portal, you will be presented with three fields: <strong class="bold">Account ID or account alias</strong>, <strong class="bold">IAM user name</strong>, and <strong class="bold">Password</strong>. Initially, when creating your account, you will only have the long account number to use, and this only has context to AWS. It is a good practice to create an account alias, enabling you to quickly associate the account ID with a logical name for the account. This becomes especially handy when adding multi-factor authentication to your accounts and seeing that logical names bring much more context than multiple rows of numbers. </p>
			<p>Once you have logged in, you can search for services, change regions, designate favorite services for quick access, and open support requests. If you have the correct permissions, you can also access the billing console. </p>
			<p>As an initial step, to perform the exercises, we need to create a user, associate that user with a role containing permissions, and then create a set of keys that we will download and use later when we configure the CLI. </p>
			<p>Once you have logged into the<a id="_idIndexMarker085"/> AWS Management Console, follow these steps to create your user and a set of access keys: </p>
			<ol>
				<li>In the top search bar, type <strong class="source-inline">IAM</strong>.</li>
				<li>Click on the <strong class="bold">IAM</strong> service; you will be taken to the IAM dashboard. </li>
				<li>In the left-hand navigation column, click on <strong class="bold">Users</strong>. </li>
				<li>Once you are in the user menu, click on the blue button at the top of the main screen, which is labeled <strong class="bold">Add user</strong>. </li>
				<li>For the username, you can choose anything, but for our user, we are going to name them <strong class="source-inline">devops</strong>.</li>
				<li>We are going to enter a custom password in the field that meets our password requirements and leave the box that is labeled <strong class="bold">Require password reset</strong> unchecked:<div id="_idContainer013" class="IMG---Figure"><img src="Images/Figure_2.2_B17405.jpg" alt="Figure 2.2 – Creating our user&#13;&#10;" width="1079" height="643"/></div><p class="figure-caption">Figure 2.2 – Creating our user</p></li>
				<li>We can then click <strong class="bold">Next</strong> to move on to the permissions. </li>
				<li>Under <strong class="bold">Set permissions</strong>, we want to choose <strong class="bold">Attach existing policies</strong> <strong class="bold">directly</strong>. </li>
				<li>For this initial user, we are going<a id="_idIndexMarker086"/> to use the <strong class="bold">AdministratorAccess</strong> job function policy. Select this policy so that the box on the left is checked, and then click the button at the bottom that says <strong class="bold">Next: Tags</strong>:<div id="_idContainer014" class="IMG---Figure"><img src="Images/Figure_2.3_B17405.jpg" alt="Figure 2.3 – Attaching the policy&#13;&#10;" width="1217" height="469"/></div><p class="figure-caption">Figure 2.3 – Attaching the policy</p></li>
				<li>Just click the button at the bottom that says <strong class="bold">Next: Review</strong>.</li>
				<li>If everything looks correct, click the blue button that is labeled <strong class="bold">Create User</strong>.</li>
				<li>Because we said that we wanted programmatic access, once we have created our user, we will be given the opportunity to both see the secret access key and download the access key and secret key pair in a CSV file. Take note of your secret key for this user or download the file as this will be the only time that the secret access key will be available. </li>
			</ol>
			<p>With that, we have set up our first user, along with their access policy, password, access key, and secret key. We will be referencing this user throughout other exercises in this book. You are free to set up any username you are comfortable with but for an administrative user account<a id="_idIndexMarker087"/> that is not using the root account, we will be using the <em class="italic">devops</em> user. </p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor060"/>Setting up and using the AWS CLI v2</h2>
			<p>You can indeed perform<a id="_idIndexMarker088"/> most tasks using the graphical web-based interface<a id="_idIndexMarker089"/> of the Management Console. As a DevOps engineer, you will want to automate items in your environment, and the CLI gives you the power, via a set of scripting abilities, to do this. The CLI is one of the favorite tools of many for both its speed and powerful capabilities. </p>
			<p class="callout-heading">Note for Previous CLI v1 Users </p>
			<p class="callout">If you have previously installed the AWS CLI v1, then it is strongly recommended that you uninstall v1 <em class="italic">before</em> installing CLI v2. Both CLI commands use the same command name of <strong class="source-inline">aws</strong>. If you do not want to uninstall CLI v1, then you can put an alias in your path. </p>
			<h3>Mac setup </h3>
			<p>So long as you have sudo privileges on your machine, you can easily install AWS CLI v2 on your Mac<a id="_idIndexMarker090"/> using the bundled installer: </p>
			<ol>
				<li value="1">Open a Terminal window. </li>
				<li>Run the following two commands to install the AWS CLI:<p class="source-code"><strong class="bold">$ curl "https://awscli.amazonaws.com/AWSCLIV2.pkg" -o "AWSCLIV2.pkg"</strong></p><p class="source-code"><strong class="bold">$ sudo installer -pkg AWSCLIV2.pkg -target /</strong></p></li>
				<li>Check that the correct version has been installed by running the following command:<p class="source-code"><strong class="bold">$ aws --version </strong></p><p>The output should be similar to the following:</p><p class="source-code"><strong class="bold">aws-cli/2.1.29 Python/3.8.8 Darwin/18.7.0 exe/x_86_64</strong></p></li>
			</ol>
			<p>So long as the number<a id="_idIndexMarker091"/> after <strong class="source-inline">cli</strong> starts with a <strong class="source-inline">2</strong>, you have successfully installed AWS CLI v2.</p>
			<p>You can now skip to the Configuring the CLI section.</p>
			<h3>PC setup </h3>
			<p class="callout-heading">Note for PC Users </p>
			<p class="callout">To run CLI v2, you need to be running at least a 64-bit version of Windows XP or a later version of the Windows operating system.</p>
			<p>If you have administrative<a id="_idIndexMarker092"/> rights to install software on your machine, then you can follow these instructions to install AWS CLI v2: </p>
			<ol>
				<li value="1">Download the AWS CLI MSI installer<a id="_idIndexMarker093"/> for Windows: <a href="https://awscli.amazonaws.com/AWSCLIV2.msi">https://awscli.amazonaws.com/AWSCLIV2.msi</a>.</li>
				<li>Run the downloaded MSI installer and follow the onscreen instructions.</li>
				<li>Confirm that the correct version has been installed by running the following command:<p class="source-code"><strong class="bold">C:\&gt; aws --version </strong></p></li>
			</ol>
			<h3>Linux setup </h3>
			<p>Before installing CLI v2 on a Linux<a id="_idIndexMarker094"/> machine, there are a few prerequisites that must be taken care of first:</p>
			<ul>
				<li>You need the ability to unzip a package, either with the system <strong class="source-inline">unzip</strong> command or some other installed package. </li>
				<li>To have AWS CLI v2 run correctly, you need to make sure that the <strong class="source-inline">glibc</strong>, <strong class="source-inline">groff</strong>, and <strong class="source-inline">less</strong> packages are installed on your distribution. Most major distributions already have these packages installed by default. </li>
				<li>AWS supports AWS CLI v2 on newer 64-bit versions of CentOS, Fedora, Ubuntu, Amazon Linux 1, and Amazon Linux 2. </li>
			</ul>
			<p>Now that the prerequisites<a id="_idIndexMarker095"/> have been met, you can use the following instructions to install the CLI:</p>
			<ol>
				<li value="1">Run the following <strong class="source-inline">curl</strong> command to download the AWS CLI v2 ZIP file: <p class="source-code"><strong class="bold">$ curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"</strong></p><p class="source-code"><strong class="bold">$ sudo installer -pkg AWSCLIV2.pkg -target /</strong></p></li>
				<li>Now that the ZIP file has been downloaded, we can unzip it: <p class="source-code"><strong class="bold">$ unzip awscliv2.zip </strong></p></li>
				<li>Once unzipped, we can run the <strong class="source-inline">install</strong> program: <p class="source-code"><strong class="bold">$ sudo ./aws/install</strong></p></li>
				<li>Check that the correct version has been installed by running the following command:<p class="source-code"><strong class="bold">$ aws --version </strong></p></li>
				<li>The output should be similar to the following:<p class="source-code"><strong class="bold">aws-cli/2.1.29 Python/3.8.8 Linux/4.14.133-133.105.amzn2.x86_64 botocore/2.0.0</strong></p></li>
			</ol>
			<p>A reference to setting up the CLI from AWS<a id="_idIndexMarker096"/> can be found in their documentation at <a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html">https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html</a>.</p>
			<h2 id="_idParaDest-61"><a id="_idTextAnchor061"/>Configuring the CLI </h2>
			<p>Now that you have installed the CLI, it is a good idea to configure it. If you had the CLI v1 installed<a id="_idIndexMarker097"/> and had used profiles in the past, you should be able to use those same profiles going forward. To quickly configure your CLI, you can use the <strong class="source-inline">aws configure</strong> command; however, as a prerequisite, it's a good idea to have already created and downloaded a key and a secret key for your user from IAM. You will need this pair of credentials if you are configuring either your default profile or a secondary profile to use with the examples shown. </p>
			<p>If you haven't already done so, log back into your AWS account via the Management Console<a id="_idIndexMarker098"/> and then navigate to the IAM service so that you can create yourself a user and role. Using<a id="_idIndexMarker099"/> this user, you can allocate an <strong class="bold">access key ID</strong> and <strong class="bold">secret key ID</strong> to input into the CLI that you can<a id="_idIndexMarker100"/> use for the tutorials in this book. </p>
			<p>Once you have your key pair, follow these steps to configure your CLI: </p>
			<ol>
				<li value="1">Run the <strong class="source-inline">aws configure</strong> command: <p class="source-code"><strong class="bold">$ aws configure </strong></p></li>
				<li>When prompted, cut and paste the access key ID. </li>
				<li>When prompted, cut and paste the secret key ID. </li>
				<li>When prompted, set your default region (we will be using <strong class="source-inline">us-east-2</strong>) for the examples.</li>
				<li>You can just hit <em class="italic">Enter</em> to exit here and use the default JSON output; however, I find setting the output as a table is a lot more user-readable.</li>
			</ol>
			<p>For many of the examples you will see in this book, a <em class="italic">profile</em> will be added to the CLI commands.</p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor062"/>Cloud compute in AWS </h1>
			<p>When we talk about compute<a id="_idIndexMarker101"/> in AWS, we are talking about several<a id="_idIndexMarker102"/> services, including Amazon EC2, Elastic Load Balancing, AWS Batch, Elastic Container Service, and Elastic Kubernetes Service, along with AWS Fargate, the managed service that allows you to run your containers with minimal overhead. It even includes Lightsail, which is one of the quickest ways to get up and running on the cloud for developers, with no need to configure software or networking:</p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="Images/Figure_2.4_B17405.jpg" alt="Figure 2.4 – Compute services in AWS&#13;&#10;" width="931" height="181"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4 – Compute services in AWS</p>
			<p>Although many services fall under the compute umbrella in AWS, the most foundational service is EC2. This is your virtualized instance in the Amazon cloud. While other services, such as Elastic Container<a id="_idIndexMarker103"/> Service, Elastic Kubernetes<a id="_idIndexMarker104"/> Service, and even Elastic Beanstalk, can allow you to run containers in AWS, at their core, they are running on EC2 instances. Thus, knowing the foundational pieces of the EC2 service, such as how to select the correct instance type, how to use the optimal load balancer (as there are three to choose from), and how to add volumes to an instance, all become relative information, both when processing questions for the DevOps professional exam and in your day-to-day duties as a professional: </p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="Images/Figure_2.5_B17405.jpg" alt="Figure 2.5 – EC2 in a real-world architecture &#13;&#10;" width="721" height="601"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5 – EC2 in a real-world architecture </p>
			<p>In the preceding diagram, we can see a real-life scenario where EC2 instances are being used in an Auto Scaling group<a id="_idIndexMarker105"/> to service the WebSphere platform. There are multiple EC2 instances<a id="_idIndexMarker106"/> in a private subnet that can only be accessed by an application load balancer that outside users see from a DNS entry in Route 53. If internal users need to access any of the WebSphere servers, there is a bastion host that allows the SSH protocol in the public subnet. The bastion host is in an Auto Scaling group that spans both Availability Zones, but only one host is up at a time. </p>
			<p>We will now take a closer look at some of these services, especially the EC2 service. </p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor063"/>Amazon Elastic Cloud Compute (EC2) </h2>
			<p>Amazon EC2 allows you to create<a id="_idIndexMarker107"/> a virtual server to perform any number of tasks in the cloud. EC2 allows for a whole array of customization. There is a multitude of operating systems that you can use to meet your application needs. Appropriating the correct amount of memory and processing power is simply a matter of choosing the correct instance type based on the needs of your workload. </p>
			<p>EC2 also has three different pricing models. Each allows you the flexibility or discounting to fit your needs. There are On-Demand Instances, Reserved Instances, and Spot Instances.</p>
			<p><strong class="bold">On-Demand Instances</strong> are the default instance type and offer no long-term commitments<a id="_idIndexMarker108"/> when requesting an EC2 compute instance. You determine when you want to start, stop, hibernate, launch, or terminate the instance without any repercussions. From a pricing standpoint, you are only paying for On-Demand Instances per second while the instance is in the <em class="italic">running</em> state. </p>
			<p>If you have known workloads<a id="_idIndexMarker109"/> with EC2 that will be running constantly for a year or need a desired capacity in a certain Availability Zone, then <strong class="bold">Reserved Instances</strong> can provide cost savings, along with reserved capacity in that particular Availability Zone. There are two different term commitments available with Reserved Instances. These are known as 1 year and 3 years, with the latter establishing greater savings for longer-term commitments. </p>
			<p>When AWS has extra capacity<a id="_idIndexMarker110"/> that isn't being utilized, then those instances are made available as <strong class="bold">Spot Instances</strong> at precipitous discounts. The supply and demand of different types of instances that are available makes the price fluctuate, sometimes rather rapidly. These savings can reach up to 80% off of the normal on-demand pricing; however, there are a few caveats that come along with it. First, you have to launch the instance immediately and you cannot<a id="_idIndexMarker111"/> stop or hibernate Spot Instances. When launching a Spot Instance, you will set a maximum price for what you are looking to spend, such as the current on-demand price, and if the price rises above the maximum price that you set, then AWS will send a signal and you will be given 2 minutes to save your work before the instance is terminated. </p>
			<h3>EC2 instance types </h3>
			<p>At the time of writing, there are over 170 instance types that allow you to customize your compute needs<a id="_idIndexMarker112"/> for any type of workload that you would want<a id="_idIndexMarker113"/> to run into in the cloud. It's not important to try and memorize all the different types and sizes of the instances, along with their compute and memory specifications. However, it is a good idea to know the different categories that EC2 breaks down into for workload specificity and which EC2 families belong to those categories. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">Only some EC2 instances allow enhanced networking and this can be a deciding factor when trying to choose the correct instance type. The enhanced networking feature can be especially important when dealing with troubleshooting or workloads as it supports higher bandwidth and higher packets per second. </p>
			<p>Since there are many types of workloads moving to the cloud, each with its own specific needs, AWS has created several different EC2 instance families. Each of these families contains one or more instance types and has a preferred application profile. </p>
			<p>These instance families can be broken down into groupings, as follows:</p>
			<ul>
				<li>General-purpose instances</li>
				<li>Compute-optimized instances</li>
				<li>Memory-optimized instances</li>
				<li>Accelerated computing instances</li>
				<li>Storage-optimized instances</li>
			</ul>
			<p>Let's take a look at these in more detail.</p>
			<h4>General-purpose instances </h4>
			<p>General-purpose instances<a id="_idIndexMarker114"/> balance memory, compute, and network resources and are a good choice for an assortment<a id="_idIndexMarker115"/> of workloads. This includes the T class of instances, which have burstable credits that build up over time. General-purpose instances are a good starting point if you don't have a full classification of your workload.</p>
			<p><em class="italic">Use cases</em>: Web servers, development and test servers, code repositories, small to mid-sized databases, and backend servers for SAP.</p>
			<h4>Compute-optimized instances </h4>
			<p>Compute-optimized instances<a id="_idIndexMarker116"/> are tailored for workloads that benefit from high-performance<a id="_idIndexMarker117"/> processors. The instances in this family also have the ability<a id="_idIndexMarker118"/> for enhanced networking pre-built in, along with being <strong class="bold">Elastic Block Store</strong> (<strong class="bold">EBS</strong>)-optimized by default. </p>
			<p><em class="italic">Use cases</em>: Batch processing, video encoding, ad serving, distributed analytics, CPU-based machine learning, gaming, scientific modeling, and high-performance science and engineering applications such as genome analysis or computational fluid dynamics. </p>
			<h4>Memory-optimized instances </h4>
			<p>This family of instances is, as its name implies, designed for memory-intensive applications. They are designed<a id="_idIndexMarker119"/> to deliver fast performance<a id="_idIndexMarker120"/> capabilities on jobs that need lots of memory. </p>
			<p><em class="italic">Use cases</em>: Open source databases, in-memory caches, and real-time analytics. </p>
			<h4>Accelerated computing instances </h4>
			<p>Accelerated computing instances<a id="_idIndexMarker121"/> contain co-processors or hardware<a id="_idIndexMarker122"/> accelerators that perform special functions much more efficiently than other processors. These functions can include data pattern matching, floating-point number<a id="_idIndexMarker123"/> calculations, and graphics processing. </p>
			<p><em class="italic">Use cases</em>: Speech recognition, high-performance computing, drug discovery, computational finance, and autonomous vehicles.</p>
			<h4>Storage-optimized instances </h4>
			<p>Storage-optimized instances provide directly attached<a id="_idIndexMarker124"/> storage options that allow for specialized storage<a id="_idIndexMarker125"/> needs. This means that the instance storage<a id="_idIndexMarker126"/> can be optimized for very customized <strong class="bold">input and output</strong> (<strong class="bold">I/O</strong>) performance in the case of H1 instances or very high<a id="_idIndexMarker127"/> storage density in the case of <strong class="bold">high storage</strong> (<strong class="bold">HS</strong>) instances. </p>
			<p><em class="italic">Use cases</em>: NoSQL databases, data warehousing, Elasticsearch, in-memory databases, traditional databases, and analytics workloads.</p>
			<p>Instances are backed by two types of storage: <strong class="bold">instance store</strong> and EBS. When selecting an instance type, it will show<a id="_idIndexMarker128"/> whether the instance is backed by EBS or an instance store. There are some major differences between these two backing types, especially when it comes to persistence. One of the major advantages of an instance store is that it has high I/O and throughput. This comes from it being directly attached to the instance. The disadvantage of instance store-based volumes comes from persistence. If you restart an EC2 instance backed by an instance store, then you will lose all ephemeral data, such as logs and temporary files. This is not the case with EBS-based instances since the storage is not directly attached to the instance. </p>
			<h3>Understanding Amazon Machine Images (AMIs)</h3>
			<p>Whenever you launch<a id="_idIndexMarker129"/> an EC2 instance, it must start from an <strong class="bold">Amazon Machine Image</strong> (<strong class="bold">AMI</strong>) so that it contains the required information<a id="_idIndexMarker130"/> to launch. These can be the base operating system images that are, for all intents and purposes, clean slates. Or they can be AMIs<a id="_idIndexMarker131"/> that you or some other entity has created as a valid checkpoint for a working system or systems running on a single instance. </p>
			<p>AMIs can be provided by Amazon itself, your own user account, shared privately with your other accounts or from a partner account, created by a member from the community, or even available for free or for a per-hour cost on the AWS Marketplace.</p>
			<h4>Use cases for AMIs </h4>
			<p>You can create your own AMIs<a id="_idIndexMarker132"/> to use in Auto Scaling groups or to speed up the launch of complex instances that require multiple steps to download, install, and configure the software.</p>
			<p>There is also a case for a base image in an organization that has a pre-approved operating system, as well as security settings pre-installed for all users to conform to.</p>
			<p>There are community AMIs available; however, you run these at your own risk since there may be unknown packages installed on them.</p>
			<p>Another option is to use marketplace AMIs provided by vendors and partners that have vetted and known software already preconfigured. These AMIs often have an extra price per hour when running the instance. </p>
			<h3>Backing up Amazon EC2 instances </h3>
			<p>If you want to back up<a id="_idIndexMarker133"/> your instance either for point-in-time recovery purposes or to use it in a launch configuration with autoscaling, then you need to create an AMI: </p>
			<ol>
				<li value="1">First, we must find the latest version of the Amazon Linux2 AMI with a little help from the Systems Manager service and throw this into a variable for the next step:<p class="source-code"><strong class="bold">$IMAGE='aws ssm get-parameters --names /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2 --query 'Parameters[0].[Value]' --output text --region us-east-2'</strong></p></li>
				<li>Launch an EC2 instance. Now that we know the latest version of our base AMI that we are going to use, we need to have an instance to back up and create the custom AMI:<p class="source-code"><strong class="bold">$ aws ec2 run-instances \</strong></p><p class="source-code"><strong class="bold">--image-id $IMAGE \</strong></p><p class="source-code"><strong class="bold">--instance-type t2.micro \</strong></p><p class="source-code"><strong class="bold">--region us-east-2</strong></p><p>Once it launches, it should return a JSON statement that contains the instance ID. We will need<a id="_idIndexMarker134"/> this instance ID for our next step to create the AMI. In my case, the instance ID that was returned was <strong class="source-inline">i-02666d4b05eee61c1</strong>.</p><p>Create the AMI as a backup: </p><p class="source-code"><strong class="bold">$aws ec2 create-image \ </strong></p><p class="source-code"><strong class="bold">--instance-id i-02666d4b05eee61c1 \</strong></p><p class="source-code"><strong class="bold">--name "DevOps_chapter2" \</strong></p><p class="source-code"><strong class="bold">--no-reboot</strong> </p><p>If the image is created successfully, then some JSON should be returned with the <strong class="source-inline">ImageId</strong> value:</p><p class="source-code"><strong class="bold">{</strong></p><p class="source-code"><strong class="bold">    "ImageId": "ami-0aa7408c42669e27b"</strong></p><p class="source-code"><strong class="bold">}</strong></p></li>
				<li>Verify the image: <p class="source-code"><strong class="bold">$aws ec2 describe-images \</strong></p><p class="source-code"><strong class="bold">--region us-east-2 \</strong></p><p class="source-code"><strong class="bold">--image-ids ami-0aa7408c42669e27b</strong></p><p>This should return a block of JSON with multiple values. However, the main value we are looking for is the line that says <strong class="source-inline">State</strong>. If the value says <strong class="source-inline">available</strong>, then you have successfully backed up your EC2 instance and are ready to test. </p></li>
				<li>Let's test our backup. <p>Here, we are going back to our original command that we used in <em class="italic">Step 2</em> to create the instance, but now, we are going to substitute the original image ID for the AMI that we have just created:</p><p class="source-code"><strong class="bold">$aws ec2 run-instances \</strong></p><p class="source-code"><strong class="bold">--image-id ami-0aa7408c42669e27b \</strong></p><p class="source-code"><strong class="bold">--instance-type t2.micro \</strong></p><p class="source-code"><strong class="bold">--region us-east-2</strong></p></li>
			</ol>
			<p>As you can see, with just a few simple commands, we have taken our running EC2 instance and not only backed it up but also created a launchable AMI from it. We could take our backup a step further and copy our AMI to another region; so long as there are no hardcoded region-specific items<a id="_idIndexMarker135"/> on the image, it should launch and run without issue. </p>
			<p>Once you have finished launching your EC2 instance and created your AMI, it is a good idea to terminate the instance and remove the AMI so that you don't get charged extra on your AWS account. </p>
			<h3>Using user data scripts to configure EC2 instances at launch </h3>
			<p>Although we can launch our EC2 instances<a id="_idIndexMarker136"/> and then configure the software and packages<a id="_idIndexMarker137"/> that we need on them by hand, this is not the most efficient approach. Apart from following manual steps leading to human mistakes, this approach is much more time-consuming than using an automated process. </p>
			<h4>Example user data script </h4>
			<p>Next, we will look at an example<a id="_idIndexMarker138"/> of a user data script that can configure an EC2 instance without user interaction by pre-forming items such as creating files from scratch, updating previously installed packages, updating software repositories, and even running commands:</p>
			<p class="source-code">#cloud-config</p>
			<p class="source-code">package_upgrade: true</p>
			<p class="source-code">repo_update: true</p>
			<p class="source-code">repo_upgrade: all</p>
			<p class="source-code">packages:</p>
			<p class="source-code"> - boto3</p>
			<p class="source-code"> - aws-cfn-bootstrap</p>
			<p class="source-code">write_files:</p>
			<p class="source-code">  - path: /test.txt</p>
			<p class="source-code">    content: |</p>
			<p class="source-code">      This is my file.</p>
			<p class="source-code">      There are many like it but this one is mine. </p>
			<p class="source-code">runcmd:</p>
			<p class="source-code">  - [ sed, -i, -e, 's/is/was/g', /test.txt]</p>
			<p class="source-code">  - echo "modified my file"</p>
			<p class="source-code">  - [cat, /test.txt]</p>
			<p>With that, we have learned how to automatically configure EC2 instances at launch using user data scripts, which can do things such as install and upgrade packages, as well as run scripts and<a id="_idIndexMarker139"/> commands. Next, we will look at networking interfaces for EC2 instances. </p>
			<h3>Elastic Networking Interfaces (ENIs) </h3>
			<p><strong class="bold">Elastic Networking Interfaces</strong> (<strong class="bold">ENIs</strong>) work like virtual networking cards, so they allow the instance to have an IP address<a id="_idIndexMarker140"/> and be connected to a specific subnet. EC2 instances allow multiple ENIs to be attached, and each<a id="_idIndexMarker141"/> of those network interfaces can be on the same subnet or can traverse different subnets for specific reasons:</p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="Images/Figure_2.6_B17405.jpg" alt="Figure 2.6 – ENIs in separate security groups&#13;&#10;" width="341" height="251"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.6 – ENIs in separate security groups</p>
			<p>Since security groups are attached<a id="_idIndexMarker142"/> at the network interface level and not at the instance level, adding additional ENIs to your instances allows you to have your instance join more than one security group for specialized purposes. If you have a web server that needs to access the public internet, you can attach one interface to the security group that serves this purpose. Also, in the same instance, you may need to SSH into the machine so that a team member can check the logs or processes running on the server. Locking the security group down that is attached to a particular ENI that allows access to the SSH port (port <strong class="source-inline">22</strong>) can be done in this manner.</p>
			<h3>Elastic Block Store (EBS) </h3>
			<p>Although EBS and EC2 are closely tied together, it is pertinent to remember that they are both separate services. EBS is a storage service<a id="_idIndexMarker143"/> that provides network-based storage that is allocated in the same Availability Zone as the instance<a id="_idIndexMarker144"/> and then mounted for usage. The amount of instance storage that's allocated to an instance varies by the instance type, and not all types of EC2 instances contain an instance store volume.</p>
			<p>EBS is different from an <strong class="bold">instance store</strong> based on some key attributes. The instance store volume is the storage<a id="_idIndexMarker145"/> that is physically attached to the EC2 instance. It is best used for temporary storage since the instance store does not persist through instance stops, terminations, or instance failures. In contrast, data stored on an EBS volume will persist:</p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="Images/Figure_2.7_B17405.jpg" alt="Figure 2.7 – EBS overview in AWS&#13;&#10;" width="506" height="209"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.7 – EBS overview in AWS</p>
			<p>EBS volumes can either be allocated at the time of instance creation or created once the instance has been placed in a service as additional storage. </p>
			<p>One of the key features to remember from a DevOps perspective, when it comes to allocating and restoring EBS volumes, is that a volume must stay in the same Availability Zone where it was created.</p>
			<p>One of the key terms to understand<a id="_idIndexMarker146"/> when trying to decide which<a id="_idIndexMarker147"/> of the different types of EBS volumes to use is <strong class="bold">input/output operations per second</strong> (<strong class="bold">IOPS</strong>) or <strong class="bold">Provisioned IOPS</strong> (<strong class="bold">PIOPs</strong>). IOPS is the measure of I/O, measured in kilobytes, that a volume can perform in a second. Using CloudWatch Metrics, you can monitor the performance of a particular volume using volume metrics such as the following: </p>
			<ul>
				<li><strong class="source-inline">VolumeReadOps </strong></li>
				<li><strong class="source-inline">VolumeReadBytes</strong></li>
				<li><strong class="source-inline">VolumeWriteOps</strong></li>
				<li><strong class="source-inline">VolumeWriteBytes</strong></li>
			</ul>
			<p>Now that we know the basics of how EBS volume<a id="_idIndexMarker148"/> performance is measured, let's look at some of the different types of EBS volumes available. </p>
			<h4>Types of EBS volumes </h4>
			<p>There are three main types<a id="_idIndexMarker149"/> of EBS volumes, all of which differ in terms of performance, optimal use cases, and cost: </p>
			<ul>
				<li><strong class="bold">Solid State Drives</strong> (<strong class="bold">SSD</strong>): This is a type of drive that's optimized for heavy read and write operations<a id="_idIndexMarker150"/> and where a higher number<a id="_idIndexMarker151"/> of IOPS is needed. There are two types of SSD EBS volumes available for provisioning:<p>a. <strong class="bold">General-purpose SSD</strong>: Gives a balance of cost<a id="_idIndexMarker152"/> and performance, best used for development and test environments. </p><p>b. <strong class="bold">Provisioned IOPS SSD</strong>: Used for mission-critical workloads<a id="_idIndexMarker153"/> where performance is key, such as databases or caches.</p></li>
				<li><strong class="bold">Hard Disk Drives</strong> (<strong class="bold">HDD</strong>): This is a type of drive that is optimized for streaming workloads<a id="_idIndexMarker154"/> where performance is a necessity<a id="_idIndexMarker155"/> due to constant reads and/or writes. There are two types of HDD EBS volumes available for provisioning:<p>a.<strong class="bold"> Throughput-optimized HDD</strong>: This type of EBS volume<a id="_idIndexMarker156"/> is best for data warehouses, log servers, or big data workloads. </p><p>b. <strong class="bold">Cold HDD</strong>: This is a low-cost HDD<a id="_idIndexMarker157"/> that is best used for infrequently accessed data. </p></li>
				<li><strong class="bold">Previous generation</strong>: This is a type of drive that is best used for smaller datasets that are not of critical<a id="_idIndexMarker158"/> importance. Rather than being on SSDs, these EBS volumes are on magnetic disks, which means they are not as performant as the other two types of EBS volumes. There is only one type of previous-generation EBS drive available and it's best used for data that is infrequently accessed. </li>
			</ul>
			<p>Looking at the EC2 service, which also covers<a id="_idIndexMarker159"/> AMIs and EBS volumes, there are many options to choose from, which allows you to choose the right size for your instance. This spans much deeper than the operating system that you choose, but also how fast you provision your instances and how performant you need your storage to be. </p>
			<p>Next, we will look at AWS Batch, a service that allows us to perform large operations with ease either on-demand or on a schedule.</p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor064"/>AWS Batch </h2>
			<p>Sometimes, you need a large amount of compute resources. </p>
			<p>An example to think about in this instance is tabulating voting results. Once voting has ended, multiple people and machines start the count at different regional centers. Each of the regional centers counts the votes and submits that sub-total to get the final results. </p>
			<p>This process of counting the votes<a id="_idIndexMarker160"/> is a batch process. If the votes were all tallied in different <strong class="bold">Comma Separated Value</strong> (<strong class="bold">CSV</strong>) files, then they could all be uploaded and processed by the AWS Batch service.   </p>
			<p>AWS Batch<a id="_idIndexMarker161"/> can run its jobs as either shell scripts, Linux executables, or Docker containers. </p>
			<p>There are five main components<a id="_idIndexMarker162"/> in AWS Batch:</p>
			<ul>
				<li>Jobs </li>
				<li>Job definitions </li>
				<li>Job queue </li>
				<li>Job scheduler </li>
				<li>Compute environments </li>
			</ul>
			<p>Let's look at these in more detail.</p>
			<h3>Jobs </h3>
			<p>Jobs in AWS Batch<a id="_idIndexMarker163"/> are nothing more than units of work. These units can take the form of a shell script, a Docker container image, or a Linux executable, but regardless of how the job is submitted, it will be run in a containerized environment. </p>
			<h3>Job definitions </h3>
			<p>Telling the job how<a id="_idIndexMarker164"/> it should be run is the function of the AWS Batch job definition. This is also the place where you give extra detail to your jobs, such as which IAM role to use if the job needs to access other AWS resources, as well as defining how much memory and compute power the job will have. </p>
			<h3>Job queue </h3>
			<p>Until a compute environment<a id="_idIndexMarker165"/> is available to run the job, the job will sit in a job queue. You are not limited to a single job queue per AWS account as queues can be associated with different types of compute environments. </p>
			<p>High-priority jobs may be placed in a queue where the compute environment is made up of On-Demand Instances. Lower-priority jobs may wait in a queue that is made up of Spot Instances and can run once the spot becomes available. </p>
			<h3>Job scheduler </h3>
			<p>Once an AWS Batch job has been submitted<a id="_idIndexMarker166"/> to a queue, the scheduler assesses the current compute environments to see if they can run the job. Since jobs may be dependent on other jobs running and/or finishing before they can start, the scheduler also considers this. It is for this fact that jobs are run in a reasonable order from which<a id="_idIndexMarker167"/> they were previously submitted to the queue, but an exact order is not always possible. </p>
			<h3>Compute environments </h3>
			<p>Once you have set up your compute environments<a id="_idIndexMarker168"/> where the job queues will perform the tasks, you have several choices. You can choose managed or unmanaged compute and specify a particular instance type, or even only run on the newest types of instances available. </p>
			<p>If one of the reasons you're using Batch is that that you are interested in cost savings, you can also determine the spot price for your compute instances. If the reliability of the compute environment is a necessity, then you are better off setting up your environment with On-Demand Instances. </p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor065"/>Virtual Private Cloud networking and Route 53 networking </h1>
			<p>The <strong class="bold">Virtual Private Cloud</strong> (<strong class="bold">VPC</strong>) service from AWS allows<a id="_idIndexMarker169"/> you to create a virtual network in the cloud. It allows your compute and database instances to either allow internet connectivity or segment it off from the internet. Security can be accomplished through either stateful or stateless virtual firewall rules, which provide the amount of network connectivity that you see fit: </p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="Images/Figure_2.8_B17405.jpg" alt="Figure 2.8 – Network traffic and security in AWS &#13;&#10;" width="351" height="431"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.8 – Network traffic and security in AWS </p>
			<p>The VPC service comprises<a id="_idIndexMarker170"/> multiple components that allow you to route and secure traffic from your AWS services and, optionally, the internet and/or your on-premises network. </p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor066"/>VPC </h2>
			<p>Although the solution<a id="_idIndexMarker171"/> architect (or possibly the network architect) will often determine the CIDR address range that's used for the VPC, many times, it falls on the DevOps<a id="_idIndexMarker172"/> engineer to implement the VPC with <strong class="bold">Infrastructure as Code</strong> (<strong class="bold">IaC</strong>). </p>
			<p>There are quite a few components that can help make up a VPC. While we will not be covering VPC extensively, we will look at some items that you should know about in case they come up in any exam questions.</p>
			<h3>Subnets</h3>
			<p>A <strong class="bold">subnet</strong> defines a range<a id="_idIndexMarker173"/> of IP addresses in a VPC. There are both<a id="_idIndexMarker174"/> public and private subnets. A public subnet should be used for resources that will be accessed by the internet. A private subnet should be used for resources that won't be accessible from the internet. Each subnet can only inhabit one Availability Zone and cannot traverse multiple Availability Zones. </p>
			<h3>Security groups </h3>
			<p><strong class="bold">Security groups</strong> act as virtual firewalls in Amazon VPC. You can have up to five security groups<a id="_idIndexMarker175"/> per EC2 instance, and security groups are enforced at the instance level (not at the subnet level). </p>
			<p>Security groups<a id="_idIndexMarker176"/> allow for stateful traffic, meaning that if the traffic is allowed in via the rules, then it's returned, notwithstanding any rules in place. You can specify inbound traffic rules based on a combination of port, IP range, or another security group. </p>
			<h3>Network Access Control Lists (NACLs)</h3>
			<p><strong class="bold">Network Access Control Lists</strong> (<strong class="bold">NACLS</strong>) work at the subnet level (unlike security groups, which<a id="_idIndexMarker177"/> work at the instance level). </p>
			<p>Whereas security groups<a id="_idIndexMarker178"/> are stateful, NACLs are stateless, and any traffic that needs to return through an NACL needs to have the port and IP range opened. NACL rules are evaluated in order, with the lowest rule being processed first.</p>
			<h3>Internet gateways</h3>
			<p>Unless you are running your VPC as a private<a id="_idIndexMarker179"/> extension of your data center, then you will need internet connectivity. <strong class="bold">Internet gateways</strong> provide the connection to the internet for VPCs<a id="_idIndexMarker180"/> in a highly available, scalable, and redundant manner. There are two main functions provided by the internet gateway: the first is to provide internet access to the subnets designated in the route table. The second is to provide network address translation for instances that have been assigned an IPv4 public address. </p>
			<h3>Egress-only internet gateways</h3>
			<p>If you are using IPv6 for<a id="_idIndexMarker181"/> your VPC and instances, then you need an <strong class="bold">egress-only internet gateway</strong> rather than a regular internet<a id="_idIndexMarker182"/> gateway. This type of internet gateway prevents the internet from initiating connections to your instances, but still offers<a id="_idIndexMarker183"/> the same scalability, redundancy, and high availability as the other internet gateway. </p>
			<h3>Network Address Translator (NAT)</h3>
			<p>When you have instances<a id="_idIndexMarker184"/> in a private subnet, by default, they cannot<a id="_idIndexMarker185"/> talk to the internet. Even if you have set up an internet gateway, you want to separate your private instances from direct internet access. This is where either a NAT instance or a NAT gateway comes into play. </p>
			<p>A <strong class="bold">NAT</strong> device forwards traffic from the instances in the private subnet to the internet or other AWS services. Since the advent of <strong class="bold">VPC Endpoints</strong>, using a NAT to talk to other AWS<a id="_idIndexMarker186"/> services in your account is considered a non-secure practice and should never be done in production environments. </p>
			<h3>VPC endpoints</h3>
			<p>If you need to talk to your AWS<a id="_idIndexMarker187"/> services securely from your VPC, then you can create VPC endpoints. A VPC endpoint allows EC2 instances<a id="_idIndexMarker188"/> and other AWS compute services (such as Lambda and Fargate) to communicate with supported AWS services such as S3 and DynamoDB, without the need to create an internet gateway or even the need to have a public DNS name.</p>
			<p>This is especially useful for EC2 instances that neither need to nor should be connecting to the internet. Using a VPC endpoint allows the data connections to travel on the internal AWS network and does not require an internet gateway, a virtual private gateway, a NAT device, a VPN connection, or an AWS Direct Connect connection. VPC endpoints are virtual devices that can scale to meet demand, along with being redundant and highly available. </p>
			<h3>DHCP option sets</h3>
			<p>When you create<a id="_idIndexMarker189"/> a VPC, Amazon automatically creates a set of <strong class="bold">Dynamic Host Configuration Protocol</strong> (<strong class="bold">DHCP</strong>) options for you<a id="_idIndexMarker190"/> by default. You can, however, customize some of the available settings by creating a new <strong class="bold">DHCP option set</strong> and then attaching that DHCP option set<a id="_idIndexMarker191"/> to your VPC (and therefore removing the default DHCP option set). </p>
			<p>Some of the allowed options<a id="_idIndexMarker192"/> include the following:</p>
			<ul>
				<li><strong class="source-inline">domain-name-servers</strong>: To use your own DNS instead of the AWS-provided DNS.</li>
				<li><strong class="source-inline">domain-name</strong>: The default domain name for unqualified servers.</li>
				<li><strong class="source-inline">ntp-servers</strong>: You can specify up to four Network Time Protocol servers.</li>
				<li><strong class="source-inline">netbios-name-servers</strong>: You can specify up to four NetBIOS name servers. </li>
				<li><strong class="source-inline">netbios-node-type</strong>: The NetBIOS node type (1, 2, 4, or 8); this setting is blank by default and Amazon does not support broadcast or multicast.<p class="callout-heading">Note</p><p class="callout">Knowing the DHCP options is useful for configuring your VPCs in the real world, but memorizing the available options is not necessary for the DevOps professional exam. </p></li>
			</ul>
			<h3>Using a custom DNS with your VPC</h3>
			<p>Amazon provides<a id="_idIndexMarker193"/> a default DNS server (Route 53 Resolver); however, you can use your own DNS server if you like. Some companies<a id="_idIndexMarker194"/> either run the DNS from their data centers in a hybrid environment, mirror their current DNS server to the cloud so that they do not have to create Route 53-hosted zones with all of the required information, or just choose to manage the DNS themselves on the platform that they feel the most comfortable with. </p>
			<p>If this is the case and you want to specify the DNS versus using the default DNS server, then you would need to create a DHCP option set and complete the value for your DNS server. Once you've done this, you can attach the DCHP option set to your VPC, and the instances in that VPC will start using the designated DNS server that you have specified. </p>
			<h3>Ways to connect multiple networks</h3>
			<p>In the world of AWS<a id="_idIndexMarker195"/> networking, multiple tools have been provided to help you connect your different networks.  </p>
			<h4>VPN connections</h4>
			<p>You can network AWS with your own <strong class="bold">Virtual Private Network</strong> (<strong class="bold">VPN</strong>) using a variety of options:</p>
			<ul>
				<li><strong class="bold">AWS Site-to-Site VPN</strong>: This option creates an IPSec connection<a id="_idIndexMarker196"/> between your VPN and the remote VPC. </li>
				<li><strong class="bold">AWS Client VPN</strong>: This option uses a managed<a id="_idIndexMarker197"/> client that allows your users to connect from almost any location using an OpenVPN-based client. </li>
				<li><strong class="bold">AWS VPN CloudHub</strong>: This option allows you to create a Site-to-Site VPN connection<a id="_idIndexMarker198"/> to multiple VPCs with your virtual private gateway. (The other options mentioned only create a VPN connection with a single VPC.) </li>
				<li><strong class="bold">Third-party VPN appliance</strong>: This option allows you to create a VPN connection using third-party software running<a id="_idIndexMarker199"/> on an EC2 instance. However, it's important to note that when choosing this option, you are responsible for the maintenance and upkeep of the EC2 instance. </li>
			</ul>
			<h4>AWS Transit Gateway </h4>
			<p><strong class="bold">AWS Transit Gateway</strong><a id="_idIndexMarker200"/> allows you to connect both on-premises networks and multiple VPCs through a central networking hub. Transit Gateway acts as a router for your cloud connections, allowing you to connect each network to the Transit Gateway<a id="_idIndexMarker201"/> only once. Once connected, those other networks can then talk to each other, using defined network rules, through the AWS global private network and not the public internet.</p>
			<h4>AWS Direct Connect </h4>
			<p>If you or your company have consistent data transfers both to and from AWS, then the <strong class="bold">Direct Connect</strong> service<a id="_idIndexMarker202"/> can provide a private network connection between your premise and AWS. These dedicated connections are available in 1 Gbps, 10 Gbps, or 100 Gbps denominations through direct connect providers. </p>
			<p>Unlike connecting directly over the public internet, using an AWS Direct Connect connection gives you consistent network performance by transferring data from your data center or office to an from AWS over dedicated channels rather than going over the public internet. </p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor067"/>Route 53 </h2>
			<p>The global DNS service that AWS provides is <strong class="bold">Route 53</strong>. This is one of the few services in AWS<a id="_idIndexMarker203"/> that is not tied to any specific region. The Route 53 service also has one of the strongest commitments, stating that it <em class="italic">will use commercially reasonable efforts to make Amazon Route 53 100% available</em> (Amazon Web Services, 2021).</p>
			<p>There are three main components<a id="_idIndexMarker204"/> of Route 53 that are of foundational importance:</p>
			<ul>
				<li>The ability to register (and manage) domain names </li>
				<li>The DNS service </li>
				<li>The ability to perform health checks (and subsequently route traffic) on your web application based on the fact that it's functional, available, and reachable </li>
			</ul>
			<p>In this section, we will cover some of the basic information about the Route 53 service, especially those topics that would be relevant to know for the DevOps exam. </p>
			<h3>Understanding the different types of records available in Route 53 </h3>
			<p>There are many different DNS record types<a id="_idIndexMarker205"/> and at the time of writing, Route 53 supports the following types<a id="_idIndexMarker206"/> of DNS records:</p>
			<ul>
				<li><strong class="bold">Address</strong> (<strong class="bold">A</strong>) record</li>
				<li>AAAA (IPV6 address record) </li>
				<li><strong class="bold">Canonical name</strong> (<strong class="bold">CNAME</strong>) record</li>
				<li><strong class="bold">Certification Authority Authorization</strong> (<strong class="bold">CAA</strong>)</li>
				<li><strong class="bold">Mail exchange</strong> (<strong class="bold">MX</strong>) record </li>
				<li><strong class="bold">Name authority pointer record</strong> (<strong class="bold">NAPTR</strong>)</li>
				<li><strong class="bold">Name server</strong> (<strong class="bold">NS</strong>) record</li>
				<li><strong class="bold">Pointer</strong> (<strong class="bold">PTR</strong>) record</li>
				<li><strong class="bold">Start of Authority</strong> (<strong class="bold">SOA</strong>) record</li>
				<li><strong class="bold">Sender policy framework</strong> (<strong class="bold">SPF</strong>) </li>
				<li><strong class="bold">Service</strong> (<strong class="bold">SRV</strong>) location</li>
				<li><strong class="bold">Text</strong> (<strong class="bold">TXT</strong>) record</li>
				<li>Alias records (which are a Route53-specific extension of DNS) </li>
			</ul>
			<p>Now that we know what types of records Route 53 supports, let's take a look at the difference between a domain and a hosted zone. </p>
			<h3>Knowing the difference between a domain and a hosted zone</h3>
			<p>One of the first things to understand about domains and a hosted zones is that, first, a domain is an internet construct<a id="_idIndexMarker207"/> of domain name servers that associates the unique name of a person or organization with a numerically addressed internet resource. </p>
			<p>Domains have zone files<a id="_idIndexMarker208"/> that are text mappings of the different resources and their associated<a id="_idIndexMarker209"/> names, addresses, and type of record that the asset is currently mapped in. A hosted zone, on the other hand, is something only found in Route 53. It is similar to a DNS zone file (and you can import DNS zone files into your Route 53 hosted zones) in terms of its structure and mapping. One major<a id="_idIndexMarker210"/> difference, however, is that<a id="_idIndexMarker211"/> it can be managed and modified using<a id="_idIndexMarker212"/> the Route 53 interface, CLI, or API. </p>
			<h3>Route 53 health checks</h3>
			<p>Route 53 allows you to check<a id="_idIndexMarker213"/> the health of your applications and then reroute traffic to other servers or resources based on the rules that you provide. You can even see the recent status of your health checks in the Route 53 web console. </p>
			<h4>Checking the health of a specific endpoint</h4>
			<p>In this case, you will be creating<a id="_idIndexMarker214"/> a check from Route 53 that performs checks at regular intervals specified by you. Your health checks are monitoring an endpoint that is either an IP address or a domain name. Route 53 then goes out at the interval and checks whether the server, the application, or another resource is available and operational. You can also request a specific web page or URL that would mirror most of the actions of your users, rather than just a simple health check page placed on the server that returns a simple 200 code if the system is up and running. </p>
			<h4>Calculated health checks (health checks that monitor other health checks) </h4>
			<p>If you have multiple resources<a id="_idIndexMarker215"/> that all perform the same function, then you may be wondering if a minimum number of resources are healthy. This is where calculated health checks come into play:  </p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="Images/Figure_2.9_B17405.jpg" alt="Figure 2.9 – An example of a calculated health check&#13;&#10;" width="512" height="401"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.9 – An example of a calculated health check</p>
			<p>The calculated health check<a id="_idIndexMarker216"/> acts as a root health check where descendant checks can fail before the origin is considered unhealthy. </p>
			<p>This type of health check is designed to fail if any of the alarms are set off. </p>
			<h4>Checking the status of a CloudWatch alarm</h4>
			<p>Route 53 can utilize the power<a id="_idIndexMarker217"/> of CloudWatch metrics and alarms. Once a CloudWatch alarm has been created, you can create a health check in Route 53 that observes the same data stream<a id="_idIndexMarker218"/> as the CloudWatch alarm. </p>
			<p>To help improve both the availability and flexibility of the CloudWatch alarm health check, Route 53 looks at the data coming from CloudWatch, which then determines whether that route will be healthy or unhealthy. It does not wait for an ALARM state to be reached before setting the route to unhealthy. </p>
			<h3>Route 53 routing policies </h3>
			<p>Routing policies<a id="_idIndexMarker219"/> in Route 53 tell the service how to handle queries. These policies can be simple to complex, as we will see next. </p>
			<h4>Simple routing</h4>
			<p>This is where the DNS is configured<a id="_idIndexMarker220"/> with no special configurations. You are simply<a id="_idIndexMarker221"/> setting the single record that the DNS file should point to. There is no weighting and no failover – just keep it simple. </p>
			<h4>Failover routing </h4>
			<p>With failover routing, you will have<a id="_idIndexMarker222"/> an alternative<a id="_idIndexMarker223"/> record in case the primary resource that is listed in the initial record set is unavailable or fails the defined health check(s). </p>
			<p>This could be another server in a different region or a backup website being served by an S3 bucket. </p>
			<h4>Geolocation routing </h4>
			<p>If you have an audience<a id="_idIndexMarker224"/> or userbase that spans either a country, a continent, or the world, and depending on their location would like them to have a custom message or website (without needing to use complex cookies and dynamic content), then you can use the <strong class="bold">geolocation routing</strong> feature of Route 53 to send them<a id="_idIndexMarker225"/> to the server or content origin that would be most relevant for their originating location. </p>
			<p>You can specify the geographic locations by continent, country, or state in the United States. </p>
			<h4>Geoproximity routing</h4>
			<p>You may have a geographically diverse userbase but want the ability to push more (or less) traffic to a certain<a id="_idIndexMarker226"/> region or set of resources. This is where <strong class="bold">geoproximity routing</strong> differs from geolocation routing. It can create a bias<a id="_idIndexMarker227"/> for each specific resource that routes more or less traffic to that resource based on the bias number and the original location of the requesting user. A bias expands or shrinks the size of the geographic region in which traffic is routed to a resource. </p>
			<h4>Latency-based routing </h4>
			<p>When you have an audience<a id="_idIndexMarker228"/> or userbase that is geographically spread out and you have resources running in multiple regions, then you can use latency-based routing to direct each user to the resource or content that will give them the quickest possible response time. </p>
			<p class="callout-heading">Question</p>
			<p class="callout">What is the difference between geolocation routing and latency-based routing?</p>
			<p class="callout">Although in both cases you are dealing<a id="_idIndexMarker229"/> with a geographically expanded user base, <strong class="bold">latency-based routing</strong> is based on latency records that Route 53 creates and stores on points of origin and IP or DNS points of record. These latency records can change from time to time, so it's best to have a uniform experience or have a customized experience based on customer settings. </p>
			<p class="callout"><strong class="bold">Geolocation routing</strong>, on the other hand, is matching<a id="_idIndexMarker230"/> the user request with the resource that has been geographically tied to that originating IP address, so you can have local content customized for the end user experience. </p>
			<h4>Multi-answer routing </h4>
			<p>Although multi-answer<a id="_idIndexMarker231"/> routing doesn't take the place of a load balancer, it does allow Route 53 to respond with up to eight healthy values chosen at random. </p>
			<p>Some scenarios where you would want to use multi-answer routing<a id="_idIndexMarker232"/> would be as follows:</p>
			<ul>
				<li>Creating more than one record of the same name type </li>
				<li>Routing traffic to multiple sources </li>
				<li>Associating a Route 53 health check with records </li>
			</ul>
			<h4>Weighted routing </h4>
			<p>If you have multiple resources<a id="_idIndexMarker233"/> that you would like the requests<a id="_idIndexMarker234"/> to be sent to while distributing the traffic unevenly, you can set up weighted routing from Route 53. This has a few applications, such as launching a canary release with major or minor updates and redirecting only a portion of the traffic to the new environment. You can then gauge key performance indicators, such as the number of abandoned carts. </p>
			<p>Implementing weighted routing starts with having more than one resource that you want the traffic to be balanced between. You can then create records inside your Route 53-hosted zone to reflect these resources. These have to be records of the same type within a domain or subdomain; that is, both need to be A records. </p>
			<p>Once the records have been set, you can move on to configuring the routing by assigning a weight to the specific resource. The weighted value can be any number from 0 through 255 (if you specify 0, then that record will no longer receive traffic). </p>
			<p>The traffic is balanced using a calculation that takes the weight of the current record. That is then divided by the sum of all the records. </p>
			<p><em class="italic">Example</em>: If you had two servers running and the record for server A had a weight of 75 and the weight for server B had a weight of 150, then 75 + 150 = 225 total weight, and server A would get 33% of the traffic using the formula 75 / 225 = 0.3333333:  </p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="Images/Figure_2.10_B17405.jpg" alt="Figure 2.10 – Creating a weighted record in Route 53&#13;&#10;" width="795" height="397"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.10 – Creating a weighted record in Route 53</p>
			<p>Route 53 will be covered in a bit more detail<a id="_idIndexMarker235"/> once we get to <a href="B17405_13_Final_JM_ePub.xhtml#_idTextAnchor338"><em class="italic">Chapter 13</em></a>, <em class="italic">Blue Green Deployments</em>, and discuss blue/green deployment<a id="_idIndexMarker236"/> strategies. </p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor068"/>Cloud databases</h1>
			<p>As you look at the following model, you may be wondering why there are so many databases. This comes from<a id="_idIndexMarker237"/> the evolution of the application architecture over the past few decades, where specialization, speed, and scale have all become keys to success in this industry:  </p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="Images/Figure_2.11_B17405.jpg" alt="Figure 2.11 – Database types and services in AWS &#13;&#10;" width="521" height="233"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.11 – Database types and services in AWS </p>
			<p>There is not enough space in this chapter to visit each type of database that AWS offers. However, as part of this foundational overview, we do want to cover some of the relational databases and their basic features. </p>
			<p>We will be covering the key-value database, Dynamo DB, in more detail in an upcoming chapter.</p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor069"/>Relational databases </h2>
			<p>The word databases usually brings relational databases<a id="_idIndexMarker238"/> to mind, which have rows, columns, and schemas. </p>
			<p>Relational databases<a id="_idIndexMarker239"/> in AWS come in three main <em class="italic">flavors</em>, as well as engines that can be classified as part of a community, commercial, or cloud native. Cloud-native engines are used in the Aurora service since they are built based on community engines with cloud-native storage, backup, and replication capabilities.  </p>
			<p class="callout-heading">Note</p>
			<p class="callout">When we talk about cloud native, we are talking about building and running applications that take advantage of the cloud computing model: loosely coupled services that are dynamically scalable, can run on a multitenant platform, and have flexible networking and storage.  </p>
			<p>Relational databases tend to follow the ACID set<a id="_idIndexMarker240"/> of database properties:</p>
			<ul>
				<li><strong class="bold">Atomic</strong>: All of your transaction succeeds or none of it does.</li>
				<li><strong class="bold">Consistent</strong>: Any data written<a id="_idIndexMarker241"/> to the database must be valid according to all defined rules. </li>
				<li><strong class="bold">Isolated</strong>: Ensures that concurrent execution of transactions leaves the database in the same state that it would have been in if the transactions were executed sequentially. </li>
				<li><strong class="bold">Durable</strong>: Once a transaction has been committed, it will remain in the system, even in the case of a system crash. </li>
			</ul>
			<h3>Relational Database Service</h3>
			<p><strong class="bold">Relational Database Service</strong> (<strong class="bold">RDS</strong>) aims to take away the tasks that were previously performed by a database<a id="_idIndexMarker242"/> administrator, who had to be on staff<a id="_idIndexMarker243"/> but finds that many of their day-to-day duties are starting to be handled by automation and scripting. This includes provisioning a new database, creating backups, scaling out to read replicas, patching and upgrading instances, as well as switching over to a high-availability instance when an incident occurs. RDS streamlines the setup and installation of software and database provisioning. Using the managed RDS service also allows an organization to achieve a standard set of consistency when creating and deploying a database. No custom scripts are needed to deploy, set up, and patch the database since this is all handled in the background. </p>
			<p>After deployment, developers and applications have a database endpoint that can be accessed readily, where connections can be made with a client or authentication application, and then queries can be performed on the database. </p>
			<p>RDS comes in several different engine<a id="_idIndexMarker244"/> formats, including two commercial<a id="_idIndexMarker245"/> engines and three popular open source engines. The two commercial engines that are supported are Oracle and Microsoft SQL Server. Both of these commercial engines are used in enterprises. RDS supports access to Oracle schemas via Oracle SQL, along with native tools such as SQL Server Management Studio for Microsoft SQL Server. In the community editions, RDS offers the ability to spin up databases using MySQL, PostgreSQL, and MariaDB. MySQL is one of the most popular community relational databases and AWS runs the community edition of this, defaulting InnoDB tables with the documentation, stating that MyISAM storage does not support reliable crash recovery. </p>
			<h3>PostgreSQL</h3>
			<p>PostgreSQL is another extremely<a id="_idIndexMarker246"/> popular database, with developers using it for its rich feature set. RDS supports common tools such as pgAdmin<a id="_idIndexMarker247"/> or connections via JDBC/ODBC drivers to connect to the database. </p>
			<p>After deciding on your engine of choice, you have the opportunity to choose an instance type that will give you varying levels of compute and memory power. There are burstable instances (T family) that are often only used for testing environments. A general (M family) and memory-optimized (R family) instance is preferred when taking your database workloads to a more productionized environment. </p>
			<p>One of the differences between RDS and the same type of engines that you would run yourself on EC2 instances is how replicas work. Read replicas are extremely easy to provision by merely clicking a button, either in the same or a different Availability Zone; however, these nodes are read-only and cannot take a write. In the same context, you can make your instances instantly highly available, which replicates your data asynchronously to the copy of the master node in another Availability Zone or region. If an incident occurs, then it will be seamless to your application and end users as the DNS that both use to connect to the database<a id="_idIndexMarker248"/> stays the same. This secondary master, however, cannot take any load off your main primary server as it cannot serve<a id="_idIndexMarker249"/> any other function (such as being a read slave) except being the failover node. Read replicas can be promoted to a standalone database and at that point, they will have the ability to take writes. However, they will no longer stay in sync with the previous master that they were replicating with. </p>
			<h3>Aurora </h3>
			<p>Amazon Aurora was built in response<a id="_idIndexMarker250"/> to customers wanting the performance of commercial-grade database engines such as Oracle or Microsoft SQL Server, without having to deal with all the hassle of the licensing handcuffs that came with those products. </p>
			<p>The other keynote about<a id="_idIndexMarker251"/> Amazon Aurora is that, unlike the other RDS engines that are backed by EBS storage, Aurora built its own storage solution from the ground up after listening to multiple customer requests over the years. </p>
			<p>Amazon Aurora comes in either MySQL-compatible or PostgreSQL versions. There are opportunities to run Aurora as a cluster or even run a serverless version of the Aurora database. The main item to know about the serverless version of Aurora is that it provides capacity on-demand as your application or users need for it. This is the distinct difference between provisioned Aurora clusters and the serverless version. </p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor070"/>Key-value databases </h2>
			<p>As applications are conceived, the number of users who will be served is not always known initially. There are usually<a id="_idIndexMarker252"/> hopes that at some point in the near future, the popularity<a id="_idIndexMarker253"/> of the application will grow from the initial set of users and testers to an exponential scale. To sustain that growth, the underlying database must be able to handle that scale seamlessly. This is one of the features of key-value databases. </p>
			<p>We will look at DynamoDB in more detail in <a href="B17405_05_Final_JM_ePub.xhtml#_idTextAnchor136"><em class="italic">Chapter 5</em></a>, <em class="italic">Amazon DynamoDB</em>. </p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor071"/>In-memory databases </h2>
			<p>When you are accessing stored<a id="_idIndexMarker254"/> items frequently from<a id="_idIndexMarker255"/> your primary data store, such as a relational database, there are times when just requesting the data from the database won't provide the user<a id="_idIndexMarker256"/> experience that your customers are expecting. This is when an in-memory database such as <strong class="bold">Amazon's Elasticache</strong> becomes a viable<a id="_idIndexMarker257"/> option. </p>
			<p>Elasticache comes in two different engine versions: Redis and Memcached. </p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor072"/>Document databases </h2>
			<p>A document database<a id="_idIndexMarker258"/> is one type of non-relational database that allows you to store documents<a id="_idIndexMarker259"/> and data in a JSON type format and query that data to find the data. One of the truly unique features of document databases is that there is no fixed schema and that they can have documents nested inside of each other. </p>
			<p>AWS offers <strong class="bold">Amazon DocumentDB</strong> (with MongoDB compatibility) as a managed database service for those that have either used MongoDB in the past or are looking for the capabilities of a document database. If you have ever been on the operating side of MongoDB before, then you know that while it is powerful and has several advanced features, including self-electing a new master if the current master becomes unavailable, there are complex setup requirements to get it up and running. </p>
			<p>It requires (in a production environment) at least three nodes – either two nodes and an arbiter or three master nodes. All of this complex setup goes away with DocumentDB. Amazon takes care of the setup and security and allows you to configure automatic backups that you can then store on S3. </p>
			<p>There are a few small things that you are giving up, such as the ability to use admin or local tables, which means you can't use the <strong class="source-inline">mongodump</strong> or <strong class="source-inline">mongorestore</strong> utilities, but there are functions that take the place of these utilities. </p>
			<p>Document databases are particularly good for teams who don't want to deal with the administrative aspects of a database, want to simplify the way initial schema values are used using JSON, and just want to start pushing data to the database, which will allow for both simple and advanced querying later. </p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor073"/>Message and queueing systems </h1>
			<p>As you start to build out cloud-scale<a id="_idIndexMarker260"/> applications, you need ways to decouple the different tiers of the application so that it can scale independently. This is for several reasons, including making your application more resilient and allowing each tier to scale independently of other tiers. Having the ability to use a managed service to perform these tasks, where you or another member of your team doesn't have to worry about the setup and maintenance of the queues or messaging systems, allows expedited usage of these techniques when moving or developing in the cloud. </p>
			<p>We will look at the messaging and queueing systems provided by AWS next and how they can be of benefit to you. </p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor074"/>Simple Notification Service (SNS) </h2>
			<p>Sometimes, you need the ability<a id="_idIndexMarker261"/> to simply send messages either from your applications or from other services in several formats, such as email or text message, that can be used in a variety of ways. These can be based on programmatic calls or events that happen in your AWS account. This is where <strong class="bold">Simple Notification Service</strong> (<strong class="bold">SNS</strong>) comes in handy.</p>
			<p>SNS is a publisher and consumer system, where publishers can push a message out to a <strong class="bold">topic</strong>. Then, any consumer who is subscribed to that topic can consume that message: </p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="Images/Figure_2.12_B17405.jpg" alt="Figure 2.12 – SNS fanout" width="534" height="407"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.12 – SNS fanout</p>
			<p>The SNS topic acts as the channel<a id="_idIndexMarker262"/> where a single message can be broadcast to one or more subscribers. The publisher, which is the application in the preceding diagram, only has to send the message once to the topic; each consumer (or subscriber) can receive the message to be processed in a way that works for them. This can be stored for archival purposes in the case of S3, consumed on-demand in the case of mobile SMS or email, or be parsed and acted upon in the case of Lambda. </p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor075"/>Simple Queue Service (SQS) </h2>
			<p>If you are looking<a id="_idIndexMarker263"/> for a fully managed queueing service that will allow you to break apart<a id="_idIndexMarker264"/> your application in a cloud-native manner, then this is where <strong class="bold">Simple Queue Service</strong> (<strong class="bold">SQS</strong>) is a real service fit. It comes in two flavors: <strong class="bold">standard queues</strong> and <strong class="bold">First In First Out</strong> (<strong class="bold">FIFO</strong>) <strong class="bold">queues</strong>. Standard queues allow for at least once delivery, whereas FIFO queues<a id="_idIndexMarker265"/> are capable of one-time delivery regarding messages placed<a id="_idIndexMarker266"/> in the queue. The other major difference between these two types of queues is that with FIFO queues, the messages are processed in the order in which they are received. A standard queue tries to preserve the ordering of the messages that are received; however, if a message is received out of order, it will not do any shuffling or re-ordering to keep the original message order. </p>
			<p>SQS is a distributed queue system, which means it is spread out across different nodes in the region. This is one of the design<a id="_idIndexMarker267"/> features that brings the benefits of SQS, such as being highly scalable, available, reliable, and durable. SQS also allows for <strong class="bold">Server-Side Encryption</strong> (<strong class="bold">SSE</strong>), either through the service key provided by AWS via KMS or a custom key. You can also control who has access to the messages that are either produced or consumed via access policies. There are three main parts of the SQS distributed queue system:</p>
			<ul>
				<li>Producers/consumers (components) </li>
				<li>The queue </li>
				<li>The messages in the queue: </li>
			</ul>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="Images/Figure_2.13_B17405.jpg" alt="Figure 2.13 – SQS distributed queue&#13;&#10;" width="561" height="161"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.13 – SQS distributed queue</p>
			<p>Messages are placed in the SQS queue by producers; they are then distributed across the queue components for redundancy. When the consumer is ready to process a message, it uses either a long or short polling technique to see whether there are any messages in the queue for processing. If it detects messages for processing, then it consumes the message for processing in the queue; however, the message is only flagged as being processed and the visibility timeout begins. Once the consumer has successfully processed the message and deletes the message, the message is removed from all the nodes of the distributed queue. </p>
			<p>Messages must be deleted<a id="_idIndexMarker268"/> by the consumer once they have been processed successfully. Because of the nature of the distributed system that SQS runs on, once a consumer has pulled down a message for processing, it is then marked so that no other consumers can pull down the same message. There is a timeout period, however, before which the consumer needs to acknowledge the message was processed. If this acknowledgment is never received by SQS, then the message is unflagged and available to another consumer for processing.</p>
			<h3>Where would you use SNS versus SQS?</h3>
			<p>When trying to decide which messaging<a id="_idIndexMarker269"/> service to use, there is a rule<a id="_idIndexMarker270"/> that you can use to help decide which one is going to serve you best; ask the following set of questions:</p>
			<ul>
				<li>Do my messages need guaranteed delivery? </li>
				<li>(If this answer is yes, then SQS is going to be the best choice as there is no option for guaranteed delivery with SNS.)</li>
				<li>Are my messages going to be consumed only by my application?<p>(If this answer is yes, then SQS is going to be the best choice.)</p></li>
				<li>Are my messages going to be consumed by multiple sources?<p>(If this answer is yes, then SNS is going to be the best choice.)</p></li>
			</ul>
			<p>By using these three questions, you can determine whether you are dealing with a closed loop of gathering the messages and then processing them for use in the application (SQS) or creating a message to be consumed outside the application (SNS). </p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor076"/>Amazon MQ </h2>
			<p>If you are migrating<a id="_idIndexMarker271"/> an application whose previous messaging system was either Apache MQ or Rabbit MQ and you no longer want to manage the instances, then <strong class="bold">Amazon MQ</strong> is a great option. Amazon MQ is more of a message broker than just a simple queue. </p>
			<p>When you go to allocate<a id="_idIndexMarker272"/> your Amazon MQ, you must select either the Rabbit MQ or Apache MQ engine type. Once you have selected your engine type, when using the Management Console to provision Amazon MQ, it will prompt you, through a series of choices, to use a single broker or have an active and standby broker for high availability. It can even create a mesh network of single or active and standby brokers for you using predefined blueprints. </p>
			<h3>Differences between Amazon MQ and SQS </h3>
			<p>While SQS<a id="_idIndexMarker273"/> is also a managed service from AWS, it is a scalable queue that doesn't require<a id="_idIndexMarker274"/> a message broker to be established. Amazon MQ's strength is that it allows previously built applications that have dependencies on APIs such as MQTT, OpenWire, or STOMP to be quickly and easily migrated to the cloud in a highly available fashion with minimal overhead. </p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor077"/>Simple Email Service (SES) </h2>
			<p><strong class="bold">Simple Email Service</strong> (<strong class="bold">SES</strong>) allows you to set up and send emails without most of the complexities of running an SMTP server. </p>
			<p>SES only runs<a id="_idIndexMarker275"/> out of three regions: <strong class="source-inline">us-east-1</strong> (North Virginia), <strong class="source-inline">us-west-2</strong> (Oregon), and <strong class="source-inline">eu-west-1</strong> (Ireland). </p>
			<p>Amazon helps you verify where your mail is coming from. It provides trust to the email that you sent your customers by adding a series of DKIM records to the DNS records we send our emails from.</p>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor078"/>Trusted Advisor </h1>
			<p>As the number of resources<a id="_idIndexMarker276"/> grows in your AWS account, it can sometimes be challenging to keep track of them all. Challenges start to arise, such as volumes that are sitting<a id="_idIndexMarker277"/> around not being used and <strong class="bold">Elastic IPs</strong> (<strong class="bold">EIPs</strong>) that are sitting there, not connected to any instance or interface, which is burning through your current budget. Once you enter the dashboard of Trusted Advisor, you will be presented with the four categories that the tool generates for automated checks: </p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="Images/Figure_2.14_B17405.jpg" alt="Figure 2.14 – Trusted Advisor dashboard&#13;&#10;" width="884" height="494"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.14 – Trusted Advisor dashboard</p>
			<p>When it comes to <strong class="bold">Fault Tolerance</strong> checks, it can tell you if you have too many instances in a single Availability Zone or region. </p>
			<p>The number of Trusted Advisor<a id="_idIndexMarker278"/> checks available increases with the support level associated with the account. With an account that has a basic or developer support level, a basic set of checks are displayed. Once the support level is raised to business or enterprise, then the account has access to all 115 Trusted Advisor checks. </p>
			<p>Even at a basic support level, Trusted Advisor provides several checks in four areas:</p>
			<ul>
				<li>Cost optimization</li>
				<li>Performance </li>
				<li>Security </li>
				<li>Fault tolerance<p class="callout-heading">Note</p><p class="callout">Did you notice that the four basic checks that Trusted Advisor provides can be mapped to four of the AWS service pillars?</p></li>
			</ul>
			<p>On top of this, Trusted Advisor<a id="_idIndexMarker279"/> can be set up to send a weekly notification to either the billing contact, the operational contact, the security contact on the account, or any combination of the three contacts listed. </p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor079"/>Accessing Trusted Advisor</h2>
			<p>Trusted Advisor has a graphical dashboard interface that can be viewed, and it even allows you to view potential<a id="_idIndexMarker280"/> monthly savings under the cost optimization icon when you view the service from the AWS console for the first time. AWS has worked to refine the console interface to make it easier to use and to be as intuitive as possible, displaying three icons under each major heading icon regarding the number of checks that have no warnings associated (green), investigation recommended (yellow), and then action recommended (red). </p>
			<p>You can also access Trusted Advisor from the AWS CLI. This can be useful from a DevOps perspective, especially from a service limit check perspective. You can use your automated pipeline or a periodic Lambda job to check service levels for particular services, and then go out and automatically request a service-level increase or send out a notification to the correct email or distribution list so that you don't face disruption in your build process. </p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor080"/>Summary</h1>
			<p>AWS continues to grow the number of services that it offers all the time. We have gone over quite a few of the services in this chapter, although briefly, but have only covered many of the essential services that are used to host and serve applications running on the AWS platform. </p>
			<p>In this chapter, we reviewed the foundational services offered by AWS. It will be these services, along with the others we will look at in the rest of part one, that we will use as the pieces of deployment in our DevOps journey. Having foundational knowledge of these services can let us do more from a development and deployment objective, and it also helps us when digesting the questions being asked on the DevOps professional exam, along with the scenarios being presented to us in our day-to-day professional lives. </p>
			<p>In the next chapter, we are going to examine identity and access management, the basis for securing our AWS cloud account and assets. </p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor081"/>Review questions </h1>
			<ol>
				<li value="1">If you wanted to bid on Amazon EC2 spare capacity, which of the following would allow you to do this?<p>a. Reserved Instances </p><p>b. Auto Scaling</p><p>c. Spot Instances </p><p>d. Savings plans </p></li>
				<li>The company you are working with wants to tighten up its network security. However, they don't currently want to have to declare egress for the return traffic. Which VPC construct can you use to help tune their security settings?<p>a. Network Access Control Lists (NACLs)</p><p>b. Network Address Translator (NAT)</p><p>c. VPC endpoint</p><p>d. Security groups</p></li>
				<li>True/False: Trusted Advisor can notify you of the service limits your account is about to reach. </li>
				<li>If your company was running a SQL Server instance in their current data center and wanted to migrate it to the AWS cloud, what options are available?<p>a. Running the database on EC2</p><p>b. Setting up Direct Connect so that cloud resources can connect to the SQL Server instance </p><p>c. SQL Server on RDS </p><p>d. SQL Server on Amazon Aurora</p></li>
				<li>Which of the following statements about FIFO SQS queues is true?<p>a. You are not guaranteed to get messages in the order that you sent them. </p><p>b. Messages will be delivered exactly in a FIFO order.</p><p>c. Messages will be delivered exactly once. </p><p>d. Messages can be delivered one or more times. </p></li>
			</ol>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor082"/>Review answers </h1>
			<ol>
				<li value="1">c</li>
				<li>d</li>
				<li>True </li>
				<li>a and c </li>
				<li>b and c </li>
			</ol>
		</div>
	</div></body></html>