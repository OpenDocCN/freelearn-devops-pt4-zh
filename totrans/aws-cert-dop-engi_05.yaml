- en: 'Chapter 4: Amazon S3 Blob Storage'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon S3 is one of the core tenants of Amazon Web Services. However, in the
    context of being a DevOps professional, there are certain nuances about the service
    that you must not only familiarize yourself with but also become comfortable with
    implementing.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon's **Simple Storage Service** (**S3**) is the entry point for many users
    and companies looking to get into the cloud. Some of the main features it provides
    are being highly available, exceedingly durable, extremely performant, and easily
    managed, along with the ability to be thoroughly secure. One of the major features
    of S3 is its eleven 9's of durability (99.999999999%), which means that it doesn't
    lose objects or data. Once you upload an object, before it returns the 200 success
    status, that object must be copied to multiple systems in multiple Availability
    Zones to prevent data loss in an array of scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: S3 concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using lifecycle policies in S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using S3 events to trigger other AWS services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S3 access logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S3 endpoints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S3 concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we dive into S3, let''s at least briefly talk about the three distinct
    types of cloud storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Object storage** – Data is saved as an object and is bundled with the associated
    metadata of that object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**File storage** – Data is stored as a single piece of information in a folder
    structure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Block storage** – Data and files are separated into blocks. Each of these
    blocks is then stored as a separate piece of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S3 is an object storage service, and although it seems to have a folder structure,
    this is really just the metadata that is tagged to the object in key/value pairs
    so that the data can be categorized more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once an S3 bucket has been created, then not only is it ready for data, but
    it is also at that point almost infinitely scalable. There are also a number of
    helper services that AWS has created to assist you in moving data into S3\. These
    range from streaming solutions such as Amazon Kinesis to **SSH File Transfer Protocol**
    (**SFTP**) alternatives such as AWS SFTP, and even bulk data load services such
    as AWS Snowball and Snowball Edge:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – S3 options for data transfer'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.1_B17405.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – S3 options for data transfer
  prefs: []
  type: TYPE_NORMAL
- en: By default, AWS accounts are allowed to provision up to one hundred S3 buckets.
    This is a soft limit, and if you need more buckets this can be raised by placing
    a service limit increase ticket.
  prefs: []
  type: TYPE_NORMAL
- en: Interacting with S3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Users have a few ways to interact with S3, starting with the AWS console. This
    will show you a graphical listing of your buckets and display your objects in
    a folder-like format based on the object tags that you have given the different
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: As you become more comfortable with S3 and its capabilities, you might find
    that the console requires more than one click to perform simple tasks such as
    deleting a file, and this is where the CLI and knowing its commands can become
    your ally.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the AWS CLI, there are a few base commands that you can use to interact
    with S3:'
  prefs: []
  type: TYPE_NORMAL
- en: There is the base `aws s3` command – this command gives you the ability to perform
    nine basic operations, such as creating a bucket, listing buckets or their contents,
    and even creating a pre-signed URL for access to a bucket.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is the `aws s3api` command – this command provides a different set of
    secondary commands for the items in the base `s3` command.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `aws s3control` command allows you granular access to the S3 control plane.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And finally, there is the `aws s3outposts` command – this command provides access
    to S3 on AWS Outposts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S3 naming guidelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Every S3 bucket name across all of AWS must be unique, not just in your account.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you create your bucket, there are a few rules that you must follow in
    order for the name to be compliant with S3:'
  prefs: []
  type: TYPE_NORMAL
- en: A bucket name must be at least 3 but no longer than 63 characters long.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bucket names can only be made up of numbers, lowercase letters, dots (.), and
    hyphens (-).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A bucket name must start and end with either a number or a letter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bucket names cannot start with the name `xn–`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A bucket name may not be formatted like an IP address (such as `192.168.2.1`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's also Amazon's recommendation that you do not use dots (.) in your bucket
    names unless you are using the S3 bucket for static web hosting.
  prefs: []
  type: TYPE_NORMAL
- en: Bucket names for enterprises
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The preceding rules are the minimum guidelines for naming S3 buckets. As you
    move into the real world, most large organizations that are already in the AWS
    cloud have a bucket naming scheme. Some small and medium firms still do not have
    organizational standards relating to how to name their S3 buckets, but this can
    be a mistake.
  prefs: []
  type: TYPE_NORMAL
- en: S3 buckets are very easy to create, not only for developers but for almost anyone
    that has access to the service. The issue arises when you start to find buckets
    that are named like `mytest-bucket123`. Once again, this comes back to those initial
    principles of trying to figure out things like who the owner of this data is and
    whether it needs to be replicated for safekeeping or can be safely deleted.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you move to an enterprise naming scheme, you and your organization need
    to come to a consensus regarding a uniform naming standard for the buckets in
    the accounts for which you are responsible:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Enterprise bucket naming example'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.2_B17405.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – Enterprise bucket naming example
  prefs: []
  type: TYPE_NORMAL
- en: Using an abbreviated schema such as `<region>-<environment>-<department>-<product>-<identifier>`
    creates a unique name for each bucket that still stays within AWS naming standards.
  prefs: []
  type: TYPE_NORMAL
- en: This helps to quickly identify who owns each bucket and allows teams and account
    managers to sort and search through resources quickly and easily, not only identifying
    who created the buckets by the product or project they belong to. This is shown
    in *Figure 4.2* and with the bucket named `oh-d-devops-pip-cont1`. This is bucket
    name is shorthand for `ohio-development-devops-pipeline-containers`.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an S3 bucket
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you want to back up your instance either for point-in-time recovery purposes
    or to use in a launch configuration with autoscaling, then you need to create
    an AMI image:'
  prefs: []
  type: TYPE_NORMAL
- en: Launch an `EC2` instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We need to have an instance to back up and create the AMI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You will need to use a different bucket name than the one shown in this example.
    You can name your bucket anything you like as long as you stick to the S3 naming
    guidelines.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If it was successful, then you should see the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You now have a bucket we can work with. If you wanted to see the bucket in
    the command line, we could list it with the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This bucket that we just created is now ready to hold files, media, logs, or
    whatever you'd like to store in it at this point.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Moving data to S3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, we have now created at least one bucket. As we go through later
    exercises, you'll notice that some of the AWS services create buckets in your
    account, such as **CloudFormation**.
  prefs: []
  type: TYPE_NORMAL
- en: If we were simply trying to move items one at a time or a folder at a time,
    then we could use the AWS Management Console or the CLI using the `aws s3 copy`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: If we had a server that was generating logs, and we wanted to put those logs
    into an S3 bucket for either storage, backup, analysis, or a combination of those
    options, then we could use the `aws s3 sync` command. The `s3 sync` command will
    sync all objects in the stated folder with the specified bucked in S3\. This works
    extremely well in concert with a `cron` or cron-like job for enacting the command
    on a schedule.
  prefs: []
  type: TYPE_NORMAL
- en: When trying to move a whole server or data center, it can be time-consuming
    to try and push all the files and objects across the wire. This is where the **Snowball**
    family of services comes into play. A **Snowball Edge Storage Optimized** (**SESO**)
    allows secure transport of your data with up to 80 TB of usable hard disk drive
    storage, which, after being uploaded to the device and shipped to AWS, is then
    offloaded to an S3 bucket that you designate.
  prefs: []
  type: TYPE_NORMAL
- en: S3 inventory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using the list command to see the different objects in your bucket is helpful
    until you start to have tens of thousands to millions of objects in your buckets.
    At that point, it can be helpful to have a more powerful tool at your disposal.
    **S3 inventory** was created for just this purpose. The S3 inventory tool creates
    a report, which is then delivered to another bucket and provides information about
    your objects on items such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Creation date
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encryption status
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replication status
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have the option to encrypt the reports either with SSE-S3 or with a **Key
    Management Service** (**KMS**) key of your choosing.
  prefs: []
  type: TYPE_NORMAL
- en: Once the reports have been delivered, then you can use tools such as Amazon
    Athena to query the reports using Standard SQL, looking for trends or anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: S3 inventory does come with a small cost to generate the reports (less than
    .003 cents per million objects); however, it should not be assumed that the tool
    comes with the S3 service for free.
  prefs: []
  type: TYPE_NORMAL
- en: S3 storage tiers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Amazon S3 service has a collection of different storage tiers that can serve
    different needs as well as different cost structures. The default storage tier
    is the standard storage tier, although this is not always the correct choice for
    storing your data, especially if your organization is looking for long-term storage
    and/or cost savings.
  prefs: []
  type: TYPE_NORMAL
- en: You can create lifecycle policies to move your objects from one storage tier
    to another or even be deleted if the object is unneeded after a set period of
    time.
  prefs: []
  type: TYPE_NORMAL
- en: S3 Standard
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once you initially set up a bucket, by default, without specifying any other
    storage tier, it will be in the S3 Standard tier. This is a highly available general
    access storage policy that provides millisecond access to objects when requesting
    their retrieval. Although this is the costliest of all the storage tiers, S3 Standard
    storage is extremely inexpensive when compared to other types of storage services
    like file and block storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key points to remember about S3 Standard:'
  prefs: []
  type: TYPE_NORMAL
- en: The Standard tier provides high throughput and low latency and performance for
    object uploads and downloads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no other storage tier is indicated, then Standard is the default storage
    class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crafted for 99.99% availability during a given year.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideal for objects that need frequent access.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideal for use cases such as data lakes, cloud-native applications, websites,
    and content distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S3 Intelligent-Tiering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are times when you might think that the algorithm that Amazon has derived
    to move your objects to the different tiers of storage may be more efficient than
    any you may be able to come up with. This is a perfect case of when to choose
    S3 Intelligent-Tiering. With Intelligent-Tiering, AWS will move your objects automatically
    between the frequently accessed tiers and the infrequently accessed tiers based
    on your usage and then charge you accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key points to remember about S3 Intelligent-Tiering are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Designed to optimize storage costs by automatically moving objects to the most
    cost-effective storage tier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designed for longer storage of at least 30 days (minimum 30-day charge) and
    Intelligent-Tiering takes 30 days to start to figure out access patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It stores objects in two access tiers and optimizes that storage based on frequently
    and infrequently accessed objects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no performance impact, and there are no additional fees when Intelligent-Tiering
    moves objects between tiers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crafted for 99.99% availability during a given year.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimized for data lakes and other datasets where the access patterns are unknown.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S3 Standard Infrequent Access (S3 Standard-IA)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have data that you don't access frequently but still need to be able
    to retrieve it in real time, Standard-IA is an option to consider. There are some
    points that need to be considered when thinking about this storage option, such
    as the files need to be stored for a minimum of 30 days before deletion (or be
    charged for the 30 days), along with having a minimum file size of 128 KB.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key points to remember about S3 Standard-IA are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Designed for files over 128 KB (smaller files will be charged as if they were
    128 KB).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designed for longer storage of at least 30 days (minimum 30-day charge).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a higher GET, PUT, COPY, POST, LIST, and SELECT charge than Standard
    but a lower storage cost, so it is designed for infrequent access, as the name
    states.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objects are available to access in real time with no delays.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crafted for 99.99% availability during a given year.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copies of data are stored in multiple Availability Zones.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S3 One Zone Infrequent Access (S3 One Zone-IA)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: S3 One Zone-IA has many of the features of Standard-IA but at a lower price
    because the data is being stored in only one availability zone instead of a minimum
    of three. This is not a good option for critical data but can present large cost
    savings for files that are infrequently accessed and can be re-created if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key points to remember about S3 One Zone-IA are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Ideal for data that can be re-created or object replicas when setting *cross-region
    replication*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designed for longer storage of at least 30 days (minimum 30-day charge).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objects are available for real-time access.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crafted for 99.95% availability during a given year.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data is subject to loss stemming from data center outages caused by disasters
    such as floods or earthquakes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S3 Glacier
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The S3 Glacier storage tier provides you an option for a low-cost, durable storage
    archive with low fees for data retrieval. Unlike the Glacier service from AWS,
    there is no need to wait for days for your objects to appear back in your S3 bucket.
    S3 Glacier has 3 tiers of retrieval speeds. The first is an expedited tier that
    can bring your objects back in just 1-5 minutes. The second is the standard retrieval
    tier, which restores objects in 3-5 hours, and the third is the bulk tier, which
    takes around 12 hours to restore objects to your bucket.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key points to remember about S3 Glacier are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Designed for longer storage of at least 90 days (minimum charge of 90 days).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crafted for 99.9% availability during a given year.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objects can be locked via the **VAULT LOCK** feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Glacier retrieval times can be configured from minutes to hours.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appropriate for low-cost data archival on infrequently accessed objects, especially
    for compliance purposes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S3 Glacier Deep Archive
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Like the Glacier service, if you have items that are rarely retrieved but need
    to be retained, then Glacier Deep Archive can be a practical solution to your
    storage problems. Often, there are cases such as moving from a tape backup system
    to a digital tape backup system where you would only be retrieving the data once
    or twice per year and could withstand waiting 12 hours for data retrieval. These
    controls come with deep savings because storage in Glacier Deep Archive costs
    only $1 per TB per month.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key points to remember about S3 Glacier Deep Archive are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Designed for long-term digital storage that may be accessed once or twice during
    a given year
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crafted for 99.9% availability during a given year
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designed for longer storage of at least 180 days (minimum 180-day charge)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An alternative to on-premises tape libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: S3 Glacier and S3 Glacier Deep Archive are storage classes within S3 and, as
    such, the object stays within the S3 service.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using lifecycle policies in S3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we just talked about the different storage classes available with S3, not
    all data that you store in S3 needs to be in the standard tier at all times, as
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – An example S3 lifecycle policy'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.3_B17405.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 – An example S3 lifecycle policy
  prefs: []
  type: TYPE_NORMAL
- en: Depending on how long you need to access your data on a regular basis, you can
    move your objects to different tiers using object lifecycles.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a lifecycle policy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following exercise will use the AWS console and the bucket we created previously
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to the AWS console (in the account in which you created the S3 bucket
    earlier in this chapter). Make sure you have logged into the account with a user
    that has S3 full access rights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the S3 page ([https://s3.console.aws.amazon.com/](https://s3.console.aws.amazon.com/)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you have more than one S3 bucket, click on the bucket that you made in the
    previous exercise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This bucket should currently have no objects and no current lifecycle policy
    associated with it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once you're on the bucket's main page, click on the **Management** tab on the
    main screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This will bring the **Lifecycle rules** section to the middle of the main screen.
    It should have a **(0)** right after the **Lifecycle rules**, meaning that there
    are no **Lifecycle rules** currently associated with this bucket:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.4 – S3 lifecycle rules via the Management tab'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/Figure_4.4_B17405.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.4 – S3 lifecycle rules via the Management tab
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now we can click on the button in the **Lifecycle Rules** section labeled **Create
    lifecycle rule**. This will bring us to the area where we can start to create
    our own lifecycle rules.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are going to create a rule that deletes objects in our bucket after one day.
    We could create multiple rules that follow the path in *Fi**gure 4.2*, but if
    you remember the key points of the Infrequent Access storage tier, any object
    that isn't kept for at least 30 days will be charged for 30 days of storage. Rather
    than have those charges when testing out how to create lifecycle rules, we will
    just create a rule that will delete the objects after a certain period of time
    instead.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Call the rule `devopspro-1day-delete`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under the rule scope, click the option that says **This rule applies to all
    objects in the bucket**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the box that appears saying **I acknowledge this rule will apply to all
    objects in the bucket**:![Figure 4.5 – Configuring the lifecycle rule
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_4.5_B17405.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.5 – Configuring the lifecycle rule
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Under the **Lifecycle rule actions**, check the box that is labeled **Expire
    current versions of objects**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the `1`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on **Create rule** at the bottom of the page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Create rule button'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.6_B17405.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.6 – Create rule button
  prefs: []
  type: TYPE_NORMAL
- en: We have now created our lifecycle rule and, in order to test it out, we need
    to upload a file into the bucket and then wait for a day so that we can see it
    be automatically deleted.
  prefs: []
  type: TYPE_NORMAL
- en: S3 endpoints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prior to the creation of S3 endpoints, all data being accessed from S3 traversed
    the public internet. If you had private information that you were passing from
    a private S3 bucket to a resource in a private subnet in your **Virtual Private
    Cloud** (**VPC**), then not only did this pose some security risks, but it also
    required some extra networking to allow the resources in the private subnet to
    talk to the internet so that the S3 buckets that you wanted to access could be
    uploaded to and downloaded from.
  prefs: []
  type: TYPE_NORMAL
- en: If we have resources in a private subnet of a VPC that do not have a public
    route to the internet via a NAT instance or a NAT gateway, then we would not be
    able to access items in our S3 buckets without setting up that NAT instance, or
    we can make a more secure connection by using an S3 endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: An S3 endpoint, which is a gateway endpoint, allows us to add an entry to the
    route table of our **VPC**. By adding this endpoint, we can now bypass the public
    internet with both our public and private instances and protect the privacy of
    our data being passed along the route. This is a much more secure solution for
    transporting your data from EC2 instances and other services residing within your
    VPC than using the public route.
  prefs: []
  type: TYPE_NORMAL
- en: S3 access control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you have data and objects uploaded into your bucket, unless you have a
    bucket with public access, then you might want to restrict who can access the
    objects within the bucket. Starting small, you may just allow the default access
    controls to whoever has authorization into the account. They may access the data
    in any non-public bucket. As you move to most corporate environments, there will
    be segments of data that will need to be cordoned off from one business unit to
    another. One product team would most likely have no need to access the data stored
    by another data team. Along those same lines, data being stored by the financial
    and business departments will probably need to restrict any technology members
    from accessing and possibly deleting the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where the access controls of S3 come into play. There are two main
    methods for implementing access controls: using S3 bucket policies on who and
    what can access the individual objects themselves and then by using IAM controls
    to limit users, groups, and resources that can access the buckets individually,
    or by using controls such as tags in an **attribute-based control model**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s generally a good idea to pick one method of access or the other and not
    try to mix the two since this can lead to some very frustrating sessions of trying
    to troubleshoot permission issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – S3 resource versus user-based policies'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.7_B17405.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.7 – S3 resource versus user-based policies
  prefs: []
  type: TYPE_NORMAL
- en: You should also know that you must explicitly make an S3 bucket public as all
    S3 buckets are private by default and block public access.
  prefs: []
  type: TYPE_NORMAL
- en: Resource-based policies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you and your organization prefer to restrict at the object level, then you
    can use **Access Control Lists** (**ACLs**) either at the bucket access level
    or at the object access level.
  prefs: []
  type: TYPE_NORMAL
- en: User-based policies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many, on the other hand, would rather control access to their S3 buckets with
    IAM policies. This allows you to control entitlement from the bucket and folder
    level, along with the ability to construct more complex conditions in the IAM
    policy based on a tag, `VPC-id`, `source-IP` address, and other factors.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-account access
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you need a user or resource in one account to be able to access the objects
    in another account, then you can set up a cross-account access role as we did
    in the IAM exercise in [*Chapter 3*](B17405_03_Final_JM_ePub.xhtml#_idTextAnchor083),
    *Identity and Access Management and Working with Secrets in AWS*.
  prefs: []
  type: TYPE_NORMAL
- en: S3 access logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When storing different objects in S3, especially those that are to be downloaded
    by various users and groups, you might want to know who is accessing the different
    files, when, and from what location.
  prefs: []
  type: TYPE_NORMAL
- en: Users can capture all the access logs and records of who is accessing various
    objects in a bucket via a simple setting in S3\. You cannot store the logs in
    the same bucket as the items that you are tracking, so you need to either create
    an entirely new bucket expressly for the purpose of capturing the logs or designate
    a previously created bucket in your current account to hold the logs.
  prefs: []
  type: TYPE_NORMAL
- en: Logs are not pushed to that bucket in real time as items are accessed, as would
    be the case for logs on a web server. Amazon pushes the logs in batches in a best-effort
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: If you don't want to set up an entirely different bucket to capture these logs,
    and if you have **CloudTrail logs** turned on for the account, then you can gather
    IAM user information on S3 API calls.
  prefs: []
  type: TYPE_NORMAL
- en: Encryption options with S3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: S3 allows encryption at rest for the objects it stores. The default option when
    you store an object is to store it unencrypted. If you are working in any kind
    of environment that requires compliance, then you will most likely need to encrypt
    the objects you are storing.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have decided that your objects stored in S3 need to be encrypted, then
    you do have options. You can choose between *server-side encryption* and *client-side
    encryption*. There are some key questions to ask before making this decision:'
  prefs: []
  type: TYPE_NORMAL
- en: Do you need to manage the encryption key?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where is the encryption key going to be stored?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who is going to do the encryption and decryption of the data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Server-side encryption
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AWS has made the process of encrypting your objects and data in S3 storage
    easy with their server-side encryption options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SSE-S3`: Using the `SSE-S3` option allows you to use the AWS S3 master key
    to encrypt your objects and data. This allows your data to be stored encrypted
    at rest without a lot of management or extra configuration on your or your team''s
    part in the setup. You simply upload your objects to the S3 bucket of your choice,
    and then once they are received successfully, the S3 service handles the encryption
    of those objects. By the same token, when an object is requested by a service
    or user, and as long as that service or user has the proper authorization to access
    the object, then the S3 service decrypts the requested object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SSE-K:MS`: Integrating the **Key Management Service** (**KMS**) into server-side
    encryption adds a small cost, but a few more features and benefits over just using
    the default encryption key provided with the S3 service. You now have another
    layer of granular control of the customer keys and of which IAM entities are allowed
    to access that key. KMS also provides an audit trail of who has accessed the key.
    And one of the main features is that you have the control to rotate the key if
    needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Client-side encryption
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When choosing client-side encryption, the full responsibility for encrypting
    and decrypting falls on you the client. This method involves encrypting the objects
    before they reach S3\. You are also responsible for any master/child key management
    along with key rotation. Client-side encryption is a good choice if your organization
    needs total control of both the master keys and the encryption algorithm used.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to take a deep dive into the protection of data in flight and at
    rest in [*Chapter 19*](B17405_19_Final_JM_ePub.xhtml#_idTextAnchor447), *Protecting
    Data in Flight and at Rest*.
  prefs: []
  type: TYPE_NORMAL
- en: Using S3 events to trigger other AWS services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The S3 service can notify other services when certain things happen to objects
    in a bucket. Two of the most common scenarios are if an object was uploaded or
    if an object was deleted from a particular bucket. The S3 bucket can then notify
    one of three other AWS services of what has happened and the bucket where the
    event occurred. The three services that allow S3 event notifications are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: AWS **Lambda**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon **Simple Queue Service** (**SQS**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon **Simple Notification Service** (**SNS**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can arrange for notifications to be issued to SQS or SNS when a new object
    is added to the bucket or overridden. Notifications can also be delivered to AWS
    Lambda for processing by a Lambda function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – S3 Event Flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.8_B17405.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.8 – S3 Event Flow
  prefs: []
  type: TYPE_NORMAL
- en: Let's think about this and how we would use this from a DevOps perspective.
    AWS Lambda is an extremely powerful tool that we will explore in detail in [*Chapter
    12*](B17405_12_Final_JM_ePub.xhtml#_idTextAnchor307), *Lambda Deployments and
    Versioning*, and it can be used to invoke almost any other AWS service. In our
    current scenario, we could have a customer who is using the AWS SFTP service to
    upload a file to an Amazon S3 bucket. That bucket could trigger a bucket event
    to AWS Lambda. The Lambda function could then kick off an AWS Pipeline build that
    would process the file, which, on passing or failing, sends a notification to
    the development team of a new build available for deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In order to use S3 events, you must grant the S3 principle the necessary permissions
    in order to use the requested services. This includes the permission to publish
    to SNS queues or SQS topics, as well as the ability to invoke Lambda.
  prefs: []
  type: TYPE_NORMAL
- en: Triggering an S3 event
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will go through the exercise of using our previously created bucket to add
    an event trigger whenever an object is uploaded to the bucket. This event will
    be something simple to start with: an email notification to ourselves. In order
    to send that email, we will need to create an SNS topic and then subscribe to
    that topic with our email. Then we can go back to our bucket and add the bucket
    event configuration so that whenever an object is uploaded, it will send us a
    notification.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know that we have to set up an SNS topic, let''s use our CLI to
    create that topic and subscribe so we can get the emails once something has been
    uploaded to our bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up your terminal and type the following commands so we can create the
    topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If the topic is created successfully, then it should return something like
    this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have our topic, we need to subscribe using our email address:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should return a JSON statement telling you that the subscription is pending:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now we need to go to our email account and find the email that the SNS service
    has just sent, and then click on the link that says **Confirm Subscription**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now let's log into our account and go to our S3 bucket so that we can configure
    the **Event Notifications**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The final step for our SNS topic to be ready is to add an IAM role that allows
    it to receive the notification events from the S3 bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will use the following policy and then need to fill in the following values
    before saving them to a file:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Account Number**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Region**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bucket name**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will create the following policy and then add this to the topic in the **Access**
    section once we log into the AWS console:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let's now log into our AWS account and go to the SNS service so we can update
    our access policy. This way, the SNS topic has the correct permissions to interact
    with the S3 event notification.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once on the SNS service, choose **Topics**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Topics** menu, you should see the topic we created via **CLI – s3events**.
    Click on the topic name so we can get to the configuration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once inside the topic, we need to press the **Edit** button near the top-right
    of the main page:![Figure 4.9 – Edit button near the top of the page
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_4.9_B17405.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.9 – Edit button near the top of the page
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now find the `JSON` policy we made earlier into this section. Once you have
    replaced the previous value with the new access policy, click the orange **Save
    changes** button at the bottom of the page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we can go to the S3 console ([https://s3.console.aws.amazon.com/](https://s3.console.aws.amazon.com/)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the bucket that you made previously in this chapter (ours was named `devopspro-beyond`).
    If you haven't made a bucket already, you could choose any bucket you have in
    your account or create a new bucket quickly. Click on that bucket name so that
    you are brought to the main bucket page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you are on the main bucket page, then click the **Properties** menu item
    in the horizontal menu in the main window frame:![Figure 4.10 – S3 horizontal
    menu with Properties highlighted
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_4.10_B17405.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.10 – S3 horizontal menu with Properties highlighted
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now scroll down the screen until you find the panel named **Event Notifications**:![Figure
    4.11 – S3 Event notifications with no created notifications
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_4.11_B17405.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.11 – S3 Event notifications with no created notifications
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now click on the **Create event notification** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the following configurations for our S3 event test:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`S3 Test`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.txt`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event Types** – Check the box labeled **Put**:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Configuring an S3 event notification'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.12_B17405.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.12 – Configuring an S3 event notification
  prefs: []
  type: TYPE_NORMAL
- en: Scroll down to the bottom of the page where you see **Destination**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the radio button that is next to `s3-event`):![Figure 4.13 – Choosing
    the destination for our S3 event notification
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_4.13_B17405.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.13 – Choosing the destination for our S3 event notification
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click the orange **Save changes** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It's now time to upload a text file and test that we receive an email notification.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'All we need to test is any text file (remember we configured the event to only
    be configured on `.txt` files and no other types of files). We will use the CLI
    to upload our file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you uploaded your file successfully, then you should see this output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once the file is uploaded, you should receive an email notification at the email
    address you used to subscribe to the SNS topic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have seen how we can trigger operations on other services, such
    as **Lambda**, **SNS**, and **SQS**, we can think about how this would be of use
    to us in the real world. In the case of SNS, you may have a client who has an
    account and would like to be notified whenever one of their clients uploads one
    or more files to their personal S3 bucket so they can review the files. In the
    case of Lambda, you may be receiving invoices from another department and need
    to extract out the data before storing it into one or more data stores, and by
    using S3 events this can all happen automatically once the file is uploaded to
    the bucket.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at S3 Batch operations and see how, with the
    help of a manifest file, we can process a few or a few thousand files at once
    using just the S3 service.
  prefs: []
  type: TYPE_NORMAL
- en: S3 Batch operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having a good tagging strategy is part of recommended AWS account hygiene. Just
    like other initiatives, many of these strategies evolve over time. There may come
    a time when you or your organization feels the need to change some of the mandatory
    tags on the objects in your current set of S3 buckets. If you have been running
    in AWS for any period of time then there are most likely too many objects to re-tag
    by hand and so you are left trying to devise a solution. This is where the power
    of AWS **S3 Batch operations** can come into play. It can perform batch operations
    on files and buckets with ease.
  prefs: []
  type: TYPE_NORMAL
- en: 'S3 Batch operations allow you to do more than just modify tags. The following
    operations can be performed with S3 Batch operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Modify objects and metadata properties.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copy objects between S3 buckets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace object tag sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modify access controls to sensitive data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restore archive objects from Glacier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Invoke AWS Lambda functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once the job has been created, then it goes through a series of statutes before
    it either reaches the completed or failed state. The following table describes
    the different statuses available to an Amazon S3 Batch job:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: S3 Batch hands on-example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To test out the power of S3 Batch, we are going to take 75 files, upload them
    to our bucket, and then use AWS Batch to add a tag to each of the files almost
    instantly.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you don't want to re-create all the files for this exercise, then simply
    go to the GitHub repository for this book; there are 75 small files available
    in [*Chapter 4*](B17405_04_Final_JM_ePub.xhtml#_idTextAnchor110), *Amazon S3 Blob
    Storage*, in the batch subfolder.
  prefs: []
  type: TYPE_NORMAL
- en: Also, since all the files have the `.txt` extension, you may want to turn off
    the S3 event notification or unsubscribe from the topic before uploading all of
    the exercise files to the S3 bucket.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now use a hands-on example with S3 Batch to update the tags on a number
    of files at once. If you have a mandatory tagging strategy in place and files
    are missing some of those tags, then this can be an efficient way of managing
    those changes rather than trying to either write a custom script to perform the
    task or changing the tags on the files manually:'
  prefs: []
  type: TYPE_NORMAL
- en: Before we start, in order for the job to be able to execute, we need to ensure
    that we have an IAM role. Let's first log in to the AWS Management Console and
    navigate to the IAM service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a role for an AWS service, choose **S3**, and then at the very bottom
    of the page, choose **S3 Batch Operations**:![Figure 4.14 – Selecting the use
    case for S3 permissions in IAM
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_4.14_B17405.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.14 – Selecting the use case for S3 permissions in IAM
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click on the blue button labeled **Next: Permissions**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When you get to policy, click on the `JSON` code from GitHub named `S3_batch_IAM.json`.
    You will need to replace the name of your S3 bucket in all the places where it
    asks for variables. (The variable names are notated as `<<TargetResource>>`, `<<ManifestBucket>>`,
    and `<<ResourceBucket>>`.) Unless you are storing the manifests and reports in
    different buckets, then just replace the same bucket name in each value. When
    you are done, you can click the blue button labeled **Next: Tags**. There is no
    need for any tags at this moment, so just click the blue button labeled **Next:
    Review**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we can name and save our role; a good descriptive name is something like
    `S3-Batch-Tagging-Role`. Add the description if you desire and then click the
    blue **Create Policy** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go back to the other tab where we were creating the role and search for the
    policy we just created, named `S3-Batch-Tagging`. Once this role has been created,
    we need to take a note of the ARN so that we can use it in our `batch` command
    later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the 75`.txt` files from the GitHub directory (or create your own set
    of files) into a single directory so that they can be uploaded into the S3 bucket
    you created earlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we will use the `s3 sync` command to quickly move the files from our
    local directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We will also need to download the manifest (`.csv` file) from the GitHub repository
    in order to start our batch job. In the manifest, you will need to replace the
    current bucket name, `devopspro-beyond`, with the bucket name where you have uploaded
    your objects. Once you have changed those values, make sure that you upload the
    manifest to the S3 bucket, as S3 Batch needs to read the manifest from an S3 location
    when using a `CSV` file and not from a locally sourced file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The final report also needs a *folder* in our bucket to reside in. We will
    use the `s3 cp` command to move a file into the new folder and have it ready to
    receive the final report:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that our objects and manifest have been uploaded, we can go back to the
    AWS management console and start the batch job. Going back to the browser window
    where you had previously made your IAM role, navigate to the **S3 service**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the left-hand menu, click on **Batch Operations**:![Figure 4.15 – Batch Operations
    menu item
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_4.15_B17405.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.15 – Batch Operations menu item
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on the orange button on the right-hand side that is labeled **Create Job**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the Manifest page, select the radio button that says `manifest.csv` file
    in your S3 bucket. The manifest ETag will populate automatically:![Figure 4.16
    – The manifest information for S3 Batch
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_4.16_B17405.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.16 – The manifest information for S3 Batch
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click the orange **Next** button at the bottom of the page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under **Operation**, choose the radio button next to **Replace All tags**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This will make another set of options appear. For the `TAG`, and for the `Chapter4`.
    Once you have done this, click on the orange **Next** button:![Figure 4.17 – Adding
    the key and value for what to change
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_4.17_B17405.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.17 – Adding the key and value for what to change
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For the completion report, browse to the `final-reports` folder that we created
    earlier by uploading a copy of the `manifest` file via the CLI:![Figure 4.18 –
    Choosing the destination for the S3 Batch reports
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_4.18_B17405.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.18 – Choosing the destination for the S3 Batch reports
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Under permission, choose from the existing IAM roles and then, in the drop-down
    box, select the **S3-Batch-Tagging-Role** that you created at the beginning of
    this exercise. Click on the orange **Next** button at the bottom of the page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the review page, scroll all the way to the bottom and click on the orange
    **Create job** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you have created the job you will now be brought back to the S3 batch main
    screen. It may take a minute or two for the job to finish being created, but once
    it has, you can select the radio button to the left and then click on the button
    labeled **Run job**. This will start processing all the tags on the files in your
    manifest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can now go back to the AWS Management Console and navigate to our S3 bucket
    so we can look at our files to see if the **Delete** tag has been added.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The manifest that has been uploaded in the GitHub repository has the S3 example
    bucket name. You will need to change the bucket name on the manifest for the S3
    bucket that you have created before uploading and running your batch job.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: S3 replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even with its high durability guarantees, there are numerous cases where you
    need to devise a plan to protect your data in case of regional outage or loss.
    You might even want to copy your original data, which is restricted by access
    policies, somewhere else so that another team can access that data. This is where
    **S3 replication** comes into play. It gives you the ability to asynchronously
    copy your data to another bucket. There are two versions of S3 replication available:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cross-Region Replication** (**CRR**): This is where the objects in the bucket
    are replicated into a separate bucket that has been created in a different region
    than the primary region in which the original bucket was created.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Single Region Replication** (**SRR**): In SRR, objects are still replicated
    to a new separate bucket from the originating bucket, but both buckets are in
    the same geographical region.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S3 versioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the S3 service, you can keep track of how files change over time using the
    versioning feature. While this feature does add additional cost, it is especially
    useful as a way to help restore deleted objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you enable versioning, each object in that S3 bucket gets a value for
    the version ID. If you haven''t enabled versioning on the bucket, then the version
    id for the objects in the bucket is set to null:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.19 – An S3 bucket with versioning enabled showing version ids'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.19_B17405.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.19 – An S3 bucket with versioning enabled showing version ids
  prefs: []
  type: TYPE_NORMAL
- en: Once you upload a subsequent version of the object with versioning turned on,
    Amazon produces a new version id for the new version of the object and then places
    that newer version of the object in the bucket.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the AWS S3 service and many of its features. We
    examined not only the basics of creating buckets and how buckets are secured with
    the different types of access policies but also how you can encrypt your data
    at rest using different encryption methods from AWS. We also saw how to trigger
    workflows using bucket events to do things such as kick off our DevOps pipeline.
    Now that we have a firm understanding of object storage, we will move on to the
    serverless NoSQL database DynamoDB.
  prefs: []
  type: TYPE_NORMAL
- en: Review questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have a department in your company that needs an S3 bucket configured where
    the objects are accessed on a weekly basis and need to be both durable and reliable.
    Which S3 storage class should you use to configure the bucket?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You have five departments in your organization. Three of the departments are
    product teams, one is the accounting department, and one is the HR department.
    The accounting department has decided to migrate their files to S3 from the data
    center in order to save costs. How can you be sure that only members of the accounting
    department have access to the accounting files and no one else?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A healthcare company is preparing for an internal audit. They need to make sure
    that all of their files stored in S3 have been encrypted and that the keys are
    rotated no less than once per year. The company has been in business for over
    15 years and has recently (in the last 5 years) made a digital push to move the
    majority of its files onto the cloud. This has resulted in over 1.5 million documents,
    including billing records, patient information, business information, and other
    documents being stored. What is the most effective way to check for this continually?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: S3 Standard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure that an IAM group has been created just for members of the accounting
    department. Create an IAM (user-based) policy that allows members of the accounting
    group to have full permissions on the accounting bucket. You could go a step further
    and create a policy boundary that explicitly denies access to the accounting bucket
    and place that on all other groups.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an S3 inventory report. Use AWS Athena to query for files that are not
    encrypted.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
