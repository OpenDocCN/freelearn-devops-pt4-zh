<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer250">
			<h1 id="_idParaDest-407"><a id="_idTextAnchor417"/>Chapter 17: Advanced and Enterprise Logging Scenarios</h1>
			<p>As we look to wrap up our section on logging, we will discuss how to implement enterprise-grade logging systems. While CloudWatch has the ability to search through logs and even present some visualizations, we will look at other native solutions offered by AWS that are more in tune with capturing, processing, storing, and visualizing massive amounts of logs streaming in constantly. </p>
			<p>In this chapter, we're going to cover the following main topics:  </p>
			<ul>
				<li>Using QuickSight to visualize data</li>
				<li>Streaming logs to Amazon Elasticsearch  </li>
				<li>Using Amazon Kinesis to process logs </li>
			</ul>
			<h1 id="_idParaDest-408"><a id="_idTextAnchor418"/>Using QuickSight to visualize data </h1>
			<p>Although there are multiple third-party visualization tools available to analyze your data and create graphical<a id="_idIndexMarker1410"/> representations of your logs, there is a native service<a id="_idIndexMarker1411"/> that Amazon has created for its customers, <strong class="bold">QuickSight</strong>. QuickSight was developed to be a cloud-scale <strong class="bold">Business Intelligence</strong> (<strong class="bold">BI</strong>) service that is easy to use and can ingest data from multiple sources.</p>
			<p>Amazon QuickSight uses a proprietary SPICE engine to calculate and serve data. <strong class="bold">SPICE</strong> stands for <strong class="bold">Super-fast, Parallel, In-memory Calculation Engine</strong>. This technology has been built to achieve<a id="_idIndexMarker1412"/> blazing-fast performance at an enterprise scale. The SPICE engine can do this by automatically replicating data, allowing thousands of users to perform queries and analysis on that underlying data at immensely fast speeds.</p>
			<p>Another key feature about Amazon QuickSight is that you can share created dashboards with members of your IAM organization. Still, it also has the ability to share access via email to those that do not have an IAM or federated account to your AWS organization. QuickSight also has an iPhone and Android app that is available for access:</p>
			<div>
				<div id="_idContainer237" class="IMG---Figure">
					<img src="Images/Figure_17.1_B17405.jpg" alt="Figure 17.1 – A flow of logs to Athena and AWS QuickSight to create visualizations&#13;&#10;" width="545" height="247"/>
				</div>
			</div>
			<p class="figure-caption">Figure 17.1 – A flow of logs to Athena and AWS QuickSight to create visualizations</p>
			<p>In the previous figure, we show a flow of AWS users creating events from actions that they are taking. </p>
			<p>When you set up QuickSight<a id="_idIndexMarker1413"/> in your AWS account, you create<a id="_idIndexMarker1414"/> a namespace, which is a logical container that is used to organize your teams, clients, and others that you will invite to your QuickSight visualizations. You can create multiple namespaces and can isolate the data viewed by the users to that namespace. Namespaces can also span over multiple regions. Once you set up the namespace, there is no further administration needed from that point forward. </p>
			<p>With an understanding of the value that the QuickSight service brings when creating visualizations to not only those in our Amazon account but others in our organization, let's now look at how the Athena service can expand on these capabilities, using the files that we already have stored in our S3 buckets. </p>
			<h2 id="_idParaDest-409"><a id="_idTextAnchor419"/>Querying data with Amazon Athena</h2>
			<p>AWS has created<a id="_idIndexMarker1415"/> a service that allows you to query data stored in S3 buckets. The service<a id="_idIndexMarker1416"/> is serverless, and hence there is no need to provision servers, and the service only charges you for the queries that you run. </p>
			<p>The Presto query engine backs <strong class="bold">Amazon Athena</strong>. This is an open source SQL engine that allows users to query large datasets with low latency. The Presto engine also fully supports joins, arrays, and window functions. </p>
			<p>The key features<a id="_idIndexMarker1417"/> of Amazon Athena are as follows:</p>
			<ul>
				<li>Since it's serverless, there's no administration or infrastructure to manage.</li>
				<li>It uses standard SQL to query the underlying data. </li>
				<li>It has extremely fast performance without the need for tuning.</li>
				<li>It allows for federated queries across multiple data sources. </li>
				<li>It is secure, allowing you to take advantage of both IAM and S3 bucket policies to control access to data. </li>
				<li>It is highly available with S3 as the data source. </li>
			</ul>
			<p>Now that we have seen how to use Amazon QuickSight to create more powerful visualizations using Amazon Athena, let's look at a few Amazon QuickSight use cases.  </p>
			<h2 id="_idParaDest-410"><a id="_idTextAnchor420"/>Amazon QuickSight use cases </h2>
			<p>The next section<a id="_idIndexMarker1418"/> will explore some of the use cases where Amazon QuickSight can be used in conjunction with other AWS services to create enterprise-grade systems to create dashboards and analysis systems to monitor logs and analytics. </p>
			<h3>Using QuickSight to visualize logs and usage analytics with the help of Athena and Glue </h3>
			<p>Amazon has built an interactive query service that allows you to use standard SQL statements to query your data, named Athena. One of the best parts about Athena, besides the fact that it uses standard SQL and hence there's no new special language to learn to use it, is that it's serverless. This means that there are no servers to provision, and you are only charged for the queries that you run on the system and the data that it scans.</p>
			<h3>The ability to add machine learning insights to your dashboards</h3>
			<p>Amazon QuickSight<a id="_idIndexMarker1419"/> extends the normal proficiencies of just displaying your data in a dashboard format by adding natural language capabilities and machine learning insights to help you gain a much grander understanding of your data. These features help users discover patterns and hidden trends arising from the underlying data without having specialized technical expertise or machine learning skills. </p>
			<h3>Connect user dashboards to your data warehouse or data lake </h3>
			<p>If you have your data stored in a data warehouse, such as using the <strong class="bold">Amazon Redshift</strong> service, then you can create<a id="_idIndexMarker1420"/> a connection in Amazon QuickSight to connect to your Redshift cluster. This Redshift cluster becomes an auto-discovered dataset and secures the connection between the Redshift cluster and QuickSight using SSL automatically without extra configuration. You can then either choose the tables you want to use in your QuickSight visualizations or create a custom SQL statement to import your data into the SPICE engine to analyze and visualize your data. </p>
			<p>If you are storing<a id="_idIndexMarker1421"/> data in a data lake, especially using <strong class="bold">Lake Formation</strong> from AWS, your data resides in Amazon S3 buckets. You can then use <strong class="bold">AWS Glue</strong> to crawl the data and create a data<a id="_idIndexMarker1422"/> catalog. Once the data catalog has been created, you can query the data with Amazon Athena and create a table and database. These tables and databases serve as containers for the schema of data held in the S3 buckets. Amazon QuickSight can then connect to Athena databases and create visualizations of the data or even perform further SQL queries:</p>
			<div>
				<div id="_idContainer238" class="IMG---Figure">
					<img src="Images/Figure_17.2_B17405.jpg" alt="Figure 17.2 – Connecting Amazon QuickSight to a data lake in AWS &#13;&#10;" width="675" height="183"/>
				</div>
			</div>
			<p class="figure-caption">Figure 17.2 – Connecting Amazon QuickSight to a data lake in AWS </p>
			<p>Now that we have gone over some of the use cases where enterprises would find value in Amazon QuickSight, let's go through a hands-on exercise using QuickSight to cement the concepts of this service. This way, if a question comes up regarding visualizations on the DevOps professional exam, we have a solid basis of when to choose Amazon QuickSight versus<a id="_idIndexMarker1423"/> CloudWatch dashboards. </p>
			<h2 id="_idParaDest-411"><a id="_idTextAnchor421"/>Creating a dashboard with Amazon QuickSight</h2>
			<p>When you create a data dashboard in Amazon QuickSight, you are publishing a collection of interactive<a id="_idIndexMarker1424"/> graphs and charts for your users to explore, using<a id="_idIndexMarker1425"/> the underlying data that not only shows them insights but gives them the tools to explore further should they feel the need.  </p>
			<p>Let's go through the process of creating a dashboard in Amazon QuickSight. We will need the assistance of the Amazon Athena service in order to get our data to a temporary database so that we can connect to it from QuickSight: </p>
			<ol>
				<li>Log on to the Amazon Management Console, and in the top search box, search for <strong class="source-inline">QuickSight</strong>. Once on the <strong class="bold">QuickSight</strong> screen, verify your AWS account number and then press the blue button labeled <strong class="bold">Sign up for QuickSight</strong>. Be sure to change the default of <strong class="bold">Enterprise</strong> to <strong class="bold">Standard</strong>: <div id="_idContainer239" class="IMG---Figure"><img src="Images/Figure_17.3_B17405.jpg" alt="Figure 17.3 – The QuickSight icon from the search menu&#13;&#10;" width="505" height="117"/></div><p class="figure-caption"> </p><p class="figure-caption">Figure 17.3 – The QuickSight icon from the search menu</p><p>You can also use the following URL: <a href="https://aws.amazon.com/quicksight/pricing/">https://aws.amazon.com/quicksight/pricing/</a>. Click on the link in the middle of the page that says <strong class="bold">Standard Edition</strong>. Once on the <strong class="bold">Standard Edition</strong> page, scroll down to the bottom of the page and click the large yellow button labeled <strong class="bold">Start Your Free Trial</strong>. </p><p>You should<a id="_idIndexMarker1426"/> now be on the page<a id="_idIndexMarker1427"/> labeled <strong class="bold">Create your QuickSight account</strong>. Keep the first selection, <strong class="bold">Use IAM federated identities &amp; QuickSight-managed users</strong>, for <strong class="bold">Authentication method</strong>. Next, for <strong class="bold">QuickSight region</strong>, change the region to <strong class="bold">US East (Ohio)</strong>:</p><div id="_idContainer240" class="IMG---Figure"><img src="Images/Figure_17.4_B17405.jpg" alt="Figure 17.4 – Setting the Authentication method and region to create the QuickSight account&#13;&#10;" width="1149" height="436"/></div><p class="figure-caption">  </p><p class="figure-caption">Figure 17.4 – Setting the Authentication method and region to create the QuickSight account</p></li>
				<li>For your QuickSight account name, which is where you declare your namespace, choose something unique that you can remember. You will also need to enter an email address. </li>
				<li>Now, we can specify which data will be available to QuickSight in our current setup. Mark the boxes next to the following items:<p>a. Enable auto discovery of data and users in your Amazon Redshift, Amazon RDS, and AWS IAM services. </p><p>b. Amazon Redshift </p><p>c. Amazon RDS </p><p>d. IAM </p><p>e. Amazon Athena. </p><p>f. Amazon S3 (select the bucket shown – <strong class="bold">chapter16-elb-logs</strong>):</p><div id="_idContainer241" class="IMG---Figure"><img src="Images/Figure_17.5_B17405.jpg" alt="Figure 17.5 – Selecting the S3 bucket for QuickSight&#13;&#10;" width="1197" height="393"/></div><p class="figure-caption">Figure 17.5 – Selecting the S3 bucket for QuickSight</p><p>We have added a bucket from our previous hands-on exercise, which should have data to query with Amazon QuickSight. </p></li>
				<li>You will now<a id="_idIndexMarker1428"/> be on a screen with<a id="_idIndexMarker1429"/> a dancing graph as AWS creates your QuickSight account. Once it is created, then you will be presented a blue button to click on labeled <strong class="bold">Go to Amazon QuickSight</strong>. Click on this button to proceed. </li>
				<li>Once you click the button, it will start to create a few samples for you and display a pop-up window welcoming you to QuickSight. Click the blue <strong class="bold">Next</strong> button on the popup to close them, or click <strong class="bold">X</strong> in the top right-hand corner. </li>
				<li>We will need to create a dataset from the data in our S3 bucket. Find the datasets icon in the left-hand vertical menu and click on it. Once on the datasets page, click on the dark blue <strong class="bold">New dataset</strong> button in the top right-hand corner.</li>
				<li>Choose <strong class="bold">Upload a file</strong> as the data<a id="_idIndexMarker1430"/> source for your dataset. Download the <strong class="source-inline">MOCK_DATA.csv</strong> file from the <strong class="source-inline">Chapter-17</strong> folder<a id="_idIndexMarker1431"/> of the GitHub repository if you have not already done so and upload it to QuickSight. Click the blue <strong class="bold">Next</strong> button on the <strong class="bold">Confirm file upload settings</strong> popup. Once it is done, click the blue <strong class="bold">Visualize</strong> button:<div id="_idContainer242" class="IMG---Figure"><img src="Images/Figure_17.6_B17405.jpg" alt="Figure 17.6 – Confirming the data has been uploaded correctly to QuickSight&#13;&#10;" width="733" height="575"/></div><p class="figure-caption">Figure 17.6 – Confirming the data has been uploaded correctly to QuickSight</p></li>
				<li>Once the data has been loaded, you should be taken to the <strong class="bold">Visualize</strong> section of Amazon QuickSight. It's now time for us to create a visualization from the data we just imported. We need to initially select some fields to show on the graph. </li>
				<li>Choose the <em class="italic">filled map</em> type of graphic, which is in the visual types to the right-hand side as the last value. Then, with that type of visualization selected, drag the <strong class="source-inline">state_province</strong> value to the <strong class="bold">Location</strong> field well and the <strong class="source-inline">zip_postal_code</strong> value to the <strong class="bold">Color</strong> field well. Click on some of the other visualization types to see how QuickSight can change how your data is presented:</li>
			</ol>
			<div>
				<div id="_idContainer243" class="IMG---Figure">
					<img src="Images/Figure_17.7_B17405.jpg" alt="Figure 17.7 – The filled map visualization type in Amazon QuickSight&#13;&#10;" width="483" height="371"/>
				</div>
			</div>
			<p class="figure-caption">Figure 17.7 – The filled map visualization type in Amazon QuickSight</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Amazon QuickSight Standard edition has a price of $12 per month when paying month to month at the time of publishing. There is a noted free 30-day trial for authors; however, if you have already used the service, you will be charged for it if you go through this tutorial.</p>
			<p>Now that<a id="_idIndexMarker1432"/> we have seen how to create interactive<a id="_idIndexMarker1433"/> dashboards and visualizations using the QuickSight service, let's look at our next service used for managing logs at an enterprise level – the Amazon Elasticsearch service. </p>
			<h1 id="_idParaDest-412"><a id="_idTextAnchor422"/>Searching and grouping logs with managed Elasticsearch </h1>
			<p>Many people<a id="_idIndexMarker1434"/> associate Elasticsearch<a id="_idIndexMarker1435"/> with ELK; however, the two<a id="_idIndexMarker1436"/> have differences. <strong class="bold">ELK</strong> stands for <strong class="bold">Elasticsearch, Logstash, and Kibana</strong>. In this configuration, Elasticsearch<a id="_idIndexMarker1437"/> serves as the storage, Logstash<a id="_idIndexMarker1438"/> serves as the log parser, and Kibana serves as the visualization<a id="_idIndexMarker1439"/> frontend of the system<a id="_idIndexMarker1440"/> where users interact<a id="_idIndexMarker1441"/> with the system:</p>
			<div>
				<div id="_idContainer244" class="IMG---Figure">
					<img src="Images/Figure_17.8_B17405.jpg" alt="Figure 17.8 – A comparison of the ELK stack versus Amazon's managed Elasticsearch service&#13;&#10;" width="556" height="323"/>
				</div>
			</div>
			<p class="figure-caption">Figure 17.8 – A comparison of the ELK stack versus Amazon's managed Elasticsearch service</p>
			<p>With Amazon's managed Elasticsearch service, there is no Logstash installed by default; however, there are other options to get the logs that you generate into your Elasticsearch cluster. </p>
			<h2 id="_idParaDest-413"><a id="_idTextAnchor423"/>Use cases for managed Elasticsearch </h2>
			<p>There are several use cases<a id="_idIndexMarker1442"/> for using the managed Elasticsearch product from AWS. Let's examine them next.</p>
			<h3>Store and search logs for application monitoring </h3>
			<p>You can stream logs that have been placed into AWS CloudWatch Logs into Amazon's managed Elasticsearch service. Once the logs have been put into the Elasticsearch cluster, they can be searched by the Lucene-backed Elasticsearch search engine. </p>
			<h3>Security Information and Event Management (SIEM) </h3>
			<p>Storing logs from multiple events and applications across your network in a central system allows you to detect and alert on security events in near real time, using the capabilities of Amazon's managed Elasticsearch service.  </p>
			<h3>An enterprise-grade search engine</h3>
			<p>Although the ELK stack<a id="_idIndexMarker1443"/> is most synonymous with collecting and displaying logs, Elasticsearch is a powerful search engine, built using the Lucene library, which allows for performing searching in near real time. You can connect to Elasticsearch using a RESTful API to send results back to your application or deliver new data to store in your search engine. </p>
			<h3>Monitoring your infrastructure</h3>
			<p>As you collect logs from the different pieces of infrastructure you manage, whether they reside in the cloud or on-premises, you can collect<a id="_idIndexMarker1444"/> them in a single solution using managed Elasticsearch from AWS. This can help you quickly research any issues that arise from all angles, helping to cut down your <strong class="bold">Mean Time To Resolution</strong> (<strong class="bold">MTTR</strong>).</p>
			<p>Please note that Amazon's Elasticsearch service is being renamed to Amazon OpenSearch. </p>
			<p>Now that we understand the use cases for the Elasticsearch service, let's see how we can get our CloudWatch logs into an Elasticsearch cluster with a hands-on example.  </p>
			<h2 id="_idParaDest-414"><a id="_idTextAnchor424"/>Streaming logs from CloudWatch Logs to the Elasticsearch service </h2>
			<p>For the hands-on exercise<a id="_idIndexMarker1445"/> with the managed Elasticsearch service, we will<a id="_idIndexMarker1446"/> deploy a simple Lambda function so that we can generate some logs. We can<a id="_idIndexMarker1447"/> then stand up a single-node Elasticsearch cluster to receive the logs. Use the Lambda function's logs that go into the CloudWatch Logs log group. We will then subscribe that log group to the Elasticsearch cluster we just created. The CloudFormation script also includes an AWS Events rule to fire off the Lambda function every five minutes so that logs will be periodically sent to our CloudWatch Logs group. Finally, we will go to the Kibana visualization interface and look for our logs. Let's get started:</p>
			<ol>
				<li value="1">We will first need to download the CloudFormation template named <strong class="source-inline">lambda_stack.yml</strong> from the <strong class="source-inline">Chapter-17</strong> folder in the GitHub repository for the book. </li>
				<li>Start by logging into the AWS Management Console. Once inside the console, navigate to the <strong class="bold">CloudFormation</strong> service so that we can quickly get our Lambda function<a id="_idIndexMarker1448"/> up and running. Create a stack<a id="_idIndexMarker1449"/> with new resources by either pressing<a id="_idIndexMarker1450"/> the orange <strong class="bold">Create stack</strong> button from the main service page or, if you are already on the <strong class="bold">Stacks</strong> page with previous stacks you have created, the white <strong class="bold">Create Stack</strong> button on the top right-hand side.</li>
				<li>Once on the <strong class="bold">Create stack</strong> page, keep the top setting as <strong class="bold">Template is ready</strong>. Under the <strong class="bold">Specify template</strong> heading, select the radio button next to the option of <strong class="bold">Upload a template file</strong>. Use the <strong class="bold">Choose a file</strong> button to select the <strong class="source-inline">lambda_stack.yml</strong> file that you had downloaded from the <strong class="source-inline">Chapter-17</strong> folder in the GitHub repository. Once the file has been uploaded, click the orange <strong class="bold">Next</strong> button at the bottom of the screen. </li>
				<li>Now that you're on the <strong class="bold">Specify stack details</strong> page, enter <strong class="source-inline">Logging-Lambda</strong> as the name for this CloudFormation stack in the text box for <strong class="bold">Stack name</strong>. After this is entered, click the orange <strong class="bold">Next</strong> button at the bottom of the screen:<div id="_idContainer245" class="IMG---Figure"><img src="Images/Figure_17.9_B17405.jpg" alt="Figure 17.9 – Entering the name of the CloudFormation stack&#13;&#10;" width="808" height="252"/></div><p class="figure-caption">Figure 17.9 – Entering the name of the CloudFormation stack</p></li>
				<li>There is nothing to do on the <strong class="bold">Configure stack options</strong> page. Scroll all the way down to the bottom of the page and click the orange <strong class="bold">Next</strong> button. </li>
				<li>On the <strong class="bold">Review</strong> page, scroll down to the bottom of the page and click the checkbox, acknowledging that this template needs to create an IAM role. After you have done this, you can click the orange <strong class="bold">Create stack</strong> button. </li>
				<li>It should<a id="_idIndexMarker1451"/> take 1 to 5 minutes for our stack<a id="_idIndexMarker1452"/> to completely create our resources; after<a id="_idIndexMarker1453"/> it has, we can click on the <strong class="bold">Resources</strong> tab on the horizontal menu and find the logical ID named <strong class="source-inline">LambdaLogGenerator</strong>. This will be our actual Lambda function. Click on the blue link next to this name, which will be in the column named <strong class="bold">Physical ID</strong>. This will open up a new window directly to our Lambda function:<div id="_idContainer246" class="IMG---Figure"><img src="Images/Figure_17.10_B17405.jpg" alt="Figure 17.10 – The Logical ID and Physical ID of the created Lambda function&#13;&#10;" width="1150" height="200"/></div><p class="figure-caption">Figure 17.10 – The Logical ID and Physical ID of the created Lambda function</p></li>
				<li>We need to wait at least 5 minutes for our Lambda function to be invoked, so in the meantime, we will create our Elasticsearch cluster so that we can stream the logs when they are ready. In the top search box of the AWS Management Console, search for the term <strong class="source-inline">Elasticsearch</strong>. When you see the <strong class="bold">Elasticsearch Service</strong> icon, right-click on the icon to open up the service in a new tab. </li>
				<li>When you come to the <strong class="bold">Amazon Elasticsearch Service</strong> page, click the blue button that is labeled <strong class="bold">Create a new domain</strong>. </li>
				<li>For <strong class="bold">Deployment type</strong>, choose <strong class="bold">Development and testing</strong>, as we only need a single availability zone. Under the heading of <strong class="bold">Version</strong>, choose the latest version. At the time of writing, the latest version available was 7.10. After you have made your selections, click the blue <strong class="bold">Next</strong> button at the bottom of the screen. </li>
				<li>We should now be on the <strong class="bold">Configure domain</strong> page. There are only a few settings on this page that need to be set. The first is <strong class="bold">Elasticsearch domain name</strong>. Use <strong class="source-inline">chapter-17</strong> for the domain name. The second setting will be under the heading of <strong class="bold">Data nodes</strong>. It is suggested for this exercise that you use the <strong class="source-inline">T3.medium.elasticsearch</strong> instances, since we are only performing a short test and not storing much data. Once you have made this change, scroll down<a id="_idIndexMarker1454"/> to the bottom<a id="_idIndexMarker1455"/> of the page and click<a id="_idIndexMarker1456"/> the blue <strong class="bold">Next</strong> button. </li>
				<li>On the <strong class="bold">Configure access and security</strong>, to simplify the access to the Kibana interface, use the following settings:<p>a. Choose <strong class="bold">Public Access</strong>. </p><p>b. Check the box for <strong class="bold">Enable fine-grained access control</strong>. </p><p>c. Select the radio button to create a master user: </p><ul><li><strong class="bold">Username</strong>: <strong class="source-inline">devops</strong></li><li><strong class="bold">Password</strong>: <strong class="source-inline">Chapter17*</strong></li></ul><p>d. Under <strong class="bold">Access policy</strong>, select the following:</p><ul><li><strong class="bold">  </strong></li></ul><p>Keep all other settings the same, and then click the blue <strong class="bold">Next</strong> button at the bottom of the page. Click the blue <strong class="bold">Next</strong> button on the <strong class="bold">Tags</strong> page, and finally scroll down to the bottom of the <strong class="bold">Review</strong> page and click the blue <strong class="bold">Confirm</strong> button. </p></li>
				<li> It will take a few minutes for our Elasticsearch cluster to spin up, and this will give us time to go back to our Lambda function. Go back to the other tab in our browser window where we previously had our Lambda function.</li>
				<li>Once on the <strong class="source-inline">Logging-Lambda-LambdaLogGenerator</strong> function, click the <strong class="bold">Monitor</strong> item in the horizontal menu. This will not only allow us to see a line where our Lambda function has been invocated but there will also be a white button<a id="_idIndexMarker1457"/> near that horizontal<a id="_idIndexMarker1458"/> menu, now labeled <strong class="bold">View logs in CloudWatch</strong>. Click on this button to be taken<a id="_idIndexMarker1459"/> directly to the logs:<div id="_idContainer247" class="IMG---Figure"><img src="Images/Figure_17.11_B17405.jpg" alt="Figure 17.11 – The View logs in CloudWatch button directly under the horizontal menu&#13;&#10;" width="930" height="215"/></div><p class="figure-caption">Figure 17.11 – The View logs in CloudWatch button directly under the horizontal menu</p></li>
				<li>We should now be in the CloudWatch Logs group for our Lambda function. Above the heading of <strong class="bold">Log streams</strong> will be a horizontal menu. Click on the menu item labeled <strong class="bold">Subscription filters</strong>.  </li>
				<li>Once the heading of <strong class="bold">Subscription filters</strong> appears, click on the white button on the right-hand side labeled <strong class="bold">Create</strong>. A menu of choices will appear. Choose the item named <strong class="bold">Create Elasticsearch subscription filter</strong>. This will bring you to a destination page. Choose the Elasticsearch cluster named <strong class="source-inline">chapter-17</strong> that we had just created from the drop-down list. For <strong class="bold">Lambda IAM Execution Role</strong>, start typing <strong class="source-inline">Logging-Lambda</strong> and select the role we created for our logging Lambda function. Under <strong class="bold">Log Format</strong>, select <strong class="bold">Common Log Format</strong> from the drop-down list. For <strong class="bold">Subscription filter name</strong>, type in <strong class="source-inline">log-test</strong>. Finally, scroll to the bottom of the page and click the orange <strong class="bold">Start streaming</strong> button. </li>
				<li>Once you click the <strong class="bold">Start streaming</strong> button, a Lambda function will be created. Now our logs should start streaming into our Elasticsearch cluster. </li>
				<li>Let's now go back to the tab where we had created our Elasticsearch cluster and click on the <strong class="bold">Dashboard</strong> menu item in the left-hand vertical menu. We should now see <strong class="screen-inline">chapter-17</strong> as a linking domain. Click on <strong class="bold">chapter-17</strong> to be taken to the details of the domain. There will be a link to Kibana on the <strong class="bold">Overview</strong> page. Click on this link. Enter in the username (<strong class="source-inline">devops</strong>) and password that was created when we provisioned our Elasticsearch cluster. </li>
				<li>We are now in the Kibana<a id="_idIndexMarker1460"/> interface for our Elasticsearch cluster. Click<a id="_idIndexMarker1461"/> on the text that says <strong class="bold">Explore on my own</strong>. When the <strong class="bold">Select your tenant</strong> dialog<a id="_idIndexMarker1462"/> box appears, just click the blue <strong class="bold">Confirm</strong> button. </li>
			</ol>
			<p>Now that we have looked at how to capture logs and store them on the managed Elasticsearch cluster, let's now look at how we can incorporate the Amazon Kinesis service with Elasticsearch. </p>
			<h1 id="_idParaDest-415"><a id="_idTextAnchor425"/>Understanding the Amazon Kinesis service</h1>
			<p>There is a service in AWS<a id="_idIndexMarker1463"/> that has been created especially for the real-time processing of streaming data. That service is <strong class="bold">Amazon Kinesis</strong>. As more and more items in the world produce data, and more and more applications want to consume that data, there need to be services that can quickly consume and do some pre-processing on that data. The Kinesis service also provides a bit of redundancy in case your main application goes down, storing the records on its shards. By default, the records can be accessed for 24 hours from when they were written. This can also be extended in the settings to save the data for up to 7 days. Data that is sent to Kinesis can be a maximum of 1 MB in size.</p>
			<p>The key features of Amazon Kinesis to understand<a id="_idIndexMarker1464"/> for the test are the following:</p>
			<ul>
				<li>It allows for real-time ingesting, processing, and streaming of data. </li>
				<li>It is a fully managed service, and hence there is no infrastructure for you to manage.</li>
				<li>It integrates with a number of other AWS services, such as Amazon Redshift.</li>
			</ul>
			<p>Knowing that the service comes with these built-in benefits, it also has evolved since its initial introduction to meet the needs of the customers who have been using it. Although it is useful for processing immense amounts of incoming data, such as logs from multiple sources, this is only one of its many uses. </p>
			<p>The Kinesis service comes to us from AWS with four separate capabilities:</p>
			<ul>
				<li><strong class="bold">Kinesis Video Streams</strong> – This capability allows you to easily process incoming video<a id="_idIndexMarker1465"/> data securely and allows<a id="_idIndexMarker1466"/> for analytics, machine learning, or other processing.</li>
				<li><strong class="bold">Kinesis Data Streams</strong> – This capability allows you to stream data from applications<a id="_idIndexMarker1467"/> to the managed<a id="_idIndexMarker1468"/> Kinesis service. </li>
				<li><strong class="bold">Kinesis Data Firehose</strong> – This capability gives users a simple way to capture, transform, and load<a id="_idIndexMarker1469"/> data streams into AWS data<a id="_idIndexMarker1470"/> stores, such as S3, ElasticSearch, and Redshift. </li>
				<li><strong class="bold">Kinesis Data Analytics</strong> – This capability allows<a id="_idIndexMarker1471"/> users to easily process data streams in real time, using<a id="_idIndexMarker1472"/> SQL or Apache Flink.  </li>
			</ul>
			<p>There are scenarios when you would use more than one of the capabilities at the same time.  </p>
			<p class="callout-heading">Tip for the Test</p>
			<p class="callout">While you are required to know the Amazon Kinesis service in much greater detail when taking the Data Analytics – Specialty certification, there are cases where scenarios are presented in the DevOps professional exam where knowing when to use the Kinesis service would be the correct solution (or incorrect solution). </p>
			<p>Now that we have a basic understanding of the Amazon Kinesis service, let's look at when it would be appropriate to use the Amazon Kinesis service to process our logs in an enterprise-type scenario.</p>
			<h2 id="_idParaDest-416"><a id="_idTextAnchor426"/>Using Amazon Kinesis to process logs</h2>
			<p>Kinesis Firehose<a id="_idIndexMarker1473"/> allows us to add an automatic call to an Amazon Lambda<a id="_idIndexMarker1474"/> function that can transform our records before insertion into the Elasticsearch cluster. This works very similarly to the way a Logstash instance in an ELK stack would take incoming logs and transform them before sending them off to an Elasticsearch instance:</p>
			<div>
				<div id="_idContainer248" class="IMG---Figure">
					<img src="Images/Figure_17.12_B17405.jpg" alt="Figure 17.12 – Kinesis Firehose inserting logs into S3 and the Elasticsearch service simultaneously&#13;&#10;" width="355" height="354"/>
				</div>
			</div>
			<p class="figure-caption">Figure 17.12 – Kinesis Firehose inserting logs into S3 and the Elasticsearch service simultaneously</p>
			<p>One of the reasons that there is an S3 bucket in the diagram is the fact that any failed records can be retried for a specified period by Kinesis Firehose to be resent to the Elasticsearch service if there is a failure during delivery. </p>
			<p>What types of things<a id="_idIndexMarker1475"/> could cause failure? Well, running out of space<a id="_idIndexMarker1476"/> in your Elasticsearch cluster is one that comes to mind. If you are constantly streaming logs and do not have the correct way to phase out older data or have not added enough nodes to your cluster, there will come a point where your cluster will run out of space and can no longer accept any new data, or logs in our case. Rather than lose that information or have to manually try and insert it into the cluster, Kinesis can queue the misfires to an S3 bucket and then try to resend the data at a later time. </p>
			<p>Now that we have an understanding of how Amazon Kinesis can be used to enhance our logging setup, let's take a look at the importance of properly tagging and categorizing our logs. </p>
			<h2 id="_idParaDest-417"><a id="_idTextAnchor427"/>Using tagging and metadata to properly categorize logs </h2>
			<p>When categorizing<a id="_idIndexMarker1477"/> your cloud resources and subsequent logs<a id="_idIndexMarker1478"/> for ingestion, tags can help classify where the resources are coming from, especially in the case where you are pushing all of the logs to a larger enterprise-logging solution.</p>
			<p>As you create your assets in AWS as a seasoned DevOps engineer, you should have a number of tags that you place on your resources as they come out of your CI/CD pipeline so that you know how to manage them effectively:</p>
			<div>
				<div id="_idContainer249" class="IMG---Figure">
					<img src="Images/Figure_17.13_B17405.jpg" alt="Figure 17.13 – Examples of tag categories and keys for use within an enterprise system&#13;&#10;" width="439" height="204"/>
				</div>
			</div>
			<p class="figure-caption">Figure 17.13 – Examples of tag categories and keys for use within an enterprise system</p>
			<p>When we talked<a id="_idIndexMarker1479"/> about processing in the previous example with<a id="_idIndexMarker1480"/> the Lambda function using Kinesis, we could look at the tags contained in the metadata. </p>
			<h1 id="_idParaDest-418"><a id="_idTextAnchor428"/>Cleaning up resources </h1>
			<p>Once again, we have created a number of resources in our AWS account during the course of the hands-on exercises<a id="_idIndexMarker1481"/> of this chapter. If not taken down, items such as the managed ElasticSearch cluster could bring about a larger bill than you might like. The Lambda function, which fires every 5 minutes, could eat into your Free Tier allowance, as there are on average 43,800 minutes in a month, so this function would be invocated 8,760 times. Simply delete the resources that you are no longer testing with and delete any CloudFormation stacks that you are no longer using to be sure that your AWS bill stays as low as possible. </p>
			<p>Also, remember to cancel your QuickSight subscription, whether it was a free trial or a single-month subscription, so that you do not get a recurring charge for the QuickSight service. </p>
			<h1 id="_idParaDest-419"><a id="_idTextAnchor429"/>Summary</h1>
			<p>In this chapter, we have discussed how to implement enterprise-grade logging systems. We had a look at other native solutions offered by AWS that are more in tune with capturing, processing, storing, and visualizing massive amounts of logs streaming in constantly. </p>
			<p>In the next chapter, we will begin looking at how to ensure that our workloads are highly available, starting with Auto Scaling groups and their life cycles. </p>
			<h1 id="_idParaDest-420"><a id="_idTextAnchor430"/>Review questions </h1>
			<ol>
				<li value="1">Your company has asked you to create a visual dashboard that shows logs in a visual format that can be accessed by the management team. The management team does not log on to the AWS account via IAM users, nor do they have AWS Management Console access. They also want data enrichment from one of the AWS Redshift databases and a CSV file that is created from an AWS Batch process and stored in an S3 bucket. How can you create an easy-to-access, secure, and dynamic dashboard for the management team to use?<p>a. Create a CloudWatch dashboard using all of the sources. Gather the management team's email addresses and send out a link for access to the dashboard. </p><p>b. Stream all the data into a managed ElasticSearch cluster via CloudWatch Logs. Create a Kibana dashboard that will display the required visualizations. Share the link to the Kibana dashboard with the executive team. </p><p>c. Use Kinesis Firehose to stream the data from Redshift to a managed Elasticsearch cluster. Create a Kibana dashboard that will display the required visualizations. Share the link to the Kibana dashboard with the executive team.  </p><p>d. Use Amazon Athena to create a temporary table of all the logs that are required to be used in the QuickSight dashboard. Import the CSV file into QuickSight for visualization purposes. Import the Redshift database from QuickSight as a data source. Gather the management team's email addresses and send out a link for access to the dashboard.</p></li>
				<li>You have recently joined a company that has been storing multiple different log files in S3 buckets. These logs have come from a variety of sources, including being pushed from on-premises servers using S3 sync, Application Load Balancer logs, VPC Flow Logs, and others. The company would like you to quickly do an analysis of the logs and find out how old the logs are from each category. How could you perform this task quickly and cost-effectively?<p>a. Set the S3 bucket, which contains the logs as a data source in Amazon QuickSight. Use the SPICE engine to analyze the logs. Create two quick visualizations, one showing the age of the logs and the other the types of logs. </p><p>b. Use the S3 inventory function to search through the files in the S3 bucket. Import the report into Excel and then sort the files by age and the types of logs.</p><p>c. Create an AWS Glue job catalog of all the items in the S3 buckets. Use Amazon Athena to be able to query the types of logs. Create a query to group the types of logs. Sort by date descending to show the oldest logs first in each group.</p><p>d. Import all the logs into a managed Elasticsearch cluster. Using the Kibana interface, run a query on the types of logs and then group the logs by age to get a count of the number of logs. </p></li>
			</ol>
			<h1 id="_idParaDest-421"><a id="_idTextAnchor431"/>Review answers </h1>
			<ol>
				<li value="1">D </li>
				<li>B</li>
			</ol>
		</div>
	</div></body></html>