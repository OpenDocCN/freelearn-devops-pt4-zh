- en: 'Chapter 17: Advanced and Enterprise Logging Scenarios'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 17 章：高级和企业级日志记录场景
- en: As we look to wrap up our section on logging, we will discuss how to implement
    enterprise-grade logging systems. While CloudWatch has the ability to search through
    logs and even present some visualizations, we will look at other native solutions
    offered by AWS that are more in tune with capturing, processing, storing, and
    visualizing massive amounts of logs streaming in constantly.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们准备结束关于日志部分的讨论时，我们将讨论如何实现企业级的日志系统。虽然 CloudWatch 可以搜索日志并呈现一些可视化内容，但我们将探索 AWS
    提供的其他本地解决方案，这些解决方案更适合捕获、处理、存储并可视化不断流入的大量日志。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下主要主题：
- en: Using QuickSight to visualize data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 QuickSight 可视化数据
- en: Streaming logs to Amazon Elasticsearch
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将日志流式传输到 Amazon Elasticsearch
- en: Using Amazon Kinesis to process logs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Amazon Kinesis 处理日志
- en: Using QuickSight to visualize data
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 QuickSight 可视化数据
- en: Although there are multiple third-party visualization tools available to analyze
    your data and create graphical representations of your logs, there is a native
    service that Amazon has created for its customers, **QuickSight**. QuickSight
    was developed to be a cloud-scale **Business Intelligence** (**BI**) service that
    is easy to use and can ingest data from multiple sources.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有多个第三方可视化工具可用于分析数据并创建日志的图形表示，但 Amazon 为其客户创建了一项本地服务，**QuickSight**。QuickSight
    是为云端规模的 **商业智能**（**BI**）服务而开发的，易于使用，且能够从多个来源导入数据。
- en: Amazon QuickSight uses a proprietary SPICE engine to calculate and serve data.
    **SPICE** stands for **Super-fast, Parallel, In-memory Calculation Engine**. This
    technology has been built to achieve blazing-fast performance at an enterprise
    scale. The SPICE engine can do this by automatically replicating data, allowing
    thousands of users to perform queries and analysis on that underlying data at
    immensely fast speeds.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon QuickSight 使用专有的 SPICE 引擎来计算和提供数据。**SPICE** 代表 **超高速并行内存计算引擎**。这项技术旨在实现企业级的极速性能。SPICE
    引擎通过自动复制数据来实现这一点，允许成千上万的用户在极快的速度下对这些基础数据进行查询和分析。
- en: 'Another key feature about Amazon QuickSight is that you can share created dashboards
    with members of your IAM organization. Still, it also has the ability to share
    access via email to those that do not have an IAM or federated account to your
    AWS organization. QuickSight also has an iPhone and Android app that is available
    for access:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon QuickSight 的另一个关键特点是，你可以与 IAM 组织成员共享创建的仪表盘。它还可以通过电子邮件与没有 IAM 或联合账户的 AWS
    组织成员共享访问权限。QuickSight 还提供适用于 iPhone 和 Android 的应用程序，方便访问：
- en: '![Figure 17.1 – A flow of logs to Athena and AWS QuickSight to create visualizations'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 17.1 – 日志流向 Athena 和 AWS QuickSight 创建可视化'
- en: '](img/Figure_17.1_B17405.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_17.1_B17405.jpg)'
- en: Figure 17.1 – A flow of logs to Athena and AWS QuickSight to create visualizations
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.1 – 日志流向 Athena 和 AWS QuickSight 创建可视化
- en: In the previous figure, we show a flow of AWS users creating events from actions
    that they are taking.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一张图中，我们展示了 AWS 用户如何从他们所采取的行动中创建事件。
- en: When you set up QuickSight in your AWS account, you create a namespace, which
    is a logical container that is used to organize your teams, clients, and others
    that you will invite to your QuickSight visualizations. You can create multiple
    namespaces and can isolate the data viewed by the users to that namespace. Namespaces
    can also span over multiple regions. Once you set up the namespace, there is no
    further administration needed from that point forward.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在 AWS 账户中设置 QuickSight 时，你会创建一个命名空间，这是一个逻辑容器，用于组织你的团队、客户以及其他你将邀请到 QuickSight
    可视化中的人员。你可以创建多个命名空间，并且可以将用户查看的数据与该命名空间隔离。命名空间也可以跨多个区域。设置好命名空间后，从此不需要进一步的管理操作。
- en: With an understanding of the value that the QuickSight service brings when creating
    visualizations to not only those in our Amazon account but others in our organization,
    let's now look at how the Athena service can expand on these capabilities, using
    the files that we already have stored in our S3 buckets.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 了解了 QuickSight 服务在为我们 Amazon 账户中的用户以及我们组织中的其他人创建可视化时带来的价值后，让我们来看一下 Athena 服务如何基于这些能力进行扩展，利用我们已经存储在
    S3 存储桶中的文件。
- en: Querying data with Amazon Athena
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Amazon Athena 查询数据
- en: AWS has created a service that allows you to query data stored in S3 buckets.
    The service is serverless, and hence there is no need to provision servers, and
    the service only charges you for the queries that you run.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 创建了一项服务，允许你查询存储在 S3 存储桶中的数据。该服务是无服务器的，因此无需配置服务器，而且该服务仅对你运行的查询收费。
- en: The Presto query engine backs **Amazon Athena**. This is an open source SQL
    engine that allows users to query large datasets with low latency. The Presto
    engine also fully supports joins, arrays, and window functions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Presto 查询引擎支持 **Amazon Athena**。这是一个开源的 SQL 引擎，允许用户以低延迟查询大规模数据集。Presto 引擎还完全支持连接、数组和窗口函数。
- en: 'The key features of Amazon Athena are as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Athena 的主要特点如下：
- en: Since it's serverless, there's no administration or infrastructure to manage.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于它是无服务器的，因此无需管理任何管理或基础设施。
- en: It uses standard SQL to query the underlying data.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用标准 SQL 来查询底层数据。
- en: It has extremely fast performance without the need for tuning.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它具有极快的性能，无需调优。
- en: It allows for federated queries across multiple data sources.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持跨多个数据源进行联合查询。
- en: It is secure, allowing you to take advantage of both IAM and S3 bucket policies
    to control access to data.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是安全的，允许你利用 IAM 和 S3 存储桶策略来控制数据访问。
- en: It is highly available with S3 as the data source.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它与 S3 作为数据源时具有高可用性。
- en: Now that we have seen how to use Amazon QuickSight to create more powerful visualizations
    using Amazon Athena, let's look at a few Amazon QuickSight use cases.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何使用 Amazon QuickSight 结合 Amazon Athena 创建更强大的可视化，让我们看看一些 Amazon QuickSight
    的应用场景。
- en: Amazon QuickSight use cases
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon QuickSight 的应用场景
- en: The next section will explore some of the use cases where Amazon QuickSight
    can be used in conjunction with other AWS services to create enterprise-grade
    systems to create dashboards and analysis systems to monitor logs and analytics.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将探讨一些使用 Amazon QuickSight 与其他 AWS 服务结合的应用场景，以创建企业级系统，构建仪表盘和分析系统，用于监控日志和分析。
- en: Using QuickSight to visualize logs and usage analytics with the help of Athena
    and Glue
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 QuickSight 可视化日志和使用分析，借助 Athena 和 Glue 的支持
- en: Amazon has built an interactive query service that allows you to use standard
    SQL statements to query your data, named Athena. One of the best parts about Athena,
    besides the fact that it uses standard SQL and hence there's no new special language
    to learn to use it, is that it's serverless. This means that there are no servers
    to provision, and you are only charged for the queries that you run on the system
    and the data that it scans.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊构建了一个交互式查询服务，允许你使用标准 SQL 语句查询数据，名为 Athena。除了使用标准 SQL 并且无需学习新的特殊语言之外，Athena
    的另一个优点是它是无服务器的。这意味着无需配置服务器，且仅对你在系统上运行的查询和扫描的数据收费。
- en: The ability to add machine learning insights to your dashboards
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将机器学习洞察集成到你的仪表盘中的能力
- en: Amazon QuickSight extends the normal proficiencies of just displaying your data
    in a dashboard format by adding natural language capabilities and machine learning
    insights to help you gain a much grander understanding of your data. These features
    help users discover patterns and hidden trends arising from the underlying data
    without having specialized technical expertise or machine learning skills.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon QuickSight 扩展了仅仅以仪表盘格式展示数据的常规功能，通过添加自然语言功能和机器学习洞察，帮助你更全面地理解数据。这些功能帮助用户发现底层数据中隐藏的模式和趋势，而无需专业的技术专长或机器学习技能。
- en: Connect user dashboards to your data warehouse or data lake
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将用户仪表盘连接到你的数据仓库或数据湖
- en: If you have your data stored in a data warehouse, such as using the **Amazon
    Redshift** service, then you can create a connection in Amazon QuickSight to connect
    to your Redshift cluster. This Redshift cluster becomes an auto-discovered dataset
    and secures the connection between the Redshift cluster and QuickSight using SSL
    automatically without extra configuration. You can then either choose the tables
    you want to use in your QuickSight visualizations or create a custom SQL statement
    to import your data into the SPICE engine to analyze and visualize your data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据存储在数据仓库中，例如使用**Amazon Redshift**服务，那么你可以在 Amazon QuickSight 中创建连接，连接到你的
    Redshift 集群。此 Redshift 集群会成为自动发现的数据集，并通过 SSL 自动保护 Redshift 集群和 QuickSight 之间的连接，无需额外配置。然后，你可以选择想要在
    QuickSight 可视化中使用的表，或者创建自定义 SQL 语句将数据导入 SPICE 引擎中，以分析和可视化数据。
- en: 'If you are storing data in a data lake, especially using **Lake Formation**
    from AWS, your data resides in Amazon S3 buckets. You can then use **AWS Glue**
    to crawl the data and create a data catalog. Once the data catalog has been created,
    you can query the data with Amazon Athena and create a table and database. These
    tables and databases serve as containers for the schema of data held in the S3
    buckets. Amazon QuickSight can then connect to Athena databases and create visualizations
    of the data or even perform further SQL queries:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将数据存储在数据湖中，尤其是使用 AWS 的 **Lake Formation**，那么数据就存储在 Amazon S3 存储桶中。接着，你可以使用
    **AWS Glue** 来爬取数据并创建数据目录。一旦数据目录创建完成，你就可以使用 Amazon Athena 查询数据并创建表和数据库。这些表和数据库充当
    S3 存储桶中数据架构的容器。Amazon QuickSight 然后可以连接到 Athena 数据库，创建数据的可视化，甚至进行进一步的 SQL 查询：
- en: '![Figure 17.2 – Connecting Amazon QuickSight to a data lake in AWS'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 17.2 – 将 Amazon QuickSight 连接到 AWS 中的数据湖'
- en: '](img/Figure_17.2_B17405.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_17.2_B17405.jpg)'
- en: Figure 17.2 – Connecting Amazon QuickSight to a data lake in AWS
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.2 – 将 Amazon QuickSight 连接到 AWS 中的数据湖
- en: Now that we have gone over some of the use cases where enterprises would find
    value in Amazon QuickSight, let's go through a hands-on exercise using QuickSight
    to cement the concepts of this service. This way, if a question comes up regarding
    visualizations on the DevOps professional exam, we have a solid basis of when
    to choose Amazon QuickSight versus CloudWatch dashboards.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讲解了一些企业在使用 Amazon QuickSight 时可能遇到的应用场景，接下来让我们通过一个动手练习来加深对该服务概念的理解。这样一来，如果在
    DevOps 专业认证考试中遇到关于可视化的问题，我们就能有一个坚实的基础，知道何时选择 Amazon QuickSight 而非 CloudWatch 仪表板。
- en: Creating a dashboard with Amazon QuickSight
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Amazon QuickSight 创建仪表板
- en: When you create a data dashboard in Amazon QuickSight, you are publishing a
    collection of interactive graphs and charts for your users to explore, using the
    underlying data that not only shows them insights but gives them the tools to
    explore further should they feel the need.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在 Amazon QuickSight 中创建数据仪表板时，你实际上是在发布一组交互式图表和图形，供用户探索。该仪表板使用的底层数据不仅展示了洞察，还为用户提供了进一步探索的工具，以防他们有此需求。
- en: 'Let''s go through the process of creating a dashboard in Amazon QuickSight.
    We will need the assistance of the Amazon Athena service in order to get our data
    to a temporary database so that we can connect to it from QuickSight:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一起走过在 Amazon QuickSight 中创建仪表板的过程。为了将数据导入临时数据库以便在 QuickSight 中连接，我们将需要借助
    Amazon Athena 服务：
- en: Log on to the Amazon Management Console, and in the top search box, search for
    `QuickSight`. Once on the **QuickSight** screen, verify your AWS account number
    and then press the blue button labeled **Sign up for QuickSight**. Be sure to
    change the default of **Enterprise** to **Standard**:![Figure 17.3 – The QuickSight
    icon from the search menu
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到 Amazon 管理控制台，在顶部搜索框中搜索 `QuickSight`。进入 **QuickSight** 页面后，验证你的 AWS 账户编号，然后按下蓝色按钮
    **注册 QuickSight**。确保将默认的 **企业版** 更改为 **标准版**：![图 17.3 – 来自搜索菜单的 QuickSight 图标
- en: '](img/Figure_17.3_B17405.jpg)'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_17.3_B17405.jpg)'
- en: Figure 17.3 – The QuickSight icon from the search menu
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 17.3 – 来自搜索菜单的 QuickSight 图标
- en: 'You can also use the following URL: [https://aws.amazon.com/quicksight/pricing/](https://aws.amazon.com/quicksight/pricing/).
    Click on the link in the middle of the page that says **Standard Edition**. Once
    on the **Standard Edition** page, scroll down to the bottom of the page and click
    the large yellow button labeled **Start Your Free Trial**.'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以使用以下 URL：[https://aws.amazon.com/quicksight/pricing/](https://aws.amazon.com/quicksight/pricing/)。点击页面中间的
    **标准版** 链接。进入 **标准版** 页面后，向下滚动到页面底部，点击大号黄色按钮 **开始你的免费试用**。
- en: 'You should now be on the page labeled **Create your QuickSight account**. Keep
    the first selection, **Use IAM federated identities & QuickSight-managed users**,
    for **Authentication method**. Next, for **QuickSight region**, change the region
    to **US East (Ohio)**:'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你现在应该在标有 **创建你的 QuickSight 账户** 的页面上。保留第一个选项 **使用 IAM 联邦身份 & QuickSight 管理用户**
    作为 **身份验证方法**。接下来，对于 **QuickSight 区域**，将区域更改为 **美国东部（俄亥俄州）**：
- en: '![Figure 17.4 – Setting the Authentication method and region to create the
    QuickSight account'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 17.4 – 设置身份验证方法和区域以创建 QuickSight 账户'
- en: '](img/Figure_17.4_B17405.jpg)'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_17.4_B17405.jpg)'
- en: Figure 17.4 – Setting the Authentication method and region to create the QuickSight
    account
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 17.4 – 设置身份验证方法和区域以创建 QuickSight 账户
- en: For your QuickSight account name, which is where you declare your namespace,
    choose something unique that you can remember. You will also need to enter an
    email address.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于您的 QuickSight 账户名称，您需要选择一个独特的名称，并且可以记住。您还需要输入一个电子邮件地址。
- en: 'Now, we can specify which data will be available to QuickSight in our current
    setup. Mark the boxes next to the following items:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以指定在当前设置中哪些数据将可供 QuickSight 使用。勾选以下项目旁边的框：
- en: a. Enable auto discovery of data and users in your Amazon Redshift, Amazon RDS,
    and AWS IAM services.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. 启用在您的 Amazon Redshift、Amazon RDS 和 AWS IAM 服务中自动发现数据和用户。
- en: b. Amazon Redshift
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 亚马逊 Redshift
- en: c. Amazon RDS
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. 亚马逊 RDS
- en: d. IAM
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d. IAM
- en: e. Amazon Athena.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e. 亚马逊 Athena。
- en: 'f. Amazon S3 (select the bucket shown – **chapter16-elb-logs**):'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: f. 亚马逊 S3（选择显示的桶 – **chapter16-elb-logs**）：
- en: '![Figure 17.5 – Selecting the S3 bucket for QuickSight'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 17.5 – 选择用于 QuickSight 的 S3 桶'
- en: '](img/Figure_17.5_B17405.jpg)'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_17.5_B17405.jpg)'
- en: Figure 17.5 – Selecting the S3 bucket for QuickSight
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 17.5 – 选择用于 QuickSight 的 S3 桶
- en: We have added a bucket from our previous hands-on exercise, which should have
    data to query with Amazon QuickSight.
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们已经从之前的实践练习中添加了一个桶，该桶应该包含可供 Amazon QuickSight 查询的数据。
- en: You will now be on a screen with a dancing graph as AWS creates your QuickSight
    account. Once it is created, then you will be presented a blue button to click
    on labeled **Go to Amazon QuickSight**. Click on this button to proceed.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您现在将看到一个屏幕，上面有一个跳动的图形，AWS 正在为您创建 QuickSight 账户。账户创建完成后，您将看到一个蓝色按钮，上面写着 **进入
    Amazon QuickSight**。点击此按钮继续。
- en: Once you click the button, it will start to create a few samples for you and
    display a pop-up window welcoming you to QuickSight. Click the blue **Next** button
    on the popup to close them, or click **X** in the top right-hand corner.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击按钮后，它将开始为您创建一些示例，并显示一个弹窗，欢迎您使用 QuickSight。点击弹窗中的蓝色 **下一步** 按钮来关闭它们，或点击右上角的
    **X**。
- en: We will need to create a dataset from the data in our S3 bucket. Find the datasets
    icon in the left-hand vertical menu and click on it. Once on the datasets page,
    click on the dark blue **New dataset** button in the top right-hand corner.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要从 S3 桶中的数据创建一个数据集。找到左侧垂直菜单中的数据集图标并点击它。进入数据集页面后，点击右上角的深蓝色 **新建数据集** 按钮。
- en: Choose `MOCK_DATA.csv` file from the `Chapter-17` folder of the GitHub repository
    if you have not already done so and upload it to QuickSight. Click the blue **Next**
    button on the **Confirm file upload settings** popup. Once it is done, click the
    blue **Visualize** button:![Figure 17.6 – Confirming the data has been uploaded
    correctly to QuickSight
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您还没有选择 `Chapter-17` 文件夹中的 `MOCK_DATA.csv` 文件，请选择它并将其上传至 QuickSight。点击 **确认文件上传设置**
    弹窗中的蓝色 **下一步** 按钮。上传完成后，点击蓝色 **可视化** 按钮：![图 17.6 – 确认数据已正确上传至 QuickSight
- en: '](img/Figure_17.6_B17405.jpg)'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_17.6_B17405.jpg)'
- en: Figure 17.6 – Confirming the data has been uploaded correctly to QuickSight
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 17.6 – 确认数据已正确上传至 QuickSight
- en: Once the data has been loaded, you should be taken to the **Visualize** section
    of Amazon QuickSight. It's now time for us to create a visualization from the
    data we just imported. We need to initially select some fields to show on the
    graph.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据加载完成，您将进入 Amazon QuickSight 的 **可视化** 部分。现在是时候从我们刚刚导入的数据中创建一个可视化了。我们需要首先选择一些字段来显示在图表上。
- en: 'Choose the *filled map* type of graphic, which is in the visual types to the
    right-hand side as the last value. Then, with that type of visualization selected,
    drag the `state_province` value to the `zip_postal_code` value to the **Color**
    field well. Click on some of the other visualization types to see how QuickSight
    can change how your data is presented:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 *填充地图* 类型的图形，它位于右侧可视化类型列表的最后一个值。然后，在选择该可视化类型后，将 `state_province` 值拖动到 `zip_postal_code`
    值的 **颜色** 字段中。点击其他一些可视化类型，看看 QuickSight 如何改变数据的展示方式：
- en: '![Figure 17.7 – The filled map visualization type in Amazon QuickSight'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 17.7 – Amazon QuickSight 中的填充地图可视化类型'
- en: '](img/Figure_17.7_B17405.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_17.7_B17405.jpg)'
- en: Figure 17.7 – The filled map visualization type in Amazon QuickSight
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.7 – Amazon QuickSight 中的填充地图可视化类型
- en: Important Note
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Amazon QuickSight Standard edition has a price of $12 per month when paying
    month to month at the time of publishing. There is a noted free 30-day trial for
    authors; however, if you have already used the service, you will be charged for
    it if you go through this tutorial.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon QuickSight 标准版的价格为每月 12 美元（按月支付）。发布时，有一个为期 30 天的免费试用期，适用于作者；但是，如果您已经使用过该服务，在通过本教程时将会被收费。
- en: Now that we have seen how to create interactive dashboards and visualizations
    using the QuickSight service, let's look at our next service used for managing
    logs at an enterprise level – the Amazon Elasticsearch service.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了如何使用 QuickSight 服务创建交互式仪表板和可视化，接下来我们来看看用于在企业级管理日志的下一个服务——Amazon Elasticsearch
    服务。
- en: Searching and grouping logs with managed Elasticsearch
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用托管 Elasticsearch 搜索和分组日志
- en: 'Many people associate Elasticsearch with ELK; however, the two have differences.
    **ELK** stands for **Elasticsearch, Logstash, and Kibana**. In this configuration,
    Elasticsearch serves as the storage, Logstash serves as the log parser, and Kibana
    serves as the visualization frontend of the system where users interact with the
    system:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人将 Elasticsearch 与 ELK 联系在一起；然而，它们之间存在差异。**ELK** 代表 **Elasticsearch、Logstash
    和 Kibana**。在这种配置中，Elasticsearch 作为存储，Logstash 作为日志解析器，Kibana 作为系统的可视化前端，用户通过它与系统交互：
- en: '![Figure 17.8 – A comparison of the ELK stack versus Amazon''s managed Elasticsearch
    service'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 17.8 – ELK 堆栈与 Amazon 托管 Elasticsearch 服务的比较'
- en: '](img/Figure_17.8_B17405.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_17.8_B17405.jpg)'
- en: Figure 17.8 – A comparison of the ELK stack versus Amazon's managed Elasticsearch
    service
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.8 – ELK 堆栈与 Amazon 托管 Elasticsearch 服务的比较
- en: With Amazon's managed Elasticsearch service, there is no Logstash installed
    by default; however, there are other options to get the logs that you generate
    into your Elasticsearch cluster.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Amazon 的托管 Elasticsearch 服务时，默认情况下没有安装 Logstash；但是，还有其他选项可以将您生成的日志导入到 Elasticsearch
    集群中。
- en: Use cases for managed Elasticsearch
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 托管 Elasticsearch 的使用场景
- en: There are several use cases for using the managed Elasticsearch product from
    AWS. Let's examine them next.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 的托管 Elasticsearch 产品有多个使用场景。接下来我们来看看这些场景。
- en: Store and search logs for application monitoring
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 存储和搜索应用监控日志
- en: You can stream logs that have been placed into AWS CloudWatch Logs into Amazon's
    managed Elasticsearch service. Once the logs have been put into the Elasticsearch
    cluster, they can be searched by the Lucene-backed Elasticsearch search engine.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将存放在 AWS CloudWatch Logs 中的日志流式传输到 Amazon 托管 Elasticsearch 服务。一旦日志进入 Elasticsearch
    集群，它们可以通过基于 Lucene 的 Elasticsearch 搜索引擎进行搜索。
- en: Security Information and Event Management (SIEM)
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安全信息和事件管理（SIEM）
- en: Storing logs from multiple events and applications across your network in a
    central system allows you to detect and alert on security events in near real
    time, using the capabilities of Amazon's managed Elasticsearch service.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 将来自网络中多个事件和应用程序的日志存储在一个集中系统中，可以帮助您使用 Amazon 托管 Elasticsearch 服务的能力，几乎实时地检测和报警安全事件。
- en: An enterprise-grade search engine
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 企业级搜索引擎
- en: Although the ELK stack is most synonymous with collecting and displaying logs,
    Elasticsearch is a powerful search engine, built using the Lucene library, which
    allows for performing searching in near real time. You can connect to Elasticsearch
    using a RESTful API to send results back to your application or deliver new data
    to store in your search engine.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 ELK 堆栈最常与收集和显示日志相关联，但 Elasticsearch 是一个强大的搜索引擎，基于 Lucene 库构建，允许进行近实时的搜索。您可以使用
    RESTful API 连接到 Elasticsearch，将结果返回到应用程序，或者将新数据发送到搜索引擎存储。
- en: Monitoring your infrastructure
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监控您的基础设施
- en: As you collect logs from the different pieces of infrastructure you manage,
    whether they reside in the cloud or on-premises, you can collect them in a single
    solution using managed Elasticsearch from AWS. This can help you quickly research
    any issues that arise from all angles, helping to cut down your **Mean Time To
    Resolution** (**MTTR**).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当您收集来自不同基础设施组件的日志时，无论它们位于云端还是本地，都可以使用 AWS 的托管 Elasticsearch 将它们集中收集到一个解决方案中。这有助于您从各个角度快速研究出现的问题，从而帮助减少您的
    **平均修复时间** (**MTTR**)。
- en: Please note that Amazon's Elasticsearch service is being renamed to Amazon OpenSearch.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Amazon 的 Elasticsearch 服务正在更名为 Amazon OpenSearch。
- en: Now that we understand the use cases for the Elasticsearch service, let's see
    how we can get our CloudWatch logs into an Elasticsearch cluster with a hands-on
    example.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 Elasticsearch 服务的使用场景，让我们通过一个实际操作的例子，看看如何将 CloudWatch 日志导入到 Elasticsearch
    集群中。
- en: Streaming logs from CloudWatch Logs to the Elasticsearch service
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 CloudWatch Logs 中的日志流式传输到 Elasticsearch 服务
- en: 'For the hands-on exercise with the managed Elasticsearch service, we will deploy
    a simple Lambda function so that we can generate some logs. We can then stand
    up a single-node Elasticsearch cluster to receive the logs. Use the Lambda function''s
    logs that go into the CloudWatch Logs log group. We will then subscribe that log
    group to the Elasticsearch cluster we just created. The CloudFormation script
    also includes an AWS Events rule to fire off the Lambda function every five minutes
    so that logs will be periodically sent to our CloudWatch Logs group. Finally,
    we will go to the Kibana visualization interface and look for our logs. Let''s
    get started:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用托管Elasticsearch服务的实操练习中，我们将部署一个简单的Lambda函数来生成一些日志。然后，我们将启动一个单节点的Elasticsearch集群来接收这些日志。利用Lambda函数生成的日志，这些日志将进入CloudWatch
    Logs日志组。接着，我们会将该日志组订阅到刚刚创建的Elasticsearch集群。CloudFormation脚本还包括一个AWS事件规则，每五分钟触发一次Lambda函数，以便定期将日志发送到我们的CloudWatch日志组。最后，我们将进入Kibana可视化界面，查看我们的日志。让我们开始吧：
- en: We will first need to download the CloudFormation template named `lambda_stack.yml`
    from the `Chapter-17` folder in the GitHub repository for the book.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先需要从GitHub仓库中的`Chapter-17`文件夹下载名为`lambda_stack.yml`的CloudFormation模板。
- en: Start by logging into the AWS Management Console. Once inside the console, navigate
    to the **CloudFormation** service so that we can quickly get our Lambda function
    up and running. Create a stack with new resources by either pressing the orange
    **Create stack** button from the main service page or, if you are already on the
    **Stacks** page with previous stacks you have created, the white **Create Stack**
    button on the top right-hand side.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，登录到AWS管理控制台。在控制台内，导航至**CloudFormation**服务，以便我们可以快速启动Lambda函数。通过点击主服务页面中的橙色**创建堆栈**按钮，或者如果你已经在**堆栈**页面并且有之前创建的堆栈，点击右上角的白色**创建堆栈**按钮来创建带有新资源的堆栈。
- en: Once on the `lambda_stack.yml` file that you had downloaded from the `Chapter-17`
    folder in the GitHub repository. Once the file has been uploaded, click the orange
    **Next** button at the bottom of the screen.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载自GitHub仓库`Chapter-17`文件夹中的`lambda_stack.yml`文件后上传。上传文件后，点击屏幕底部的橙色**下一步**按钮。
- en: Now that you're on the `Logging-Lambda` as the name for this CloudFormation
    stack in the text box for **Stack name**. After this is entered, click the orange
    **Next** button at the bottom of the screen:![Figure 17.9 – Entering the name
    of the CloudFormation stack
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在文本框中的**堆栈名称**字段内输入`Logging-Lambda`作为此CloudFormation堆栈的名称。输入后，点击屏幕底部的橙色**下一步**按钮：![图17.9
    – 输入CloudFormation堆栈名称](img/Figure_17.9_B17405.jpg)
- en: '](img/Figure_17.9_B17405.jpg)'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_17.9_B17405.jpg)'
- en: Figure 17.9 – Entering the name of the CloudFormation stack
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图17.9 – 输入CloudFormation堆栈名称
- en: There is nothing to do on the **Configure stack options** page. Scroll all the
    way down to the bottom of the page and click the orange **Next** button.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**配置堆栈选项**页面上无需操作。向下滚动到页面底部，点击橙色的**下一步**按钮。
- en: On the **Review** page, scroll down to the bottom of the page and click the
    checkbox, acknowledging that this template needs to create an IAM role. After
    you have done this, you can click the orange **Create stack** button.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**审核**页面上，向下滚动到页面底部，勾选复选框，确认此模板需要创建一个IAM角色。完成后，可以点击橙色的**创建堆栈**按钮。
- en: It should take 1 to 5 minutes for our stack to completely create our resources;
    after it has, we can click on the `LambdaLogGenerator`. This will be our actual
    Lambda function. Click on the blue link next to this name, which will be in the
    column named **Physical ID**. This will open up a new window directly to our Lambda
    function:![Figure 17.10 – The Logical ID and Physical ID of the created Lambda
    function
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建资源的过程应该需要1到5分钟；完成后，我们可以点击`LambdaLogGenerator`。这将是我们实际的Lambda函数。点击此名称旁边的蓝色链接，它位于**物理ID**列下。这将直接打开一个新窗口，显示我们的Lambda函数：![图17.10
    – 创建的Lambda函数的逻辑ID和物理ID](img/Figure_17.10_B17405.jpg)
- en: '](img/Figure_17.10_B17405.jpg)'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_17.10_B17405.jpg)'
- en: Figure 17.10 – The Logical ID and Physical ID of the created Lambda function
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图17.10 – 创建的Lambda函数的逻辑ID和物理ID
- en: We need to wait at least 5 minutes for our Lambda function to be invoked, so
    in the meantime, we will create our Elasticsearch cluster so that we can stream
    the logs when they are ready. In the top search box of the AWS Management Console,
    search for the term `Elasticsearch`. When you see the **Elasticsearch Service**
    icon, right-click on the icon to open up the service in a new tab.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要等待至少5分钟以便Lambda函数被调用，因此在此期间我们将创建Elasticsearch集群，以便在日志准备好时能够进行流式传输。在AWS管理控制台的顶部搜索框中，搜索`Elasticsearch`。当你看到**Elasticsearch服务**图标时，右键点击图标，在新标签页中打开该服务。
- en: When you come to the **Amazon Elasticsearch Service** page, click the blue button
    that is labeled **Create a new domain**.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你进入**Amazon Elasticsearch服务**页面时，点击标记为**创建新域**的蓝色按钮。
- en: For **Deployment type**, choose **Development and testing**, as we only need
    a single availability zone. Under the heading of **Version**, choose the latest
    version. At the time of writing, the latest version available was 7.10\. After
    you have made your selections, click the blue **Next** button at the bottom of
    the screen.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于**部署类型**，选择**开发与测试**，因为我们只需要一个可用区。在**版本**部分，选择最新版本。在撰写时，最新版本是7.10。选择完毕后，点击屏幕底部的蓝色**下一步**按钮。
- en: We should now be on the `chapter-17` for the domain name. The second setting
    will be under the heading of `T3.medium.elasticsearch` instances, since we are
    only performing a short test and not storing much data. Once you have made this
    change, scroll down to the bottom of the page and click the blue **Next** button.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们应该在`chapter-17`域名设置页面。第二个设置将在`T3.medium.elasticsearch`实例下，因为我们只是进行短期测试，且不需要存储大量数据。做完这些更改后，滚动到页面底部并点击蓝色的**下一步**按钮。
- en: On the `devops`
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`devops`中
- en: '`Chapter17*`'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Chapter17*`'
- en: 'd. Under **Access policy**, select the following:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: d. 在**访问策略**下，选择以下内容：
- en: Keep all other settings the same, and then click the blue **Next** button at
    the bottom of the page. Click the blue **Next** button on the **Tags** page, and
    finally scroll down to the bottom of the **Review** page and click the blue **Confirm**
    button.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 保持其他设置不变，然后点击页面底部的蓝色**下一步**按钮。点击**标签**页面上的蓝色**下一步**按钮，最后滚动到**审查**页面的底部，点击蓝色的**确认**按钮。
- en: It will take a few minutes for our Elasticsearch cluster to spin up, and this
    will give us time to go back to our Lambda function. Go back to the other tab
    in our browser window where we previously had our Lambda function.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Elasticsearch集群启动需要几分钟，这段时间我们可以回到Lambda函数的页面。返回我们浏览器窗口中的另一个标签页，在那里我们之前已经打开了Lambda函数。
- en: Once on the `Logging-Lambda-LambdaLogGenerator` function, click the **Monitor**
    item in the horizontal menu. This will not only allow us to see a line where our
    Lambda function has been invocated but there will also be a white button near
    that horizontal menu, now labeled **View logs in CloudWatch**. Click on this button
    to be taken directly to the logs:![Figure 17.11 – The View logs in CloudWatch
    button directly under the horizontal menu
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦进入`Logging-Lambda-LambdaLogGenerator`函数，点击横向菜单中的**监控**项。这不仅会让我们看到Lambda函数被调用的日志行，还会有一个白色按钮，位于横向菜单旁边，现标记为**在CloudWatch中查看日志**。点击该按钮将直接带你进入日志页面：![图17.11
    - 直接位于横向菜单下方的在CloudWatch中查看日志按钮
- en: '](img/Figure_17.11_B17405.jpg)'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_17.11_B17405.jpg)'
- en: Figure 17.11 – The View logs in CloudWatch button directly under the horizontal
    menu
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图17.11 - 直接位于横向菜单下方的在CloudWatch中查看日志按钮
- en: We should now be in the CloudWatch Logs group for our Lambda function. Above
    the heading of **Log streams** will be a horizontal menu. Click on the menu item
    labeled **Subscription filters**.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们应该在Lambda函数的CloudWatch日志组中。**日志流**标题上方会有一个横向菜单。点击菜单项**订阅过滤器**。
- en: Once the heading of `chapter-17` that we had just created from the drop-down
    list. For `Logging-Lambda` and select the role we created for our logging Lambda
    function. Under `log-test`. Finally, scroll to the bottom of the page and click
    the orange **Start streaming** button.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦从下拉列表中选择我们刚刚创建的`chapter-17`标题。选择`Logging-Lambda`并选择为日志Lambda函数创建的角色。选择`log-test`。最后，滚动到页面底部并点击橙色的**开始流式传输**按钮。
- en: Once you click the **Start streaming** button, a Lambda function will be created.
    Now our logs should start streaming into our Elasticsearch cluster.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦点击**开始流式传输**按钮，一个Lambda函数将被创建。现在，我们的日志应该开始流入Elasticsearch集群中。
- en: Let's now go back to the tab where we had created our Elasticsearch cluster
    and click on the `chapter-17` as a linking domain. Click on `devops`) and password
    that was created when we provisioned our Elasticsearch cluster.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在回到我们创建 Elasticsearch 集群的标签页，并点击 `chapter-17` 作为链接域。点击 `devops`）并输入在配置 Elasticsearch
    集群时创建的密码。
- en: We are now in the Kibana interface for our Elasticsearch cluster. Click on the
    text that says **Explore on my own**. When the **Select your tenant** dialog box
    appears, just click the blue **Confirm** button.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在在 Elasticsearch 集群的 Kibana 界面中。点击显示的文字 **Explore on my own**。当弹出 **Select
    your tenant** 对话框时，只需点击蓝色的 **Confirm** 按钮。
- en: Now that we have looked at how to capture logs and store them on the managed
    Elasticsearch cluster, let's now look at how we can incorporate the Amazon Kinesis
    service with Elasticsearch.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看过如何捕获日志并将其存储在托管的 Elasticsearch 集群中，接下来我们将探讨如何将 Amazon Kinesis 服务与 Elasticsearch
    结合使用。
- en: Understanding the Amazon Kinesis service
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解 Amazon Kinesis 服务
- en: There is a service in AWS that has been created especially for the real-time
    processing of streaming data. That service is **Amazon Kinesis**. As more and
    more items in the world produce data, and more and more applications want to consume
    that data, there need to be services that can quickly consume and do some pre-processing
    on that data. The Kinesis service also provides a bit of redundancy in case your
    main application goes down, storing the records on its shards. By default, the
    records can be accessed for 24 hours from when they were written. This can also
    be extended in the settings to save the data for up to 7 days. Data that is sent
    to Kinesis can be a maximum of 1 MB in size.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 中有一个专门为实时处理流数据而创建的服务。这个服务就是 **Amazon Kinesis**。随着世界上越来越多的物品产生数据，越来越多的应用程序希望消费这些数据，必须有能够快速消费并对数据进行一些预处理的服务。Kinesis
    服务还提供了一些冗余，以防主应用程序出现故障，通过其分片存储记录。默认情况下，记录可以在写入后 24 小时内访问。此设置也可以在配置中扩展，最多保存数据 7
    天。发送到 Kinesis 的数据最大为 1 MB。
- en: 'The key features of Amazon Kinesis to understand for the test are the following:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 需要了解的 Amazon Kinesis 关键功能如下：
- en: It allows for real-time ingesting, processing, and streaming of data.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许实时摄取、处理和流式传输数据。
- en: It is a fully managed service, and hence there is no infrastructure for you
    to manage.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是一个完全托管的服务，因此无需管理基础设施。
- en: It integrates with a number of other AWS services, such as Amazon Redshift.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它与其他多个 AWS 服务集成，如 Amazon Redshift。
- en: Knowing that the service comes with these built-in benefits, it also has evolved
    since its initial introduction to meet the needs of the customers who have been
    using it. Although it is useful for processing immense amounts of incoming data,
    such as logs from multiple sources, this is only one of its many uses.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 了解到该服务具备这些内建的优势，它自首次推出以来也经历了演变，以满足使用者的需求。尽管它在处理大量传入数据（如来自多个来源的日志）时非常有用，但这只是它众多用途中的一种。
- en: 'The Kinesis service comes to us from AWS with four separate capabilities:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis 服务来自 AWS，提供四个独立的功能：
- en: '**Kinesis Video Streams** – This capability allows you to easily process incoming
    video data securely and allows for analytics, machine learning, or other processing.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kinesis 视频流** – 该功能允许您轻松、安全地处理传入的视频数据，并支持分析、机器学习或其他处理。'
- en: '**Kinesis Data Streams** – This capability allows you to stream data from applications
    to the managed Kinesis service.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kinesis 数据流** – 该功能允许您将数据从应用程序流式传输到托管的 Kinesis 服务。'
- en: '**Kinesis Data Firehose** – This capability gives users a simple way to capture,
    transform, and load data streams into AWS data stores, such as S3, ElasticSearch,
    and Redshift.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kinesis 数据火 hose** – 该功能为用户提供了一种简单的方法，将数据流捕获、转换并加载到 AWS 数据存储中，如 S3、ElasticSearch
    和 Redshift。'
- en: '**Kinesis Data Analytics** – This capability allows users to easily process
    data streams in real time, using SQL or Apache Flink.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kinesis 数据分析** – 该功能允许用户使用 SQL 或 Apache Flink 轻松地实时处理数据流。'
- en: There are scenarios when you would use more than one of the capabilities at
    the same time.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些场景中，您可能会同时使用多个功能。
- en: Tip for the Test
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 测试小贴士
- en: While you are required to know the Amazon Kinesis service in much greater detail
    when taking the Data Analytics – Specialty certification, there are cases where
    scenarios are presented in the DevOps professional exam where knowing when to
    use the Kinesis service would be the correct solution (or incorrect solution).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在参加数据分析 – 专业认证考试时，你需要对 Amazon Kinesis 服务有更深入的了解，但在 DevOps 专业考试中，也有一些场景会考察你是否知道何时使用
    Kinesis 服务是正确的解决方案（或错误的解决方案）。
- en: Now that we have a basic understanding of the Amazon Kinesis service, let's
    look at when it would be appropriate to use the Amazon Kinesis service to process
    our logs in an enterprise-type scenario.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对 Amazon Kinesis 服务有了基本的了解，让我们看看在企业场景中使用 Amazon Kinesis 服务来处理日志的合适时机。
- en: Using Amazon Kinesis to process logs
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Amazon Kinesis 处理日志
- en: 'Kinesis Firehose allows us to add an automatic call to an Amazon Lambda function
    that can transform our records before insertion into the Elasticsearch cluster.
    This works very similarly to the way a Logstash instance in an ELK stack would
    take incoming logs and transform them before sending them off to an Elasticsearch
    instance:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis Firehose 允许我们添加一个自动调用 Amazon Lambda 函数的功能，该函数可以在将记录插入 Elasticsearch
    集群之前对其进行转换。这与 ELK 堆栈中的 Logstash 实例处理传入日志并在发送到 Elasticsearch 实例之前进行转换的方式非常相似：
- en: '![Figure 17.12 – Kinesis Firehose inserting logs into S3 and the Elasticsearch
    service simultaneously'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 17.12 – Kinesis Firehose 同时将日志插入 S3 和 Elasticsearch 服务'
- en: '](img/Figure_17.12_B17405.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_17.12_B17405.jpg)'
- en: Figure 17.12 – Kinesis Firehose inserting logs into S3 and the Elasticsearch
    service simultaneously
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.12 – Kinesis Firehose 同时将日志插入 S3 和 Elasticsearch 服务
- en: One of the reasons that there is an S3 bucket in the diagram is the fact that
    any failed records can be retried for a specified period by Kinesis Firehose to
    be resent to the Elasticsearch service if there is a failure during delivery.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图中有一个 S3 存储桶的原因之一是，Kinesis Firehose 可以在指定时间内重试任何失败的记录，以便在交付失败时重新发送到 Elasticsearch
    服务。
- en: What types of things could cause failure? Well, running out of space in your
    Elasticsearch cluster is one that comes to mind. If you are constantly streaming
    logs and do not have the correct way to phase out older data or have not added
    enough nodes to your cluster, there will come a point where your cluster will
    run out of space and can no longer accept any new data, or logs in our case. Rather
    than lose that information or have to manually try and insert it into the cluster,
    Kinesis can queue the misfires to an S3 bucket and then try to resend the data
    at a later time.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 什么因素可能导致失败？嗯，一个常见的原因是 Elasticsearch 集群的空间不足。如果你持续流式传输日志，但没有正确的方式来淘汰旧数据，或者没有向集群添加足够的节点，那么在某个时刻，集群将没有足够的空间，无法再接受任何新数据，或者在我们的案例中，无法接收新的日志。与其丢失这些信息或手动尝试将其插入集群，不如让
    Kinesis 将错误的记录排入 S3 存储桶，然后在稍后重新尝试发送数据。
- en: Now that we have an understanding of how Amazon Kinesis can be used to enhance
    our logging setup, let's take a look at the importance of properly tagging and
    categorizing our logs.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了如何使用 Amazon Kinesis 来增强日志记录设置，让我们来看看正确标记和分类日志的重要性。
- en: Using tagging and metadata to properly categorize logs
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用标签和元数据正确分类日志
- en: When categorizing your cloud resources and subsequent logs for ingestion, tags
    can help classify where the resources are coming from, especially in the case
    where you are pushing all of the logs to a larger enterprise-logging solution.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在对云资源和后续日志进行分类以便摄取时，标签可以帮助分类资源的来源，特别是在将所有日志推送到大型企业日志解决方案的情况下。
- en: 'As you create your assets in AWS as a seasoned DevOps engineer, you should
    have a number of tags that you place on your resources as they come out of your
    CI/CD pipeline so that you know how to manage them effectively:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个经验丰富的 DevOps 工程师，在 AWS 中创建资源时，你应该为从 CI/CD 管道中出来的资源添加多个标签，以便有效管理这些资源：
- en: '![Figure 17.13 – Examples of tag categories and keys for use within an enterprise
    system'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 17.13 – 用于企业系统中的标签类别和键的示例'
- en: '](img/Figure_17.13_B17405.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_17.13_B17405.jpg)'
- en: Figure 17.13 – Examples of tag categories and keys for use within an enterprise
    system
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.13 – 用于企业系统中的标签类别和键的示例
- en: When we talked about processing in the previous example with the Lambda function
    using Kinesis, we could look at the tags contained in the metadata.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在前面的示例中讨论使用 Lambda 函数和 Kinesis 处理时，我们可以查看元数据中包含的标签。
- en: Cleaning up resources
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清理资源
- en: Once again, we have created a number of resources in our AWS account during
    the course of the hands-on exercises of this chapter. If not taken down, items
    such as the managed ElasticSearch cluster could bring about a larger bill than
    you might like. The Lambda function, which fires every 5 minutes, could eat into
    your Free Tier allowance, as there are on average 43,800 minutes in a month, so
    this function would be invocated 8,760 times. Simply delete the resources that
    you are no longer testing with and delete any CloudFormation stacks that you are
    no longer using to be sure that your AWS bill stays as low as possible.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的实践过程中，我们在 AWS 账户中创建了许多资源。如果没有及时删除，像托管的 ElasticSearch 集群这样的资源可能会导致账单超出预期。每隔
    5 分钟触发一次的 Lambda 函数可能会消耗您的免费套餐配额，因为每月大约有 43,800 分钟，因此此函数将被调用 8,760 次。只需删除不再使用的资源，并删除任何不再使用的
    CloudFormation 堆栈，以确保您的 AWS 账单尽可能保持在最低水平。
- en: Also, remember to cancel your QuickSight subscription, whether it was a free
    trial or a single-month subscription, so that you do not get a recurring charge
    for the QuickSight service.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请记得取消您的 QuickSight 订阅，无论是免费试用还是单月订阅，以免因 QuickSight 服务而产生持续收费。
- en: Summary
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have discussed how to implement enterprise-grade logging
    systems. We had a look at other native solutions offered by AWS that are more
    in tune with capturing, processing, storing, and visualizing massive amounts of
    logs streaming in constantly.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们讨论了如何实施企业级日志记录系统。我们还了解了 AWS 提供的其他本地解决方案，这些方案更适合捕获、处理、存储和可视化大量不断流入的日志数据。
- en: In the next chapter, we will begin looking at how to ensure that our workloads
    are highly available, starting with Auto Scaling groups and their life cycles.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始探讨如何确保我们的工作负载具有高度可用性，从 Auto Scaling 组及其生命周期开始。
- en: Review questions
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复习问题
- en: Your company has asked you to create a visual dashboard that shows logs in a
    visual format that can be accessed by the management team. The management team
    does not log on to the AWS account via IAM users, nor do they have AWS Management
    Console access. They also want data enrichment from one of the AWS Redshift databases
    and a CSV file that is created from an AWS Batch process and stored in an S3 bucket.
    How can you create an easy-to-access, secure, and dynamic dashboard for the management
    team to use?
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您的公司要求您创建一个可视化仪表盘，以图形化的方式展示日志，并且管理团队可以访问。管理团队不会通过 IAM 用户登录 AWS 账户，也没有 AWS 管理控制台访问权限。他们还希望从
    AWS Redshift 数据库和通过 AWS Batch 过程创建并存储在 S3 桶中的 CSV 文件中进行数据增强。如何为管理团队创建一个易于访问、安全且动态的仪表盘？
- en: a. Create a CloudWatch dashboard using all of the sources. Gather the management
    team's email addresses and send out a link for access to the dashboard.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. 使用所有数据源创建一个 CloudWatch 仪表盘。收集管理团队的电子邮件地址，并发送仪表盘访问链接。
- en: b. Stream all the data into a managed ElasticSearch cluster via CloudWatch Logs.
    Create a Kibana dashboard that will display the required visualizations. Share
    the link to the Kibana dashboard with the executive team.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 通过 CloudWatch Logs 将所有数据流式传输到托管的 ElasticSearch 集群。创建一个 Kibana 仪表盘，展示所需的可视化内容。将
    Kibana 仪表盘的链接与执行团队共享。
- en: c. Use Kinesis Firehose to stream the data from Redshift to a managed Elasticsearch
    cluster. Create a Kibana dashboard that will display the required visualizations.
    Share the link to the Kibana dashboard with the executive team.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. 使用 Kinesis Firehose 将 Redshift 中的数据流式传输到托管的 ElasticSearch 集群。创建一个 Kibana
    仪表盘，展示所需的可视化内容。将 Kibana 仪表盘的链接与执行团队共享。
- en: d. Use Amazon Athena to create a temporary table of all the logs that are required
    to be used in the QuickSight dashboard. Import the CSV file into QuickSight for
    visualization purposes. Import the Redshift database from QuickSight as a data
    source. Gather the management team's email addresses and send out a link for access
    to the dashboard.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d. 使用 Amazon Athena 创建一个包含所有需要用于 QuickSight 仪表盘日志的临时表。将 CSV 文件导入 QuickSight
    以进行可视化。将 Redshift 数据库作为数据源从 QuickSight 导入。收集管理团队的电子邮件地址，并发送仪表盘访问链接。
- en: You have recently joined a company that has been storing multiple different
    log files in S3 buckets. These logs have come from a variety of sources, including
    being pushed from on-premises servers using S3 sync, Application Load Balancer
    logs, VPC Flow Logs, and others. The company would like you to quickly do an analysis
    of the logs and find out how old the logs are from each category. How could you
    perform this task quickly and cost-effectively?
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您最近加入了一家公司，该公司在S3存储桶中存储了多种不同的日志文件。这些日志来自各种来源，包括使用S3同步从本地服务器推送，应用程序负载均衡器日志，VPC流日志等。公司希望您快速分析这些日志，并查明每个类别的日志有多老。您如何能够快速且经济有效地执行此任务？
- en: a. Set the S3 bucket, which contains the logs as a data source in Amazon QuickSight.
    Use the SPICE engine to analyze the logs. Create two quick visualizations, one
    showing the age of the logs and the other the types of logs.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. 将包含日志的S3存储桶设置为Amazon QuickSight中的数据源。使用SPICE引擎分析日志。创建两个快速可视化，一个显示日志的年龄，另一个显示日志的类型。
- en: b. Use the S3 inventory function to search through the files in the S3 bucket.
    Import the report into Excel and then sort the files by age and the types of logs.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 使用S3库存功能搜索S3存储桶中的文件。将报告导入Excel，然后按年龄和日志类型对文件进行排序。
- en: c. Create an AWS Glue job catalog of all the items in the S3 buckets. Use Amazon
    Athena to be able to query the types of logs. Create a query to group the types
    of logs. Sort by date descending to show the oldest logs first in each group.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. 创建一个AWS Glue作业目录，列出所有S3存储桶中的项目。使用Amazon Athena查询日志类型。创建一个查询来对日志类型进行分组。按日期降序排序，以便在每个组中首先显示最旧的日志。
- en: d. Import all the logs into a managed Elasticsearch cluster. Using the Kibana
    interface, run a query on the types of logs and then group the logs by age to
    get a count of the number of logs.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d. 将所有日志导入托管的Elasticsearch集群。使用Kibana界面，运行一个关于日志类型的查询，然后按年龄对日志进行分组以获取日志数量的统计信息。
- en: Review answers
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查看答案
- en: D
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: B
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
