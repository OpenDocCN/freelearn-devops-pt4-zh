- en: 'Chapter 17: Advanced and Enterprise Logging Scenarios'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we look to wrap up our section on logging, we will discuss how to implement
    enterprise-grade logging systems. While CloudWatch has the ability to search through
    logs and even present some visualizations, we will look at other native solutions
    offered by AWS that are more in tune with capturing, processing, storing, and
    visualizing massive amounts of logs streaming in constantly.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Using QuickSight to visualize data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming logs to Amazon Elasticsearch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Amazon Kinesis to process logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using QuickSight to visualize data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although there are multiple third-party visualization tools available to analyze
    your data and create graphical representations of your logs, there is a native
    service that Amazon has created for its customers, **QuickSight**. QuickSight
    was developed to be a cloud-scale **Business Intelligence** (**BI**) service that
    is easy to use and can ingest data from multiple sources.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon QuickSight uses a proprietary SPICE engine to calculate and serve data.
    **SPICE** stands for **Super-fast, Parallel, In-memory Calculation Engine**. This
    technology has been built to achieve blazing-fast performance at an enterprise
    scale. The SPICE engine can do this by automatically replicating data, allowing
    thousands of users to perform queries and analysis on that underlying data at
    immensely fast speeds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another key feature about Amazon QuickSight is that you can share created dashboards
    with members of your IAM organization. Still, it also has the ability to share
    access via email to those that do not have an IAM or federated account to your
    AWS organization. QuickSight also has an iPhone and Android app that is available
    for access:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 17.1 – A flow of logs to Athena and AWS QuickSight to create visualizations'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_17.1_B17405.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17.1 – A flow of logs to Athena and AWS QuickSight to create visualizations
  prefs: []
  type: TYPE_NORMAL
- en: In the previous figure, we show a flow of AWS users creating events from actions
    that they are taking.
  prefs: []
  type: TYPE_NORMAL
- en: When you set up QuickSight in your AWS account, you create a namespace, which
    is a logical container that is used to organize your teams, clients, and others
    that you will invite to your QuickSight visualizations. You can create multiple
    namespaces and can isolate the data viewed by the users to that namespace. Namespaces
    can also span over multiple regions. Once you set up the namespace, there is no
    further administration needed from that point forward.
  prefs: []
  type: TYPE_NORMAL
- en: With an understanding of the value that the QuickSight service brings when creating
    visualizations to not only those in our Amazon account but others in our organization,
    let's now look at how the Athena service can expand on these capabilities, using
    the files that we already have stored in our S3 buckets.
  prefs: []
  type: TYPE_NORMAL
- en: Querying data with Amazon Athena
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AWS has created a service that allows you to query data stored in S3 buckets.
    The service is serverless, and hence there is no need to provision servers, and
    the service only charges you for the queries that you run.
  prefs: []
  type: TYPE_NORMAL
- en: The Presto query engine backs **Amazon Athena**. This is an open source SQL
    engine that allows users to query large datasets with low latency. The Presto
    engine also fully supports joins, arrays, and window functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key features of Amazon Athena are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Since it's serverless, there's no administration or infrastructure to manage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses standard SQL to query the underlying data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has extremely fast performance without the need for tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It allows for federated queries across multiple data sources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is secure, allowing you to take advantage of both IAM and S3 bucket policies
    to control access to data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is highly available with S3 as the data source.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have seen how to use Amazon QuickSight to create more powerful visualizations
    using Amazon Athena, let's look at a few Amazon QuickSight use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon QuickSight use cases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next section will explore some of the use cases where Amazon QuickSight
    can be used in conjunction with other AWS services to create enterprise-grade
    systems to create dashboards and analysis systems to monitor logs and analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Using QuickSight to visualize logs and usage analytics with the help of Athena
    and Glue
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Amazon has built an interactive query service that allows you to use standard
    SQL statements to query your data, named Athena. One of the best parts about Athena,
    besides the fact that it uses standard SQL and hence there's no new special language
    to learn to use it, is that it's serverless. This means that there are no servers
    to provision, and you are only charged for the queries that you run on the system
    and the data that it scans.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to add machine learning insights to your dashboards
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Amazon QuickSight extends the normal proficiencies of just displaying your data
    in a dashboard format by adding natural language capabilities and machine learning
    insights to help you gain a much grander understanding of your data. These features
    help users discover patterns and hidden trends arising from the underlying data
    without having specialized technical expertise or machine learning skills.
  prefs: []
  type: TYPE_NORMAL
- en: Connect user dashboards to your data warehouse or data lake
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have your data stored in a data warehouse, such as using the **Amazon
    Redshift** service, then you can create a connection in Amazon QuickSight to connect
    to your Redshift cluster. This Redshift cluster becomes an auto-discovered dataset
    and secures the connection between the Redshift cluster and QuickSight using SSL
    automatically without extra configuration. You can then either choose the tables
    you want to use in your QuickSight visualizations or create a custom SQL statement
    to import your data into the SPICE engine to analyze and visualize your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are storing data in a data lake, especially using **Lake Formation**
    from AWS, your data resides in Amazon S3 buckets. You can then use **AWS Glue**
    to crawl the data and create a data catalog. Once the data catalog has been created,
    you can query the data with Amazon Athena and create a table and database. These
    tables and databases serve as containers for the schema of data held in the S3
    buckets. Amazon QuickSight can then connect to Athena databases and create visualizations
    of the data or even perform further SQL queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 17.2 – Connecting Amazon QuickSight to a data lake in AWS'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_17.2_B17405.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17.2 – Connecting Amazon QuickSight to a data lake in AWS
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have gone over some of the use cases where enterprises would find
    value in Amazon QuickSight, let's go through a hands-on exercise using QuickSight
    to cement the concepts of this service. This way, if a question comes up regarding
    visualizations on the DevOps professional exam, we have a solid basis of when
    to choose Amazon QuickSight versus CloudWatch dashboards.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a dashboard with Amazon QuickSight
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you create a data dashboard in Amazon QuickSight, you are publishing a
    collection of interactive graphs and charts for your users to explore, using the
    underlying data that not only shows them insights but gives them the tools to
    explore further should they feel the need.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go through the process of creating a dashboard in Amazon QuickSight.
    We will need the assistance of the Amazon Athena service in order to get our data
    to a temporary database so that we can connect to it from QuickSight:'
  prefs: []
  type: TYPE_NORMAL
- en: Log on to the Amazon Management Console, and in the top search box, search for
    `QuickSight`. Once on the **QuickSight** screen, verify your AWS account number
    and then press the blue button labeled **Sign up for QuickSight**. Be sure to
    change the default of **Enterprise** to **Standard**:![Figure 17.3 – The QuickSight
    icon from the search menu
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_17.3_B17405.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 17.3 – The QuickSight icon from the search menu
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can also use the following URL: [https://aws.amazon.com/quicksight/pricing/](https://aws.amazon.com/quicksight/pricing/).
    Click on the link in the middle of the page that says **Standard Edition**. Once
    on the **Standard Edition** page, scroll down to the bottom of the page and click
    the large yellow button labeled **Start Your Free Trial**.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You should now be on the page labeled **Create your QuickSight account**. Keep
    the first selection, **Use IAM federated identities & QuickSight-managed users**,
    for **Authentication method**. Next, for **QuickSight region**, change the region
    to **US East (Ohio)**:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 17.4 – Setting the Authentication method and region to create the
    QuickSight account'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/Figure_17.4_B17405.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 17.4 – Setting the Authentication method and region to create the QuickSight
    account
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For your QuickSight account name, which is where you declare your namespace,
    choose something unique that you can remember. You will also need to enter an
    email address.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we can specify which data will be available to QuickSight in our current
    setup. Mark the boxes next to the following items:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Enable auto discovery of data and users in your Amazon Redshift, Amazon RDS,
    and AWS IAM services.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Amazon Redshift
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. Amazon RDS
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d. IAM
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: e. Amazon Athena.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'f. Amazon S3 (select the bucket shown – **chapter16-elb-logs**):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 17.5 – Selecting the S3 bucket for QuickSight'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/Figure_17.5_B17405.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 17.5 – Selecting the S3 bucket for QuickSight
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We have added a bucket from our previous hands-on exercise, which should have
    data to query with Amazon QuickSight.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You will now be on a screen with a dancing graph as AWS creates your QuickSight
    account. Once it is created, then you will be presented a blue button to click
    on labeled **Go to Amazon QuickSight**. Click on this button to proceed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you click the button, it will start to create a few samples for you and
    display a pop-up window welcoming you to QuickSight. Click the blue **Next** button
    on the popup to close them, or click **X** in the top right-hand corner.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will need to create a dataset from the data in our S3 bucket. Find the datasets
    icon in the left-hand vertical menu and click on it. Once on the datasets page,
    click on the dark blue **New dataset** button in the top right-hand corner.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose `MOCK_DATA.csv` file from the `Chapter-17` folder of the GitHub repository
    if you have not already done so and upload it to QuickSight. Click the blue **Next**
    button on the **Confirm file upload settings** popup. Once it is done, click the
    blue **Visualize** button:![Figure 17.6 – Confirming the data has been uploaded
    correctly to QuickSight
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_17.6_B17405.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 17.6 – Confirming the data has been uploaded correctly to QuickSight
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once the data has been loaded, you should be taken to the **Visualize** section
    of Amazon QuickSight. It's now time for us to create a visualization from the
    data we just imported. We need to initially select some fields to show on the
    graph.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Choose the *filled map* type of graphic, which is in the visual types to the
    right-hand side as the last value. Then, with that type of visualization selected,
    drag the `state_province` value to the `zip_postal_code` value to the **Color**
    field well. Click on some of the other visualization types to see how QuickSight
    can change how your data is presented:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 17.7 – The filled map visualization type in Amazon QuickSight'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_17.7_B17405.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17.7 – The filled map visualization type in Amazon QuickSight
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Amazon QuickSight Standard edition has a price of $12 per month when paying
    month to month at the time of publishing. There is a noted free 30-day trial for
    authors; however, if you have already used the service, you will be charged for
    it if you go through this tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen how to create interactive dashboards and visualizations
    using the QuickSight service, let's look at our next service used for managing
    logs at an enterprise level – the Amazon Elasticsearch service.
  prefs: []
  type: TYPE_NORMAL
- en: Searching and grouping logs with managed Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Many people associate Elasticsearch with ELK; however, the two have differences.
    **ELK** stands for **Elasticsearch, Logstash, and Kibana**. In this configuration,
    Elasticsearch serves as the storage, Logstash serves as the log parser, and Kibana
    serves as the visualization frontend of the system where users interact with the
    system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 17.8 – A comparison of the ELK stack versus Amazon''s managed Elasticsearch
    service'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_17.8_B17405.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17.8 – A comparison of the ELK stack versus Amazon's managed Elasticsearch
    service
  prefs: []
  type: TYPE_NORMAL
- en: With Amazon's managed Elasticsearch service, there is no Logstash installed
    by default; however, there are other options to get the logs that you generate
    into your Elasticsearch cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Use cases for managed Elasticsearch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several use cases for using the managed Elasticsearch product from
    AWS. Let's examine them next.
  prefs: []
  type: TYPE_NORMAL
- en: Store and search logs for application monitoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can stream logs that have been placed into AWS CloudWatch Logs into Amazon's
    managed Elasticsearch service. Once the logs have been put into the Elasticsearch
    cluster, they can be searched by the Lucene-backed Elasticsearch search engine.
  prefs: []
  type: TYPE_NORMAL
- en: Security Information and Event Management (SIEM)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Storing logs from multiple events and applications across your network in a
    central system allows you to detect and alert on security events in near real
    time, using the capabilities of Amazon's managed Elasticsearch service.
  prefs: []
  type: TYPE_NORMAL
- en: An enterprise-grade search engine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although the ELK stack is most synonymous with collecting and displaying logs,
    Elasticsearch is a powerful search engine, built using the Lucene library, which
    allows for performing searching in near real time. You can connect to Elasticsearch
    using a RESTful API to send results back to your application or deliver new data
    to store in your search engine.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring your infrastructure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you collect logs from the different pieces of infrastructure you manage,
    whether they reside in the cloud or on-premises, you can collect them in a single
    solution using managed Elasticsearch from AWS. This can help you quickly research
    any issues that arise from all angles, helping to cut down your **Mean Time To
    Resolution** (**MTTR**).
  prefs: []
  type: TYPE_NORMAL
- en: Please note that Amazon's Elasticsearch service is being renamed to Amazon OpenSearch.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the use cases for the Elasticsearch service, let's see
    how we can get our CloudWatch logs into an Elasticsearch cluster with a hands-on
    example.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming logs from CloudWatch Logs to the Elasticsearch service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the hands-on exercise with the managed Elasticsearch service, we will deploy
    a simple Lambda function so that we can generate some logs. We can then stand
    up a single-node Elasticsearch cluster to receive the logs. Use the Lambda function''s
    logs that go into the CloudWatch Logs log group. We will then subscribe that log
    group to the Elasticsearch cluster we just created. The CloudFormation script
    also includes an AWS Events rule to fire off the Lambda function every five minutes
    so that logs will be periodically sent to our CloudWatch Logs group. Finally,
    we will go to the Kibana visualization interface and look for our logs. Let''s
    get started:'
  prefs: []
  type: TYPE_NORMAL
- en: We will first need to download the CloudFormation template named `lambda_stack.yml`
    from the `Chapter-17` folder in the GitHub repository for the book.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start by logging into the AWS Management Console. Once inside the console, navigate
    to the **CloudFormation** service so that we can quickly get our Lambda function
    up and running. Create a stack with new resources by either pressing the orange
    **Create stack** button from the main service page or, if you are already on the
    **Stacks** page with previous stacks you have created, the white **Create Stack**
    button on the top right-hand side.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once on the `lambda_stack.yml` file that you had downloaded from the `Chapter-17`
    folder in the GitHub repository. Once the file has been uploaded, click the orange
    **Next** button at the bottom of the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that you're on the `Logging-Lambda` as the name for this CloudFormation
    stack in the text box for **Stack name**. After this is entered, click the orange
    **Next** button at the bottom of the screen:![Figure 17.9 – Entering the name
    of the CloudFormation stack
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_17.9_B17405.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 17.9 – Entering the name of the CloudFormation stack
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There is nothing to do on the **Configure stack options** page. Scroll all the
    way down to the bottom of the page and click the orange **Next** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Review** page, scroll down to the bottom of the page and click the
    checkbox, acknowledging that this template needs to create an IAM role. After
    you have done this, you can click the orange **Create stack** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It should take 1 to 5 minutes for our stack to completely create our resources;
    after it has, we can click on the `LambdaLogGenerator`. This will be our actual
    Lambda function. Click on the blue link next to this name, which will be in the
    column named **Physical ID**. This will open up a new window directly to our Lambda
    function:![Figure 17.10 – The Logical ID and Physical ID of the created Lambda
    function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_17.10_B17405.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 17.10 – The Logical ID and Physical ID of the created Lambda function
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We need to wait at least 5 minutes for our Lambda function to be invoked, so
    in the meantime, we will create our Elasticsearch cluster so that we can stream
    the logs when they are ready. In the top search box of the AWS Management Console,
    search for the term `Elasticsearch`. When you see the **Elasticsearch Service**
    icon, right-click on the icon to open up the service in a new tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you come to the **Amazon Elasticsearch Service** page, click the blue button
    that is labeled **Create a new domain**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For **Deployment type**, choose **Development and testing**, as we only need
    a single availability zone. Under the heading of **Version**, choose the latest
    version. At the time of writing, the latest version available was 7.10\. After
    you have made your selections, click the blue **Next** button at the bottom of
    the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We should now be on the `chapter-17` for the domain name. The second setting
    will be under the heading of `T3.medium.elasticsearch` instances, since we are
    only performing a short test and not storing much data. Once you have made this
    change, scroll down to the bottom of the page and click the blue **Next** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the `devops`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Chapter17*`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'd. Under **Access policy**, select the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Keep all other settings the same, and then click the blue **Next** button at
    the bottom of the page. Click the blue **Next** button on the **Tags** page, and
    finally scroll down to the bottom of the **Review** page and click the blue **Confirm**
    button.
  prefs: []
  type: TYPE_NORMAL
- en: It will take a few minutes for our Elasticsearch cluster to spin up, and this
    will give us time to go back to our Lambda function. Go back to the other tab
    in our browser window where we previously had our Lambda function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once on the `Logging-Lambda-LambdaLogGenerator` function, click the **Monitor**
    item in the horizontal menu. This will not only allow us to see a line where our
    Lambda function has been invocated but there will also be a white button near
    that horizontal menu, now labeled **View logs in CloudWatch**. Click on this button
    to be taken directly to the logs:![Figure 17.11 – The View logs in CloudWatch
    button directly under the horizontal menu
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_17.11_B17405.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 17.11 – The View logs in CloudWatch button directly under the horizontal
    menu
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We should now be in the CloudWatch Logs group for our Lambda function. Above
    the heading of **Log streams** will be a horizontal menu. Click on the menu item
    labeled **Subscription filters**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the heading of `chapter-17` that we had just created from the drop-down
    list. For `Logging-Lambda` and select the role we created for our logging Lambda
    function. Under `log-test`. Finally, scroll to the bottom of the page and click
    the orange **Start streaming** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you click the **Start streaming** button, a Lambda function will be created.
    Now our logs should start streaming into our Elasticsearch cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's now go back to the tab where we had created our Elasticsearch cluster
    and click on the `chapter-17` as a linking domain. Click on `devops`) and password
    that was created when we provisioned our Elasticsearch cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are now in the Kibana interface for our Elasticsearch cluster. Click on the
    text that says **Explore on my own**. When the **Select your tenant** dialog box
    appears, just click the blue **Confirm** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have looked at how to capture logs and store them on the managed
    Elasticsearch cluster, let's now look at how we can incorporate the Amazon Kinesis
    service with Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Amazon Kinesis service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a service in AWS that has been created especially for the real-time
    processing of streaming data. That service is **Amazon Kinesis**. As more and
    more items in the world produce data, and more and more applications want to consume
    that data, there need to be services that can quickly consume and do some pre-processing
    on that data. The Kinesis service also provides a bit of redundancy in case your
    main application goes down, storing the records on its shards. By default, the
    records can be accessed for 24 hours from when they were written. This can also
    be extended in the settings to save the data for up to 7 days. Data that is sent
    to Kinesis can be a maximum of 1 MB in size.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key features of Amazon Kinesis to understand for the test are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It allows for real-time ingesting, processing, and streaming of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is a fully managed service, and hence there is no infrastructure for you
    to manage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It integrates with a number of other AWS services, such as Amazon Redshift.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowing that the service comes with these built-in benefits, it also has evolved
    since its initial introduction to meet the needs of the customers who have been
    using it. Although it is useful for processing immense amounts of incoming data,
    such as logs from multiple sources, this is only one of its many uses.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Kinesis service comes to us from AWS with four separate capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kinesis Video Streams** – This capability allows you to easily process incoming
    video data securely and allows for analytics, machine learning, or other processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kinesis Data Streams** – This capability allows you to stream data from applications
    to the managed Kinesis service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kinesis Data Firehose** – This capability gives users a simple way to capture,
    transform, and load data streams into AWS data stores, such as S3, ElasticSearch,
    and Redshift.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kinesis Data Analytics** – This capability allows users to easily process
    data streams in real time, using SQL or Apache Flink.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are scenarios when you would use more than one of the capabilities at
    the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Tip for the Test
  prefs: []
  type: TYPE_NORMAL
- en: While you are required to know the Amazon Kinesis service in much greater detail
    when taking the Data Analytics – Specialty certification, there are cases where
    scenarios are presented in the DevOps professional exam where knowing when to
    use the Kinesis service would be the correct solution (or incorrect solution).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of the Amazon Kinesis service, let's
    look at when it would be appropriate to use the Amazon Kinesis service to process
    our logs in an enterprise-type scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Using Amazon Kinesis to process logs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kinesis Firehose allows us to add an automatic call to an Amazon Lambda function
    that can transform our records before insertion into the Elasticsearch cluster.
    This works very similarly to the way a Logstash instance in an ELK stack would
    take incoming logs and transform them before sending them off to an Elasticsearch
    instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 17.12 – Kinesis Firehose inserting logs into S3 and the Elasticsearch
    service simultaneously'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_17.12_B17405.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17.12 – Kinesis Firehose inserting logs into S3 and the Elasticsearch
    service simultaneously
  prefs: []
  type: TYPE_NORMAL
- en: One of the reasons that there is an S3 bucket in the diagram is the fact that
    any failed records can be retried for a specified period by Kinesis Firehose to
    be resent to the Elasticsearch service if there is a failure during delivery.
  prefs: []
  type: TYPE_NORMAL
- en: What types of things could cause failure? Well, running out of space in your
    Elasticsearch cluster is one that comes to mind. If you are constantly streaming
    logs and do not have the correct way to phase out older data or have not added
    enough nodes to your cluster, there will come a point where your cluster will
    run out of space and can no longer accept any new data, or logs in our case. Rather
    than lose that information or have to manually try and insert it into the cluster,
    Kinesis can queue the misfires to an S3 bucket and then try to resend the data
    at a later time.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an understanding of how Amazon Kinesis can be used to enhance
    our logging setup, let's take a look at the importance of properly tagging and
    categorizing our logs.
  prefs: []
  type: TYPE_NORMAL
- en: Using tagging and metadata to properly categorize logs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When categorizing your cloud resources and subsequent logs for ingestion, tags
    can help classify where the resources are coming from, especially in the case
    where you are pushing all of the logs to a larger enterprise-logging solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you create your assets in AWS as a seasoned DevOps engineer, you should
    have a number of tags that you place on your resources as they come out of your
    CI/CD pipeline so that you know how to manage them effectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 17.13 – Examples of tag categories and keys for use within an enterprise
    system'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_17.13_B17405.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17.13 – Examples of tag categories and keys for use within an enterprise
    system
  prefs: []
  type: TYPE_NORMAL
- en: When we talked about processing in the previous example with the Lambda function
    using Kinesis, we could look at the tags contained in the metadata.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning up resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once again, we have created a number of resources in our AWS account during
    the course of the hands-on exercises of this chapter. If not taken down, items
    such as the managed ElasticSearch cluster could bring about a larger bill than
    you might like. The Lambda function, which fires every 5 minutes, could eat into
    your Free Tier allowance, as there are on average 43,800 minutes in a month, so
    this function would be invocated 8,760 times. Simply delete the resources that
    you are no longer testing with and delete any CloudFormation stacks that you are
    no longer using to be sure that your AWS bill stays as low as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Also, remember to cancel your QuickSight subscription, whether it was a free
    trial or a single-month subscription, so that you do not get a recurring charge
    for the QuickSight service.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have discussed how to implement enterprise-grade logging
    systems. We had a look at other native solutions offered by AWS that are more
    in tune with capturing, processing, storing, and visualizing massive amounts of
    logs streaming in constantly.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will begin looking at how to ensure that our workloads
    are highly available, starting with Auto Scaling groups and their life cycles.
  prefs: []
  type: TYPE_NORMAL
- en: Review questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Your company has asked you to create a visual dashboard that shows logs in a
    visual format that can be accessed by the management team. The management team
    does not log on to the AWS account via IAM users, nor do they have AWS Management
    Console access. They also want data enrichment from one of the AWS Redshift databases
    and a CSV file that is created from an AWS Batch process and stored in an S3 bucket.
    How can you create an easy-to-access, secure, and dynamic dashboard for the management
    team to use?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Create a CloudWatch dashboard using all of the sources. Gather the management
    team's email addresses and send out a link for access to the dashboard.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Stream all the data into a managed ElasticSearch cluster via CloudWatch Logs.
    Create a Kibana dashboard that will display the required visualizations. Share
    the link to the Kibana dashboard with the executive team.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. Use Kinesis Firehose to stream the data from Redshift to a managed Elasticsearch
    cluster. Create a Kibana dashboard that will display the required visualizations.
    Share the link to the Kibana dashboard with the executive team.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d. Use Amazon Athena to create a temporary table of all the logs that are required
    to be used in the QuickSight dashboard. Import the CSV file into QuickSight for
    visualization purposes. Import the Redshift database from QuickSight as a data
    source. Gather the management team's email addresses and send out a link for access
    to the dashboard.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You have recently joined a company that has been storing multiple different
    log files in S3 buckets. These logs have come from a variety of sources, including
    being pushed from on-premises servers using S3 sync, Application Load Balancer
    logs, VPC Flow Logs, and others. The company would like you to quickly do an analysis
    of the logs and find out how old the logs are from each category. How could you
    perform this task quickly and cost-effectively?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Set the S3 bucket, which contains the logs as a data source in Amazon QuickSight.
    Use the SPICE engine to analyze the logs. Create two quick visualizations, one
    showing the age of the logs and the other the types of logs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Use the S3 inventory function to search through the files in the S3 bucket.
    Import the report into Excel and then sort the files by age and the types of logs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. Create an AWS Glue job catalog of all the items in the S3 buckets. Use Amazon
    Athena to be able to query the types of logs. Create a query to group the types
    of logs. Sort by date descending to show the oldest logs first in each group.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d. Import all the logs into a managed Elasticsearch cluster. Using the Kibana
    interface, run a query on the types of logs and then group the logs by age to
    get a count of the number of logs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Review answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
