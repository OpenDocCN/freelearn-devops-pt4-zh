<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer313">
			<h1 id="_idParaDest-504"><a id="_idTextAnchor516"/>Chapter 24: Practice Exam 1</h1>
			<p>This section will have sample questions that are similar to the test so that you can become familiar with how to read and answer questions in the allotted time. </p>
			<p>In this chapter, we're going to cover the following main topics:  </p>
			<ul>
				<li>SDLC automation</li>
				<li>Configuration management and infrastructure as code</li>
				<li>Monitoring and logging </li>
				<li>Policies and standards automation</li>
				<li>Incident and event response </li>
				<li>High availability, fault tolerance, and disaster recovery</li>
			</ul>
			<p>Before you start to answer the following questions, it is suggested that you use a timer of some kind to keep track of how long it is taking you to answer each question. None of the questions are actual AWS test questions. They are, however, made to be in the format that the test appears in. This includes long scenario-type questions along with multiple-choice answers that are very similar. Small details can mean the difference between the correct answer and one of the incorrect answers. </p>
			<p>Both the questions and answers are long. Many of the answers have the same components in them as well, which makes this different than some of the associate tests where you can just eliminate a few of the answers that you know are wrong and then pick the answer from the one or two that are left. </p>
			<h1 id="_idParaDest-505"><a id="_idTextAnchor517"/>Test questions</h1>
			<ol>
				<li>You have developed a set of CloudFormation templates that can be used by your current company to deploy its middleware solution for processing files. This application is deployed to a number of EC2 spot instances, and there is a user data component of CloudFormation that downloads a number of scripts, including an initialization shell script from an S3 bucket to help configure those instances. The S3 bucket that holds the scripts has versioning enabled. The only known copy of the scripts at the company is located in that S3 bucket. A new intern accidentally deletes the script when trying to upload a different static asset to the bucket. What is the quickest way to restore the script so that the servers may be redeployed using the CloudFormation templates?<p>a. You will need to recreate the script and upload it with the same name to the S3 bucket. With file versioning enabled, you can only see the differences between files, not recovered deleted files. </p><p>b. You will need to make a modification to the CloudFormation scripts so that they use the previous version ID of the script, which got deleted in the S3 bucket. </p><p>c. You will need to go into the AWS Management Console, navigate to the S3 bucket that holds your deployment scripts, and then choose the List versions option. You can then find the script that was deleted and remove the delete marker. </p><p>d. You will need to go into the AWS Management Console, navigate to the S3 bucket that holds your deployment script, and then choose the List versions option. You can then download the previous version of the script, which was deleted. After the script has been downloaded, you can then upload it back up to the bucket and rename it the original script name so that the EC2 instances can find it from the CloudFormation template. </p></li>
				<li>You are part of a team that has recently developed a Spring Boot application and is ready to deploy it to the AWS cloud. Because the traffic to the application varies, you have configured the application to be launched in an Auto Scaling group. It is important to make sure that the application is running, so you have created a Bash script that will run on each EC2 instance and check the application's health periodically. If the instance is unhealthy, then it should be marked as such, and that instance should be replaced by a new, healthy one. What is the best way to construct the Bash script to perform this task?<p>a. Construct the script to reboot the instance if the health check fails. Run the script as the root user. Once the Auto Scaling group detects the reboot, it will terminate the instance and then create a new instance in its place.</p><p>b. Construct the script to use the AWS CLI. Have the instance use the <strong class="source-inline">autoscaling set-instance-health</strong> command, letting the Auto Scaling group know that the instance is unhealthy. The Auto Scaling group will then terminate the instance and create a new one in its place. </p><p>c. Construct the script to use the AWS CLI. Have the instance use the <strong class="source-inline">autoscaling put-notification-configuration</strong> command to notify the Auto Scaling group that the instance is unhealthy. The Auto Scaling group will then terminate the instance and create a new one in its place. </p><p>d. Construct the script to use the AWS CLI. Have the instance use the <strong class="source-inline">autoscaling enter-standby</strong> command, letting the Auto Scaling group know that the instance is unhealthy. The Auto Scaling group will then terminate the instance and create a new one in its place. </p></li>
				<li>You have been brought into a company that has recently established configuration and tagging standards for the infrastructure resources that it is running on AWS. You have been asked to design and build a near-real-time dashboard showing the compliance standpoint that will emphasize any violations. <p>a. Define the tagging and resource requirements in Amazon Inspector. Create a Simple Notification Service (SNS) topic to be notified if anomalies are found. Have Inspector check periodically for the compliance requirements and then send a notification to the SNS topic if found. Use AWS Security Hub to quickly visualize the violations. </p><p>b. Create a customized CloudWatch metric to track all the resource and tagging standards. Use the CloudWatch service to create a dashboard to visually track any resources that do not meet the tagging standards. </p><p>c. Enable the AWS Config service and use the configuration recorder to record all resources that are created and deleted. Have the configuration changes sent to an S3 bucket. Use Amazon QuickSight to create turn the data in the S3 bucket into visual information and dashboards for analysis so non-compliant resources can be easily spotted. </p><p>D. Define multiple resource configurations in AWS Service Catalog. Use Amazon CloudWatch to monitor any compliance violations from Service Catalog. Use the CloudWatch service to create a dashboard to visually track any resources that do not meet the tagging standards.</p></li>
				<li>You have joined an application team that is moving their MySQL database from on-premises to the AWS cloud. This is a critical application, and therefore it needs more stability than the single on-premises server can give it. The application is read-intensive and has a 10/1 read-to-write ratio. Cost is the main objective, yet they do need to be aware of the project's budget. None of the current members of the team are skilled as DBAs, and all would rather focus on application development. What setup would you suggest to keep their database running should an Availability Zone fail in their current Region? <p>a. Create a MySQL RDS instance. When setting up the instance, choose the Multi-AZ feature. Once the data has been imported, then create a read replica and program the application to utilized the read replica for the heavy reads. </p><p>b. Create a MySQL RDS instance. Once the data has been imported, create a read replica in the alternate Region and program the application to utilized the read replica for the heavy reads. </p><p>c. Create a CloudFormation template that stands up an Auto Scaling group and, in the launch template, configures the MySQL server on each of the two EC2 instances in different Regions. Have the configuration make the MySQL service run in a Master-Master setup so that any of the servers could be pointed to in case of failure. </p><p>d. Create a CloudFormation template that stands up an Auto Scaling group and, in the launch template, configures the MySQL server on each of the two EC2 instances in different regions. Have a third smaller EC2 instance stood up as part of the template as a MySQL proxy so that the application servers can point to the proxy. The proxy will switch automatically between the current master and the slave. </p></li>
				<li>You have previously built a CI/CD pipeline in AWS CodePipeline for your team's application. With the current pipeline, there were automated stages for code checkout, build, test, deploy to dev, and deploy to test. You are now updating CodePipeline to add a new stage to Deploy to Prod and adding a manual approval for the product owner before the code is released into the production environment. On the initial test of the new stage, the product owner informed you that after logging into the AWS Management Console to the CodePipeline pipeline with their IAM user, they found that she did not have permission to approve the build. How can you remedy this issue?<p>a. Create a new SNS topic for the Deploy to Prod stage. Add the product owner to the topic. Have the product owner click the approval link in the SNS message when it is sent. </p><p>b. From the AWS Management Console, go to the specific CodePipeline and add the product owner's IAM user as an approver for that pipeline.</p><p>c. From the AWS Management Console, go to the IAM service. Create a new group called <strong class="source-inline">CodePipeline Approvers</strong>. Attach the <strong class="source-inline">AWSCodePipelineFullAccess</strong> managed IAM policy to the group. Add the product owner to the group. </p><p>d. From the AWS Management Console, go to the IAM service. Create a new group called <strong class="source-inline">CodePipeline Approvers</strong>. Attach the <strong class="source-inline">AWSCodePipelineApproverAccess</strong> managed IAM policy to the group. Add the product owner to the group.  </p></li>
				<li>An organization has moved its code versioning system for its developers to AWS CodeCommit. Teams of developers create and work on feature branches while testing and then create a pull request when ready to merge into the main branch. The organization has set guidelines that no one should be able to directly commit to the main branch. Any team member who is a developer is part of the IAM group developers. This group was recently modified to add the <strong class="source-inline">AWSCodeCommitPowerUser</strong> managed policy and now all members of this group are able to commit to the main branch of any repository in the organization's AWS CodeCommit. What steps need to be taken to prevent this and enforce the organization's guidelines?<p>a. Create an added IAM policy that would allow the <strong class="source-inline">codecommit:GitPush</strong> action. Add a condition to the policy that specifies the CodeCommit repositories in the resource statement.</p><p>b. Create an added IAM policy that would deny the <strong class="source-inline">codecommit:GitPush</strong> action. Add a condition to the policy that specifies the CodeCommit repositories in the resource statement.</p><p>c. Alter the IAM policy and remove the <strong class="source-inline">codecommit:GitPush</strong> action.</p><p>d. Alter the IAM policy and add a deny rule for <strong class="source-inline">codecommit:GitPush</strong> that specifies the specific repositories where push is not allowed. </p></li>
				<li>You have configured AWS CodeDeploy to automate deployments to both EC2 instances in your development and test environments located in your AWS account, along with a few RHEL servers that are still located on-premises. There is a deployment group configured that defines each of the specific instances that are included for deployment. There has been an announcement stating that there will be a hardware refresh on-premises for one of the instances in the deployment group, which will take 2 weeks to complete. During this period, no new deployments should be pushed to these instances. Which method is the most suitable for enacting this 2-week freeze for the specific on-premises server?<p>a. Create a new deployment group with a tag that is only used by the user.</p><p>b. Verify the tags used by both the deployment groups. Use the AWS CLI to remove the specific instance that will be in service with the <strong class="source-inline">aws deploy</strong> <strong class="source-inline">remove-tags-from-on-premises-instances</strong> command.</p><p>c. Use the AWS CLI to deregister the on-premises instance from the CodeDeploy deployment using <strong class="source-inline">deploy deregister</strong>. </p><p>d. Use the AWS CLI to uninstall the CodeDeploy agent from the on-premises instance using <strong class="source-inline">deploy uninstall</strong>. </p></li>
				<li>A DevOps engineer on your team has submitted the following <strong class="source-inline">buildspec.yaml</strong> file for security review. The security review failed. You have been tasked with helping the junior engineer to review the file:<p class="figure-caption">  Figure 24.1 –<a id="_idTextAnchor518"/> The security review file</p><div id="_idContainer312" class="IMG---Figure"><img src="Images/Figure_24.1_B17405.jpg" alt="Figure 24.1 – The security review file&#13;&#10;" width="1218" height="700"/></div><p>What changes would you recommend that the junior DevOps engineer make so that the <strong class="source-inline">buildspec</strong> file complies with security best practices (choose three)? </p><p>a. Add permissions to the CodeBuild role so that the necessary actions can be performed during the build process. Remove the access key and secret key from the file. </p><p>b. Use KMS encryption for the environment variables so that they don't appear in plaintext on the file. </p><p>c. Write all environment variables to a file. Store the file in S3 and pull the file down at execution time so that the variables don't appear in plaintext on the file.</p><p>d. Use AWS Secrets Manager to store the <strong class="source-inline">DB_PASSWORD</strong> value. Remove the DB value from the environment values once stored and then retrieve them when needed.  </p><p>e. Create a run command in Systems Manager that would perform the commands. Use System Manager instead of SSH and SCP directly from the instance. </p></li>
				<li>You have been brought into a company that is expanding its presence on the AWS cloud. They want to build out their footprint using CloudFormation. However, they would like to use common components and patterns throughout their various applications. Many of the underlying components such as the infrastructure and networking will not be modified frequently after being generated. The company would like to manage all of the common component items independently and allow other application stacks to reuse the components when they need to. How can you achieve this objective?<p>a. Create a CloudFormation stack to hold all of the common resources. Other CloudFormation stacks will be able to use its resources by importing the resources from the AWS Management Console. </p><p>b. Create a CloudFormation stack to generate all of the common resources. Export the output values so that other CloudFormation stacks can import the values using the <strong class="source-inline">GetAttribute</strong> function. </p><p>c. Construct a CloudFormation stack to generate all of the common resources. Export the output values so that other CloudFormation stacks can import the values using the <strong class="source-inline">ImportValue</strong> function. </p><p>d. Create a CloudFormation stack to generate all of the common resources. Any application stack can be created as a nested stack from this stack to use all of the common resources.</p></li>
				<li>A company has just launched a new booking service that has both a website and a mobile application. Thanks in part to the marketing team's effort, the service has been a smash hit with customers that have tried it and it keeps growing in popularity. The CTO has implemented a new directive for the upcoming quarter to make the application as efficient as possible and make any necessary tweaks to the performance. In order to achieve this, the development team will need to monitor all the different details of the application to see the root causes of any issues, errors, and latency issues. How can they achieve this using native AWS tools and services?<p>a. Configure Amazon Inspector to view the application. Periodically read the Inspector assessment reports for any latency issues found along with errors. Use timestamps to trace logs in CloudWatch Logs. </p><p>b. Configure Amazon Elasticsearch to subscribe to the CloudWatch log groups for the application. Use Kibana to graph out the latency times from user click to application response. Create a custom Kibana visualization to count the number of errors. </p><p>c. Configure a custom metric in Amazon CloudWatch to track latency. Create a dashboard on CloudWatch to track the metric. </p><p>d. Configure the AWS X-Ray SDK for the application. Send the segments to X-Ray for processing. View the service graphs and traces in the X-Ray service console. </p></li>
				<li> You have set up AWS CodeCommit for your company to use as the code versioning service. The main application team is developing a mobile phone app and submits the source code in one CodeCommit repository (repository A) in the AWS account (account A). The company has just acquired another company that had its own AWS account (account B) and some of the developers have been tasked to help the development of a new feature of the mobile phone app. What actions should you take to configure cross-account access so that the new developers who have accounts and IAM users in account B have the ability to access repository A in account A (choose two)? <p>a. Go to the AWS CodeCommit service in the AWS Management Console in account A. Share repository A with account B so that users in account B will have access to the repository. </p><p>b. Go to the AWS CodeCommit service in the AWS Management Console in account A. Add the IAM users of account B to the repository as users. </p><p>c. In AWS account A, go to the IAM console and create an IAM policy that allows access to CodeCommit's repository A. Then create an IAM role that can be assumed by another account and attach this policy. Allow users in account B to assume this role. </p><p>d. In AWS account B, go to the IAM console and create an IAM policy that allows full access to the CodeCommit service and is connected to the repository A ARN resource. Attach this new policy to all users that need access to repository A in AWS account A. </p><p>e. In AWS account B, create an IAM policy that allows for Security Token Service to assume role action so that a cross-account role can be assumed. Attach this new policy to all users that need access to repository A in AWS account A.</p></li>
				<li>The company you are working for has just undergone an audit. The corrections that came back included the need to retain and store all system logs for 6 years. The development and operations teams need 30–60 days' worth of logs for troubleshooting purposes. The marketing department needs at least 6 months of web traffic logs for their analytics analysis. The management wants to make sure that the solution you come up with is cost-effective as well as meeting the auditor's requirements. How can you satisfy both management and the auditor's needs?<p>a. Put the logs onto an EBS volume. Create monthly EBS snapshots for long-term storage of the logs after 60 days.  </p><p>b. Put the logs into Amazon S3 Glacier.</p><p>c. Put the logs into the Amazon CloudWatch Logs service and set the retention policy on the log groups to 6 years. </p><p>d. Put the logs into an Amazon S3 bucket. Create a bucket policy that moves the logs to infrequent access after 60 days and then to Amazon Glacier after 1 year. </p></li>
				<li>Your company runs a .NET application in AWS that relies on around 50 Windows servers for the underlying infrastructure. The company has a policy that all of the servers in the development, test, and production environments must be kept up to date with the latest security patches. These Window servers have all been built from a master AMI image. The DevOps team that is responsible for the patching of the instances only consists of yourself and one other team member, so creating an automated way to perform this process is imperative; otherwise, you will be pressed to complete all of the updates in the allotted time window of 1 A.M.–4 A.M. on Saturday morning when there is very little customer traffic. How can you automate this process using native AWS services?<p>a. Create a Lambda function that can download and run the updates in PowerShell. Schedule the Lambda to run every week at 1 A.M. using CloudWatch Events.</p><p>b. Apply AWS Systems Manager Patch Manager to the Windows instance fleet. Use System Manager run commands to install the updates.</p><p>c. Apply AWS System Manager Patch Manager to the Windows instance fleet. Use System Manager Maintenance Windows to schedule the updates to run every week at 1 A.M.  </p><p>d. Create custom Chef scripts in OpsWorks to download and install the updates in PowerShell. Create a task in OpsWorks to schedule and run the updates every week at 1 A.M.  </p></li>
				<li>The CTO has recently approached you, concerned about the security of the company's AWS account. They would like you to implement monitoring for any possible attacks that could be coming in against the company's AWS resources. They specifically emphasized monitoring against port scans, brute force attacks, or any SSH attacks. If an attack was detected, they would like it posted to the company's Microsoft Teams security channel. How can you go about achieving this?<p>a. Set up Amazon GuardDuty. If suspicious activity is detected, trigger a Lambda function that will post to the Microsoft Teams channel.</p><p>b. Create a Lambda function that will scan the CloudTrail logs for suspicious activity. If suspicious activity is found, it will post it to the Microsoft Teams channel. </p><p>c. Set up Amazon Inspector. If suspicious activity is detected, trigger a Lambda function that will post to the Microsoft Teams channel. </p><p>d. Create a Lambda function that will scan the VPC flow logs. If suspicious activity is found, it will post it to the Microsoft Teams channel.</p></li>
				<li>The CEO has visited the DevOps team personally, stating that the company is promising six 9s of uptime to customers or the company would be giving large refunds. This leaves you and your team only 31.56 % of downtime per year. The main workload comprises multiple EC2 instances that are configured in Auto Scaling groups, running behind Application Load Balancers. How can you configure the workload to ensure that the company maintains that uptime promise even in the case of regional failure?<p>a. Configure Amazon CloudFront in front of the load balancers and instances. CloudFront will cache the workload for customers in case of failure.</p><p>b. Set up a Route 53 geoproximity routing record. Make sure that the Auto Scaling groups are set to utilize two Availability Zones. Have the Route 53 routing record point to the Application Load Balancers. </p><p>c. Set up a Route 53 weighted routing record. Make sure that the Auto Scaling groups are set to utilize two Availability Zones. Have the Route 53 routing record point to the Application Load Balancers.</p><p>d. Set up a Route 53 latency routing record. Implement your workload EC2 instances, Auto Scaling groups, and load balancers in two different regions. Have the Route 53 routing record point to the Application Load Balancers. </p></li>
				<li>Your company wants to implement the Apache Cassandra NoSQL database on AWS. There is no managed service for Cassandra, so you will have to build this on EC2 instances. Your team is looking for you to choose the correct type of EBS volume so that they get the optimum performance for this high-performance NoSQL database. Which type of volume should you choose when building the EC2 instances in the cluster?<p>a. Use IO1 EBS volumes when creating the instances.</p><p>b. Use standard EBS volumes when creating the instances. </p><p>c. Use GPL EBS volumes when creating the instances.</p><p>d. Use GP2 EBS volumes when creating the instances.</p></li>
				<li> You have been brought into an organization to help create a new AWS CodePipeline pipeline so that the team can implement continuous integration. The pipeline needs to pull the source code and then have a test stage run by AWS CodeBuild. The test includes extracting data from a database that requires a username and password. You will need to put these in the test stage using environment variables. How can you securely configure these variables in the CodeBuild stage of the pipeline? <p>a. Use the CodeBuild environment variable options to store the secrets. Select the <strong class="source-inline">Plaintext</strong> type when storing the values and use a KMS key to encrypt the values. </p><p>b. Use the CodeBuild environment variable options to store the secrets. Select the <strong class="source-inline">SecureString</strong> type when storing the values. </p><p>c. Use AWS Secrets Manager to store the values. Update your CodeBuild environment values for the variables and use the names of the secrets as the values in <strong class="source-inline">Plaintext</strong>. </p><p>d. Use AWS Systems Manager Parameter Store to store the values. Update your CodeBuild environment values for the variables and use the parameter names as the values as <strong class="source-inline">Parameter</strong> types.  </p></li>
				<li>You have been asked to help a team that is using OpsWorks to enhance the monitoring of their stack. This team is rather skilled in developing automations using Chef but just seems to know the basics to run their services in AWS. Which of the following will not help them enhance the monitoring of their application being deployed in OpsWorks?<p>a. Utilize Amazon CloudWatch metrics and create a custom metric to track the application.</p><p>b. Utilize Amazon Cloud Trail to make sure that only authorized calls are being made to the application. </p><p>c. Utilize Amazon CloudWatch Logs to gather the logs from the application.</p><p>d. Utilize AWS Config to gather the application's configuration changes.</p></li>
				<li>You have been brought into a small start-up whose marketing website is nothing but static content. The marketing department has been complaining that load times are currently too long, and this is affecting their search engine rating. They have only a limited budget for upgrades and would like to make the most effective use of their money. The start-up is also in the process of moving all digital assets to their new AWS account. They also want to make their site as fast as possible. Which of the following suggestions would best meet their needs?<p>a. Serve their website using an EC2 server. Add Amazon CloudFront in front as the content delivery network.</p><p>b. Move all static assets to S3. Serve the website on EC2 spot instances. Add Amazon CloudFront in front as the content delivery network. </p><p>c. Move the entire website to S3. Add Amazon CloudFront in front as the content delivery network. </p><p>d. Serve their website on EC2 spot instances. Add Amazon ElastiCache for content caching to speed up page load times. </p></li>
				<li>An organization is running a successful room booking mobile application on AWS. They are using DynamoDB to store all the records of the transactions and confirmation codes obtained from credit card companies once the reservations have been made. They chose DynamoDB for its ability to quickly autoscale and handle any type of capacity that is needed without much management. These transactional records are of vital importance to the company and must not be lost lost due to any server failures. The organization has a policy that the financial transactions must be stored for 3 years in case of any customer disputes. What is the most cost-effective and reliable way to accomplish this?<p>a. Use CloudWatch Logs to capture the records from DynamoDB. Set the retention period in CloudWatch Logs to 3 years. </p><p>b. Use DynamoDB Streams to stream transition records to a Lambda function. Have the Lambda function write the record to an S3 bucket. Use a life cycle policy to move objects to S3 Glacier storage for cost savings. </p><p>c. Use DynamoDB global tables to replicate the data to a secondary region. Create a Lambda function that trims records based on the creation date of the record minus 3 years and runs on a nightly basis. </p><p>d. Use DynamoDB Streams to stream transition records to an S3 bucket. Use a life cycle policy to move objects to S3 Glacier storage for cost savings. </p></li>
				<li>A company has set up multiple accounts using AWS organizations. They have just started implementing event-driven automation and are taking the first steps to have notifications being sent from the CloudWatch event bus located in the master account via SNS to topics. How can they set up their master account and grant access to all child accounts so that the events can be sent to the event bus located in the master account?<p>a. Create an IAM policy that allows for sending CloudWatch events. Attach that policy to a role in the master account that can be assumed by all child accounts in the organization. </p><p>b. Create an IAM policy in each of the child accounts that allows for sending CloudWatch events and specifies the ARN of the event bus in the master account. Attach this policy for any service-based role that needs to send events to the master account. </p><p>c. In the master account, go to the CloudWatch Events console and then choose your event bus. Go to add permission and then choose to add the entire organization by entering the ID of the organization.  </p><p>d. In the master account, go to the CloudWatch Events console and then choose your event bus. Go to add permission and then add the ID for each of the child accounts that need permission to send events to the event bus. </p></li>
				<li>You are working with a development team that is trying to track the performance of an application that they build and are running on a group of EC2 instances. This team is especially interested in any error messages that are being generated from their Java code and would like the full team to be notified if more than 5 error messages occur in a 5-minute time period. Which of the following solutions could you implement to fulfill the team's requirements?<p>a. Configure the instances to have all the Java logs write to the syslog on the EC2 instances in the user-data script. Use Kinesis Data Firehose to pull the syslog data for the instances and count the number of error messages. Create an SNS topic for the group of developers. Have Kinesis send a notification to the topic if there are more than 5 error logs in a 5-minute period. </p><p>b. Configure the instances to write to a single log group in Amazon CloudWatch Logs. Use Amazon Elasticsearch to subscribe to the log group. Build a Kibana dashboard so that the developers can see how many error logs are being generated on a minute-by-minute basis. </p><p>c. Configure the instance to install AWS Systems Manager Agent. Have the agent pull the logs to Amazon EventBridge. Create an SNS topic for the group of developers. Create a Lambda function that will send a notification to the SNS topic if there are more than 5 error logs in a 5-minute period.</p><p>d. Configure the instance to install the unified CloudWatch agent. Create a custom metric to count the number of errors in the Java logs. Create an SNS topic for the group of developers. Push the logs and the custom metric to Amazon CloudWatch. Create a CloudWatch alarm that will send a notification to the SNS topic if the custom metric reaches a value greater than 5 in a 5-minute period. </p></li>
				<li>All of the developers in your organization currently have the ability to start and stop any of the EC2 instances that are currently running in the development account simply by logging onto the AWS Management Console and choosing to stop the instance under the Instance State settings. Some teams have been complaining that their workloads have been disrupted by other developers mistakenly stopping the wrong EC2 instances. How can you implement security measures so that only members from a particular team can start and stop their own EC2 instances using native AWS features?<p>a. For each development team in the company, create a policy that restricts the starting and stopping of instances to the <strong class="source-inline">${aws:Principal/Team}</strong> tag as the resource. </p><p>b. Add a <strong class="source-inline">Team</strong> tag to all the EC2 instances that can help restrict access by comparing the <strong class="source-inline">${aws:Principal/Team}</strong> tag attached by the individual developer to the  <strong class="source-inline">ec2:ResourceTag/Team</strong> tag on the instance and seeing whether they match. </p><p>c. Add a <strong class="source-inline">Team</strong> tag to all the EC2 instances. Restrict access to each team in the developer policy by seeing whether the EC2 instance matches the team tag.  </p><p>d. In the IAM developer role, remove the ability to start and stop instances. Create a CodePipeline job for each team that will allow them to see, start, and stop all of the instances for their development team.</p></li>
				<li>You have an Extract, Transform, and Load (ETL) application that is sending its logs to CloudWatch Logs. The logs are landing in a CloudWatch log group and are formatted in JSON. The following is a sample of the log file:<p class="source-code"><strong class="bold">{</strong></p><p class="source-code"><strong class="bold">  "eventType": "Process",</strong></p><p class="source-code"><strong class="bold">  "eventObject": "File",</strong></p><p class="source-code"><strong class="bold">  "errorCode": "CorruptFile"</strong></p><p class="source-code"><strong class="bold">    	}</strong></p></li>
				<li>How can you create a metric filter so you can find all events where the error code was <strong class="source-inline">"CorruptFile"</strong>?<p>a. Filter Pattern: <strong class="source-inline">{ $.errorCode = "CorruptFile" }</strong></p><p>b. Filter Pattern: <strong class="source-inline">{ $.errorCode == "CorruptFile" }</strong></p><p>c. Filter Pattern <strong class="source-inline">{ errorCode = "CorruptFile" }</strong> </p><p>d. Filter Pattern <strong class="source-inline">{ errorCode == "CorruptFile" }</strong>  </p></li>
				<li>Your company has requested that you create a reliable and durable logging solution for their three AWS accounts so that they can track the changes being made to their AWS resources. Which of the following options would help you successfully do this?<p>a. Create a new CloudTrail with an existing bucket to store the logs. Select the global services option when creating the trail. Use IAM roles on the S3 bucket and S3 encryption to secure the bucket.  </p><p>b. Create three new CloudTrails with a single new bucket to store the logs. One trail will be for the AWS Management Console, one will be for SDK commands, and one will be for the AWS CLI. Use multi-factor authentication for any delete actions and S3 encryption on the buckets.</p><p>c. Create a new CloudTrail with a new bucket to store the logs. Select the global services option when creating the trail. Use multi-factor authentication for any delete actions and S3 encryption on the bucket. </p><p>d. Create three new CloudTrails with three new buckets to store the logs. One trail will be for the AWS Management Console, one will be for SDK commands, and one will be for the AWS CLI. Use IAM roles on the S3 bucket and S3 encryption to secure the buckets.  </p></li>
				<li>Your company has used AWS Organizations to both create and manage their AWS account. There are multiple accounts, which include child accounts that contain organizational units that have been created using AWS Organizations. As the company grows, there is now a need to add uniform roles to each of the accounts. What is the most effective way to add all of the roles throughout the organization?<p>a. In the master account, use CloudFormation to deploy a template with which to create the new roles. Use CloudFormation StackSets to replicate the changes across the whole organization's child accounts.</p><p>b. In the master account of the organization, create a Service Control Policy (SCP), which will then add the roles to all the child accounts. </p><p>c. In the master account, use CloudFormation to deploy a template with which to create the new roles. Use CloudFormation change sets to replicate the changes across the whole organization's child accounts. </p><p>d. In the master account, create a run command in Systems Manager to create the new IAM roles. Have Systems Manager perform the command on all of the child accounts to create the new role.</p></li>
				<li>Members of your DevOps team have come to you because they have noticed a problem with one of the Auto Scaling groups that has just been updated. Instead of reaching a steady state and serving traffic, the application is constantly scaling up and down numerous times an hour. Using the native features of CloudFormation, which settings could you help your team tune in order to stabilize the application?<p>a. Examine the current Auto Scaling group termination policy and change the value to terminate the oldest instance first so that newer instances stay online. </p><p>b. Examine the current Auto Scaling group termination policy and change the value to terminate to <strong class="source-inline">ClosestToNextInstanceHour</strong> so that the instances become more stabilized. </p><p>c. Find the previous version of the Auto Scaling launch template and deploy that version to stabilize the application. </p><p>d. Examine the current Auto Scaling group Health Check grace period and expand the time currently allocated for the instances to come online and become healthy.  </p></li>
				<li>A medium-sized software company has hired you as a DevOps consultant to help set up their deployment pipeline. They want to be able to push their tested code into their production environment in a quick manner but do not want the possibility of dealing with any downtime for their customers. You have worked with the application team to configure their application to run on containers and be deployed to Amazon Elastic Container Service (ECS). Their DNS is hosted on a third-party service and changes for the DNS would require a change ticket. What deployment method would you implement?<p>a. In the service settings of ECS, set the <strong class="source-inline">minimumHealthyPercent</strong> and <strong class="source-inline">maximumHealthyPercent</strong> values of tasks before you begin your rolling update to the service. </p><p>b. Create a CodeDeploy job for your updates. Use a blue/green deployment type. Set the configuration of the blue/green deployment to all-at-once. </p><p>c. Create a CodeDeploy job for your updates. Use a blue/green deployment type. Set the configuration of the blue/green deployment to linear.</p><p>d. Create a CodeDeploy job for your updates. Use a blue/green deployment type. Set the configuration of the blue/green deployment to canary.</p></li>
				<li>Your team has recently been subject to a code audit and there were multiple findings of plaintext database usernames and passwords in the application code. This has been flagged by the company as unacceptable and the team has been given 30 days to remedy the problem. According to the company guidelines, the team needs to be able to store the secrets securely in an encrypted manner using a native AWS service that also has the ability to rotate the secret automatically every 60 days. How can you and your team remedy this issue?<p>a. Remove the database values that were previously set in the code base. Add environment variables to the deployment process. Insert the username and password as the corresponding deployment variables. </p><p>b. Remove the database values that were previously set in the code base. Store the username and values in AWS Systems Manager Parameter Store. Update your IAM role to allow access to retrieve the secret from Parameter Store. </p><p>c. Remove the database values that were previously set in the code base. Store the username and password in AWS Secrets Manager. Update your IAM role to allow access to retrieve the secret from Secrets Manager. </p><p>d. Use KMS to encrypt the values for the username and password for the database. Replace the previous values in the code with the newly encrypted values. </p></li>
				<li>Your developers have created a DynamoDB table and seem to find that the performance always slows down after 20–25 minutes of their testing process. They can see from the basic monitoring on the AWS Management Console that their requests are being throttled. What can you do to help pinpoint the issue?<p>a. Increase the Read Capacity Units (RCUs) on the table so that the queries are no longer throttled.</p><p>b. Add enhanced CloudWatch monitoring with alarms whenever throttling occurs. </p><p>c. Enable Contributor Insights on the table so that the keys that are being throttled the most are shown. </p><p>d. Add adaptive capacity to the table so that the extra RCUs are spread evenly across partitions that are becoming hot. </p></li>
			</ol>
			<h1 id="_idParaDest-506"><a id="_idTextAnchor519"/>Test answers </h1>
			<ol>
				<li value="1">c<p>Since the bucket had versioning turned on, removing the delete marker restores the object, and any current or future deployments using that script will be able to find it. </p></li>
				<li>b <p>You can use the AWS CLI and the <strong class="source-inline">autoscaling set-instance-health</strong> command along with the <strong class="source-inline">--health-status Unhealthy</strong> flag to have the instance be out of service.  </p><p>More information can be found on the documentation page for the AWS CLI at the following link: <a href="https://docs.aws.amazon.com/cli/latest/reference/autoscaling/set-instance-health.html">https://docs.aws.amazon.com/cli/latest/reference/autoscaling/set-instance-health.html</a>.</p></li>
				<li>c <p>With the configuration recorder, AWS Config can evaluate new resources being created in an account. Items are recorded as JSON snapshots to an S3 bucket declared in the setup.  </p></li>
				<li>a <p>RDS uses DNS to switch over to the standby replica for a seamless transition in a Multi-AZ implementation. </p></li>
				<li>d <p>Although the <strong class="source-inline">AWSCodePipeline_FullAccess</strong> policy would give approval access, it doesn't follow the AWS principle of least privilege. This policy would give the product owner more privileges than they need. Hence <strong class="source-inline">AWSCodePipelineApproverAccess</strong> would add the access that they were missing. </p></li>
				<li>b <p>You cannot modify a managed AWS policy, and hence this disqualifies both answers c and d. You are trying to prevent the action of users directly pushing to the master branch. </p></li>
				<li>b <p>You remove an on-premises instance tag from an on-premises instance when that tag is no longer being used, or if you want to remove the on-premises instance from any deployment groups that rely on that tag. </p></li>
				<li>a, d, and e<p>Writing all the values to a file in S3 would not guarantee their integrity. It is a much more secure practice to use roles and to have database values stored in a credential store, such as Secrets Manager or Systems Manager Parameter Store. </p></li>
				<li>c<p>The <strong class="source-inline">Fn::ImportValue</strong> intrinsic function returns the value of an exported value of a previously created stack. This is used to create cross-stack references.</p></li>
				<li>d<p>Amazon Inspector is a security service and would not find latencies in an application. The X-Ray service helps developers identify the root cause of performance issues and errors. </p></li>
				<li>c and e <p>In order to gain access to resources in another AWS account, a cross-account IAM role needs to be created so that it can be assumed by the other AWS account. Likewise, the other account's users must have a policy attached to them that will allow for the assumption of the role.</p></li>
				<li>d <p>The use of Amazon S3 life cycle policies will allow you and your team to have both immediate access to the current logs and the ability to store them on the low-cost option in Amazon Glacier as the auditor requires. </p></li>
				<li>c<p>Using AWS Systems Manager, you can use the combination of Patch Manager and Maintenance Windows to successfully automate this task in the recommended way by AWS.</p></li>
				<li>a <p>Amazon GuardDuty can detect all of the different types of events that the CTO was concerned about. Adding a Lambda function that will post to the company's Microsoft Teams channel will satisfy the request. </p></li>
				<li>d <p>Only by deploying to multiple regions can you make sure that you are protected against a regional failure. Using a latency-based record in Route 53 will automatically point to the set of servers that is responding the quickest to a request in case of a failure. </p></li>
				<li>a <p>io1 volumes are crafted for workloads that require sustained IOPS performance and I/O-intensive database workloads. </p></li>
				<li>d<p>Sensitive values should be stored in either Systems Manager Parameter Store or Secrets Manager. If Secrets Manager is used for CodeBuild, then the variable type should be selected as Secrets Manager and not <strong class="source-inline">Plaintext</strong>. </p></li>
				<li>d <p>Amazon Config is not a service used for monitoring and metrics. </p></li>
				<li>c </li>
				<li>c<p>Since the full site is made of static content, and S3 is the least expensive and reliable solution, this is the most optimum choice. Using S3 as the origin and being fronted by Amazon CloudFront will allow for assets to be served faster to the end user via edge locations. </p></li>
				<li>b <p>DynamoDB Streams cannot directly stream to S3 as a source, so a Lambda Function would need to first <strong class="source-inline">GetRecords</strong> and then put them to the specified S3 bucket with the life cycle policy. </p></li>
				<li>c <p>A CloudWatch event bus allows you to add permissions on an organizational level. This also helps if your organization grows, as you don't have to keep track of which accounts you have added to the event bus or remember to take the extra step of adding the account to the event bus permissions when it is created. </p></li>
				<li>d</li>
				<li>b <p>Using tags on EC2 instances can be the first part of differentiating the ownership of EC2 instances between teams. <strong class="source-inline">ec2:ResourceTag</strong> is a tag that exists on an EC2 resource and can be verified against an IAM policy. </p></li>
				<li>a  <p>The syntax of the metric filter would be <strong class="source-inline">{ $.errorCode = "CorruptFile" }</strong>.</p></li>
				<li>c <p>The use of the global option will send all of the API actions recorded to a single S3 bucket. Adding in MFA will prevent any unauthorized deletions of the logs. </p></li>
				<li>a<p>AWS CloudFormation StackSets extends the functionality of CloudFormation stacks, allowing you to create, update, or delete stacks across multiple accounts and regions with a single operation.</p></li>
				<li>d <p>Although you may be able to roll back with a code versioning system, adjusting the current health check of the Auto Scaling group will allow your instances to come online and become healthy. </p></li>
				<li> a <p>A rolling type of deployment would be the most optimal type of deployment, especially when the DNS is hosted on a third-party provider.   </p></li>
				<li>c <p>Although both Systems Manager Parameter Store and AWS Secrets Manager will safely secure secrets according to the new guidelines in this scenario, only Secrets Manager will automatically rotate the database secrets.</p></li>
				<li>c <p>Amazon CloudWatch Contributor Insights integrates with DynamoDB to provide information about the most accessed and throttled items in a table or global secondary index. </p></li>
			</ol>
			<h1 id="_idParaDest-507"><a id="_idTextAnchor520"/>Question breakdown</h1>
			<p>If you are interested in how you are performing in a particular domain, then we have how the sample test questions would be grouped based on the test domains, as follows:</p>
			<p><strong class="bold">Domain 1 – SDLC automation</strong>:</p>
			<ul>
				<li>Question 6</li>
				<li>Question 11 </li>
				<li>Question 17 </li>
				<li>Question 23 </li>
				<li>Question 26</li>
				<li>Question 27</li>
				<li>Question 28</li>
			</ul>
			<p><strong class="bold">Domain 2 – Configuration management and infrastructure as code:</strong></p>
			<ul>
				<li>Question 5 </li>
				<li>Question 7 </li>
				<li>Question 9 </li>
				<li>Question 16</li>
			</ul>
			<p><strong class="bold">Domain 3 – Monitoring and logging:</strong></p>
			<ul>
				<li>Question 10 </li>
				<li>Question 18 </li>
				<li>Question 21 </li>
				<li>Question 22</li>
				<li>Question 24 </li>
				<li>Question 30</li>
			</ul>
			<p><strong class="bold">Domain 4 – Policies and standards automation:</strong></p>
			<ul>
				<li>Question 3</li>
				<li>Question 8 </li>
				<li>Question 12 </li>
				<li>Question 13 </li>
			</ul>
			<p><strong class="bold">Domain 5 – Incident and event response:</strong></p>
			<ul>
				<li>Question 1 </li>
				<li>Question 2 </li>
				<li>Question 14 </li>
				<li>Question 19 </li>
			</ul>
			<p><strong class="bold">Domain 6 – High availability, fault tolerance, and disaster recovery:</strong></p>
			<ul>
				<li>Question 4 </li>
				<li>Question 15 </li>
				<li>Question 20 </li>
				<li>Question 25 </li>
			</ul>
			<p>If you are missing questions particularly in a specific area, then go back and reread those chapters, look to the end of <a href="B17405_23_Final_JM_ePub.xhtml#_idTextAnchor501"><em class="italic">Chapter 23</em></a>, <em class="italic">Overview of the DevOps Professional Certification Test</em>, for one of the AWS Whitepapers that could give you more insight into that topic, or even watch some of the past re:Invent talks or AWS TechTalk videos to gain a better understanding of the domain. </p>
			<p>Now, let's have one final summary of our journey to certification. </p>
			<h1 id="_idParaDest-508"><a id="_idTextAnchor521"/>Summary</h1>
			<p>In this chapter, you have been presented with a number of sample DevOps professional exam questions so that you can practice all of the items that you have learned up to this point, as well as reading and comprehending the question and answer format that will be on the test. </p>
			<p>Hopefully, at this point, you feel confident to take and pass the DevOps Professional certification exam. Once you pass, you will join the small subset of individuals who can be quickly recognized for their skill in not only DevOps but also AWS and cloud technologies. </p>
		</div>
	</div></body></html>