<html><head></head><body>
		<div>
			<div id="_idContainer221" class="Content">
			</div>
		</div>
		<div id="_idContainer222" class="Content">
			<h1 id="_idParaDest-244">10. <a id="_idTextAnchor249"/>Serverless in Azure – Working with Azure Functions</h1>
		</div>
		<div id="_idContainer255" class="Content">
			<p>In the previous chapter, you learned about various big data solutions available on Azure. In this chapter, you will learn how serverless technology can help you deal with a large amount of data. </p>
			<p>Serverless is one of the hottest buzzwords in technology these days, and everyone wants to ride this bandwagon. Serverless brings a lot of advantages in overall computing, software development processes, infrastructure, and technical implementation. There is a lot going on in the industry: at one end of the spectrum is <strong class="bold">Infrastructure as a Service (IaaS)</strong>, and at the other is serverless. In between the two are <strong class="bold">Platform as a Service (PaaS)</strong> and containers. I have met many developers and it seems to me that there is some confusion among them about IaaS, PaaS, containers, and serverless computing. Also, there is much confusion about use cases, applicability, architecture, and implementation for the serverless paradigm. Serverless is a new paradigm that is changing not only technology but also the culture and processes within organizations.</p>
			<p>We will begin this chapter by introducing serverless, and will cover the following topics as we progress:</p>
			<ul>
				<li>Functions as a Service</li>
				<li>Azure Functions</li>
				<li>Azure Durable Functions</li>
				<li>Azure Event Grid</li>
			</ul>
			<h2 id="_idParaDest-245"><a id="_idTextAnchor250"/>Serverless</h2>
			<p>Serverless refers to a deployment model in which users are responsible for only their application code and configuration. In serverless computing, customers do not have to bother about bringing their own underlying platform and infrastructure and, instead, can concentrate on solving their business problems.</p>
			<p>Serverless does not mean that there are no servers. Code and configuration will always need compute, storage, and networks to run. However, from the customer's perspective, there is no visibility of such compute, storage, and networks. They do not care about the underlying platform and infrastructure. They do not need to manage or monitor infrastructure and the platform. Serverless provides an environment that can scale up and down, in and out, automatically, without the customer even knowing about it. All operations related to platforms and infrastructures happen behind the scenes and are executed by the cloud provider. Customers are provided with performance-related <strong class="bold">service-level agreements (SLAs)</strong> and Azure ensures that it meets those SLAs irrespective of the total demand.</p>
			<p>Customers are required to only bring in their code; it is the responsibility of the cloud provider to provide the infrastructure and platform needed to run the code. Let's go ahead and dive into the various advantages of Azure Functions.</p>
			<h2 id="_idParaDest-246"><a id="_idTextAnchor251"/>The advantages of Azure Functions</h2>
			<p>Serverless computing is a relatively new paradigm that helps organizations convert large functionalities into smaller, discrete, on-demand functions that can be invoked and executed through automated triggers and scheduled jobs. They are also known as <strong class="bold">Functions as a Service (FaaS)</strong>, in which organizations can focus on their domain challenges instead of the underlying infrastructure and platform. FaaS also helps in devolving solution architectures into smaller, reusable functions, thereby increasing return on investment.</p>
			<p>There is a plethora of serverless compute platforms available. Some of the important ones are listed here:</p>
			<ul>
				<li>Azure Functions</li>
				<li>AWS Lambda</li>
				<li>IBM OpenWhisk</li>
				<li>Iron.io</li>
				<li>Google Cloud Functions</li>
			</ul>
			<p>In fact, every few days it feels like there is a new platform/framework being introduced, and it is becoming increasingly difficult for enterprises to decide on the framework that works best for them. Azure provides a rich serverless environment known as Azure Functions, and what follows are some of the features that it supports:</p>
			<ul>
				<li>Numerous ways to invoke a function—manually, on a schedule, or based on an event.</li>
				<li>Numerous types of binding support.</li>
				<li>The ability to run functions synchronously as well as asynchronously.</li>
				<li>The ability to execute functions based on multiple types of triggers.</li>
				<li>The ability to run both long- and short-duration functions. However, large and long-running functions are not recommended as they may lead to unexpected timeouts.</li>
				<li>The ability to use proxy features for different function architectures.</li>
				<li>Multiple usage models including consumption, as well as the App Service model.</li>
				<li>The ability to author functions using multiple languages, such as JavaScript, Python, and C#.</li>
				<li>Authorization based on OAuth.</li>
				<li>The Durable Functions extension helps in writing stateful functions.</li>
				<li>Multiple authentication options, including Azure AD, Facebook, Twitter, and other identity providers.</li>
				<li>The ability to easily configure inbound and outbound parameters.</li>
				<li>Visual Studio integration for authoring Azure functions.</li>
				<li>Massive parallelism.</li>
			</ul>
			<p>Let's take a look at FaaS and what roles it plays in serverless architecture.</p>
			<h2 id="_idParaDest-247"><a id="_idTextAnchor252"/>FaaS</h2>
			<p>Azure provides FaaS. These are serverless implementations from Azure. With Azure Functions, code can be written in any language the user is comfortable with and Azure Functions will provide a runtime to execute it. Based on the language chosen, an appropriate platform is provided for users to bring their own code. Functions are a unit of deployment and can automatically be scaled out and in. When dealing with functions, users cannot view the underlying virtual machines and platform, but Azure Functions provides a small window to view them via the <a id="_idTextAnchor253"/><strong class="bold">Kudu Console</strong>.</p>
			<p>There are two main components of Azure Functions:</p>
			<ul>
				<li>The Azure Functions runtime</li>
				<li>Azure Functions binding and triggers</li>
			</ul>
			<p>Let's learn about these components in detail.</p>
			<h3 id="_idParaDest-248"><a id="_idTextAnchor254"/>The Azure Functions runtime</h3>
			<p>The core of Azure Functions is its runtime. The precursor to Azure Functions was Azure WebJobs. The code for Azure WebJobs also forms the core for Azure Functions. There are additional features and extensions added to Azure WebJobs to create Azure Functions. The Azure Functions runtime is the magic that makes functions work. Azure Functions is hosted within Azure App Service. Azure App Service loads the Azure runtime and either waits for an external event or a manual activity to occur. On arrival of a request or the occurrence of a trigger, App Service loads the incoming payload, reads the function's <strong class="inline">function.json</strong> file to find the function's bindings and trigger, maps the incoming data to incoming parameters, and invokes the function with parameter values. Once the function completes its execution, the value is again passed back to the Azure Functions runtime by way of an outgoing parameter defined as a binding in the <strong class="inline">function.json</strong> file. The function runtime returns the values to the caller. The Azure Functions runtime acts as the glue that enables the entire performance of functions.</p>
			<p>The current Azure runtime version is ~3. It is based on the .NET Core 3.1 framework. Prior to this, version ~2 was based on the .NET Core 2.2 framework. The first version, ~1, was based on the .NET 4.7 framework.</p>
			<p>There were substantial changes from version 1 to 2 because of changes in the underlying framework itself. However, there are very few breaking changes from version 2 to 3 and most functions written in version 2 would continue to run on version 3 as well. However, it is recommended that adequate testing is done after migrating from version 2 to 3. There were also breaking changes from version 1 to 2 with regard to triggers and bindings. Triggers and bindings are now available as extensions, with each one in a different assembly in versions 2 and 3.</p>
			<h3 id="_idParaDest-249"><a id="_idTextAnchor255"/>Azure Functions bindings and triggers</h3>
			<p>If the Azure Functions runtime is the brain of Azure Functions, then Azure Functions bindings and triggers are its heart. Azure Functions promote loose coupling and high cohesion between services using triggers and bindings. Applications written targeting non-serverless environments implement code using imperative syntax for incoming and outgoing parameters and return values. Azure Functions uses a declarative mechanism to invoke functions using triggers and configures the flow of data using bindings.</p>
			<p>Binding refers to the process of creating a connection between the incoming data and the Azure function along with mapping the data types. The connection could be in a single direction from the runtime to Azure Functions and vice versa or could be multi-directional—the binding can transmit data between the Azure runtime and Azure Functions in both directions. Azure Functions defines bindings declaratively.</p>
			<p>Triggers are a special type of binding through which functions can be invoked based on external events. Apart from invoking a function, triggers also pass the incoming data, payload, and metadata to the function.</p>
			<p>Bindings are defined in the <strong class="inline">function.json</strong> file as follows:</p>
			<p class="snippet">{  "bindings": [    {      "name": "checkOut",      "type": "queueTrigger",      "direction": "in",      "queueName": "checkout-items",      "connection": "AzureWebJobsDashboard"    },    {      "name": "Orders",      "type": "table",      "direction": "out",      "tableName": "OrderDetails",      "connection": "&lt;&lt;Connection to table storage account&gt;&gt;"   }  ],  "disabled": false }</p>
			<p>In this example, a trigger is declared that invokes the function whenever there is a new item in the storage queue. The type is <strong class="inline">queueTrigger</strong>, the direction is inbound, <strong class="inline">queueName</strong> is <strong class="inline">checkout-items</strong>, and details about the target storage account connection and table name are also shown. All these values are important for the functioning of this binding. The <strong class="inline">checkOut</strong> name can be used within the function's code as a variable.</p>
			<p>Similarly, a binding for the return value is declared. Here, the return value is named <strong class="inline">Orders</strong> and the data is the output from Azure Functions. The binding writes the return data into Azure Table Storage using the connection string provided.</p>
			<p>Both bindings and triggers can be modified and authored using the <strong class="bold">Integrate</strong> tab in Azure Functions. In the backend, the <strong class="inline">function.json</strong> file is updated. The <strong class="inline">checkOut</strong> trigger is declared as shown here:</p>
			<div>
				<div id="_idContainer223" class="IMG---Figure">
					<img src="image/B15432_10_01.jpg" alt="Modifying triggers through the Integrate tab of Azure Functions"/>
				</div>
			</div>
			<h6>Figure 10.1: The Triggers section of the Integrate tab</h6>
			<p>The <strong class="bold">Orders</strong> output is shown next:</p>
			<div>
				<div id="_idContainer224" class="IMG---Figure">
					<img src="image/B15432_10_02.jpg" alt="Adding output details for the storage account through the Integrate tab"/>
				</div>
			</div>
			<h6>Figure 10.2: Adding output details for the storage account</h6>
			<p>The authors of Azure functions do not need to write any plumbing code to get data from multiple sources. They just decide the type of data expected from the Azure runtime. This is shown in the next code segment. Notice that the checkout is available as a string to the function. Multiple data types can be used as binding for functions. For example, a queue binding can provide the following:</p>
			<ul>
				<li>A plain old CLR (Common Language Runtime) object (POCO)</li>
				<li>A string</li>
				<li>A byte</li>
				<li><strong class="inline">CloudQueueMessage</strong></li>
			</ul>
			<p>The author of the function can use any one of these data types, and the Azure Functions runtime will ensure that a proper object is sent to the function as a parameter. The following is a code snippet for accepting string data and the Functions runtime will encapsulate incoming data into a <strong class="inline">string</strong> data type before invoking the function. If the runtime is unable to cast the incoming data to a string, it will generate an exception:</p>
			<p class="snippet">using System;public static void Run(string checkOut, TraceWriter log){    log.Info($"C# Queue trigger function processed: { checkOut }");}</p>
			<p>It is also important to know that, in <em class="italics">Figure 10.2</em>, the storage account names are <strong class="inline">AzureWebJobsStorage</strong> and <strong class="inline">AzureWebJobsDashboard</strong>. Both of these are keys defined in the <strong class="inline">appSettings</strong> section and contain storage account connection strings. These storage accounts are used internally by Azure Functions to maintain its state and the status of function execution.</p>
			<p>For more information on Azure bindings and triggers, refer to <a href="https://docs.microsoft.com/azure/azure-functions/functions-bindings-storage-queue">https://docs.microsoft.com/azure/azure-functions/functions-bindings-storage-queue</a>.</p>
			<p>Now that we have a fair understanding of Azure bindings and triggers, let's check out the various configuration options offered by Azure Functions.</p>
			<h3 id="_idParaDest-250"><a id="_idTextAnchor256"/>Azure Functions configuration</h3>
			<p>Azure Functions provides configuration options at multiple levels. It provides configuration for the following:</p>
			<ul>
				<li>The platform itself</li>
				<li>Functions App Services</li>
			</ul>
			<p>These settings affect every function contained by them. More information about these settings are available at <a href="https://docs.microsoft.com/azure/azure-functions/functions-how-to-use-azure-function-app-settings">https://docs.microsoft.com/azure/azure-functions/functions-how-to-use-azure-function-app-settings</a>.</p>
			<p><strong class="bold">Platform configuration</strong></p>
			<p>Azure functions are hosted within Azure App Service, so they get all of its features. Diagnostic and monitoring logs can be configured easily using platform features. Furthermore, App Service provides options for assigning SSL certificates, using a custom domain, authentication, and authorization as part of its networking features. </p>
			<p>Although customers are not concerned about the infrastructure, operating system, file system, or platform on which functions actually execute, Azure Functions provides the necessary tooling to peek within the underlying system and make changes. The in-portal console and the Kudu Console are the tools used for this purpose. They provide a rich editor to author Azure functions and edit their configuration.</p>
			<p>Azure Functions, just like App Service, lets you store the configuration information within the <strong class="inline">web.config</strong> application settings section, which can be read on demand. Some of the platform features of function apps are shown in <em class="italics">Figure 10.3</em>:</p>
			<div>
				<div id="_idContainer225" class="IMG---Figure">
					<img src="image/B15432_10_03.jpg" alt="Platform features of the function app"/>
				</div>
			</div>
			<h6>Figure 10.3: Platform features of a function app</h6>
			<p>These platform features can be used to configure authentication, custom domains, SSL, and so on. Also, the <strong class="bold">Platform Features</strong> tab provides an overview of the development tools that can be used with the function app. In the next section, we will take a look at the function app settings that are available in the platform features.</p>
			<p><strong class="bold">App Service function settings</strong></p>
			<p>These settings affect all functions. Application settings can be managed here. Proxies in Azure Functions can be enabled and disabled. We will discuss proxies later in this chapter. They also help in changing the edit mode of a function application and the deployment to slots:</p>
			<div>
				<div id="_idContainer226" class="IMG---Figure">
					<img src="image/B15432_10_04.jpg" alt="Configuring the function app’s settings"/>
				</div>
			</div>
			<h6>Figure 10.4: Function app settings</h6>
			<p>Budget is a very important aspect of the success of any project. Let's explore the various plans offered for Azure Functions.</p>
			<h3 id="_idParaDest-251"><a id="_idTextAnchor257"/>Azure Functions cost plans</h3>
			<p>Azure Functions is based on the Azure App Service and provides a pocket-friendly model for users. There are three cost models.</p>
			<p><strong class="bold">A consumption plan</strong></p>
			<p>This is based on the per-second consumption and execution of functions. This plan calculates the cost based on the compute usage during the actual consumption and execution of the function. If a function is not executed, there is no cost associated with it. However, it does not mean that performance is compromised in this plan. Azure functions will automatically scale out and in based on demand, to ensure basic minimum performance levels are maintained. A function execution is allowed 10 minutes for completion.</p>
			<p>One of the major drawbacks of this plan is that if there is no consumption of functions for a few seconds, the function might get cold and the next request that comes up might face a short delay in getting a response as the function is idle. This phenomenon is called a cold start. However, there are workarounds that can keep functions warm even when there are no legitimate requests. This can be done by writing a scheduled function that keeps invoking the target function to keep it warm.</p>
			<p><strong class="bold">A premium plan</strong></p>
			<p>This is a relatively new plan and provides lots of benefits compared to both App Service and a consumption plan. In this plan, there are no cold starts for Azure functions. Functions can be associated with a private network and customers can choose their own virtual machine sizes for executing functions. It provides numerous out-of-the-box facilities that were not possible previously with the other two types of plans.</p>
			<p><strong class="bold">An App Service plan</strong></p>
			<p>This plan provides functions with completely dedicated virtual machines in the backend, and so the cost is directly proportional to the cost of the virtual machine and its size. There is a fixed cost associated with this plan, even if functions are not invoked. Function code can run for as long as necessary. Although there is no time restriction, the default limit is set to 30 minutes. This can be changed by changing the value in the <strong class="inline">hosts.json</strong> file. Within the App Service plan, the function runtime goes idle if not used for a few minutes and can be activated only using an HTTP trigger. There is an <strong class="bold">Always On</strong> setting that can be used to prevent the function runtime from going idle. Scaling is either manual or based on autoscale settings.</p>
			<p>Along with the flexible pricing option, Azure also offers various hosting options for architecture deployment.</p>
			<h3 id="_idParaDest-252"><a id="_idTextAnchor258"/>Azure Functions destination hosts</h3>
			<p>The Azure Functions runtime can be hosted on Windows as well as on Linux hosts. PowerShell Core, Node.js, Java, Python, and .NET Core-based functions can run on both Windows as well as Linux operating systems. It is important to know which type of underlying operating system is required for the functions because this configuration setting is tied to the function app and in turn to all functions that are contained in it. </p>
			<p>Also, it is possible to run functions within Docker containers. This is because Azure provides Docker images that have a pre-built function runtime installed in them and functions can be hosted using such images. Now, Docker images can be used to create containers within Kubernetes Pods and hosted on Azure Kubernetes Service, Azure Container Instances, or on unmanaged Kubernetes clusters. These images can be stored within Docker Hub, Azure Container Registry, or any other global as well as private image repositories.</p>
			<p>To have a clearer understanding, let's look into some of the most prominent use cases for Azure Functions.</p>
			<h3 id="_idParaDest-253"><a id="_idTextAnchor259"/>Azure Functions use cases</h3>
			<p>Azure Functions has many implementations. Let's have a look at some of these use cases.</p>
			<p><strong class="bold">Implementing microservices</strong></p>
			<p>Azure Functions helps in breaking down large applications into smaller, discrete functional code units. Each unit is treated independently of others and evolves in its own life cycle. Each such code unit has its own compute, hardware, and monitoring requirements. Each function can be connected to all other functions. These units are woven together by orchestrators to build complete functionality. For example, in an e-commerce application, there can be individual functions (code units), each responsible for listing catalogs, recommendations, categories, subcategories, shopping carts, checkouts, payment types, payment gateways, shipping addresses, billing addresses, taxes, shipping charges, cancellations, returns, emails, SMS, and so on. Some of these functions are brought together to create use cases for e-commerce applications, such as product browsing and checkout flow.</p>
			<p><strong class="bold">Integration between multiple endpoints</strong></p>
			<p>Azure Functions can build overall application functionality by integrating multiple functions. The integration can be based on the triggering of events or it could be on a push basis. This helps in decomposing large monolithic applications into small components.</p>
			<p><strong class="bold">Data processing</strong></p>
			<p>Azure Functions can be used for processing incoming data in batches. It can help in processing data in multiple formats, such as XML, CSV, JSON, and TXT. It can also run conversion, enrichment, cleaning, and filtering algorithms. In fact, multiple functions can be used, each doing either conversion or enrichment, cleaning or filtering. Azure Functions can also be used to incorporate advanced cognitive services, such as <strong class="bold">optical character recognition (OCR)</strong>, computer vision, and image manipulation and conversion. This is ideal if you want to process API responses and convert them.</p>
			<p><strong class="bold">Integrating legacy applications</strong></p>
			<p>Azure Functions can help in integrating legacy applications with newer protocols and modern applications. Legacy applications might not be using industry-standard protocols and formats. Azure Functions can act as a proxy for these legacy applications, accepting requests from users or other applications, converting the data into a format understood by a legacy application, and talking to it on protocols it understands. This opens a world of opportunity for integrating and bringing old and legacy applications into the mainstream portfolio.</p>
			<p><strong class="bold">Scheduled jobs</strong></p>
			<p>Azure Functions can be used to execute continuously or periodically for certain application functions. These application functions can perform tasks such as periodically taking backups, restoring, running batch jobs, exporting and importing data, and bulk emailing.</p>
			<p><strong class="bold">Communication gateways</strong></p>
			<p>Azure Functions can be used in communication gateways when using notification hubs, SMS, email, and so on. For example, you can use Azure Functions to send a push notification to Android and iOS devices using Azure Notification Hubs.</p>
			<p>A<a id="_idTextAnchor260"/>zure functions are available in different types, which must be selected based on their relationship to optimizing workloads. Let's have a closer look at them.</p>
			<h3 id="_idParaDest-254"><a id="_idTextAnchor261"/>Types of Azure functions</h3>
			<p>Azure functions can be categorized into three different types:</p>
			<ul>
				<li><strong class="bold">On-demand functions</strong>: These are functions that are executed when they are explicitly called or invoked. Examples of such functions include HTTP-based functions and webhooks.</li>
				<li><strong class="bold">Scheduled functions</strong>: These functions are like timer jobs and execute functions on fixed intervals. </li>
				<li><strong class="bold">Event-based functions</strong>: These functions are executed based on external events. For example, uploading a new file to Azure Blob storage generates an event that could start the execution of Azure functions.</li>
			</ul>
			<p>In the following section, you will learn how to create an event-driven function that will be connected to an Azure Storage account.</p>
			<h2 id="_idParaDest-255"><a id="_idTextAnchor262"/>Creating an event-driven function</h2>
			<p>In this example, an Azure function will be authored and connected to an Azure Storage account. The Storage account has a container for holding all Blob files. The name of the Storage account is <strong class="bold">incomingfiles</strong> and the container is <strong class="bold">orders</strong>, as shown in <em class="italics">Figure 10.5</em>:</p>
			<div>
				<div id="_idContainer227" class="IMG---Figure">
					<img src="image/B15432_10_05.jpg" alt="The Storage account and container used for the Azure function"/>
				</div>
			</div>
			<h6>Figure 10.5: Storage account details </h6>
			<p>Perform the following steps to create a new Azure function from the Azure portal: </p>
			<ol>
				<li>Click on the <strong class="bold">+</strong> button beside the <strong class="bold">Functions</strong> menu on the left.</li>
				<li>Select <strong class="bold">In-Portal</strong> from the resultant screen and click on the <strong class="bold">Continue</strong> button.</li>
				<li>Select <strong class="bold">Azure Blob Storage trigger</strong>, as shown in <em class="italics">Figure 10.6</em>:<div id="_idContainer228" class="IMG---Figure"><img src="image/B15432_10_06.jpg" alt="Creating an Azure Blob Storage trigger function"/></div></li>
			</ol>
			<h6>Figure 10.6: Selecting Azure Blob Storage trigger</h6>
			<p>Right now, this Azure function does not have connectivity to the Storage account. Azure functions need connection information for the Storage account, and that is available from the <strong class="bold">Access keys</strong> tab in the Storage account. The same information can be obtained using the Azure Functions editor environment. In fact, that environment allows the creation of a new Storage account from the same editor environment.</p>
			<p>The Azure Blob Storage trigger can be added using the <strong class="bold">New</strong> button beside the <strong class="bold">Storage account connection</strong> input type. It allows the selection of an existing Storage account or the creation of a new Storage account. Since I already have a couple of Storage accounts, I am reusing them, but you should create a separate Azure Storage account. Selecting a Storage account will update the settings in the <strong class="bold">appSettings</strong> section with the connection string added to it.</p>
			<p>Ensure that a container already exists within the Blob service of the target Azure Storage account. The path input refers to the path to the container. In this case, the <strong class="bold">orders</strong> container already exists within the Storage account. The <strong class="bold">Create</strong> button shown here will provision the new function monitoring the Storage account container: </p>
			<div>
				<div id="_idContainer229" class="IMG---Figure">
					<img src="image/B15432_10_07.jpg" alt="Creating a function for monitoring a Storage account container"/>
				</div>
			</div>
			<h6>Figure 10.7: Creating a function that monitors the Storage account container</h6>
			<p>The code for the <strong class="inline">storagerelatedfunctions</strong> function is as follows:</p>
			<p class="snippet">public static void Run(Stream myBlob, TraceWriter log){    log.Info($"C# Blob trigger function Processed blob\n  \n Size {myBlob.Length} Bytes");}</p>
			<p>The bindings are shown here:</p>
			<p class="snippet">{  "bindings": [    {      "name": "myBlob",      "type": "blobTrigger",      "direction": "in",      "path": "orders",      "connection": "azureforarchitead2b_STORAGE"    }  ],  "disabled": false }</p>
			<p>Now, uploading any blob file to the <strong class="inline">orders</strong> container should trigger the function:</p>
			<p> </p>
			<div>
				<div id="_idContainer230" class="IMG---Figure">
					<img src="image/B15432_10_08.jpg" alt="Initiating a trigger through a blob file"/>
				</div>
			</div>
			<h6>Figure 10.8: C# Blob trigger function processed blob</h6>
			<p>In the next section, we will dive into Azure Function Proxies, which will help you to efficiently handle the requests and responses of your APIs.</p>
			<h2 id="_idParaDest-256"><a id="_idTextAnchor263"/>Function Proxies</h2>
			<p>Azure Function Proxies is a relatively new addition to Azure Functions. Function Proxies helps in hiding the details of Azure functions and exposing completely different endpoints to customers. Function Proxies can receive requests on endpoints, modify the content, body, headers, and URL of the request by changing the values, and augment them with additional data and pass it internally to Azure functions. Once they get a response from these functions, they can again convert, override, and augment the response and send it back to the client.</p>
			<p>It also helps in invoking different functions for CRUD (create, read, delete, and update) operations using different headers, thereby breaking large functions into smaller ones. It provides a level of security by not exposing the original function endpoint and also helps in changing the internal function implementation and endpoints without impacting its caller. Function Proxies helps by providing clients with a single function URL and then invoking multiple Azure functions in the backend to complete workflows. More information about Azure Function Proxies can be found at <a href="https://docs.microsoft.com/azure/azure-functions/functions-proxies">https://docs.microsoft.com/azure/azure-functions/functions-proxies</a>.</p>
			<p>In the next section, we will cover Durable Functions in detail.</p>
			<h2 id="_idParaDest-257"><a id="_idTextAnchor264"/>Durable Functions</h2>
			<p>Durable Functions is one of the latest additions to Azure Functions. It allows architects to write stateful workflows in an Orchestrator function, which is a new function type. As a developer, you can choose to code it or use any form of IDE. Some advantages of using Durable Functions are:</p>
			<ul>
				<li>Function output can be saved to local variables and you can call other functions synchronously and asynchronously.</li>
				<li>The state is preserved for you.</li>
			</ul>
			<p>The following is the basic mechanism for invoking Durable Functions:</p>
			<div>
				<div id="_idContainer231" class="IMG---Figure">
					<img src="image/B15432_10_09.jpg" alt="The basic mechanism for invoking Durable Functions"/>
				</div>
			</div>
			<h6>Figure 10.9: Mechanism for invoking Durable Functions</h6>
			<p>Azure Durable Functions can be invoked by any trigger provided by Azure Functions. These triggers include HTTP, Blob storage, Table Storage, Service Bus queues, and more. They can be triggered manually by someone with access to them, or by an application. <em class="italics">Figure 10.9</em> shows a couple of triggers as an example. These are also known as starter Durable Functions. The starter durable functions invoke the <strong class="bold">durable orchestrator trigger</strong>, which contains the main logic for orchestration, and orchestrates the invocation of activity functions.  </p>
			<p>The code written within the durable orchestrator must be deterministic. This means that no matter the number of times the code is executed, the values returned by it should remain the same. The Orchestrator function is a long-running function by nature. This means it can be hydrated, state-serialized, and it goes to sleep after it calls a durable activity function. This is because it does not know when the durable activity function will complete and does not want to wait for it. When the durable activity function finishes its execution, the Orchestrator function is executed again. The function execution starts from the top and executes until it either calls another durable activity function or finishes the execution of the function. It has to re-execute the lines of code that it already executed earlier and should get the same results that it got earlier. Note that the code written within the durable orchestrator must be deterministic. This means that no matter the number of times the code is executed, the values returned by it should remain the same. </p>
			<p>Let me explain this with the help of an example. If we use a general .NET Core datetime class and return the current date time, it will result in a new value every time we execute the function. The Durable Functions context object provides <strong class="inline">CurrentUtcDateTime</strong>, which will return the same datetime value during re-execution that it returned the first time.</p>
			<p>These orchestration functions can also wait for external events and enable scenarios related to human hand-off. This concept will be explained later in this section. </p>
			<p>These activity functions can be called with or without a retry mechanism. Durable Functions can help to solve many challenges and provides features to write functions that can do the following:</p>
			<ul>
				<li>Execute long-running functions</li>
				<li>Maintain state</li>
				<li>Execute child functions in parallel or sequence</li>
				<li>Recover from failure easily</li>
				<li>Orchestrate the execution of functions in a workflow</li>
			</ul>
			<p>Now that you have a fair understanding of the inner workings of a durable function, let's explore how to create a durable function in Visual Studio.</p>
			<h3 id="_idParaDest-258"><a id="_idTextAnchor265"/>Steps for creating a durable function using Visual Studio</h3>
			<p>The following are the steps to create a durable function:</p>
			<ol>
				<li value="1">Navigate to the Azure portal and click on <strong class="bold">Resource groups</strong> in the left menu.</li>
				<li>Click on the <strong class="bold">+Add</strong> button in the top menu to create a new resource group.</li>
				<li>Provide the resource group information on the resultant form and click on the <strong class="bold">Create</strong> button, as shown here:<div id="_idContainer232" class="IMG---Figure"><img src="image/B15432_10_10.jpg" alt="Creating a resource group for creating a Durable Function"/></div><h6>Figure 10.10: Creating a resource group</h6></li>
				<li>Navigate to the newly created resource group and add a new function app by clicking on the <strong class="bold">+Add</strong> button in the top menu and search for <strong class="inline">function app</strong> in the resultant search box.</li>
				<li>Select <strong class="bold">Function App</strong> and click on the <strong class="bold">Create</strong> button. Fill in the resultant function app form and click on the <strong class="bold">Create</strong> button. You can also reuse the function app we created earlier.</li>
				<li>Once the function app is created, we will get into our local development environment with visual studio 2019 installed on it. We will get started with Visual Studio and create a new project of type <strong class="inline">Azure functions</strong>, provide it with a name, and select <strong class="bold">Azure Functions v3</strong> (<strong class="bold">.NET core</strong>) for <strong class="bold">Function runtime</strong>.</li>
				<li>After the project is created, we need to add the <strong class="inline">DurableTask</strong> NuGet package to the project for working with Durable Functions. The version used at the time of writing this chapter is 2.2.2:<div id="_idContainer233" class="IMG---Figure"><img src="image/B15432_10_11.jpg" alt="Adding a DurableTask NuGet package for the project"/></div><h6>Figure 10.11: Adding a DurableTask NuGet package</h6></li>
				<li>Now, we can code our durable functions within Visual Studio. Add a new function, provide it with a name, and select the <strong class="bold">Durable Functions Orchestration</strong> trigger type:<div id="_idContainer234" class="IMG---Figure"><img src="image/B15432_10_12.jpg" alt="Adding a Durable Functions Orchestration trigger"/></div><h6>Figure 10.12: Selecting a Durable Functions Orchestration trigger </h6></li>
				<li>Visual Studio generates the boilerplate code for Durable Functions, and we are going to use it to learn about Durable Functions. Durable Functions activities are functions that are invoked by the main Orchestrator function. There is generally one main Orchestrator function and multiple Durable Functions activities. Once the extension is installed, provide a name for the function and write code that does something useful, such as sending an email or an SMS, connecting to external systems and executing logic, or executing services using their endpoints, such as cognitive services.<p>Visual Studio generates three sets of functions in a single line of code:</p><ul><li><strong class="bold">HttpStart</strong>: This is the starter function. This means that it is responsible for starting the durable function orchestration. The code generated consists of an HTTP trigger starter function; however, it could be any trigger-based function, such as <strong class="inline">BlobTrigger</strong>, a <strong class="inline">ServiceBus</strong> queue, or a trigger-based function.</li><li><strong class="bold">RunOrchestrator</strong>: This is the main durable orchestration function. It is responsible for accepting parameters from the starter function and in turn, invokes multiple durable task functions. Each durable task function is responsible for a functionality and these durable tasks can be invoked either in parallel or in sequence depending on the need. </li><li><strong class="bold">SayHello</strong>: This is the durable task function that is invoked from the durable function orchestrator to do a particular job.</li></ul></li>
				<li>The code for the starter function (<strong class="inline">HttpStart</strong>) is shown next. This function has a trigger of type HTTP and it accepts an additional binding of type <strong class="inline">DurableClient</strong>. This <strong class="inline">DurableClient</strong> object helps in invoking the Orchestrator function:<div id="_idContainer235" class="IMG---Figure"><img src="image/B15432_10_13.jpg" alt="Code for the HttpStart starter function"/></div><h6>Figure 10.13: Code for the starter function</h6></li>
				<li>The code for the Orchestrator function (<strong class="inline">RunOrchestrator</strong>) is shown next. This function has a trigger of type <strong class="inline">OrchestrationTrigger</strong> and accepts a parameter of type <strong class="inline">IDurableOrchestrationContext</strong>. This context object helps in invoking durable tasks:<div id="_idContainer236" class="IMG---Figure"><img src="image/B15432_10_14.jpg" alt="Code for the orchestrator trigger function"/></div><h6>Figure 10.14: Code for orchestrator trigger function</h6></li>
				<li>The code for the durable task function (<strong class="inline">HelloFunction</strong>) is shown next. This function has a trigger of type <strong class="inline">ActivityTrigger</strong> and accepts a parameter that can be any type needed for it to execute its functionality. It has a return value of type <strong class="inline">string</strong> and the function is responsible for returning a string value to the orchestration function:<div id="_idContainer237" class="IMG---Figure"><img src="image/B15432_10_15.jpg" alt="Code for the durable task function"/></div></li>
			</ol>
			<h6>Figure 10.15: Code for the durable task function</h6>
			<p>Next, we can execute the function locally, which will start a storage emulator if one's not already started, and will provide a URL for the HTTP trigger function:</p>
			<div>
				<div id="_idContainer238" class="IMG---Figure">
					<img src="image/B15432_10_16.jpg" alt="Executing a function to start the storage emulator and provide a URL for the HTTP trigger function"/>
				</div>
			</div>
			<h6>Figure 10.16: Starting the storage emulator</h6>
			<p>We are going to invoke this URL using a tool known as <strong class="bold">Postman</strong> (this can be downloaded from <a href="https://www.getpostman.com/">https://www.getpostman.com/</a>). We just need to copy the URL and execute it in Postman. This activity is shown in <em class="italics">Figure 10.17</em>:</p>
			<div>
				<div id="_idContainer239" class="IMG---Figure">
					<img src="image/B15432_10_17.jpg" alt="Invoking URLs using Postman"/>
				</div>
			</div>
			<h6>Figure 10.17: Invoking URLs using Postman</h6>
			<p>Notice that five URLs are generated when you start the orchestrator:</p>
			<ul>
				<li>The <strong class="inline">statusQueryGetUri</strong> URL is used to find the current status of the orchestrator. Clicking this URL on Postman opens a new tab, and if we execute this request, it shows the status of the workflow:<div id="_idContainer240" class="IMG---Figure"><img src="image/B15432_10_18.jpg" alt="The current status of the orchestrator"/></div></li>
			</ul>
			<h6>Figure 10.18: Current status of the orchestrator</h6>
			<ul>
				<li>The <strong class="inline">terminatePostUri</strong> URL is used for stopping an already running Orchestrator function.</li>
				<li>The <strong class="inline">sendEventPostUri</strong> URL is used to post an event to a suspended durable function. Durable functions can be suspended if they are waiting for an external event. This URL is used in those cases.</li>
				<li>The <strong class="inline">purgeHistoryDeleteUri</strong> URL is used to delete the history maintained by Durable Functions for a particular invocation from its Table Storage account.</li>
			</ul>
			<p>Now that you know how to work with Durable Functions using Visual Studio, let's cover another aspect of Azure functions: chaining them together.</p>
			<h2 id="_idParaDest-259"><a id="_idTextAnchor266"/>Creating a connected architecture with functions</h2>
			<p>A connected architecture with functions refers to creating multiple functions, whereby the output of one function triggers another function and provides data for the next function to execute its logic. In this section, we will continue with the previous scenario of the Storage account. In this case, the output of the function being triggered using Azure Storage Blob files will write the size of the file to Azure Cosmos DB.</p>
			<p>The configuration of Cosmos DB is shown next. By default, there are no collections created in Cosmos DB.</p>
			<p>A collection will automatically be created when creating a function that will be triggered when Cosmos DB gets any data:</p>
			<div>
				<div id="_idContainer241" class="IMG---Figure">
					<img src="image/B15432_10_19.jpg" alt="Creating an Azure Cosmos DB account"/>
				</div>
			</div>
			<h6>Figure 10.19: Creating an Azure Cosmos DB account</h6>
			<p>Let's follow the below steps to retrieve data for the next function from the output of one function.</p>
			<ol>
				<li value="1">Create a new database, <strong class="inline">testdb</strong>, within Cosmos DB, and create a new collection named <strong class="inline">testcollection</strong> within it. You need both the database and the collection name when configuring Azure functions: <div id="_idContainer242" class="IMG---Figure"><img src="image/B15432_10_20.jpg" alt="Creating a new database within a container"/></div><h6> Figure 10.20: Adding a container</h6></li>
				<li>Create a new function that will have a Blob Storage trigger and output CosmosDB binding. The value returned from the function will be the size of the data for the uploaded file. This returned value will be written to Cosmos DB. The output binding will write to the Cosmos DB collection. Navigate to the <strong class="bold">Integrate</strong> tab and click on the <strong class="bold">New Output</strong> button below the <strong class="bold">Outputs</strong> label and select <strong class="bold">Azure Cosmos DB</strong>:<div id="_idContainer243" class="IMG---Figure"><img src="image/B15432_10_21.jpg" alt="Binding output to Azure Cosmos DB"/></div><h6>Figure 10.21: Binding output to Azure Cosmos DB</h6></li>
				<li>Provide the appropriate names for the database and collection (check the checkbox to create the collection if it does not exist), click on the <strong class="bold">New</strong> button to select your newly created Azure Cosmos DB, and leave the parameter name as <strong class="bold">outputDocument</strong>:<div id="_idContainer244" class="IMG---Figure"><img src="image/B15432_10_22.jpg" alt="Selecting an Azure Cosmos DB account as output"/></div><h6>Figure 10.22: Newly created Azure Cosmos DB</h6></li>
				<li>Modify the function as shown in <em class="italics">Figure 10.23</em>:<div id="_idContainer245" class="IMG---Figure"><img src="image/B15432_10_23.jpg" alt="Modifying the function"/></div><h6> Figure 10.23: Modifying the function</h6></li>
				<li>Now, uploading a new file to the orders collection in the Azure Storage account will execute a function that will write to the Azure Cosmos DB collection. Another function can be written with the newly created Azure Cosmos DB account as a trigger binding. It will provide the size of files and the function can act on it. This is shown here:<div id="_idContainer246" class="IMG---Figure"><img src="image/B15432_10_24.jpg" alt="Creating a trigger binding function"/></div></li>
			</ol>
			<h6>Figure 10.24: Writing a trigger binding function</h6>
			<p>This section covered how the output of one function can be used to retrieve data for the next function. In the next section, you will learn about how to enable serverless eventing by understanding about Azure Event Grid.</p>
			<h2 id="_idParaDest-260"><a id="_idTextAnchor267"/>Azure Event Grid</h2>
			<p>Azure Event Grid is a relatively new service. It has also been referred to as a serverless eventing platform. It helps with the creation of applications based on events (also known as <strong class="bold">event-driven design</strong>). It is important to understand what events are and how we dealt with them prior to Event Grid. An event is something that happened – that is, an activity that changed the state of a subject. When a subject undergoes a change in its state, it generally raises an event.</p>
			<p>Events typically follow the publish/subscribe pattern (also popularly known as the <strong class="bold">pub/sub pattern</strong>), in which a subject raises an event due to its state change, and that event can then be subscribed to by multiple interested parties, also known as <strong class="bold">subscribers</strong>. The job of the event is to notify the subscribers of such changes and also provide them with data as part of its context. The subscribers can take whatever action they deem necessary, which varies from subscriber to subscriber.</p>
			<p>Prior to Event Grid, there was no service that could be described as a real-time event platform. There were separate services, and each provided its own mechanism for handling events.</p>
			<p>For example, Log Analytics, also known as <strong class="bold">Operations Management Suite</strong> (<strong class="bold">OMS</strong>), provides an infrastructure for capturing environment logs and telemetry on which alerts can be generated. These alerts can be used to execute a runbook, a webhook, or a function. This is near to real time, but they are not completely real time. Moreover, it was quite cumbersome to trap individual logs and act on them. Similarly, there is Application Insights, which provides similar features to Log Analytics but for applications instead.</p>
			<p>There are other logs, such as activity logs and diagnostic logs, but again, they rely on similar principles as other log-related features. Solutions are deployed on multiple resource groups in multiple regions, and events raised from any of these should be available to the resources that are deployed elsewhere.</p>
			<p>Event Grid removes all barriers, and as a result, events can be generated by most resources (they are increasingly becoming available), and even custom events can be generated. These events can then be subscribed to by any resource, in any region, and in any resource group within the subscription.</p>
			<p>Event Grid is already laid down as part of the Azure infrastructure, along with data centers and networks. Events raised in one region can easily be subscribed to by resources in other regions, and since these networks are connected, it is extremely efficient for the delivery of events to subscribers.</p>
			<h3 id="_idParaDest-261"><a id="_idTextAnchor268"/>Event Grid</h3>
			<p>Event Grid lets you create applications with event-based architecture. There are publishers of events and there are consumers of events; however, there can be multiple subscribers for the same event.</p>
			<p>The publisher of an event can be an Azure resource, such as Blob storage, <strong class="bold">Internet of Things</strong> (<strong class="bold">IoT</strong>) hubs, and many others. These publishers are also known as event sources. These publishers use out-of-the-box Azure topics to send their events to Event Grid. There is no need to configure either the resource or the topic. The events raised by Azure resources are already using topics internally to send their events to Event Grid. Once the event reaches the grid, it can be consumed by the subscribers.</p>
			<p>The subscribers, or consumers, are resources who are interested in events and want to execute an action based on these events. These subscribers provide an event handler when they subscribe to the topic. The event handlers can be Azure functions, custom webhooks, logic apps, or other resources. Both the event sources and subscribers that execute event handlers are shown in <em class="italics">Figure 10.25</em>:</p>
			<div>
				<div id="_idContainer247" class="IMG---Figure">
					<img src="image/B15432_10_25.jpg" alt="The Event Grid architecture"/>
				</div>
			</div>
			<h6>Figure 10.25: The Event Grid architecture</h6>
			<p>When an event reaches a topic, multiple event handlers can be executed simultaneously, each taking its own action.</p>
			<p>It is also possible to raise a custom event and send a custom topic to Event Grid. Event Grid provides features for creating custom topics, and these topics are automatically attached to Event Grid. These topics know the storage for Event Grid and automatically send their messages to it. Custom topics have two important properties, as follows:</p>
			<ul>
				<li><strong class="bold">An endpoint</strong>: This is the endpoint of the topic. Publishers and event sources use this endpoint to send and publish their events to Event Grid. In other words, topics are recognized using their endpoints.</li>
				<li><strong class="bold">Keys</strong>: Custom topics provide a couple of keys. These keys enable security for the consumption of the endpoint. Only publishers with these keys can send and publish their messages to Event Grid.</li>
			</ul>
			<p>Each event has an event type and it is recognized by it. For example, Blob storage provides event types, such as <strong class="inline">blobAdded</strong> and <strong class="inline">blobDeleted</strong>. Custom topics can be used to send a custom-defined event, such as a custom event of the <strong class="inline">KeyVaultSecretExpired</strong> type.</p>
			<p>On the other hand, subscribers have the ability to accept all messages or only get events based on filters. These filters can be based on the event type or other properties within the event payload.</p>
			<p>Each event has at least the following five properties:</p>
			<ul>
				<li><strong class="inline">id</strong>: This is the unique identifier for the event.</li>
				<li><strong class="inline">eventType</strong>: This is the event type.</li>
				<li><strong class="inline">eventTime</strong>: This is the date and time when the event was raised.</li>
				<li><strong class="inline">subject</strong>: This is a short description of the event.</li>
				<li><strong class="inline">data</strong>: This is a dictionary object and contains either resource-specific data or any custom data (for custom topics).</li>
			</ul>
			<p>Currently, Event Grid's functionalities are not available with all resources; however, Azure is continually adding more and more resources with Event Grid functionality.</p>
			<p>To find out more about the resources that can raise events related to Event Grid and handlers that can handle these events, please go to <a href="https://docs.microsoft.com/azure/event-grid/overview">https://docs.microsoft.com/azure/event-grid/overview</a>.</p>
			<h3 id="_idParaDest-262"><a id="_idTextAnchor269"/>Resource events</h3>
			<p>In this section, the following steps are provided to create a solution in which events that are raised by Blob storage are published to Event Grid and ultimately routed to an Azure function:</p>
			<ol>
				<li value="1">Log in to the Azure portal using the appropriate credentials and create a new Storage account in an existing or a new resource group. The Storage account should be either <strong class="bold">StorageV2</strong> or <strong class="bold">Blob storage</strong>. As demonstrated in <em class="italics">Figure 10.26</em>, Event Grid will not work with <strong class="bold">StorageV1</strong>:<div id="_idContainer248" class="IMG---Figure"><img src="image/B15432_10_26.jpg" alt="Creating a storage account of the StorageV2 type"/></div><h6>Figure 10.26: Creating a new storage account</h6></li>
				<li>Create a new function app or reuse an existing function app to create an Azure function. The Azure function will be hosted within the function app.</li>
				<li>Create a new function using the <strong class="bold">Azure Event Grid trigger</strong> template. Install the <strong class="bold">Microsoft.Azure.WebJobs.Extensions.EventGrid</strong> extension if it's not already installed, as shown in <em class="italics">Figure 10.27</em>:<div id="_idContainer249" class="IMG---Figure"><img src="image/B15432_10_27.jpg" alt="Installing extensions for Azure Event Grid trigger"/></div><h6>Figure 10.27: Installing extensions for an Azure Event Grid trigger</h6></li>
				<li>Name the <strong class="inline">StorageEventHandler</strong> function and create it. The following default generated code will be used as the event handler:<div id="_idContainer250" class="IMG---Figure"><img src="image/B15432_10_28.jpg" alt="Code for the event handler"/></div><h6>Figure 10.28: Event handler code</h6><p>The subscription to Storage events can be configured either from the Azure Functions <strong class="bold">user interface (UI)</strong> by clicking on <strong class="bold">Add Event Grid subscription</strong>, or from the storage account itself.</p></li>
				<li>Click on the <strong class="bold">Add Event Grid subscription</strong> link in the Azure Functions UI to add a subscription to the events raised by the storage account created in the previous step. Provide a meaningful name for the subscription, and then choose <strong class="bold">Event Schema</strong> followed by <strong class="bold">Event Grid Schema</strong>. Set <strong class="bold">Topic Types</strong> as <strong class="bold">Storage Accounts</strong>, set an appropriate <strong class="bold">Subscription</strong>, and the resource group containing the storage account:<div id="_idContainer251" class="IMG---Figure"><img src="image/B15432_10_29.jpg" alt="Creating an Event Grid subscription"/></div><h6>Figure 10.29: Creating an Event Grid subscription</h6><p>Ensure that the <strong class="bold">Subscribe to all event types</strong> checkbox is checked and click on the <strong class="bold">Create</strong> button (it should be enabled as soon as a storage account is selected).</p></li>
				<li>If we now navigate to the storage account in the Azure portal and click on the <strong class="bold">Events</strong> link in the left-hand menu, the subscription for the storage account should be visible:<div id="_idContainer252" class="IMG---Figure"><img src="image/B15432_10_30.jpg" alt="List of event subscriptions"/></div><h6>Figure 10.30:  Event subscription list</h6></li>
				<li>Upload a file to the Blob storage after creating a container, and the Azure function should be executed. The upload action will trigger a new event of the <strong class="inline">blobAdded</strong> type and send it to the Event Grid topic for storage accounts. As shown in <em class="italics">Figure 10.31</em>, the subscription is already set to get all the events from this topic, and the function gets executed as part of the event handler:<div id="_idContainer253" class="IMG---Figure"><img src="image/B15432_10_31.jpg" alt="Using a blob file to trigger a new event"/></div></li>
			</ol>
			<h6>Figure 10.31:  Triggering a new event</h6>
			<p>In this section, you learned how events raised by Blob storage can be routed to an Azure function. In the next section, you will learn how to leverage custom events.</p>
			<h3 id="_idParaDest-263"><a id="_idTextAnchor270"/>Custom events</h3>
			<p>In this example, instead of using out-of-box resources to generate events, custom events will be used. We will use PowerShell to create this solution and reuse the same Azure function that was created in the last exercise as the handler:</p>
			<ol>
				<li value="1">Log in and connect to your Azure subscription of choice using <strong class="inline">Login-AzAccount</strong> and <strong class="inline">Set-AzContext cmdlet</strong>.</li>
				<li>The next step is to create a new Event Grid topic in Azure in a resource group. The <strong class="inline">New-AzEventGridTopic</strong> cmdlet is used to create a new topic:<p class="snippet">New-AzEventGridTopic -ResourceGroupName CustomEventGridDemo -Name "KeyVaultAssetsExpiry" -Location "West Europe"</p></li>
				<li>Once the topic is created, its endpoint URL and key should be retrieved as they are needed to send and publish the event to it. The <strong class="inline">Get-AzEventGridTopic</strong> and <strong class="inline">Get-AzEventGridTopicKey</strong> cmdlets are used to retrieve these values. Note that <strong class="inline">Key1</strong> is retrieved to connect to the endpoint:<p class="snippet">$topicEndpoint = (Get-AzEventGridTopic -ResourceGroupName containers -Name KeyVaultAssetsExpiry).Endpoint $keys = (Get-AzEventGridTopicKey -ResourceGroupName containers -Name KeyVaultAssetsExpiry).Key1</p></li>
				<li>A new hash table is created with all five important Event Grid event properties. A new <strong class="inline">id</strong> property is generated for the ID, the <strong class="inline">subject</strong> property is set to <strong class="inline">Key vault</strong> <strong class="inline">Asset Expiry</strong>, <strong class="inline">eventType</strong> is set to <strong class="inline">Certificate Expiry</strong>, <strong class="inline">eventTime</strong> is set to the current time, and <strong class="inline">data</strong> contains information regarding the certificate:<p class="snippet">$eventgridDataMessage = @{id = [System.guid]::NewGuid()subject = "Key Vault Asset Expiry"eventType = "Certificate Expiry"eventTime = [System.DateTime]::UtcNow data = @{CertificateThumbprint = "sdfervdserwetsgfhgdg"ExpiryDate = "1/1/2019"Createdon = "1/1/2018"}}</p></li>
				<li>Since Event Grid data should be published in the form of a JSON array, the payload is converted in the JSON array. The <strong class="inline">"["</strong>,<strong class="inline">"]"</strong> square brackets represent a JSON array:<p class="snippet">$finalBody = "[" + $(ConvertTo-Json $eventgridDataMessage) + "]"</p></li>
				<li>The event will be published using the HTTP protocol, and the appropriate header information has to be added to the request. The request is sent using the application/JSON content type and the key belonging to the topic is assigned to the <strong class="inline">aeg-sas-key</strong> header. It is mandatory to name the header and key set to <strong class="inline">aeg-sas-key</strong>:<p class="snippet">$header = @{"contentType" = "application/json""aeg-sas-key" = $keys}</p></li>
				<li>A new subscription is created to the custom topic with a name, the resource group containing the topic, the topic name, the webhook endpoint, and the actual endpoint that acts as the event handler. The event handler in this case is the Azure function:<p class="snippet">New-AzEventGridSubscription -TopicName KeyVaultAssetsExpiry -EventSubscriptionName "customtopicsubscriptionautocar" -ResourceGroupName CustomEventGridDemo -EndpointType webhook '-Endpoint "https://durablefunctiondemoapp.azurewebsites.net/runtime/webhooks/EventGrid?functionName=StorageEventHandler&amp;code=0aSw6sxvtFmafXHvt7iOw/Dsb8o1M9RKKagzVchTUkwe9EIkzl4mCg=='-Verbose</p><p>The URL of the Azure function is available from the <strong class="bold">Integrate</strong> tab, as shown in <em class="italics">Figure 10.31</em>:</p><div id="_idContainer254" class="IMG---Figure"><img src="image/B15432_10_32.jpg" alt="Event Grid Subscription URL in the Integrate tab"/></div><h6>Figure 10.32: Event Grid Subscription URL in the Integrate tab</h6></li>
				<li>By now, both the subscriber (event handler) and the publisher have been configured. The next step is to send and publish an event to the custom topic. The event data was already created in the previous step and, by using the <strong class="inline">Invoke-WebRequest</strong> cmdlet, the request is sent to the endpoint along with the body and the header:<p class="snippet">Invoke-WebRequest -Uri $topicEndpoint -Body $finalBody -Headers $header -Method Post</p></li>
			</ol>
			<p>The API call will trigger the event and the Event Grid will message the endpoint we configured, which is the function app. With this activity, we are winding up this chapter.</p>
			<h2 id="_idParaDest-264"><a id="_idTextAnchor271"/>Summary</h2>
			<p>The evolution of functions from traditional methods has led to the design of the loosely coupled, independently evolving, self-reliant serverless architecture that was only a concept in earlier days. Functions are a unit of deployment and provide an environment that does not need to be managed by the user at all. All they have to care about is the code written for the functionality. Azure provides a mature platform for hosting functions and integrating them seamlessly, based on events or on demand. Nearly every resource in Azure can participate in an architecture composed of Azure functions. The future is functions, as more and more organizations want to stay away from managing infrastructures and platforms. They want to offload this to cloud providers. Azure Functions is an essential feature to master for every architect dealing with Azure.</p>
			<p>This chapter went into the details of Azure Functions, Functions as a Service, Durable Functions, and Event Grid. The next chapter will focus on Azure Logic Apps, and we will build a complete end-to-end solution combining multiple serverless services along with other Azure services, such as Azure Key Vault and Azure Automation.</p>
		</div>
	</body></html>