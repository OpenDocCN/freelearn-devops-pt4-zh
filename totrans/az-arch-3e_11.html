<html><head></head><body>
		<div>
			<div id="_idContainer256" class="Content">
			</div>
		</div>
		<div id="_idContainer257" class="Content">
			<h1 id="_idParaDest-265">11. <a id="_idTextAnchor272"/>Azure solutions using Azure Logic Apps, Event Grid, and Functions</h1>
		</div>
		<div id="_idContainer297" class="Content">
			<p>This chapter continues from the previous chapter and will go into further depth about serverless services available within Azure. In the previous chapter, you learned in detail about Azure Functions, functions as a service, Durable Functions, and Event Grid. Going forward, this chapter will focus on understanding Logic Apps and then move on to creating a complete end-to-end serverless solution that combines multiple serverless and other kinds of services, such as Key Vault and Azure Automation.</p>
			<p>In this chapter, we will further explore Azure services by covering the following topics:</p>
			<ul>
				<li>Azure Logic Apps</li>
				<li>Creating an end-to-end solution using serverless technologies</li>
			</ul>
			<h2 id="_idParaDest-266"><a id="_idTextAnchor273"/>Azure Logic Apps  </h2>
			<p>Logic Apps is a serverless workflow offering from Azure. It has all the features of serverless technologies, such as consumption-based costing and unlimited scalability. Logic Apps helps us to build a business process and workflow solution with ease using the Azure portal. It provides a drag-and-drop UI to create and configure workflows.</p>
			<p>Using Logic Apps is the preferred way to integrate services and data, create business projects, and create a complete flow of logic. There are several important concepts that should be understood before building a logic app.</p>
			<h3 id="_idParaDest-267"><a id="_idTextAnchor274"/>Activities</h3>
			<p>An activity is a single unit of work. Examples of activities include converting XML to JSON, reading blobs from Azure Storage, and writing to a Cosmos DB document collection. Logic Apps provides a workflow definition consisting of multiple co-related activities in a sequence. There are two types of activity in Logic Apps:</p>
			<ul>
				<li><strong class="bold">Trigger</strong>: A trigger refers to the initiation of an activity. All logic apps have a single trigger that forms the first activity. It is the trigger that creates an instance of the logic app and starts the execution. Examples of triggers are the arrival of Event Grid messages, an email, an HTTP request, or a schedule.</li>
				<li><strong class="bold">Actions:</strong> Any activity that is not a trigger is a step activity, and each of them is responsible to perform one task. Steps are connected to each other in a workflow. Each step will have an action that needs to be completed before going to the next step.</li>
			</ul>
			<h3 id="_idParaDest-268"><a id="_idTextAnchor275"/>Connectors</h3>
			<p>Connectors are Azure resources that help connect a logic app to external services. These services can be in the cloud or on-premises. For example, there is a connector for connecting logic apps to Event Grid. Similarly, there is another connector to connect to Office 365 Exchange. Almost all types of connectors are available in Logic Apps, and they can be used to connect to services. Connectors contain connection information and also logic to connect to external services using this connection information.</p>
			<p>The entire list of connectors is available at <a href="https://docs.microsoft.com/connectors">https://docs.microsoft.com/connectors</a>.</p>
			<p>Now that you know about connectors, you need to understand how they can be aligned in a step-by-step manner to make the workflow work as expected. In the next section, we will be focusing on the workings of a logic app.</p>
			<h3 id="_idParaDest-269"><a id="_idTextAnchor276"/>The workings of a logic app</h3>
			<p>Let's create a Logic Apps workflow that gets triggered when an email account receives an email. It replies to the sender with a default email and performs sentiment analysis on the content of the email. For sentiment analysis, the Text Analytics resource from Cognitive Services should be provisioned before creating the logic app:</p>
			<ol>
				<li>Navigate to the Azure portal, log in to your account, and create a <strong class="bold">Text Analytics</strong> resource in a resource group. Text Analytics is part of Cognitive Services and has features such as sentiment analysis, key phrase extraction, and language detection. You can find the service in the Azure portal<a id="_idTextAnchor277"/>, as shown in <em class="italics">Figure 11.1</em>:<div id="_idContainer258" class="IMG---Figure"><img src="image/Figure_11.1.jpg" alt="Entering the keywords ‘Text Analytics’ in the search box of the Azure portal to find the Text Analytics service."/></div><h6>Figure 11.1: Navigating to the Text Analytics service from the Azure portal</h6></li>
				<li>Provide the <strong class="bold">Name</strong>, <strong class="bold">Location</strong>, <strong class="bold">Subscription</strong>, <strong class="bold">Resource group</strong>, and <strong class="bold">Pricing tier</strong> values. We'll be using the free tier (F0 tier) of this service for this demo.</li>
				<li>Once the resource is provisioned, navigate to the <strong class="bold">Overview</strong> page, and copy the endpoint URL. Store it in a temporary location. This value will be required when configuring the logic app.</li>
				<li>Navigate to the <strong class="bold">Keys</strong> page and copy the value from <strong class="bold">Key 1</strong> and store it in a temporary location. This value will be needed when configuring the logic app.</li>
				<li>The next step is to create a logic app. To create a logic app, navigate to the resource group in the Azure portal in which the logic app should be created. Search for Logic App and create it by providing <strong class="bold">Name</strong>, <strong class="bold">Location</strong>, <strong class="bold">Resource group</strong>, and <strong class="bold">Subscription</strong> values.</li>
				<li>After the logic app has been created, navigate to the resource, click on <strong class="bold">Logic app designer</strong> in the left-hand menu, and then select the <strong class="bold">When a new email is received in Outlook.com</strong> template to create a new workflow. The template provides a head start by adding boilerplate triggers and activities. This will add an Office 365 Outlook trigger automatically to the workflow.</li>
				<li>Click on the <strong class="bold">Sign in</strong> button on the trigger; it will open a new Internet Explorer window. Then, sign in to your account. After successfully signing in, a new Office 365 mail connector will be created, containing the connection information to the account.</li>
				<li>Click on the <strong class="bold">Continue</strong> button and configure the trigger with a 3-minute poll frequency, as shown in <em class="italics">Figure 11.2</em>:<div id="_idContainer259" class="IMG---Figure"><img src="image/Figure_11.2.jpg" alt="Configuring the trigger with a 3-minute poll frequency by setting Interval as ‘3’ and Frequency as minute."/></div><h6>Figure 11.2: Configuring the trigger with a 3-minute poll frequency</h6></li>
				<li>Click on <strong class="bold">Next step</strong> to add another action and type the keyword <strong class="inline">variable</strong> in the search bar. Then, select the <strong class="bold">Initialize variable</strong> action, as demonstrated in <em class="italics">Figure 11.3</em>:<div id="_idContainer260" class="IMG---Figure"><img src="image/Figure_11.3.jpg" alt="In the ‘Choose an action’ pane, entering the keyword ‘variable’ and then selecting the Initialize variable action."/></div><h6>Figure 11.3: Adding the Initialize variable action</h6></li>
				<li>Next, configure the variable action. When the <strong class="bold">Value</strong> box is clicked on, a pop-up window appears that shows <strong class="bold">Dynamic content</strong> and <strong class="bold">Expression</strong>. Dynamic content refers to properties that are available to the current action and are filled with runtime values from previous actions and triggers. Variables help in keeping workflows generic. From this window, select <strong class="bold">Body</strong> from <strong class="bold">Dynamic content</strong>:<div id="_idContainer261" class="IMG---Figure"><img src="image/Figure_11.4.jpg" alt="In the ‘Initialize variable’ pane, adding the name as emailContent, the type as string, and the value as Body."/></div><h6>Figure 11.4: Configuring the variable action</h6></li>
				<li>Add another action by clicking on <strong class="bold">Add step</strong>, typing <strong class="inline">outlook</strong> in the search bar, and then selecting the <strong class="bold">Reply to email</strong> action:<div id="_idContainer262" class="IMG---Figure"><img src="image/Figure_11.5.jpg" alt="Searching with the keyword ‘Outlook’ in the searchbox of the ‘Choose an action’ pane and then adding the action ‘Reply to email’."/></div><h6>Figure 11.5: Adding the Reply to email action</h6></li>
				<li>Configure the new action. Ensure that <strong class="bold">Message Id</strong> is set with the dynamic content, <strong class="bold">Message Id</strong>, and then type the reply in the <strong class="bold">Comment</strong> box that you'd like to send to the recipient:<div id="_idContainer263" class="IMG---Figure"><img src="image/Figure_11.6.jpg" alt="Configuring the Reply to email action by adding the Message id, the comment “Thanks for your email. We will get back to you”, and the Reply All option as ‘Yes’."/></div><h6>Figure 11.6: Configuring the Reply to email action</h6></li>
				<li>Add another action, type <strong class="inline">text analytics</strong> in the search bar, and then select <strong class="bold">Detect Sentiment (preview)</strong>:<div id="_idContainer264" class="IMG---Figure"><img src="image/Figure_11.7.jpg" alt="Adding the Detect Sentiment (preview) action."/></div><h6>Figure 11.7: Adding the Detect Sentiment (preview) action</h6></li>
				<li>Configure the sentiment action as shown in <em class="italics">Figure 11.8</em>—both the endpoint and key values should be used here. Now click on the <strong class="bold">Create</strong> button, as demonstrated in <em class="italics">Figure 11.8</em>:<div id="_idContainer265" class="IMG---Figure"><img src="image/Figure_11.8.jpg" alt="Configuring the sentiment action by providing the Connection name as EmailAnalysis, and entering the account key and the Site URL."/></div><h6>Figure 11.8: Configuring the Detect Sentiment (preview) action</h6></li>
				<li>Provide the text to the action by adding dynamic content and selecting the previously created variable, <strong class="bold">emailContent</strong>. Then, click on <strong class="bold">Show advanced options</strong> and select <strong class="bold">en</strong> for <strong class="bold">Language</strong>:<div id="_idContainer266" class="IMG---Figure"><img src="image/Figure_11.9.jpg" alt="In the ‘Detect Sentiment(preview) pane, setting text field as emailContent and Language as en."/></div><h6>Figure 11.9: Selecting the language for the sentiment action</h6></li>
				<li>Next, add a new action by selecting <strong class="bold">Outlook</strong>, and then select <strong class="bold">Send an email</strong>. This action sends the original recipient the email content with the sentiment score in its subject. It should be configured as shown in <em class="italics">Figure 11.10</em>. If the score is not visible in the dynamic content window, click on the <strong class="bold">See more</strong> link beside it:<div id="_idContainer267" class="IMG---Figure"><img src="image/Figure_11.10.jpg" alt="Configuring the Send an email action and setting Body as emailCOntent, Subject as Score, and To as ‘To’ from (Outlook)."/></div><h6>Figure 11.10: Adding the Send an email action</h6></li>
				<li>Save the logic app, navigate back to the overview page, and click on <strong class="bold">Run trigger</strong>. The trigger will check for new emails every 3 minutes, reply to the senders, perform sentiment analysis, and send an email to the original recipient. A sample email with negative connotations is sent to the given email ID:<div id="_idContainer268" class="IMG---Figure"><img src="image/Figure_11.11.jpg" alt="A sample email with negative connotations, sent to the given email ID."/></div><h6>Figure 11.11: Sample email</h6></li>
				<li>After a few seconds, the logic app executes, and the sender gets the following reply:<div id="_idContainer269" class="IMG---Figure"><img src="image/Figure_11.12.jpg" alt="The autogenerated reply from the logic app with the text specified in the ‘Reply to email’ action."/></div><h6>Figure 11.12: Reply email to the original sender </h6></li>
				<li>The original recipient gets an email with the sentiment score and the original email text, as shown in <em class="italics">Figure 11.13</em>:<div id="_idContainer270" class="IMG---Figure"><img src="image/Figure_11.13.jpg" alt="An HTML view of the email message with the sentiment score and the original email text."/></div></li>
			</ol>
			<h6>Figure 11.13: HTML view of the email message</h6>
			<p>From the activity, we were able to understand the workings of a logic app. The app was triggered when an email was received in the inbox of the user and the process followed the sequence of steps that were given in the logic app. In the next section, you will learn how to create an end-to-end solution using serverless technologies.</p>
			<h2 id="_idParaDest-270"><a id="_idTextAnchor278"/>Creating an end-to-end solution using serverless technologies</h2>
			<p>In this section, we will create an end-to-end solution comprising serverless technologies that we discussed in the previous sections. The following example will give you an idea of how workflows can be intelligently implemented to avoid management overhead. In the next activity, we will create a workflow to notify the users when the keys, secrets, and certificates get stored in Azure Key Vault. We will take this as a problem statement, figure out a solution, architect the solution, and implement it.</p>
			<h3 id="_idParaDest-271"><a id="_idTextAnchor279"/>The problem statement</h3>
			<p>The problem that we are going to solve here is that users and organizations are not notified regarding the expiration of secrets in their key vault, and applications stop working when they expire. Users are complaining that Azure does not provide the infrastructure to monitor Key Vault secrets, keys, and certificates.</p>
			<h3 id="_idParaDest-272"><a id="_idTextAnchor280"/>Solution</h3>
			<p>The solution to this problem is to combine multiple Azure services and integrate them so that users can be proactively notified of the expiration of secrets. The solution will send notifications using two channels—email and SMS.</p>
			<p>The Azure services used to create this solution include the following:</p>
			<ul>
				<li>Azure Key Vault</li>
				<li><strong class="bold">Azure Active Directory</strong> (<strong class="bold">Azure AD</strong>)</li>
				<li>Azure Event Grid</li>
				<li>Azure Automation</li>
				<li>Logic Apps</li>
				<li>Azure Functions</li>
				<li>SendGrid</li>
				<li>Twilio SMS</li>
			</ul>
			<p>Now that you know the services that will be used as part of the solution, let's go ahead and create an architecture for this solution.</p>
			<h3 id="_idParaDest-273"><a id="_idTextAnchor281"/>Architecture</h3>
			<p>In the previous section, we explored the list of services that will be used in the solution. If we want to implement the solution, the services should be laid out in the proper order. The architecture will help us to develop the workflow and take a step closer to the solution.</p>
			<p>The architecture of the solution comprises multiple services, as shown in <em class="italics">Figure 11.14</em>:</p>
			<div>
				<div id="_idContainer271" class="IMG---Figure">
					<img src="image/Figure_11.14.jpg" alt="An architectural diagram of solution services such as PowerShell Runbook, Event Grid, Key Vault, Logic Apps, and so on, showing the flow of messages between the services."/>
				</div>
			</div>
			<h6>Figure 11.14: Solution architecture</h6>
			<p>Let's go through each of these services and understand their roles and the functionality that they provide in the overall solution.</p>
			<p><strong class="bold">Azure Automation</strong></p>
			<p>Azure Automation provides runbooks, and these runbooks can be executed to run logic using PowerShell, Python, and other scripting languages. Scripts can be executed either on-premises or in the cloud, which provides rich infrastructure and facilities to create scripts. These kinds of scripts are known as <strong class="bold">runbooks</strong>. Typically, runbooks implement a scenario such as stopping or starting a virtual machine, or creating and configuring storage accounts. It is quite easy to connect to the Azure environment from runbooks with the help of assets such as variables, certificates, and connections.</p>
			<p>In the current solution, we want to connect to Azure Key Vault, read all the secrets and keys stored within it, and fetch their expiry dates. These expiry dates should be compared with today's date and, if the expiry date is within a month, the runbook should raise a custom event on Event Grid using an Event Grid custom topic.</p>
			<p>An Azure Automation runbook using a PowerShell script will be implemented to achieve this. Along with the runbook, a scheduler will also be created that will execute the runbook once a day at 12.00 AM.</p>
			<p><strong class="bold">A custom Azure Event Grid topic</strong></p>
			<p>Once the runbook identifies that a secret or key is going to expire within a month, it will raise a new custom event and publish it to the custom topic created specifically for this purpose. Again, we will go into the details of the implementation in the next section.</p>
			<p><strong class="bold">Azure Logic Apps</strong></p>
			<p>A logic app is a serverless service that provides workflow capabilities. Our logic app will be configured to be triggered as and when an event is published on the custom Event Grid topic. After it is triggered, it will invoke the workflow and execute all the activities in it one after another. Generally, there are multiple activities, but for the purpose of this example, we will invoke one Azure function that will send both email and SMS messages. In a full-blown implementation, these notification functions should be implemented separately in separate Azure functions.</p>
			<p><strong class="bold">Azure Functions</strong></p>
			<p>Azure Functions is used to notify users and stakeholders about the expiration of secrets and keys using email and SMS. SendGrid is used to send emails, while Twilio is used to send SMS messages from Azure Functions.  </p>
			<p>In the next section, we will take a look at the prerequisites before implementing the solution.</p>
			<h3 id="_idParaDest-274"><a id="_idTextAnchor282"/>Prerequisites</h3>
			<p>You will need an Azure subscription with contributor rights at the very least. As we are only deploying services to Azure and no external services are deployed, the subscription is the only prerequisite. Let's go ahead and implement the solution.</p>
			<h3 id="_idParaDest-275"><a id="_idTextAnchor283"/>Implementation</h3>
			<p>A key vault should already exist. If not, one should be created.</p>
			<p>This step should be performed if a new Azure Key Vault instance needs to be provisioned. Azure provides multiple ways in which to provision resources. Prominent among them are Azure PowerShell and the Azure CLI. The Azure CLI is a command-line interface that works across platforms. The first task will be to provision a key vault in Azure. In this implementation, we will use Azure PowerShell to provision the key vault.</p>
			<p>Before Azure PowerShell can be used to create a key vault, it is important to log into Azure so that subsequent commands can be executed successfully to create the key vault.</p>
			<p><strong class="bold">Step 1: Provisioning an Azure Key Vault instance</strong></p>
			<p>The first step is to prepare the environment for the sample. This involves logging into the Azure portal, selecting an appropriate subscription, and then creating a new Azure resource group and a new Azure Key Vault resource:</p>
			<ol>
				<li value="1">Execute the <strong class="inline">Connect-AzAccount</strong> command to log into Azure. It will prompt for credentials in a new window.</li>
				<li>After a successful login, if there are multiple subscriptions available for the login ID provided, they will all be listed. It is important to select an appropriate subscription—this can be done by executing the <strong class="inline">Set-AzContext</strong> cmdlet:<p class="snippet">Set-AzContext -SubscriptionId xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx</p></li>
				<li>Create a new resource group in your preferred location. In this case, the name of the resource group is <strong class="inline">IntegrationDemo</strong> and it is created in the <strong class="inline">West Europe</strong> region:<p class="snippet">New-AzResourceGroup -Name IntegrationDemo -Location "West Europe" -Verbose</p></li>
				<li>Create a new Azure Key Vault resource—the name of the vault, in this case, is <strong class="inline">keyvaultbook</strong>, and it is enabled for deployment, template deployment, disk encryption, soft delete, and purge protection:<p class="snippet">New-AzKeyVault -Name keyvaultbook -ResourceGroupName IntegrationDemo -Location "West Europe" -EnabledForDeployment -EnabledForTemplateDeployment -EnabledForDiskEncryption -EnablePurgeProtection -Sku Standard - Verbose</p></li>
			</ol>
			<p>Please note that the key vault name needs to be unique. You may not be able to use the same name for two key vaults. The preceding command, when executed successfully, will create a new Azure Key Vault resource. The next step is to provide access to a service principal on the key vault.</p>
			<p><strong class="bold">Step 2: Creating a service principal</strong></p>
			<p>Instead of using an individual account to connect to Azure, Azure provides service principals, which are, in essence, service accounts that can be used to connect to Azure Resource Manager and run activities. Adding a user to an Azure directory/tenant makes them available everywhere, including in all resource groups and resources, due to the nature of security inheritance in Azure. Access must be explicitly revoked from resource groups for users if they are not allowed to access them. Service principals help by assigning granular access and control to resource groups and resources, and, if required, they can be given access to the subscription scope. They can also be assigned granular permissions, such as reader, contributor, or owner permissions.</p>
			<p>In short, service principals should be the preferred mechanism to consume Azure services. They can be configured either with a password or with a certificate key. Service principals can be created using the <strong class="inline">New-AzAdServicePrinicipal</strong> command, as shown here:</p>
			<p class="snippet">$sp = New-AzADServicePrincipal -DisplayName "keyvault-book" -Scope "/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx" -Role Owner -StartDate ([datetime]::Now) -EndDate $([datetime]::now.AddYears(1)) -Verbose</p>
			<p>The important configuration values are the scope and role. The scope determines the access area for the service application—it is currently shown at the subscription level. Valid values for scope are as follows:</p>
			<p class="snippet">/subscriptions/{subscriptionId}/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/subscriptions/{subscriptionId}/resourcegroups/{resourceGroupName}/providers/{resourceProviderNamespace}/{resourceType}/{resourceName}/subscriptions/{subscriptionId}/resourcegroups/{resourceGroupName}/providers/{resourceProviderNamespace}/{parentResourcePath}/{resourceType}/{resourceName}</p>
			<p>The role provides permissions to the assigned scope. The valid values are as follows:</p>
			<ul>
				<li>Owner</li>
				<li>Contributor</li>
				<li>Reader</li>
				<li>Resource-specific permissions</li>
			</ul>
			<p>In the preceding command, owner permissions have been provided to the newly created service principal.</p>
			<p>We can also use certificates if needed. For simplicity, we will proceed with the password.</p>
			<p>With the service principal we created, the secret will be hidden. To find out the secret, you can try the following commands:</p>
			<p class="snippet">$BSTR = [System.Runtime.InteropServices.Marshal]::SecureStringToBSTR($sp. Secret) </p>
			<p class="snippet">$UnsecureSecret = [System.Runtime.InteropServices.Marshal]::PtrToStringAuto($BSTR)</p>
			<p><strong class="inline">$UnsecureSecret</strong> will have your secret key.</p>
			<p>Along with the service principal, an Application Directory application will be created. The application acts as the global representation of our application across directories and the principal is like a local representation of the application. We can create multiple principals using the same application in a different directory. We can get the details of the application created using the <strong class="inline">Get-AzAdApplication</strong> command. We will save the output of this command to a variable, <strong class="inline">$app</strong>, as we will need this later:</p>
			<p class="snippet">$app = Get-AzAdApplication -DisplayName $sp.DisplayName</p>
			<p>We have now created a service principal using a secret; another secure way of creating one is using certificates. In the next section, we will create a service principal using certificates.</p>
			<p><strong class="bold">Step 3: Creating a service principal using certificates</strong></p>
			<p>To create a service principal using certificates, the following steps should be executed:</p>
			<ol>
				<li value="1"><strong class="bold">Create a self-signed certificate or purchase a certificate</strong>: A self-signed certificate is used to create this example end-to-end application. For real-life deployments, a valid certificate should be purchased from a certificate authority.<p>To create a self-signed certificate, the following command can be run. The self-signed certificate is exportable and stored in a personal folder on the local machine—it also has an expiry date:</p><p class="snippet">$currentDate = Get-Date $expiryDate = $currentDate.AddYears(1)$finalDate = $expiryDate.AddYears(1)$servicePrincipalName = "https://automation.book.com"$automationCertificate = New-SelfSignedCertificate -DnsName $servicePrincipalName -KeyExportPolicy Exportable -Provider "Microsoft Enhanced RSA and AES Cryptographic Provider" -NotAfter $finalDate -CertStoreLocation "Cert:\LocalMachine\My"</p></li>
				<li><strong class="bold">Export the newly created certificate</strong>: The new certificate must be exported to the filesystem so that later, it can be uploaded to other destinations, such as Azure AD, to create a service principal.<p>The commands used to export the certificate to the local filesystem are shown next. Please note that this certificate has both public and private keys, and so while it is exported, it must be protected using a password, and the password must be a secure string:</p><p class="snippet">$securepfxpwd = ConvertTo-SecureString -String 'password' -AsPlainText -Force # Password for the private key PFX certificate $cert1 = Get-Item -Path Cert:\LocalMachine\My\$($automationCertificate.Thumbprint)Export-PfxCertificate -Password $securepfxpwd -FilePath " C:\azureautomation.pfx" -Cert $cert1</p><p>The <strong class="inline">Get-Item</strong> cmdlet reads the certificate from the certificate store and stores it in the <strong class="inline">$cert1</strong> variable. The <strong class="inline">Export-PfxCertificate</strong> cmdlet actually exports the certificate in the certificate store to the filesystem. In this case, it is in the <strong class="inline">C:\book</strong> folder.</p></li>
				<li><strong class="bold">Read the content from the newly generated PFX file</strong>: An object of <strong class="inline">X509Certificate2</strong> is created to hold the certificate in memory, and the data is converted to a Base64 string using the <strong class="inline">System.Convert</strong> function:<p class="snippet">$newCert = New-Object System.Security.Cryptography.X509Certificates.X509Certificate2 -ArgumentList "C:\azureautomation.pfx", $securepfxpwd $newcertdata = [System.Convert]::ToBase64String($newCert.GetRawCertData())</p><p>We will be using this same principal to connect to Azure from the Azure Automation account. It is important that the application ID, tenant ID, subscription ID, and certificate thumbprint values are stored in a temporary location so that they can be used to configure subsequent resources:</p><p class="snippet">$adAppName = "azure-automation-sp"$ServicePrincipal = New-AzADServicePrincipal -DisplayName $adAppName -CertValue $newcertdata -StartDate $newCert.NotBefore -EndDate $newCert.NotAfter</p><p class="snippet">Sleep 10	</p><p class="snippet">New-AzRoleAssignment -ServicePrincipalName $ServicePrincipal.ApplicationId -RoleDefinitionName Owner -Scope /subscriptions/xxxxx-xxxxxxx-xxxxxx-xxxxxxx  </p></li>
			</ol>
			<p>We have our service principal ready. The key vault we created doesn't have an access policy set, which means no user or application will be able to access the vault. In the next step, we will grant permissions to the Application Directory application we created to access the key vault.</p>
			<p><strong class="bold">Step 4: Creating a key vault policy</strong></p>
			<p>At this stage, we have created the service principal and the key vault. However, the service principal still does not have access to the key vault. This service principal will be used to query and list all the secrets, keys, and certificates from the key vault, and it should have the necessary permissions to do so.</p>
			<p>To provide the newly created service principal permission to access the key vault, we will go back to the Azure PowerShell console and execute the following command:</p>
			<p>Set-AzKeyVaultAccessPolicy -VaultName keyvaultbook -ResourceGroupName IntegrationDemo -ObjectId $ServicePrincipal.Id -PermissionsToKeys get,list,create -PermissionsToCertificates get,list,import -PermissionsToSecrets get,list -Verbose</p>
			<p>Referring to the previous command block, take a look at the following points:</p>
			<ul>
				<li><strong class="inline">Set-AzKeyVaultAccessPolicy</strong> provides access permissions to users, groups, and service principals. It accepts the key vault name and the service principal object ID. This object is different from the application ID. The output of the service principal contains an <strong class="inline">Id</strong> property, as shown here:</li>
			</ul>
			<div>
				<div id="_idContainer272" class="IMG---Figure">
					<img src="image/Figure_11.15.jpg" alt="The output displaying the details of the service principal, such as Service principal name, Application ID, Object Type, and so on."/>
				</div>
			</div>
			<h6>Figure 11.15: Finding the object ID of the service principal</h6>
			<ul>
				<li><strong class="inline">PermissionsToKeys</strong> provides access to keys in the key vault, and the <strong class="inline">get</strong>, <strong class="inline">list</strong>, and <strong class="inline">create</strong> permissions are provided to this service principal. There is no write or update permission provided to this principal.</li>
				<li><strong class="inline">PermissionsToSecrets</strong> provides access to secrets in the key vault, and the <strong class="inline">get</strong> and <strong class="inline">list</strong> permissions are provided to this service principal. There is no write or update permission provided to this principal.</li>
				<li><strong class="inline">PermissionsToCertificates</strong> provides access to secrets in the key vault, and <strong class="inline">get</strong>, <strong class="inline">import</strong>, and <strong class="inline">list</strong> permissions are provided to this service principal. There is no write or update permission provided to this principal.</li>
			</ul>
			<p>At this point, we have configured the service principal to work with the Azure key vault. The next part of the solution is to create an Automation account.</p>
			<p><strong class="bold">Step 5: Creating an Automation account</strong></p>
			<p>Just like before, we will be using Azure PowerShell to create a new Azure Automation account within a resource group. Before creating a resource group and an Automation account, a connection to Azure should be established. However, this time, use the credentials for the service principal to connect to Azure. The steps are as follows:</p>
			<ol>
				<li value="1">The command to connect to Azure using the service application is as follows. The value is taken from the variables that we initialized in the previous steps:<p class="snippet">Login-AzAccount -ServicePrincipal -CertificateThumbprint $newCert.Thumbprint -ApplicationId $ServicePrincipal.ApplicationId -Tenant "xxxx-xxxxxx-xxxxx-xxxxx" </p></li>
				<li>Make sure that you have access by checking <strong class="inline">Get-AzContext</strong> as shown here. Make a note of the subscription ID as it will be needed in subsequent commands:<p class="snippet">Get-AzContext</p></li>
				<li>After connecting to Azure, a new resource containing the resources for the solution and a new Azure Automation account should be created. You are naming the resource group <strong class="inline">VaultMonitoring</strong>, and creating it in the <strong class="inline">West Europe</strong> region. You will be creating the remainder of the resources in this resource group as well:<p class="snippet">$IntegrationResourceGroup = "VaultMonitoring"$rgLocation = "West Europe"$automationAccountName = "MonitoringKeyVault"New-AzResourceGroup -name $IntegrationResourceGroup -Location $rgLocation New-AzAutomationAccount -Name $automationAccountName -ResourceGroupName $IntegrationResourceGroup -Location $rgLocation -Plan Free</p></li>
				<li>Next, create three automation variables. The values for these, that is, the subscription ID, tenant ID, and application ID, should already be available using the previous steps:<p class="snippet">New-AzAutomationVariable -Name "azuresubscriptionid" -AutomationAccountName $automationAccountName -ResourceGroupName $IntegrationResourceGroup -Value " xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx " -Encrypted $true New-AzAutomationVariable -Name "azuretenantid" -AutomationAccountName $automationAccountName -ResourceGroupName $IntegrationResourceGroup -Value " xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx " -Encrypted $true New-AzutomationVariable -Name "azureappid" -AutomationAccountName $automationAccountName -ResourceGroupName $IntegrationResourceGroup -Value " xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx " -Encrypted $true</p></li>
				<li>Now it's time to upload a certificate, which will be used to connect to Azure from Azure Automation:<p class="snippet">$securepfxpwd = ConvertTo-SecureString -String 'password' -AsPlainText -Force # Password for the private key PFX certificate New-AzAutomationCertificate -Name "AutomationCertifcate" -Path "C:\book\azureautomation.pfx" -Password $securepfxpwd -AutomationAccountName $automationAccountName -ResourceGroupName $IntegrationResourceGroup</p></li>
				<li>The next step is to install PowerShell modules related to Key Vault and Event Grid in the Azure Automation account, as these modules are not installed by default.</li>
				<li>From the Azure portal, navigate to the already-created <strong class="inline">VaultMonitoring</strong> resource group by clicking on <strong class="bold">Resource Groups</strong> in the left-hand menu.</li>
				<li>Click on the already-provisioned Azure Automation account, <strong class="bold">MonitoringKeyVault</strong>, and then click on <strong class="bold">Modules</strong> in the left-hand menu. The Event Grid module is dependent on the <strong class="inline">Az.profile</strong> module, and so we have to install it before the Event Grid module.</li>
				<li>Click on <strong class="bold">Browse Gallery</strong> in the top menu and type <strong class="inline">Az.profile</strong> in the search box, as shown in <em class="italics">Figure 11.16</em>:<div id="_idContainer273" class="IMG---Figure"><img src="image/Figure_11.16.jpg" alt="Searching for the Az.profile module in the Browser Gallery."/></div><h6> Figure 11.16: The Az.Profile module in the module gallery</h6></li>
				<li>From the search results, select <strong class="inline">Az.Profile</strong> and click on the <strong class="bold">Import</strong> button in the top menu. Finally, click on the <strong class="bold">OK</strong> button. This step takes a few seconds to complete. After a few seconds, the module should be installed.</li>
				<li>The status of the installation can be checked from the <strong class="bold">Module</strong> menu item. <em class="italics">Figure 11.17</em> demonstrates how we can import a module:<div id="_idContainer274" class="IMG---Figure"><img src="image/Figure_11.17.jpg" alt="The Az.Profile pane displaying the status of the module"/></div><h6>Figure 11.17: Az.Profile module status</h6></li>
				<li>Perform <em class="italics">steps 9</em>, <em class="italics">10</em>, and <em class="italics">11</em> again in order to import and install the <strong class="inline">Az.EventGrid</strong> module. If you are warned to install any dependencies before proceeding, go ahead and install the dependencies first.</li>
				<li>Perform <em class="italics">steps 9</em>, <em class="italics">10</em>, and <em class="italics">11</em> again in order to import and install the <strong class="inline">Az.KeyVault</strong> module. If you are warned to install any dependencies before proceeding, go ahead and install the dependency first.</li>
			</ol>
			<p>Since we have imported the necessary modules, let's go ahead and create the Event Grid topic.</p>
			<p><strong class="bold">Step 6: Creating an Event Grid topic</strong></p>
			<p>If you recall the architecture that we used, we need an Event Grid topic. Let's create one.</p>
			<p>The command that's used to create an Event Grid topic using PowerShell is as follows:</p>
			<p class="snippet">New-AzEventGridTopic -ResourceGroupName VaultMonitoring -Name azureforarchitects-topic -Location "West Europe" </p>
			<p>The process of creating an Event Grid topic using the Azure portal is as follows:</p>
			<ol>
				<li value="1">From the Azure portal, navigate to the already-created <strong class="inline">Vaultmonitoring</strong> resource group by clicking on <strong class="bold">Resource Groups</strong> in the left-hand menu.</li>
				<li>Next, click on the <strong class="bold">+Add</strong> button and search for <strong class="inline">Event Grid Topic</strong> in the search box. Select it and then click on the <strong class="bold">Create</strong> button.</li>
				<li>Fill in the appropriate values in the resultant form by providing a name, selecting a subscription, and selecting the newly created resource group, the location, and the event schema.</li>
			</ol>
			<p>As <a id="_idTextAnchor284"/>we already discussed, the Event Grid topic provides an endpoint where the source will send the data. Since we have our topic ready, let's prepare the source Automation account.</p>
			<p><strong class="bold">Step 7: Setting up the runbook</strong></p>
			<p>This step will focus on creating an Azure Automation account and PowerShell runbooks that will contain the core logic of reading Azure key vaults and retrieving secrets stored within them. The steps required for configuring Azure Automation are as follows:</p>
			<ol>
				<li value="1"><strong class="bold">Create the Azure Automation runbook</strong>: From the Azure portal, navigate to the already-created <strong class="inline">Vaultmonitoring</strong> resource group by clicking on <strong class="bold">Resource Groups</strong> in the left-hand menu.</li>
				<li>Click on the already-provisioned Azure Automation account, <strong class="inline">MonitoringKeyVault</strong>. Then, click on <strong class="bold">Runbooks</strong> in the left-hand menu, and click on <strong class="bold">+Add a Runbook</strong> from the top menu.</li>
				<li>Click on <strong class="bold">Create a new Runbook</strong> and provide a name. Let's call this runbook <strong class="bold">CheckExpiredAssets</strong>, and then set <strong class="bold">Runbook type</strong> to <strong class="bold">PowerShell</strong>:<div id="_idContainer275" class="IMG---Figure"><img src="image/Figure_11.18.jpg" alt="Creating a Runbook and giving the Runbook name as CheckExpiredAssets, and Runbook type as PowerShell."/></div><h6>Figure 11.18: Creating a runbook</h6></li>
				<li><strong class="bold">Code the runbook</strong>: Declare a few variables to hold the subscription ID, tenant ID, application ID, and certificate thumbprint information. These values should be stored in Azure Automation variables, and the certificate should be uploaded to Automation certificates. The key used for the uploaded certificate is <strong class="inline">AutomationCertifcate</strong>. The values are retrieved from these stores and are assigned to the variables, as shown next:<p class="snippet">$subscriptionID = get-AutomationVariable "azuresubscriptionid"$tenantID = get-AutomationVariable "azuretenantid"$applicationId = get-AutomationVariable "azureappid"$cert = get-AutomationCertificate "AutomationCertifcate"$certThumbprint = ($cert.Thumbprint).ToString()</p></li>
				<li>The next code within the runbook helps to log into Azure using the service principal with values from the variables declared previously. Also, the code selects an appropriate subscription. The code is shown next:<p class="snippet">Login-AzAccount -ServicePrincipal -CertificateThumbprint $certThumbprint -ApplicationId $applicationId -Tenant $tenantID Set-AzContext -SubscriptionId $subscriptionID</p><p>Since Azure Event Grid was provisioned in <em class="italics">step 6</em> of this section, its endpoint and keys are retrieved using the <strong class="inline">Get-AzEventGridTopic</strong> and <strong class="inline">Get-AzEventGridTopicKey</strong> cmdlets.</p><p>Azure Event Grid generates two keys—a primary and a secondary. The first key reference is taken as follows:</p><p class="snippet">$eventGridName = "ExpiredAssetsKeyVaultEvents"$eventGridResourceGroup = "VaultMonitoring"$topicEndpoint = (Get-AzEventGridTopic -ResourceGroupName $eventGridResourceGroup -Name $eventGridName).Endpoint $keys = (Get-AzEventGridTopicKey -ResourceGroupName $eventGridResourceGroup -Name $eventGridName ).Key1</p></li>
				<li>Next, all key vaults that were provisioned within the subscription are retrieved using iteration. While looping, all secrets are retrieved using the <strong class="inline">Get-AzKeyVaultSecret</strong> cmdlet.<p>The expiry date of each secret is compared to the current date, and if the difference is less than a month, it generates an Event Grid event and publishes it using the <strong class="inline">invoke-webrequest</strong> command.</p><p>The same steps are executed for certificates stored within the key vault. The cmdlet used to retrieve all the certificates is <strong class="inline">Get-AzKeyVaultCertificate</strong>.</p><p>The event that is published to Event Grid should be in the JSON array. The generated message is converted to JSON using the <strong class="inline">ConvertTo-Json</strong> cmdlet and then converted to an array by adding <strong class="inline">[</strong> and <strong class="inline">]</strong> as a prefix and suffix.</p><p>In order to connect to Azure Event Grid and publish the event, the sender should supply the key in its header. The request will fail if this data is missing in the request payload:</p><p class="snippet">$keyvaults = Get-AzureRmKeyVault foreach($vault in $keyvaults) {$secrets = Get-AzureKeyVaultSecret -VaultName $vault.VaultName foreach($secret in $secrets) {if( ![string]::IsNullOrEmpty($secret.Expires) ) {if($secret.Expires.AddMonths(-1) -lt [datetime]::Now){$secretDataMessage = @{id = [System.guid]::NewGuid()subject = "Secret Expiry happening soon !!"eventType = "Secret Expiry"eventTime = [System.DateTime]::UtcNow data = @{"ExpiryDate" = $secret.Expires "SecretName" = $secret.Name.ToString()"VaultName" = $secret.VaultName.ToString()"SecretCreationDate" = $secret.Created.ToString()"IsSecretEnabled" = $secret.Enabled.ToString()"SecretId" = $secret.Id.ToString()}}...Invoke-WebRequest -Uri $topicEndpoint -Body $finalBody -Headers $header -Method Post -UseBasicParsing }}Start-Sleep -Seconds 5 }}</p></li>
				<li>Publish the runbook by clicking on the <strong class="bold">Publish</strong> button, as shown in <em class="italics">Figure 11.19</em>:<div id="_idContainer276" class="IMG---Figure"><img src="image/Figure_11.19.jpg" alt="The Runbook is published and the script is displayed on the screen in the portal."/></div><h6>Figure 11.19: Publishing the runbook</h6></li>
				<li><strong class="bold">Scheduler</strong>: Create an Azure Automation scheduler asset to execute this runbook once every day at 12.00 AM. Click on <strong class="bold">Schedules</strong> from the left-hand menu of Azure Automation and click on <strong class="bold">+Add a schedule</strong> in the top menu. </li>
				<li>Provide scheduling information in the resulting form.</li>
			</ol>
			<p>This should conclude the configuration of the Azure Automation account.</p>
			<p><strong class="bold">Step 8: Working with SendGrid </strong></p>
			<p>In this step, we will be creating a new SendGrid resource. The SendGrid resource is used to send emails from the application without needing to install a <strong class="bold">Simple Mail Transfer Protocol (SMTP)</strong> server. It provides a REST API and a C# <strong class="bold">Software Development Kit (SDK)</strong>, by means of which it is quite easy to send bulk emails. In the current solution, Azure Functions will be used to invoke the SendGrid APIs to send emails, and so this resource needs to be provisioned. This resource has separate costing and is not covered as part of the Azure cost—there is a free tier available that can be used for sending emails:</p>
			<ol>
				<li value="1">A <strong class="inline">SendGrid</strong> resource is created just like any other Azure resource. Search for <strong class="inline">sendgrid</strong>, and we will get <strong class="bold">SendGrid Email Delivery</strong> in the results.</li>
				<li>Select the resource and click on the <strong class="bold">Create</strong> button to open its configuration form.</li>
				<li>Select an appropriate pricing tier.</li>
				<li>Provide the appropriate contact details. </li>
				<li>Tick the <strong class="bold">Terms of use</strong> check box.</li>
				<li>Complete the form and then click on the <strong class="bold">Create</strong> button.</li>
				<li>After<a id="_idTextAnchor285"/> the resource is provisioned, click on the <strong class="bold">Manage</strong> button in the top menu—this will open the SendGrid website. The website may request email configuration. Then, select <strong class="bold">API Keys</strong> from the <strong class="bold">Settings</strong> section and click on the <strong class="bold">Create API Key</strong> button:<div id="_idContainer277" class="IMG---Figure"><img src="image/Figure_11.20.jpg" alt="The API Keys window displaying the message “Get started creating API Keys”."/></div><h6>Figure 11.20: Creating API keys for SendGrid</h6></li>
				<li>From the resulting window, select <strong class="bold">Full Access</strong> and click on the <strong class="bold">Create &amp; View</strong> button. This will create the key for the SendGrid resource; keep a note of this key, as it will be used with the Azure Functions configuration for SendGrid:<div id="_idContainer278" class="IMG---Figure"><img src="image/Figure_11.21.jpg" alt="The ‘Create API Key’ pane displaying the option for API key permissions as Full Access, Restricted Access, and Billing Access; of which we select the Full Access option."/></div></li>
			</ol>
			<h6>Figure 11.21: Setti<a id="_idTextAnchor286"/>ng up the access level in the SendGrid portal</h6>
			<p>Now that we have configured access levels for SendGrid, let's configure another third-party service, which is called Twilio.</p>
			<p><strong class="bold">Step 9: Getting started with Twilio</strong></p>
			<p>In this step, we will be creating a new Twilio account. Twilio is used for sending bulk SMS messages. To create an account with Twilio, navigate to <a href="http://twilio.com">twilio.com</a> and create a new account. After successfully creating an account, a mobile number is generated that can be used to send SMS messages to receivers:</p>
			<div>
				<div id="_idContainer279" class="IMG---Figure">
					<img src="image/Figure_11.22.jpg" alt="Choosing the first Twilio number."/>
				</div>
			</div>
			<h6>Figure 11.22: Choosing a Twilio number</h6>
			<p>The Twilio account provides both production and test keys. Copy the test key and token to a temporary location, such as Notepad, as they will be required later within Azure Functions:</p>
			<div>
				<div id="_idContainer280" class="IMG---Figure">
					<img src="image/Figure_11.23.jpg" alt="The Twilio account dashboard displaying project info such as Trial Balance, Trial number, Account SID, and Auth token."/>
				</div>
			</div>
			<h6>Figure 11.23: Setting up Twilio</h6>
			<p>We have SendGrid and Twilio in place for the notification service; however, we need something that can take the event and notify the users. Here comes the role of a function app. In the next section, we will be creating a function app that will help with sending SMS and emails.</p>
			<p><strong class="bold">Step 10: Setting up a function app</strong></p>
			<p>In this step, we will be creating a new function app responsible for sending emails and SMS notifications. The purpose of the function app within the solution is to send notification messages to users regarding the expiry of secrets in the key vault. A single function will be responsible for sending both emails and SMS messages—note that this could have been divided into two separate functions. The first step is to create a new function app and host a function within it:</p>
			<ol>
				<li value="1">As we have done before, navigate to your resource group, click on the <strong class="bold">+Add</strong> button in the top menu, and search for the <strong class="inline">function app</strong> resource. Then, click on the <strong class="bold">Create</strong> button to get the <strong class="bold">Function App</strong> form.</li>
				<li>Fill in the <strong class="bold">Function App</strong> form and click on the <strong class="bold">Create</strong> button. The name of the function app must be unique across Azure.</li>
				<li>Once the function app is provisioned, create a new function called <strong class="inline">SMSandEMailFunction</strong> by clicking on the <strong class="bold">+</strong> button next to the <strong class="bold">Functions</strong> item in the left-hand menu. Then, select <strong class="bold">In-portal</strong> from the central dashboard.</li>
				<li>Select <strong class="bold">HTTP trigger</strong> and name it <strong class="inline">SMSandEMailFunction</strong>. Then, click on the <strong class="bold">Create</strong> button—the <strong class="bold">Authorization level</strong> option can be any value.</li>
				<li>Remove the default code, replace it with the code shown in the following listing, and then click on the <strong class="bold">Save</strong> button in the top menu:<p class="snippet">#r "SendGrid"#r "Newto<a id="_idTextAnchor287"/>nsoft.Json"#r "Twilio.Api"using System.Net;using System;using SendGrid.Helpers.Mail;using Microsoft.Azure.WebJobs.Host;using Newtonsoft.Json;using Twilio;using System.Configuration;public static HttpResponseMessage Run(HttpRequestMessage req, TraceWriter log, out Mail message,out SMSMessage sms){log.Info("C# HTTP trigger function processed a request.");string alldata = req.Content.ReadAsStringAsync().GetAwaiter().GetResult();message = new Mail();var personalization = new Personalization();personalization.AddBcc(new Email(ConfigurationManager.AppSettings["bccStakeholdersEmail"]));personalization.AddTo(new Email(ConfigurationManager.AppSettings["toStakeholdersEmail"]));var messageContent = new Content("text/html", alldata);message.AddContent(messageContent);message.AddPersonalization(personalization);message.Subject = "Key Vault assets Expiring soon..";message.From = new Email(ConfigurationManager.AppSettings["serviceEmail"]);string msg = alldata;sms = new SMSMessage();sms.Body = msg;sms.To = ConfigurationManager.AppSettings["adminPhone"];sms.From = ConfigurationManager.AppSettings["servicePhone"];return req.CreateResponse(HttpStatusCode.OK, "Hello ");}</p></li>
				<li>Click on the function app name in the left-hand menu and click again on the <strong class="bold">Application settings</strong> link in the main window:<div id="_idContainer281" class="IMG---Figure"><img src="image/Figure_11.24.jpg" alt="Navigating to the NotificationFunctionAppBook pane and then selecting the Application settings option."/></div><h6>Figure 11.24: Navigating to Application settings</h6></li>
				<li>Navigate to the <strong class="bold">Application settings</strong> section, as shown in <em class="italics">Figure 11.24</em>, and add a few entries by clicking on <strong class="bold">+ Add new setting</strong> for each entry.<p>Note that the entries are in the form of key-value pairs, and the values should be actual real-time values. Both <strong class="inline">adminPhone</strong> and <strong class="inline">servicePhone</strong> should already be configured on the Twilio website. <strong class="inline">servicePhone</strong> is the phone number generated by Twilio that is used for sending SMS messages, and <strong class="inline">adminPhone</strong> is the phone number of the administrator to whom the SMS should be sent.</p><p>Also note that Twilio expects the destination phone number to be in a particular format depending on the country (for India, the format is <strong class="inline">+91 xxxxx xxxxx</strong>). Note the spaces and country code in the number.</p><p>We also need to add the keys for both SendGrid and Twilio within the application settings. These settings are mentioned in the following list. You may already have these values handy because of activities performed in earlier steps:</p><ul><li>The value of <strong class="inline">SendGridAPIKeyAsAppSetting</strong> is the key for SendGrid.</li><li><strong class="inline">TwilioAccountSid</strong> is the system identifier for the Twilio account. This value was already copied and stored in a temporary location in <em class="italics">Step 9: Getting started with Twilio</em>.</li><li><strong class="inline">TwilioAuthToken</strong> is the token for the Twilio account. This value was already copied and stored in a temporary place in an earlier step.</li></ul></li>
			</ol>
			<ol>
				<li value="8">Save the settings by clicking on the <strong class="bold">Save</strong> button in the top menu:<div id="_idContainer282" class="IMG---Figure"><img src="image/Figure_11.25.jpg" alt="The App settings pane displaying the App setting name and the value for it."/></div><h6>Figure 11.25: Configuring application settings</h6></li>
				<li>Click on the <strong class="bold">Integrate</strong> link in the left-hand menu just below the name of the function, and click on <strong class="bold">+ New Output</strong>. This is to add an output for the SendGrid service:<div id="_idContainer283" class="IMG---Figure"><img src="image/Figure_11.26.jpg" alt="Adding output to the function app."/></div><h6>Figure 11.26: Adding an output to the function app</h6></li>
				<li>Next, select <strong class="bold">SendGrid</strong>; it might ask you to install the SendGrid extension. Install the extension, which will take a couple of minutes:<div id="_idContainer284" class="IMG---Figure"><img src="image/Figure_11.27.jpg" alt="Configuring the function app by selecting the SendGrid option."/></div><h6>Figure 11.27: Configuring a function app</h6></li>
				<li>After installing the extension, the output configuration form appears. The important configuration items in this form are <strong class="bold">Message parameter name</strong> and <strong class="bold">SendGrid API Key App Setting</strong>. Leave the default value for <strong class="bold">Message parameter name</strong> and click on the drop-down list to select <strong class="bold">SendGridAPIKeyAsAppSetting</strong> as the API app setting key. This was already configured in a previous step within the app settings configuration. The form should be configured as shown in <em class="italics">Figure 11.28</em>, and then you need to click on the <strong class="bold">Save</strong> button:<div id="_idContainer285" class="IMG---Figure"><img src="image/Figure_11.28.jpg" alt="Setting up the SendGrid output by adding values in fields such as Message parameter name, To address, From address, and so on."/></div><h6>Figure 11.28: Setting up SendGrid</h6></li>
				<li>Click on <strong class="bold">+ New Output</strong> again; this is to add an output for the Twilio service.</li>
				<li>Then, select <strong class="bold">Twilio SMS</strong>. It might ask you to install the Twilio SMS extension. Install the extension, which will take a couple of minutes.</li>
				<li>After installing the extension, the output configuration form appears. The important configuration items in this form are <strong class="bold">Message parameter name</strong>, <strong class="bold">Account SID setting</strong>, and <strong class="bold">Auth Token setting</strong>. Change the default value for <strong class="bold">Message parameter name</strong> to <strong class="inline">sms</strong>. This is done because the <strong class="inline">message</strong> parameter is already used for the SendGrid service parameter. Ensure that the value of <strong class="bold">Account SID setting</strong> is <strong class="inline">TwilioAccountSid</strong> and that the value of <strong class="bold">Auth Token setting</strong> is <strong class="bold">TwilioAuthToken</strong>. These values were already configured in a previous step of the app settings configuration. The form should be configured as shown in <em class="italics">Figure 11.29</em>, and then you should click on <strong class="bold">Save</strong>:<div id="_idContainer286" class="IMG---Figure"><img src="image/Figure_11.29.jpg" alt="Setting up Twilio SMS output."/></div></li>
			</ol>
			<h6>Figure 11.29: Setting up Twilio SMS output</h6>
			<p>Our SendGrid and Twilio accounts are ready. Now it's time to use the connectors and add them to the logic app. In the next part, we will create the logic app and will use connectors to work with the resources we have created so far.</p>
			<p><strong class="bold">Step 11: Creating a logic app</strong></p>
			<p>In this step, we will be creating a new logic app workflow. We have authored an Azure Automation runbook that queries all the secrets in all key vaults and publishes an event if it finds any of them expiring within a month. The logic app's workflow acts as a subscriber to these events:</p>
			<ol>
				<li value="1">The first step within the <strong class="bold">Logic App</strong> menu is to create a logic app workflow.</li>
				<li>Fill in the resultant form after clicking on the <strong class="bold">Create</strong> button. We are provisioning the logic app in the same resource group as the other resources for this solution.</li>
				<li>After the logic app is provisioned, it opens the designer window. Select <strong class="bold">Blank Logic App</strong> from the <strong class="bold">Templates</strong> section.</li>
				<li>In the resultant window, add a trigger that can subscribe to Event Grid. Logic Apps provides a trigger for Event Grid, and you can search for this to see whether it's available.</li>
				<li>Next, select the <strong class="bold">When a resource event occurs (preview)</strong> trigger:<div id="_idContainer287" class="IMG---Figure"><img src="image/Figure_11.30.jpg" alt="In the Azure Event Grid pane, selecting a trigger from the Event Grid."/></div><h6>Figure 11.30: Selecting a trigger from Event Grid</h6></li>
				<li>In the resultant window, select <strong class="bold">Connect with Service Principal</strong>.<p>Provide the service principal details, including the application ID (<strong class="bold">Client ID</strong>), tenant ID, and password. This trigger does not accept a service principal that authenticates with the certificate—it accepts a service principal only with a password. Create a new service principal at this stage that authenticates with a password (the steps for creating a service principal based on password authentication were covered earlier, in <em class="italics">step 2</em>) and use the details of the newly created service principal for Azure Event Grid configuration, as shown in <em class="italics">Figure 11.31</em>:</p><div id="_idContainer288" class="IMG---Figure"><img src="image/Figure_11.31.jpg" alt="Adding the service principal details for the connection, such as Connection Name, Client ID, Client Secret, and Tenant."/></div><h6>Figure 11.31: Providing the service principal details for connection</h6></li>
				<li>Select the subscription. Based on the scope of the service principal, this will get auto-filled. Select <strong class="bold">Microsoft.EventGrid.Topics</strong> as the <strong class="bold">Resource Type</strong> value and set the name of the custom topic as <strong class="bold">ExpiredAssetsKeyVaultEvents</strong>:<div id="_idContainer289" class="IMG---Figure"><img src="image/Figure_11.32.jpg" alt="On the ‘When a resource event occurs (preview)’ pane, adding Event Grid trigger details such as Subscription, Resource type, Resource Name, and so on."/></div><h6>Figure 11.32: Providing Event Grid trigger details</h6></li>
				<li>The previous step will create a connector, and the connection information can be changed by clicking on <strong class="bold">Change connection</strong>.</li>
				<li>The final configuration of the Event Grid trigger should be similar to <em class="italics">Figure 11.33</em>:<div id="_idContainer290" class="IMG---Figure"><img src="image/Figure_11.33.jpg" alt="The ‘When a resource event occurs (preview)’ pane displaying the final overview of the Event Grid trigger."/></div><h6>Figure 11.33: Event Grid trigger overview</h6></li>
				<li>Add a new <strong class="bold">Parse JSON</strong> activity after the Event Grid trigger—this activity needs the JSON schema. Generally, the schema is not available, but this activity helps generate the schema if valid JSON is provided to it:<div id="_idContainer291" class="IMG---Figure"><img src="image/Figure_11.34.jpg" alt="The Parse JSON activity showing the Content as Data object and the schema as JSON script."/></div><h6>Figure 11.34: Parse JSON activity</h6></li>
				<li>Click on <strong class="bold">Use sample payload to generate schema</strong> and provide the following data:<p class="snippet">{"ExpiryDate": "","SecretName": "","VaultName": "","SecretCreationDate": "","IsSecretEnabled": "","SecretId": ""}</p><p>A question might arise here regarding the sample payload. At this stage, how do you calculate the payload that's generated by the Event Grid publisher? The answer to this lies in the fact that this sample payload is exactly the same as is used in the data element in the Azure Automation runbook. You can take a look at that code snippet again:</p><p class="snippet">data = @{"ExpiryDate" = $certificate.Expires "CertificateName" = $certificate.Name.ToString()"VaultName" = $certificate.VaultName.ToString()"CertificateCreationDate" = $certificate.Created.ToString()"IsCertificateEnabled" = $certificate.Enabled.ToString()"CertificateId" = $certificate.Id.ToString()}</p></li>
				<li>The <strong class="bold">Content</strong> box should contain dynamic content coming out from the previous trigger, as demonstrated in <em class="italics">Figure 11.35</em>:<div id="_idContainer292" class="IMG---Figure"><img src="image/Figure_11.35.jpg" alt="Providing dynamic content to the Parse JSON activity."/></div><h6>	Figure 11.35: Providing dynamic content to the Parse JSON activity </h6></li>
				<li>Add another <strong class="bold">Azure Functions</strong> action after <strong class="bold">Parse JSON</strong>, and then select <strong class="bold">Choose an Azure function</strong>. Select the Azure function apps called <strong class="bold">NotificationFunctionAppBook</strong> and <strong class="bold">SMSAndEmailFunction</strong>, which were created earlier:<div id="_idContainer293" class="IMG---Figure"><img src="image/Figure_11.36.jpg" alt="In the Choose action pane, entering the keywords ‘Azure function’ in the search box and then selecting the Azure Functions action."/></div><h6>Figure 11.36: Adding an Azure Functions action</h6></li>
				<li>Click on the <strong class="bold">Request Body</strong> text area and fill it with the following code. This is done to convert the data into JSON before sending it to the Azure function:<p class="snippet">{"alldata" :}</p></li>
				<li>Place the cursor after the <strong class="inline">":"</strong> in the preceding code and click on <strong class="bold">Add dynamic content | Body</strong> from the previous activity:<div id="_idContainer294" class="IMG---Figure"><img src="image/Figure_11.37.jpg" alt="Converting data to JSON before sending it to an Azure function."/></div><h6>Figure 11.37: Converting data to JSON before sending it to an Azure function</h6></li>
				<li>Save the entire logic app; it should look as follows:<div id="_idContainer295" class="IMG---Figure"><img src="image/Figure_11.38.jpg" alt="The Logic App workflow displayed in the widget, showing the action flow from when the resource event occured, to the Parse JSON action, and finally, reaching the SMSandEmailFunction action."/></div></li>
			</ol>
			<h6>Figure 11.38: Logic app workflow</h6>
			<p>Once you save the logic app, your solution is ready to be tested. If you don't have any keys or secrets, try adding them with an expiry date so that you can confirm whether your solution is working.</p>
			<h3 id="_idParaDest-276"><a id="_idTextAnchor288"/>Testing</h3>
			<p>Upload some secrets and certificates that have expiry dates to Azure Key Vault and execute the Azure Automation runbook. The runbook is scheduled to run on a schedule. Additionally, the runbook will publish events to Event Grid. The logic app should be enabled, and it will pick the event and finally invoke the Azure function to send email and SMS notifications.</p>
			<p>The email should look as follows:</p>
			<div>
				<div id="_idContainer296" class="IMG---Figure">
					<img src="image/Figure_11.39.jpg" alt="A widget showing that the email has been received regarding the expiring keys and the header reads “Key Vault Assets Expiring Soon…”"/>
				</div>
			</div>
			<h6>Figure 11.39: Email received regarding the expiring keys</h6>
			<p>In this exercise, we had a problem, we architected a solution, and we implemented it. This is exactly what happens in the role of an architect. Customers will have specific requirements and, based on those, you must develop a solution. On that note, we are concluding this chapter. Let's do a quick recap of what we have discussed.</p>
			<h2 id="_idParaDest-277"><a id="_idTextAnchor289"/>Summary</h2>
			<p>This chapter introduced Logic A<a id="_idTextAnchor290"/>pps and demonstrated a complete end-to-end solution using multiple Azure services. The chapter focused heavily on creating an architecture that integrated multiple Azure services to create an end-to-end solution. The services used in the solution were Azure Automation, Azure Logic Apps, Azure Event Grid, Azure Functions, SendGrid, and Twilio. These services were implemented through the Azure portal and PowerShell using service principals as service accounts. The chapter also showed a number of ways of creating service principals with password and certificate authentication.</p>
			<p>A solution to a problem can be found in multiple ways. You could use an Outlook trigger in a logic app instead of SendGrid. There will be many solutions to a problem—the one to go with depends on what approach you are taking. The more familiar you are with the services, the greater the number of options you will have. In the next chapter, you will learn about the importance of events in both Azure and Azure application architecture.</p>
		</div>
	</body></html>