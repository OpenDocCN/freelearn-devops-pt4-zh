<html><head></head><body>
		<div>
			<div id="_idContainer298" class="Content">
			</div>
		</div>
		<div id="_idContainer299" class="Content">
			<h1 id="_idParaDest-278">12. <a id="_idTextAnchor291"/>Azure Big Data eventing solutions</h1>
		</div>
		<div id="_idContainer332" class="Content">
			<p>Events are everywhere! Any activity or task that changes the state of a work item generates an event. Due to a lack of infrastructure and the non-availability of cheap devices, there previously was not much traction for the <strong class="bold">Internet of Things</strong> (<strong class="bold">IoT</strong>). Historically, organizations used hosted environments from <strong class="bold">internet service providers</strong> (<strong class="bold">ISPs</strong>) that just had monitoring systems on top of them. These monitoring systems raised events that were few and far between.</p>
			<p>However, with the advent of the cloud, things are changing rapidly. With increased deployments on the cloud, especially of <strong class="bold">Platform as a Service</strong> (<strong class="bold">PaaS</strong>) services, organizations no longer need much control over the hardware and the platform, and now every time there is a change in an environment, an event is raised. With the emergence of cloud events, IoT has gained a lot of prominence and events have started to take center stage.</p>
			<p>Another recent phenomenon has been the rapid burst of growth in the availability of data. The velocity, variety, and volume of data has spiked, and so has the need for solutions for storing and processing data. Multiple solutions and platforms have emerged, such as Hadoop, data lakes for storage, data lakes for analytics, and machine learning services.</p>
			<p>Apart from storage and analytics, there is also a need for services that are capable of ingesting millions upon millions of events and messages from various sources. There is also a need for services that can work on temporal data, rather than working on an entire snapshot of data. For example, event data/IoT data is used in applications that make decisions based on real-time or near real-time data, such as traffic management systems or systems that monitor temperature.</p>
			<p>Azure provides a plethora of services that help in capturing and analyzing real-time data from sensors. In this chapter, we will go through a couple of eventing services in Azure, as listed here:</p>
			<ul>
				<li>Azure Event Hubs</li>
				<li>Azure Stream Analytics</li>
			</ul>
			<p>There are other eventing services, such as Azure Event Grid, that are not covered in this chapter; however, they are extensively covered in <em class="italics">Chapter 10, Azure Integration Services with Azure functions (Durable Functions and Proxy functions)</em>.</p>
			<h2 id="_idParaDest-279"><a id="_idTextAnchor292"/>Introducing events</h2>
			<p>Events are important constructs in both Azure and Azure application architecture. Events are everywhere within the software ecosystem. Generally, any action that is taken results in an event that can be trapped, and then further action can be taken. To take this discussion forward, it is important to first understand the basics of events.</p>
			<p>Events help in capturing the new state of a target resource. A message is a lightweight notification of a condition or a state change. Events are different than messages. Messages are related to business functionality, such as sending order details to another system. They contain raw data and can be large in size. In comparison, events are different; for instance, a virtual machine being stopped is an event. <em class="italics">Figure 12.1</em> demonstrates this transition from the current state to the target state:</p>
			<div>
				<div id="_idContainer300" class="IMG---Figure">
					<img src="image/B15432_12_01.jpg" alt="Change of state of a work item due to an event"/>
				</div>
			</div>
			<h6>Figure 12.1: Transition of a state due to an event</h6>
			<p>Events can be stored in durable storage as historical data and events can also be used to find patterns that are emerging on an ongoing basis. Events can be thought of as data being streamed constantly. To capture, ingest, and perform analysis on a stream of data, special infrastructure components that can read a small window of data and provide insights are needed, and that is where the Stream Analytics service comes into the picture.</p>
			<h3 id="_idParaDest-280"><a id="_idTextAnchor293"/>Event streaming</h3>
			<p>Processing events as they are ingested and streamed over a time window provides real-time insights about data. The time window could 15 minutes or an hour—the window is defined by the user and depends on the insights that are to be extracted from data. Take credit card swipes, for instance—millions of credit card swipes happen every minute, and fraud detection can be done over streamed events for a time window of one or two minutes.</p>
			<p>Event streaming refers to services that can accept data as and when it arises, rather than accepting it periodically. For example, event streams should be capable of accepting temperature information from devices as and when they send it, rather than making the data wait in a queue or a staging environment.</p>
			<p>Event streaming also has the capability of querying data while in transit. This is temporal data that is stored for a while, and the queries occur on the moving data; therefore, the data is not stationary. This capability is not available on other data platforms, which can only query stored data and not temporal data that has just been ingested.</p>
			<p>Event streaming services should be able to scale easily to accept millions or even billions of events. They should be highly available such that sources can send events and data to them at any time. Real-time data ingestion and being able to work on that data, rather than data that's stored in a different location, is the key to event streaming.</p>
			<p>But when we already have so many data platforms with advanced query execution capabilities, why do we need event steaming? One of the main advantages of event streaming is that it provides real-time insights and information whose usefulness is time-dependent. The same information found after a few minutes or hours might not be that useful. Let's consider some scenarios in which working on incoming data is quite important. These scenarios can't be effectively and efficiently solved by existing data platforms:</p>
			<ul>
				<li><strong class="bold">Credit card fraud detection</strong>: This should happen as and when a fraudulent transaction happens.</li>
				<li><strong class="bold">Telemetry information from sensors</strong>: In the case of IoT devices sending vital information about their environments, the user should be notified as and when an anomaly is detected.</li>
				<li><strong class="bold">Live dashboards</strong>: Event streaming is needed to create dashboards that show live information.</li>
				<li><strong class="bold">Datacenter environment telemetry</strong>: This will let the user know about any intrusions, security breaches, failures of components, and more.</li>
			</ul>
			<p>There are many possibilities for applying event streaming within an enterprise, and its importance cannot be stressed enough.</p>
			<h3 id="_idParaDest-281"><a id="_idTextAnchor294"/>Event Hubs</h3>
			<p>Azure Event Hubs is a streaming platform that provides functionality related to the ingestion and storage of streaming-related events.</p>
			<p>It can ingest data from a variety of sources; these sources could be IoT sensors or any applications using the Event Hubs <strong class="bold">Software Development Kit</strong> (<strong class="bold">SDK</strong>). It supports multiple protocols for ingesting and storing data. These protocols are industry standard, and they include the following:</p>
			<ul>
				<li><strong class="bold">HTTP</strong>: This is a stateless option and does not require an active session.</li>
				<li><strong class="bold">Advanced Messaging Queuing Protocol</strong> (<strong class="bold">AMQP</strong>): This requires an active session (that is, an established connection using sockets) and works with <strong class="bold">Transport Layer Security</strong> (<strong class="bold">TLS</strong>) and <strong class="bold">Secure Socket Layer</strong> (<strong class="bold">SSL</strong>).</li>
				<li><strong class="bold">Apache Kafka</strong>: This is a distributed streaming platform similar to Stream Analytics.   However, Stream Analytics is designed to run real-time analytics on multiple streams of data from various sources, such as IoT sensors and websites.</li>
			</ul>
			<p>Event Hubs is an event ingestion service. It can't query a request and output query results to another location. That is the responsibility of Stream Analytics, which is covered in the next section.</p>
			<p>To create an Event Hubs instance from the portal, search for Event Hubs in Marketplace and click on <strong class="bold">Create</strong>. Select a subscription and an existing resource group (or create a new one). Provide a name for the Event Hubs namespace, the preferred Azure region to host it in, the pricing tier (Basic or Standard, explained later), and the number of throughput units (explained later):</p>
			<div>
				<div id="_idContainer301" class="IMG---Figure">
					<img src="image/B15432_12_02.jpg" alt="Creating an Event Hub namespace in the Azure portal"/>
				</div>
			</div>
			<h6>Figure 12.2: Creating an Event Hubs namespace</h6>
			<p>Event Hubs, being a PaaS service, is highly distributed, highly available, and highly scalable.</p>
			<p>Event Hubs comes with the following two SKUs or pricing tiers:</p>
			<ul>
				<li><strong class="bold">Basic</strong>: This comes with one consumer group and can retain messages for 1 day. It can have a maximum of 100 brokered connections.</li>
				<li><strong class="bold">Standard</strong>: This comes with a maximum of 20 consumer groups and can retain messages for 1 day with additional storage for 7 days. It can have a maximum of 1,000 brokered connections. It is also possible to define policies in this SKU.</li>
			</ul>
			<p><em class="italics">Figure 12.3</em> shows the different SKUs available while creating a new Event Hubs namespace. It provides an option to choose an appropriate pricing tier, along with other important details:</p>
			<div>
				<div id="_idContainer302" class="IMG---Figure">
					<img src="image/B15432_12_03.jpg" alt="Comparing Basic and Standard SKU features"/>
				</div>
			</div>
			<h6>Figure 12.3: Event Hubs SKUs</h6>
			<p>Throughput can also be configured at the namespace level. Namespaces are containers that consist of multiple event hubs in the same subscription and region. The throughput is calculated as <strong class="bold">throughput units</strong> (<strong class="bold">TUs</strong>). Each TU provides:</p>
			<ul>
				<li>Up to 1 MB per second of ingress or a maximum of 1,000 ingress events and management operations per second.</li>
				<li>Up to 2 MB per second of egress or a maximum of 4,096 events and management operations per second.</li>
				<li>Up to 84 GB of storage.</li>
			</ul>
			<p>The TUs can range from 1 to 20 and they are billed on an hourly basis.</p>
			<p>It is important to note that the SKU cannot be changed after provisioning an Event Hubs namespace. Due consideration and planning should be undertaken before selecting an SKU. The planning process should include planning the number of consumer groups required and the number of applications interested in reading events from the event hub.</p>
			<p>Also, the Standard SKU is not available in every region. It should be checked for availability at the time of the design and implementation of the event hub. The URL for checking region availability is <a href="https://azure.microsoft.com/global-infrastructure/services/?products=event-hubs">https://azure.microsoft.com/global-infrastructure/services/?products=event-hubs</a>.</p>
			<h2 id="_idParaDest-282"><a id="_idTextAnchor295"/>Event Hubs architecture</h2>
			<p>There are three main components of the Event Hubs architecture: The <strong class="bold">Event Producers</strong>, the <strong class="bold">Event Hub</strong>, and the <strong class="bold">Event Consumer</strong>, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer303" class="IMG---Figure">
					<img src="image/B15432_12_04.jpg" alt="A basic Event Hubs architecture"/>
				</div>
			</div>
			<h6>Figure 12.4: Event Hubs architecture</h6>
			<p><strong class="bold">Event Producers</strong> generate events and send them to the <strong class="bold">Event Hub</strong>. The <strong class="bold">Event Hub</strong> stores the ingested events and provides that data to the <strong class="bold">Event Consumer</strong>. The <strong class="bold">Event Consumer</strong> is whatever is interested in those events, and it connects to the <strong class="bold">Event Hub</strong> to fetch the data.</p>
			<p>Event hubs cannot be created without an Event Hubs namespace. The Event Hubs namespace acts as a container and can host multiple event hubs. Each Event Hubs namespace provides a unique REST-based endpoint that is consumed by clients to send data to Event Hubs. This namespace is the same namespace that is needed for Service Bus artifacts, such as topics and queues.</p>
			<p>The connection string of an Event Hubs namespace is composed of its URL, policy name, and key. A sample connection string is shown in the following code block:</p>
			<p class="snippet">Endpoint=sb://demoeventhubnsbook.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=M/E4eeBsr7DAlXcvw6ziFqlSDNbFX6E49Jfti8CRkbA=</p>
			<p>This connection string can be found in the <strong class="bold">Shared Access Signature</strong> (<strong class="bold">SAS</strong>) menu item of the namespace. There can be multiple policies defined for a namespace, each having different levels of access to the namespace. The three levels of access are as follows:</p>
			<ul>
				<li><strong class="bold">Manage</strong>: This can manage the event hub from an administrative perspective. It also has rights for sending and listening to events.</li>
				<li><strong class="bold">Send</strong>: This can write events to Event Hubs.</li>
				<li><strong class="bold">Listen</strong>: This can read events from Event Hubs.</li>
			</ul>
			<p>By default, the <strong class="inline">RootManageSharedAccessKey</strong> policy is created when creating an event hub, as shown in <em class="italics">Figure 12.5</em>. Policies help in creating granular access control on Event Hubs. The key associated with each policy is used by consumers to determine their identity; additional policies can also be created with any combination of the three previously mentioned access levels:</p>
			<div>
				<div id="_idContainer304" class="IMG---Figure">
					<img src="image/B15432_12_05.jpg" alt="A list of shared access policies in Event Hubs"/>
				</div>
			</div>
			<h6>Figure 12.5: Shared access policies in Event Hubs</h6>
			<p>Event hubs can be created from the Event Hubs namespace service by performing the following actions:</p>
			<ol>
				<li>Click on <strong class="bold">Event Hubs</strong> in the left-hand menu and click on <strong class="bold">+ Event Hub</strong> in the resultant screen:<div id="_idContainer305" class="IMG---Figure"><img src="image/B15432_12_06.jpg" alt="Creating an Event Hub from the Azure portal"/></div><h6>Figure 12.6: Creating an event hub from the Azure portal</h6></li>
				<li>Next, provide values for the <strong class="bold">Partition Count</strong> and <strong class="bold">Message Retention</strong> fields, along with the name of your choice. Then, select <strong class="bold">Off</strong> for <strong class="bold">Capture</strong>, as demonstrated in <em class="italics">Figure 12.7</em>:</li>
			</ol>
			<div>
				<div id="_idContainer306" class="IMG---Figure">
					<img src="image/B15432_12_07.jpg" alt="Providing the Event Hub parameters"/>
				</div>
			</div>
			<h6>Figure 12.7: Creating a new event hub</h6>
			<p>After the event hub is created, you will see it in the list of event hubs, as shown in <em class="italics">Figure 12.8</em>:</p>
			<div>
				<div id="_idContainer307" class="IMG---Figure">
					<img src="image/B15432_12_08.jpg" alt="List of created event hubs in Azure"/>
				</div>
			</div>
			<h6>Figure 12.8: List of created event hubs</h6>
			<p>Event Hubs also allows the storage of events to a storage account or data lake directly using a feature known as Capture.</p>
			<p>Capture helps in the automatic storage of ingested data to either an Azure storage account or an Azure data lake. This feature ensures that the ingestion and storage of events happens in a single step, rather than transferring data into storage being a separate activity:</p>
			<div>
				<div id="_idContainer308" class="IMG---Figure">
					<img src="image/B15432_12_09.jpg" alt="Capture feature options"/>
				</div>
			</div>
			<h6>Figure 12.9: Capture feature options</h6>
			<p>Separate policies can be assigned to each event hub by adding a new policy at the event hub level.</p>
			<p>After creating the policy, the connection string is available from the <strong class="bold">Secure Access Signature</strong> left-menu item in the Azure portal.</p>
			<p>Since a namespace can consist of multiple event hubs, the connection string for an individual event hub will be similar to the following code block. The difference here is in the key value and the addition of <strong class="inline">EntityPath</strong> with the name of the event hub:</p>
			<p class="snippet">Endpoint=sb://azuretwittereventdata.servicebus.windows</p>
			<p class="snippet">=rxEu5K4Y2qsi5wEeOKuOvRnhtgW8xW35UBex4VlIKqg=;EntityPath=myeventhub</p>
			<p>We had to keep the <strong class="bold">Capture</strong> option set to <strong class="bold">Off</strong> while creating the event hub, and it can be switched back on after creating the event hub. It helps to save events to Azure Blob storage or an Azure Data Lake storage account automatically. The configuration for the size and time interval is shown in <em class="italics">Figure 12.10</em>:</p>
			<div>
				<div id="_idContainer309" class="IMG---Figure">
					<img src="image/B15432_12_10.jpg" alt="Selecting size and time intervals for capturing events"/>
				</div>
			</div>
			<h6>Figure 12.10: Selecting the size and time interval for capturing events</h6>
			<p>We did not cover the concepts of partitions and message retention options while creating event hubs.</p>
			<p>Partitioning is an important concept related to the scalability of any data store. Events are retained within event hubs for a specific period of time. If all events are stored within the same data store, then it becomes extremely difficult to scale that data store. Every event producer will connect to the same data store and send their events to it. Compare this with a data store that can partition the same data into multiple smaller data stores, each being uniquely identified with a value. </p>
			<p>The smaller data store is called a <strong class="bold">partition</strong>, and the value that defines the partition is known as the <strong class="bold">partition key</strong>. This partition key is part of the event data.</p>
			<p>Now the event producers can connect to the event hub, and based on the value of the partition key, the event hub will store the data in an appropriate partition. This will allow the event hub to ingest multiple events at the same time in parallel.</p>
			<p>Deciding on the number of partitions is a crucial aspect of the scalability of an event hub. <em class="italics">Figure 12.11</em> shows that ingested data is stored in the appropriate partition internally by Event Hubs using the partition key:</p>
			<div>
				<div id="_idContainer310" class="IMG---Figure">
					<img src="image/B15432_12_11.jpg" alt="The partitioning concept in Event Hub"/>
				</div>
			</div>
			<h6>Figure 12.11: Partitioning in an event hub</h6>
			<p>It is important to understand that one partition might have multiple keys. The user decides how many partitions are required, and the event hub internally decides the best way to allocate the partition keys between them. Each partition stores data in an orderly way using a timestamp, and newer events are appended toward the end of the partition.</p>
			<p>It is important to note that it is not possible to change the number of partitions once the event hub is created.</p>
			<p>It is also important to remember that partitions also help in bringing parallelism and concurrency for applications reading the events. For example, if there are 10 partitions, 10 parallel readers can read the events without any degradation in performance.</p>
			<p>Message retention refers to the time period for which events should be stored. After the expiry of the retention period, the events are discarded.</p>
			<h3 id="_idParaDest-283"><a id="_idTextAnchor296"/>Consumer groups</h3>
			<p>Consumers are applications that read events from an event hub. Consumer groups are created for consumers to connect to in order to read the events. There can be multiple consumer groups for an event hub, and each consumer group has access to all the partitions within an event hub. Each consumer group forms a query on the events in events hubs. Applications can use consumer groups and each application will get a different view of the event hub events. A default <strong class="inline">$default</strong> consumer group is created when creating an event hub. It is good practice for one consumer to be associated with one consumer group for optimal performance. However, it is possible to have five readers on each partition in a consumer group:</p>
			<div>
				<div id="_idContainer311" class="IMG---Figure">
					<img src="image/B15432_12_12.jpg" alt="Event receivers in a consumer group"/>
				</div>
			</div>
			<h6>Figure 12.12: Event receivers in a consumer group</h6>
			<p>Now that you understand consumer groups, it is time to go deeper into the concept of Event Hubs throughput.</p>
			<h3 id="_idParaDest-284"><a id="_idTextAnchor297"/>Throughput</h3>
			<p>Partitions help with scalability, while throughput helps with capacity per second. So, what is capacity in terms of Event Hubs? It is the amount of data that can be handled per second.</p>
			<p>In Event Hubs, a single TU allows the following:</p>
			<ul>
				<li>1 MB of ingestion data per second or 1,000 events per second (whichever happens first)</li>
				<li>2 MB of egress data per second or 4,096 events per second (whichever happens first)</li>
			</ul>
			<p>The auto-inflate option helps in increasing the throughput automatically if the number of incoming/outgoing events or the incoming/outgoing total size crosses a threshold. Instead of throttling, the throughput will scale up and down. The configuration of throughput at the time of the creation of the namespace is shown in <em class="italics">Figure 12.13</em>. Again, careful thought should go into deciding the TUs:</p>
			<div>
				<div id="_idContainer312" class="IMG---Figure">
					<img src="image/B15432_12_13.jpg" alt="Selecting the throughput units and enabling auto-inflate"/>
				</div>
			</div>
			<h6>Figure 12.13: Selecting the TUs along with auto-inflate</h6>
			<h2 id="_idParaDest-285"><a id="_idTextAnchor298"/>A primer on Stream Analytics</h2>
			<p>Event Hubs is a highly scalable data streaming platform, so we need another service that can process these events as a stream rather than just as stored data. Stream Analytics helps in processing and examining a stream of big data, and Stream Analytics jobs help to execute the processing of events.</p>
			<p>Stream Analytics can process millions of events per second and it is quite easy to get started with it. Azure Stream Analytics is a PaaS that is completely managed by Azure. Customers of Stream Analytics do not have to manage the underlying hardware and platform.</p>
			<p>Each job comprises multiple inputs, outputs, and a query, which transforms the incoming data into new output. The whole architecture of Stream Analytics is shown in <em class="italics">Figure 12.14</em>:</p>
			<div>
				<div id="_idContainer313" class="IMG---Figure">
					<img src="image/B15432_12_14.jpg" alt="Azure Stream Analytics architecture"/>
				</div>
			</div>
			<h6>Figure 12.14: Azure Stream Analytics architecture</h6>
			<p>In <em class="italics">Figure 12.14</em>, the event sources are displayed on the extreme left. These are the sources that produce the events. They could be IoT devices, custom applications written in any programming language, or events coming from other Azure platforms, such as Log Analytics or Application Insights.</p>
			<p>These events must first be ingested into the system, and there are numerous Azure services that can help to ingest this data. We've already looked at Event Hubs and how they help in ingesting data. There are other services, such as IoT Hub, that also help in ingesting device-specific and sensor-specific data. IoT Hub and ingestion are covered in detail in <em class="italics">Chapter 11, Designing IoT Solutions</em>. This ingested data undergoes processing as it arrives in a stream, and this processing is done using Stream Analytics. The output from Stream Analytics could be fed to a presentation platform, such as Power BI, to show real-time data to stakeholders, or a storage platform such as Cosmos DB, Data Lake Storage, or Azure Storage, from which the data can be read and actioned later by Azure Functions and Service Bus queues.</p>
			<p>Stream Analytics helps in gathering insights from real-time ingested data within a time window frame and helps in identifying patterns.</p>
			<p>It does so through three different tasks:</p>
			<ul>
				<li><strong class="bold">Input</strong>: The data should be ingested within the analytics process. The data can originate from Event Hubs, IoT Hub, or Azure Blob storage. Multiple separate reference inputs using a storage account and SQL Database can be used for lookup data within queries.</li>
				<li><strong class="bold">Query</strong>: This is where Stream Analytics does the core job of analyzing the ingested data and extracting meaningful insights and patterns. It does so with the help of JavaScript user-defined functions, JavaScript user-defined aggregates, Azure Machine Learning, and Azure Machine Learning studio.</li>
				<li><strong class="bold">Output</strong>: The result of the queries can be sent to multiple different types of destinations, and prominent among them are Cosmos DB, Power BI, Synapse Analytics, Data Lake Storage, and Functions:</li>
			</ul>
			<div>
				<div id="_idContainer314" class="IMG---Figure">
					<img src="image/B15432_12_15.jpg" alt="Stream Analytics process"/>
				</div>
			</div>
			<h6>Figure 12.15: Stream Analytics process</h6>
			<p>Stream Analytics is capable of ingesting millions of events per second and can execute queries on top of them. </p>
			<p>Input data is supported in any of the three following formats:</p>
			<ul>
				<li><strong class="bold">JavaScript Object Notation</strong> (<strong class="bold">JSON</strong>): This is a lightweight, plaintext-based format that is human readable. It consists of name-value pairs; an example of a JSON event follows:<p class="snippet">{ "SensorId" : 2, "humidity" : 60, "temperature" : 26C }</p></li>
				<li><strong class="bold">Comma-Separated Values</strong> (<strong class="bold">CSV</strong>): These are also plaintext values, which are separated by commas. An example of CSV is shown in <em class="italics">Figure 12.16</em>. The first row is the header, containing three fields, followed by two rows of data:</li>
			</ul>
			<div>
				<div id="_idContainer315" class="IMG---Figure">
					<img src="image/B15432_12_16.jpg" alt="Input in the CSV datatype"/>
				</div>
			</div>
			<h6>Figure 12.16: Plaintext values</h6>
			<ul>
				<li><strong class="bold">Avro</strong>: This format is similar to JSON; however, it is stored in a binary format rather than a text format:<p class="snippet">{</p><p class="snippet">	"firstname": "Ritesh",</p><p class="snippet">   "lastname": "Modi",</p><p class="snippet">	"email": "ritesh.modi@outlook.com"</p><p class="snippet">}</p></li>
			</ul>
			<p>However, this does not mean that Stream Analytics can only ingest data using these three formats. It can also create custom .NET-based deserializers, using which any format of data can be ingested, depending upon the deserializers' implementation. The steps you can follow to write a custom deserializer are available at <a href="https://docs.microsoft.com/azure/stream-analytics/custom-deserializer-examples">https://docs.microsoft.com/azure/stream-analytics/custom-deserializer-examples</a>.</p>
			<p>Not only can Stream Analytics receive events, but it also provides advanced query capability for the data that it receives. The queries can extract important insights from the temporal data streams and output them.</p>
			<p>As shown in <em class="italics">Figure 12.17</em>, there is an input dataset and an output dataset; the query moves the events from the input to the output. The <strong class="inline">INTO</strong> clause refers to the output location, and the <strong class="inline">FROM</strong> clause refers to the input location. The queries are very similar to SQL queries, so the learning curve is not too steep for SQL programmers:</p>
			<div>
				<div id="_idContainer316" class="IMG---Figure">
					<img src="image/B15432_12_17.jpg" alt="Stream Analytics query for receiving Twitter data"/>
				</div>
			</div>
			<h6>Figure 12.17: Stream Analytics query for receiving Twitter data</h6>
			<p>Event Hubs provides mechanisms for sending outputs from queries to target destinations. At the time of writing, Stream Analytics supports multiple destinations for events and query outputs, as shown before. </p>
			<p>It is also possible to define custom functions that can be reused within queries. There are four options provided to define custom functions.</p>
			<ul>
				<li>Azure Machine Learning</li>
				<li>JavaScript user-defined functions</li>
				<li>JavaScript user-defined aggregates </li>
				<li>Azure Machine Learning studio</li>
			</ul>
			<h3 id="_idParaDest-286"><a id="_idTextAnchor299"/>The hosting environment</h3>
			<p>Stream Analytics jobs can run on hosts that are running on the cloud, or they can run on IoT edge devices. IoT edge devices are devices that are near to IoT sensors, rather than on the cloud. <em class="italics">Figure 12.18</em> shows the <strong class="bold">New Stream Analytics job</strong> pane:</p>
			<div>
				<div id="_idContainer317" class="IMG---Figure">
					<img src="image/B15432_12_18.jpg" alt="Creating a new Stream Analytics job"/>
				</div>
			</div>
			<h6>Figure 12.18: Creating a new Stream Analytics job</h6>
			<p><a id="_idTextAnchor300"/>Let's check out streaming units in detail.</p>
			<h3 id="_idParaDest-287"><a id="_idTextAnchor301"/>Streaming units</h3>
			<p>From <em class="italics">Figure 12.18</em>, you can see that the only configuration that is unique to Stream Analytics is streaming units. Streaming units refers to the resources (that is, CPU and memory) that are assigned for running a Stream Analytics job. The minimum and maximum streaming units are 1 and 120, respectively.</p>
			<p>Streaming units must be pre-allocated according to the amount of data and the number of queries executed on that data; otherwise, the job will fail.</p>
			<p>It is possible to scale streaming units up and down from the Azure portal.</p>
			<h2 id="_idParaDest-288"><a id="_idTextAnchor302"/>A sample application using Event Hubs and Stream Analytics</h2>
			<p>In this section, we will be creating a sample application comprising multiple Azure services, including Azure Logic Apps, Azure Event Hubs, Azure Storage, and Azure Stream Analytics.</p>
			<p>In this sample application, we will be reading all tweets containing the word "Azure" and storing them in an Azure storage account.</p>
			<p>To create this solution, we first need to provision all the necessary resources.</p>
			<h2 id="_idParaDest-289"><a id="_idTextAnchor303"/>Provisioning a new resource group</h2>
			<p>Navigate to the Azure portal, log in using valid credentials, click on <strong class="bold">+ Create a resource</strong>, and search for <strong class="bold">Resource group</strong>. Select <strong class="bold">Resource group</strong> from the search results and create a new resource group. Then, provide a name and choose an appropriate location. Note that all resources should be hosted in the same resource group and location so that it is easy to delete them:</p>
			<div>
				<div id="_idContainer318" class="IMG---Figure">
					<img src="image/B15432_12_19.jpg" alt="Provisioning a new resource group in the Azure portal"/>
				</div>
			</div>
			<h6>Figure 12.19: Provisioning a new resource group in the Azure portal </h6>
			<p>Next, we will create an Event Hubs namespace.</p>
			<h3 id="_idParaDest-290"><a id="_idTextAnchor304"/>Creating an Event Hubs namespace</h3>
			<p>Click on <strong class="bold">+ Create a resource</strong> and search for <strong class="bold">Event Hubs</strong>. Select <strong class="bold">Event Hubs</strong> from the search results and create a new event hub. Then, provide a name and location, and select a subscription based on the resource group that was created earlier. Select <strong class="bold">Standard</strong> as the pricing tier and also select <strong class="bold">Enable Auto-inflate</strong>, as shown in <em class="italics">Figure 12.20</em>:</p>
			<div>
				<div id="_idContainer319" class="IMG---Figure">
					<img src="image/B15432_12_20.jpg" alt="Creating an Event Hubs namespace"/>
				</div>
			</div>
			<h6>Figure 12.20: Creating an Event Hubs namespace</h6>
			<p>By now, an Event Hubs namespace should have been created. It is a pre-requisite to have a namespace before an event hub can be created. The next step is to provision an event hub.</p>
			<h3 id="_idParaDest-291"><a id="_idTextAnchor305"/>Creating an event hub</h3>
			<p>From the Event Hubs namespace service, click on <strong class="bold">Events Hubs</strong> in the left-hand menu, and then click on <strong class="bold">+ Event hubs</strong> to create a new event hub. Name it <strong class="bold">azuretwitterdata</strong> and provide an optimal number of partitions and a <strong class="bold">Message Retention</strong> value:</p>
			<div>
				<div id="_idContainer320" class="IMG---Figure">
					<img src="image/B15432_12_21.jpg" alt="Creating an event hub with the desired credentials"/>
				</div>
			</div>
			<h6>Figure 12.21: Creating the azuretwitterdata event hub</h6>
			<p>After this step, you will have an event hub that can be used to send event data, which is stored in durable storage such as a data lake or an Azure Storage account, to be used by downstream services.</p>
			<h3 id="_idParaDest-292"><a id="_idTextAnchor306"/>Provisioning a logic app</h3>
			<p>After the resource group is provisioned, click on <strong class="bold">+ Create a resource</strong> and search for <strong class="bold">Logic Apps</strong>. Select <strong class="bold">Logic Apps</strong> from the search results and create a new logic app. Then, provide a name and location, and select a subscription based on the resource group created earlier. It is good practice to enable <strong class="bold">Log Analytics</strong>. Logic Apps is covered in more detail in <em class="italics">Chapter 11, Azure Solutions using Azure Logic Apps, Event Grid, and Functions</em>. The logic app is responsible for connecting to Twitter using an account and fetching all the tweets with <strong class="bold">Azure</strong> in them:</p>
			<div>
				<div id="_idContainer321" class="IMG---Figure">
					<img src="image/B15432_12_22.jpg" alt="Creating a logic app with the desired credentials"/>
				</div>
			</div>
			<h6>Figure 12.22: Creating a logic app</h6>
			<p>After the logic app is created, select the <strong class="bold">When a new tweet is posted</strong> trigger on the design surface, sign in, and then configure it as shown in <em class="italics">Figure 12.23</em>. You will need a valid Twitter account before configuring this trigger:</p>
			<div>
				<div id="_idContainer322" class="IMG---Figure">
					<img src="image/B15432_12_23.jpg" alt="Configuring the frequency of incoming tweets"/>
				</div>
			</div>
			<h6>Figure 12.23: Configuring the frequency of incoming tweets</h6>
			<p>Next, drop a <strong class="bold">Send event</strong> action on the designer surface; this action is responsible for sending tweets to the event hub:</p>
			<div>
				<div id="_idContainer323" class="IMG---Figure">
					<img src="image/B15432_12_24.jpg" alt="Adding an action to send tweets to the event hub"/>
				</div>
			</div>
			<h6>Figure 12.24: Adding an action to send tweets to the event hub</h6>
			<p>Select the name of the event hub that was created in an earlier step.</p>
			<p>The value specified in the content textbox is an expression that has been dynamically composed using Logic Apps–provided functions and Twitter data. Clicking on <strong class="bold">Add dynamic content</strong> provides a dialog through which the expression can be composed:</p>
			<div>
				<div id="_idContainer324" class="IMG---Figure">
					<img src="image/B15432_12_25.jpg" alt="Configuring Logic Apps activity using dynamic expressions"/>
				</div>
			</div>
			<h6>Figure 12.25: Configuring Logic Apps activity using dynamic expressions</h6>
			<p>The value of the expression is as follows:</p>
			<p class="snippet">json(concat('{','tweetdata:' ,'"',triggerBody()?['TweetText'],'"', '}'))</p>
			<p>In the next section, we will provision the storage account.</p>
			<h3 id="_idParaDest-293"><a id="_idTextAnchor307"/>Provisioning the storage account</h3>
			<p>Click on <strong class="bold">+ Create a resource</strong> and search for <strong class="bold">Storage Account</strong>. Select <strong class="bold">Storage Account</strong> from the search results and create a new storage account. Then, provide a name and location, and select a subscription based on the resource group that was created earlier. Finally, select <strong class="bold">StorageV2</strong> for <strong class="bold">Account Kind</strong>, <strong class="bold">Standard</strong> for <strong class="bold">Performance</strong>, and <strong class="bold">Locally-redundant storage</strong> (<strong class="bold">LRS</strong>) for the <strong class="bold">Replication</strong> field.</p>
			<p>Next, we will create a Blob storage container to store the data coming out of Stream Analytics.</p>
			<h3 id="_idParaDest-294"><a id="_idTextAnchor308"/>Creating a storage container</h3>
			<p>Stream Analytics will output the data as files, which will be stored within a Blob storage container. A container named <strong class="bold">twitter</strong> will be created within Blob storage, as shown in <em class="italics">Figure 12.26</em>:</p>
			<div>
				<div id="_idContainer325" class="IMG---Figure">
					<img src="image/B15432_12_26.jpg" alt="Creating a storage container for Twitter data"/>
				</div>
			</div>
			<h6>Figure 12.26: Creating a storage container</h6>
			<p>Let's create a new Stream Analytics job with a hosting environment on the cloud and set the streaming units to the default settings.</p>
			<h3 id="_idParaDest-295"><a id="_idTextAnchor309"/>Creating Stream Analytics jobs</h3>
			<p>The input for this Stream Analytics job comes from the event hub, and so we need to configure this from the <strong class="bold">Inputs</strong> menu:</p>
			<div>
				<div id="_idContainer326" class="IMG---Figure">
					<img src="image/B15432_12_27.jpg" alt="Creating an input Stream Analytics job"/>
				</div>
			</div>
			<h6>Figure 12.27: Creating an input Stream Analytics job</h6>
			<p>The output for the Stream Analytics job is a Blob storage account, so you need to configure the output accordingly. Provide a path pattern that is suitable for this exercise; for example, <strong class="bold">{datetime:ss}</strong> is the path pattern that we are using for this exercise:</p>
			<div>
				<div id="_idContainer327" class="IMG---Figure">
					<img src="image/B15432_12_28.jpg" alt="Creating a Blob Storage account as output"/>
				</div>
			</div>
			<h6>Figure 12.28: Creating a Blob storage account as output</h6>
			<p>The query is quite simple; you are just copying the data from the input to the output:</p>
			<div>
				<div id="_idContainer328" class="IMG---Figure">
					<img src="image/B15432_12_29.jpg" alt="Query for copying Twitter feeds"/>
				</div>
			</div>
			<h6>Figure 12.29: Query for copying Twitter feeds</h6>
			<p>While this example just involves copying data, there can be more complex queries for performing transformation before loading data into a destination.</p>
			<p>This concludes all the steps for the application; now you should be able to run it.</p>
			<h3 id="_idParaDest-296"><a id="_idTextAnchor310"/>Running the application</h3>
			<p>The logic app should be enabled and Stream Analytics should be running. Now, run the logic app; it will create a job to run all the activities within it, as shown in <em class="italics">Figure 12.30</em>:</p>
			<div>
				<div id="_idContainer329" class="IMG---Figure">
					<img src="image/B15432_12_30.jpg" alt="Overview of the GetAzureTwitterData application"/>
				</div>
			</div>
			<h6>Figure 12.30: Overview of the GetAzureTwitterData application</h6>
			<p>The <strong class="bold">Storage Account</strong> container should get data, as shown in <em class="italics">Figure 12.31</em>:</p>
			<div>
				<div id="_idContainer330" class="IMG---Figure">
					<img src="image/B15432_12_31.jpg" alt="Checking the storage account container data"/>
				</div>
			</div>
			<h6>Figure 12.31: Checking the Storage Account container data</h6>
			<p>As an exercise, you can extend this sample solution and evaluate the sentiment of the tweets every three minutes. The Logic Apps workflow for such an exercise would be as follows:</p>
			<div>
				<div id="_idContainer331" class="IMG---Figure">
					<img src="image/B15432_12_32.jpg" alt="Flowchart for analyzing tweet sentiments"/>
				</div>
			</div>
			<h6>Figure 12.32: Flowchart for analyzing tweet sentiment</h6>
			<p>To detect sentiment, you'll need to use the Text Analytics API, which should be configured before being used in Logic Apps.</p>
			<h2 id="_idParaDest-297"><a id="_idTextAnchor311"/>Summary</h2>
			<p>This chapter focused on topics related to the streaming and storage of events. Events have become an important consideration in overall solution architecture. We covered important resources, such as Event Hubs and Stream Analytics, and foundational concepts, such as consumer groups and throughputs, as well as creating an end-to-end solution using them along with Logic Apps. You learned that events are raised from multiple sources, and in order to get insights in real time about activities and their related events, services such as Event Hubs and Stream Analytics play a significant role. In the next chapter, we will learn about integrating Azure DevOps and Jenkins and implementing some of the industry's best practices while developing solutions.</p>
		</div>
	</body></html>