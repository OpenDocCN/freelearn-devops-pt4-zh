- en: 12\. Azure Big Data eventing solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Events are everywhere! Any activity or task that changes the state of a work
    item generates an event. Due to a lack of infrastructure and the non-availability
    of cheap devices, there previously was not much traction for the **Internet of
    Things** (**IoT**). Historically, organizations used hosted environments from **internet
    service providers** (**ISPs**) that just had monitoring systems on top of them.
    These monitoring systems raised events that were few and far between.
  prefs: []
  type: TYPE_NORMAL
- en: However, with the advent of the cloud, things are changing rapidly. With increased
    deployments on the cloud, especially of **Platform as a Service** (**PaaS**) services,
    organizations no longer need much control over the hardware and the platform,
    and now every time there is a change in an environment, an event is raised. With
    the emergence of cloud events, IoT has gained a lot of prominence and events have
    started to take center stage.
  prefs: []
  type: TYPE_NORMAL
- en: Another recent phenomenon has been the rapid burst of growth in the availability
    of data. The velocity, variety, and volume of data has spiked, and so has the
    need for solutions for storing and processing data. Multiple solutions and platforms
    have emerged, such as Hadoop, data lakes for storage, data lakes for analytics,
    and machine learning services.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from storage and analytics, there is also a need for services that are
    capable of ingesting millions upon millions of events and messages from various
    sources. There is also a need for services that can work on temporal data, rather
    than working on an entire snapshot of data. For example, event data/IoT data is
    used in applications that make decisions based on real-time or near real-time
    data, such as traffic management systems or systems that monitor temperature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Azure provides a plethora of services that help in capturing and analyzing
    real-time data from sensors. In this chapter, we will go through a couple of eventing
    services in Azure, as listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: Azure Event Hubs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Stream Analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other eventing services, such as Azure Event Grid, that are not covered
    in this chapter; however, they are extensively covered in *Chapter 10, Azure Integration
    Services with Azure functions (Durable Functions and Proxy functions)*.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing events
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Events are important constructs in both Azure and Azure application architecture.
    Events are everywhere within the software ecosystem. Generally, any action that
    is taken results in an event that can be trapped, and then further action can
    be taken. To take this discussion forward, it is important to first understand
    the basics of events.
  prefs: []
  type: TYPE_NORMAL
- en: 'Events help in capturing the new state of a target resource. A message is a
    lightweight notification of a condition or a state change. Events are different
    than messages. Messages are related to business functionality, such as sending
    order details to another system. They contain raw data and can be large in size.
    In comparison, events are different; for instance, a virtual machine being stopped
    is an event. *Figure 12.1* demonstrates this transition from the current state
    to the target state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Change of state of a work item due to an event](img/B15432_12_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.1: Transition of a state due to an event'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Events can be stored in durable storage as historical data and events can also
    be used to find patterns that are emerging on an ongoing basis. Events can be
    thought of as data being streamed constantly. To capture, ingest, and perform
    analysis on a stream of data, special infrastructure components that can read
    a small window of data and provide insights are needed, and that is where the
    Stream Analytics service comes into the picture.
  prefs: []
  type: TYPE_NORMAL
- en: Event streaming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Processing events as they are ingested and streamed over a time window provides
    real-time insights about data. The time window could 15 minutes or an hour—the
    window is defined by the user and depends on the insights that are to be extracted
    from data. Take credit card swipes, for instance—millions of credit card swipes
    happen every minute, and fraud detection can be done over streamed events for
    a time window of one or two minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Event streaming refers to services that can accept data as and when it arises,
    rather than accepting it periodically. For example, event streams should be capable
    of accepting temperature information from devices as and when they send it, rather
    than making the data wait in a queue or a staging environment.
  prefs: []
  type: TYPE_NORMAL
- en: Event streaming also has the capability of querying data while in transit. This
    is temporal data that is stored for a while, and the queries occur on the moving
    data; therefore, the data is not stationary. This capability is not available
    on other data platforms, which can only query stored data and not temporal data
    that has just been ingested.
  prefs: []
  type: TYPE_NORMAL
- en: Event streaming services should be able to scale easily to accept millions or
    even billions of events. They should be highly available such that sources can
    send events and data to them at any time. Real-time data ingestion and being able
    to work on that data, rather than data that's stored in a different location,
    is the key to event streaming.
  prefs: []
  type: TYPE_NORMAL
- en: 'But when we already have so many data platforms with advanced query execution
    capabilities, why do we need event steaming? One of the main advantages of event
    streaming is that it provides real-time insights and information whose usefulness
    is time-dependent. The same information found after a few minutes or hours might
    not be that useful. Let''s consider some scenarios in which working on incoming
    data is quite important. These scenarios can''t be effectively and efficiently
    solved by existing data platforms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Credit card fraud detection**: This should happen as and when a fraudulent
    transaction happens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Telemetry information from sensors**: In the case of IoT devices sending
    vital information about their environments, the user should be notified as and when an
    anomaly is detected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Live dashboards**: Event streaming is needed to create dashboards that show
    live information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Datacenter environment telemetry**: This will let the user know about any
    intrusions, security breaches, failures of components, and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many possibilities for applying event streaming within an enterprise,
    and its importance cannot be stressed enough.
  prefs: []
  type: TYPE_NORMAL
- en: Event Hubs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Azure Event Hubs is a streaming platform that provides functionality related
    to the ingestion and storage of streaming-related events.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can ingest data from a variety of sources; these sources could be IoT sensors
    or any applications using the Event Hubs **Software Development Kit** (**SDK**).
    It supports multiple protocols for ingesting and storing data. These protocols
    are industry standard, and they include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**HTTP**: This is a stateless option and does not require an active session.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advanced Messaging Queuing Protocol** (**AMQP**): This requires an active
    session (that is, an established connection using sockets) and works with **Transport
    Layer Security** (**TLS**) and **Secure Socket Layer** (**SSL**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Kafka**: This is a distributed streaming platform similar to Stream
    Analytics. However, Stream Analytics is designed to run real-time analytics on
    multiple streams of data from various sources, such as IoT sensors and websites.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Event Hubs is an event ingestion service. It can't query a request and output
    query results to another location. That is the responsibility of Stream Analytics,
    which is covered in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create an Event Hubs instance from the portal, search for Event Hubs in
    Marketplace and click on **Create**. Select a subscription and an existing resource
    group (or create a new one). Provide a name for the Event Hubs namespace, the
    preferred Azure region to host it in, the pricing tier (Basic or Standard, explained
    later), and the number of throughput units (explained later):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating an Event Hub namespace in the Azure portal](img/B15432_12_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.2: Creating an Event Hubs namespace'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Event Hubs, being a PaaS service, is highly distributed, highly available, and
    highly scalable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Event Hubs comes with the following two SKUs or pricing tiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Basic**: This comes with one consumer group and can retain messages for 1
    day. It can have a maximum of 100 brokered connections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standard**: This comes with a maximum of 20 consumer groups and can retain
    messages for 1 day with additional storage for 7 days. It can have a maximum of
    1,000 brokered connections. It is also possible to define policies in this SKU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 12.3* shows the different SKUs available while creating a new Event
    Hubs namespace. It provides an option to choose an appropriate pricing tier, along
    with other important details:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparing Basic and Standard SKU features](img/B15432_12_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.3: Event Hubs SKUs'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Throughput can also be configured at the namespace level. Namespaces are containers
    that consist of multiple event hubs in the same subscription and region. The throughput
    is calculated as **throughput units** (**TUs**). Each TU provides:'
  prefs: []
  type: TYPE_NORMAL
- en: Up to 1 MB per second of ingress or a maximum of 1,000 ingress events and management
    operations per second.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Up to 2 MB per second of egress or a maximum of 4,096 events and management
    operations per second.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Up to 84 GB of storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TUs can range from 1 to 20 and they are billed on an hourly basis.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the SKU cannot be changed after provisioning an
    Event Hubs namespace. Due consideration and planning should be undertaken before
    selecting an SKU. The planning process should include planning the number of consumer
    groups required and the number of applications interested in reading events from
    the event hub.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the Standard SKU is not available in every region. It should be checked
    for availability at the time of the design and implementation of the event hub.
    The URL for checking region availability is [https://azure.microsoft.com/global-infrastructure/services/?products=event-hubs](https://azure.microsoft.com/global-infrastructure/services/?products=event-hubs).
  prefs: []
  type: TYPE_NORMAL
- en: Event Hubs architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are three main components of the Event Hubs architecture: The **Event
    Producers**, the **Event Hub**, and the **Event Consumer**, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A basic Event Hubs architecture](img/B15432_12_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.4: Event Hubs architecture'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Event Producers** generate events and send them to the **Event Hub**. The **Event
    Hub** stores the ingested events and provides that data to the **Event Consumer**.
    The **Event Consumer** is whatever is interested in those events, and it connects
    to the **Event Hub** to fetch the data.'
  prefs: []
  type: TYPE_NORMAL
- en: Event hubs cannot be created without an Event Hubs namespace. The Event Hubs
    namespace acts as a container and can host multiple event hubs. Each Event Hubs
    namespace provides a unique REST-based endpoint that is consumed by clients to
    send data to Event Hubs. This namespace is the same namespace that is needed for
    Service Bus artifacts, such as topics and queues.
  prefs: []
  type: TYPE_NORMAL
- en: 'The connection string of an Event Hubs namespace is composed of its URL, policy
    name, and key. A sample connection string is shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This connection string can be found in the **Shared Access Signature** (**SAS**)
    menu item of the namespace. There can be multiple policies defined for a namespace,
    each having different levels of access to the namespace. The three levels of access
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Manage**: This can manage the event hub from an administrative perspective.
    It also has rights for sending and listening to events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Send**: This can write events to Event Hubs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Listen**: This can read events from Event Hubs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By default, the `RootManageSharedAccessKey` policy is created when creating
    an event hub, as shown in *Figure 12.5*. Policies help in creating granular access control on
    Event Hubs. The key associated with each policy is used by consumers to determine
    their identity; additional policies can also be created with any combination of
    the three previously mentioned access levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A list of shared access policies in Event Hubs](img/B15432_12_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.5: Shared access policies in Event Hubs'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Event hubs can be created from the Event Hubs namespace service by performing
    the following actions:'
  prefs: []
  type: TYPE_NORMAL
- en: Click on **Event Hubs** in the left-hand menu and click on **+ Event Hub** in
    the resultant screen:![Creating an Event Hub from the Azure portal](img/B15432_12_06.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 12.6: Creating an event hub from the Azure portal'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, provide values for the **Partition Count** and **Message Retention** fields,
    along with the name of your choice. Then, select **Off** for **Capture**, as demonstrated
    in *Figure 12.7*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Providing the Event Hub parameters](img/B15432_12_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.7: Creating a new event hub'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'After the event hub is created, you will see it in the list of event hubs,
    as shown in *Figure 12.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![List of created event hubs in Azure](img/B15432_12_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.8: List of created event hubs'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Event Hubs also allows the storage of events to a storage account or data lake
    directly using a feature known as Capture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Capture helps in the automatic storage of ingested data to either an Azure
    storage account or an Azure data lake. This feature ensures that the ingestion
    and storage of events happens in a single step, rather than transferring data
    into storage being a separate activity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Capture feature options](img/B15432_12_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.9: Capture feature options'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Separate policies can be assigned to each event hub by adding a new policy at
    the event hub level.
  prefs: []
  type: TYPE_NORMAL
- en: After creating the policy, the connection string is available from the **Secure
    Access Signature** left-menu item in the Azure portal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since a namespace can consist of multiple event hubs, the connection string
    for an individual event hub will be similar to the following code block. The difference
    here is in the key value and the addition of `EntityPath` with the name of the
    event hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We had to keep the **Capture** option set to **Off** while creating the event
    hub, and it can be switched back on after creating the event hub. It helps to
    save events to Azure Blob storage or an Azure Data Lake storage account automatically.
    The configuration for the size and time interval is shown in *Figure 12.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Selecting size and time intervals for capturing events](img/B15432_12_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.10: Selecting the size and time interval for capturing events'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We did not cover the concepts of partitions and message retention options while
    creating event hubs.
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning is an important concept related to the scalability of any data
    store. Events are retained within event hubs for a specific period of time. If
    all events are stored within the same data store, then it becomes extremely difficult
    to scale that data store. Every event producer will connect to the same data store
    and send their events to it. Compare this with a data store that can partition
    the same data into multiple smaller data stores, each being uniquely identified
    with a value.
  prefs: []
  type: TYPE_NORMAL
- en: The smaller data store is called a **partition**, and the value that defines
    the partition is known as the **partition key**. This partition key is part of
    the event data.
  prefs: []
  type: TYPE_NORMAL
- en: Now the event producers can connect to the event hub, and based on the value
    of the partition key, the event hub will store the data in an appropriate partition.
    This will allow the event hub to ingest multiple events at the same time in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deciding on the number of partitions is a crucial aspect of the scalability
    of an event hub. *Figure 12.11* shows that ingested data is stored in the appropriate
    partition internally by Event Hubs using the partition key:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The partitioning concept in Event Hub](img/B15432_12_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.11: Partitioning in an event hub'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is important to understand that one partition might have multiple keys. The
    user decides how many partitions are required, and the event hub internally decides
    the best way to allocate the partition keys between them. Each partition stores
    data in an orderly way using a timestamp, and newer events are appended toward
    the end of the partition.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that it is not possible to change the number of partitions
    once the event hub is created.
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to remember that partitions also help in bringing parallelism
    and concurrency for applications reading the events. For example, if there are
    10 partitions, 10 parallel readers can read the events without any degradation
    in performance.
  prefs: []
  type: TYPE_NORMAL
- en: Message retention refers to the time period for which events should be stored.
    After the expiry of the retention period, the events are discarded.
  prefs: []
  type: TYPE_NORMAL
- en: Consumer groups
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consumers are applications that read events from an event hub. Consumer groups
    are created for consumers to connect to in order to read the events. There can
    be multiple consumer groups for an event hub, and each consumer group has access
    to all the partitions within an event hub. Each consumer group forms a query on
    the events in events hubs. Applications can use consumer groups and each application
    will get a different view of the event hub events. A default `$default` consumer
    group is created when creating an event hub. It is good practice for one consumer
    to be associated with one consumer group for optimal performance. However, it
    is possible to have five readers on each partition in a consumer group:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Event receivers in a consumer group](img/B15432_12_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.12: Event receivers in a consumer group'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that you understand consumer groups, it is time to go deeper into the concept
    of Event Hubs throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Throughput
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Partitions help with scalability, while throughput helps with capacity per second.
    So, what is capacity in terms of Event Hubs? It is the amount of data that can
    be handled per second.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Event Hubs, a single TU allows the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 1 MB of ingestion data per second or 1,000 events per second (whichever happens
    first)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2 MB of egress data per second or 4,096 events per second (whichever happens
    first)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The auto-inflate option helps in increasing the throughput automatically if
    the number of incoming/outgoing events or the incoming/outgoing total size crosses
    a threshold. Instead of throttling, the throughput will scale up and down. The
    configuration of throughput at the time of the creation of the namespace is shown
    in *Figure 12.13*. Again, careful thought should go into deciding the TUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Selecting the throughput units and enabling auto-inflate](img/B15432_12_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.13: Selecting the TUs along with auto-inflate'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A primer on Stream Analytics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Event Hubs is a highly scalable data streaming platform, so we need another
    service that can process these events as a stream rather than just as stored data.
    Stream Analytics helps in processing and examining a stream of big data, and Stream
    Analytics jobs help to execute the processing of events.
  prefs: []
  type: TYPE_NORMAL
- en: Stream Analytics can process millions of events per second and it is quite easy
    to get started with it. Azure Stream Analytics is a PaaS that is completely managed
    by Azure. Customers of Stream Analytics do not have to manage the underlying hardware
    and platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each job comprises multiple inputs, outputs, and a query, which transforms
    the incoming data into new output. The whole architecture of Stream Analytics
    is shown in *Figure 12.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Azure Stream Analytics architecture](img/B15432_12_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.14: Azure Stream Analytics architecture'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In *Figure 12.14*, the event sources are displayed on the extreme left. These
    are the sources that produce the events. They could be IoT devices, custom applications
    written in any programming language, or events coming from other Azure platforms,
    such as Log Analytics or Application Insights.
  prefs: []
  type: TYPE_NORMAL
- en: These events must first be ingested into the system, and there are numerous
    Azure services that can help to ingest this data. We've already looked at Event
    Hubs and how they help in ingesting data. There are other services, such as IoT
    Hub, that also help in ingesting device-specific and sensor-specific data. IoT
    Hub and ingestion are covered in detail in *Chapter 11, Designing IoT Solutions*.
    This ingested data undergoes processing as it arrives in a stream, and this processing
    is done using Stream Analytics. The output from Stream Analytics could be fed
    to a presentation platform, such as Power BI, to show real-time data to stakeholders,
    or a storage platform such as Cosmos DB, Data Lake Storage, or Azure Storage,
    from which the data can be read and actioned later by Azure Functions and Service
    Bus queues.
  prefs: []
  type: TYPE_NORMAL
- en: Stream Analytics helps in gathering insights from real-time ingested data within
    a time window frame and helps in identifying patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'It does so through three different tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**: The data should be ingested within the analytics process. The data
    can originate from Event Hubs, IoT Hub, or Azure Blob storage. Multiple separate
    reference inputs using a storage account and SQL Database can be used for lookup
    data within queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Query**: This is where Stream Analytics does the core job of analyzing the
    ingested data and extracting meaningful insights and patterns. It does so with
    the help of JavaScript user-defined functions, JavaScript user-defined aggregates,
    Azure Machine Learning, and Azure Machine Learning studio.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output**: The result of the queries can be sent to multiple different types
    of destinations, and prominent among them are Cosmos DB, Power BI, Synapse Analytics,
    Data Lake Storage, and Functions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Stream Analytics process](img/B15432_12_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.15: Stream Analytics process'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Stream Analytics is capable of ingesting millions of events per second and can
    execute queries on top of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input data is supported in any of the three following formats:'
  prefs: []
  type: TYPE_NORMAL
- en: '**JavaScript Object Notation** (**JSON**): This is a lightweight, plaintext-based
    format that is human readable. It consists of name-value pairs; an example of
    a JSON event follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Comma-Separated Values** (**CSV**): These are also plaintext values, which
    are separated by commas. An example of CSV is shown in *Figure 12.16*. The first
    row is the header, containing three fields, followed by two rows of data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Input in the CSV datatype](img/B15432_12_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.16: Plaintext values'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Avro**: This format is similar to JSON; however, it is stored in a binary
    format rather than a text format:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: However, this does not mean that Stream Analytics can only ingest data using
    these three formats. It can also create custom .NET-based deserializers, using
    which any format of data can be ingested, depending upon the deserializers' implementation.
    The steps you can follow to write a custom deserializer are available at [https://docs.microsoft.com/azure/stream-analytics/custom-deserializer-examples](https://docs.microsoft.com/azure/stream-analytics/custom-deserializer-examples).
  prefs: []
  type: TYPE_NORMAL
- en: Not only can Stream Analytics receive events, but it also provides advanced
    query capability for the data that it receives. The queries can extract important
    insights from the temporal data streams and output them.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 12.17*, there is an input dataset and an output dataset;
    the query moves the events from the input to the output. The `INTO` clause refers
    to the output location, and the `FROM` clause refers to the input location. The
    queries are very similar to SQL queries, so the learning curve is not too steep
    for SQL programmers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stream Analytics query for receiving Twitter data](img/B15432_12_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.17: Stream Analytics query for receiving Twitter data'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Event Hubs provides mechanisms for sending outputs from queries to target destinations.
    At the time of writing, Stream Analytics supports multiple destinations for events
    and query outputs, as shown before.
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to define custom functions that can be reused within queries.
    There are four options provided to define custom functions.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Machine Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JavaScript user-defined functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JavaScript user-defined aggregates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Machine Learning studio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hosting environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Stream Analytics jobs can run on hosts that are running on the cloud, or they
    can run on IoT edge devices. IoT edge devices are devices that are near to IoT
    sensors, rather than on the cloud. *Figure 12.18* shows the **New Stream Analytics
    job** pane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a new Stream Analytics job](img/B15432_12_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.18: Creating a new Stream Analytics job'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let's check out streaming units in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming units
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From *Figure 12.18*, you can see that the only configuration that is unique to
    Stream Analytics is streaming units. Streaming units refers to the resources (that
    is, CPU and memory) that are assigned for running a Stream Analytics job. The
    minimum and maximum streaming units are 1 and 120, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming units must be pre-allocated according to the amount of data and the
    number of queries executed on that data; otherwise, the job will fail.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to scale streaming units up and down from the Azure portal.
  prefs: []
  type: TYPE_NORMAL
- en: A sample application using Event Hubs and Stream Analytics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will be creating a sample application comprising multiple
    Azure services, including Azure Logic Apps, Azure Event Hubs, Azure Storage, and
    Azure Stream Analytics.
  prefs: []
  type: TYPE_NORMAL
- en: In this sample application, we will be reading all tweets containing the word
    "Azure" and storing them in an Azure storage account.
  prefs: []
  type: TYPE_NORMAL
- en: To create this solution, we first need to provision all the necessary resources.
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning a new resource group
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Navigate to the Azure portal, log in using valid credentials, click on **+
    Create a resource**, and search for **Resource group**. Select **Resource group** from
    the search results and create a new resource group. Then, provide a name and choose
    an appropriate location. Note that all resources should be hosted in the same
    resource group and location so that it is easy to delete them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Provisioning a new resource group in the Azure portal](img/B15432_12_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.19: Provisioning a new resource group in the Azure portal'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Next, we will create an Event Hubs namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an Event Hubs namespace
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Click on **+ Create a resource** and search for **Event Hubs**. Select **Event
    Hubs** from the search results and create a new event hub. Then, provide a name
    and location, and select a subscription based on the resource group that was created
    earlier. Select **Standard** as the pricing tier and also select **Enable Auto-inflate**,
    as shown in *Figure 12.20*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating an Event Hubs namespace](img/B15432_12_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.20: Creating an Event Hubs namespace'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By now, an Event Hubs namespace should have been created. It is a pre-requisite
    to have a namespace before an event hub can be created. The next step is to provision
    an event hub.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an event hub
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From the Event Hubs namespace service, click on **Events Hubs** in the left-hand
    menu, and then click on **+ Event hubs** to create a new event hub. Name it **azuretwitterdata** and
    provide an optimal number of partitions and a **Message Retention** value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating an event hub with the desired credentials](img/B15432_12_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.21: Creating the azuretwitterdata event hub'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: After this step, you will have an event hub that can be used to send event data,
    which is stored in durable storage such as a data lake or an Azure Storage account,
    to be used by downstream services.
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning a logic app
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After the resource group is provisioned, click on **+ Create a resource** and
    search for **Logic Apps**. Select **Logic Apps** from the search results and create
    a new logic app. Then, provide a name and location, and select a subscription
    based on the resource group created earlier. It is good practice to enable **Log
    Analytics**. Logic Apps is covered in more detail in *Chapter 11, Azure Solutions
    using Azure Logic Apps, Event Grid, and Functions*. The logic app is responsible
    for connecting to Twitter using an account and fetching all the tweets with **Azure** in
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a logic app with the desired credentials](img/B15432_12_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.22: Creating a logic app'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'After the logic app is created, select the **When a new tweet is posted** trigger
    on the design surface, sign in, and then configure it as shown in *Figure 12.23*.
    You will need a valid Twitter account before configuring this trigger:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Configuring the frequency of incoming tweets](img/B15432_12_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.23: Configuring the frequency of incoming tweets'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, drop a **Send event** action on the designer surface; this action is responsible for
    sending tweets to the event hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Adding an action to send tweets to the event hub](img/B15432_12_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.24: Adding an action to send tweets to the event hub'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Select the name of the event hub that was created in an earlier step.
  prefs: []
  type: TYPE_NORMAL
- en: 'The value specified in the content textbox is an expression that has been dynamically
    composed using Logic Apps–provided functions and Twitter data. Clicking on **Add
    dynamic content** provides a dialog through which the expression can be composed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Configuring Logic Apps activity using dynamic expressions](img/B15432_12_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.25: Configuring Logic Apps activity using dynamic expressions'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The value of the expression is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will provision the storage account.
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning the storage account
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Click on **+ Create a resource** and search for **Storage Account**. Select **Storage
    Account** from the search results and create a new storage account. Then, provide
    a name and location, and select a subscription based on the resource group that
    was created earlier. Finally, select **StorageV2** for **Account Kind**, **Standard** for **Performance**,
    and **Locally-redundant storage** (**LRS**) for the **Replication** field.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will create a Blob storage container to store the data coming out of
    Stream Analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a storage container
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Stream Analytics will output the data as files, which will be stored within
    a Blob storage container. A container named **twitter** will be created within
    Blob storage, as shown in *Figure 12.26*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a storage container for Twitter data](img/B15432_12_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.26: Creating a storage container'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let's create a new Stream Analytics job with a hosting environment on the cloud
    and set the streaming units to the default settings.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Stream Analytics jobs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The input for this Stream Analytics job comes from the event hub, and so we
    need to configure this from the **Inputs** menu:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating an input Stream Analytics job](img/B15432_12_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.27: Creating an input Stream Analytics job'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The output for the Stream Analytics job is a Blob storage account, so you need
    to configure the output accordingly. Provide a path pattern that is suitable for
    this exercise; for example, **{datetime:ss}** is the path pattern that we are
    using for this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a Blob Storage account as output](img/B15432_12_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.28: Creating a Blob storage account as output'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The query is quite simple; you are just copying the data from the input to
    the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Query for copying Twitter feeds](img/B15432_12_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.29: Query for copying Twitter feeds'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While this example just involves copying data, there can be more complex queries
    for performing transformation before loading data into a destination.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes all the steps for the application; now you should be able to
    run it.
  prefs: []
  type: TYPE_NORMAL
- en: Running the application
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The logic app should be enabled and Stream Analytics should be running. Now,
    run the logic app; it will create a job to run all the activities within it, as
    shown in *Figure 12.30*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Overview of the GetAzureTwitterData application](img/B15432_12_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.30: Overview of the GetAzureTwitterData application'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The **Storage Account** container should get data, as shown in *Figure 12.31*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Checking the storage account container data](img/B15432_12_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.31: Checking the Storage Account container data'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As an exercise, you can extend this sample solution and evaluate the sentiment
    of the tweets every three minutes. The Logic Apps workflow for such an exercise would
    be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Flowchart for analyzing tweet sentiments](img/B15432_12_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.32: Flowchart for analyzing tweet sentiment'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To detect sentiment, you'll need to use the Text Analytics API, which should be
    configured before being used in Logic Apps.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter focused on topics related to the streaming and storage of events.
    Events have become an important consideration in overall solution architecture.
    We covered important resources, such as Event Hubs and Stream Analytics, and foundational
    concepts, such as consumer groups and throughputs, as well as creating an end-to-end
    solution using them along with Logic Apps. You learned that events are raised
    from multiple sources, and in order to get insights in real time about activities
    and their related events, services such as Event Hubs and Stream Analytics play
    a significant role. In the next chapter, we will learn about integrating Azure
    DevOps and Jenkins and implementing some of the industry's best practices while
    developing solutions.
  prefs: []
  type: TYPE_NORMAL
