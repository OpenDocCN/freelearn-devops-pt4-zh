- en: 12\. Azure Big Data eventing solutions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12. Azure 大数据事件解决方案
- en: Events are everywhere! Any activity or task that changes the state of a work
    item generates an event. Due to a lack of infrastructure and the non-availability
    of cheap devices, there previously was not much traction for the **Internet of
    Things** (**IoT**). Historically, organizations used hosted environments from **internet
    service providers** (**ISPs**) that just had monitoring systems on top of them.
    These monitoring systems raised events that were few and far between.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 事件无处不在！任何改变工作项状态的活动或任务都会生成一个事件。由于基础设施不足以及廉价设备的缺乏，**物联网**（**IoT**）此前并未获得太多关注。从历史上看，组织使用的是来自**互联网服务提供商**（**ISP**）的托管环境，这些环境上仅有监控系统。这些监控系统产生的事件少之又少。
- en: However, with the advent of the cloud, things are changing rapidly. With increased
    deployments on the cloud, especially of **Platform as a Service** (**PaaS**) services,
    organizations no longer need much control over the hardware and the platform,
    and now every time there is a change in an environment, an event is raised. With
    the emergence of cloud events, IoT has gained a lot of prominence and events have
    started to take center stage.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着云计算的兴起，事物正在迅速变化。随着云端部署的增加，尤其是**平台即服务**（**PaaS**）服务的普及，组织不再需要过多地控制硬件和平台，每当环境发生变化时，都会触发一个事件。随着云事件的出现，物联网变得越来越重要，事件也开始成为中心。
- en: Another recent phenomenon has been the rapid burst of growth in the availability
    of data. The velocity, variety, and volume of data has spiked, and so has the
    need for solutions for storing and processing data. Multiple solutions and platforms
    have emerged, such as Hadoop, data lakes for storage, data lakes for analytics,
    and machine learning services.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个最近的现象是数据可用性迅速增长。数据的速度、种类和数量急剧增加，存储和处理数据的需求也随之上升。多个解决方案和平台相继出现，例如 Hadoop、用于存储的数据湖、用于分析的数据湖和机器学习服务。
- en: Apart from storage and analytics, there is also a need for services that are
    capable of ingesting millions upon millions of events and messages from various
    sources. There is also a need for services that can work on temporal data, rather
    than working on an entire snapshot of data. For example, event data/IoT data is
    used in applications that make decisions based on real-time or near real-time
    data, such as traffic management systems or systems that monitor temperature.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 除了存储和分析外，还有一种需求是能够从各种来源摄取成千上万事件和消息的服务。还需要能够处理时间数据的服务，而不是处理整个数据快照的服务。例如，事件数据/物联网数据被用于做出基于实时或近实时数据的决策的应用程序，例如交通管理系统或监控温度的系统。
- en: 'Azure provides a plethora of services that help in capturing and analyzing
    real-time data from sensors. In this chapter, we will go through a couple of eventing
    services in Azure, as listed here:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 提供了大量帮助捕获和分析传感器实时数据的服务。在本章中，我们将介绍 Azure 中的几种事件服务，具体如下：
- en: Azure Event Hubs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure Event Hubs
- en: Azure Stream Analytics
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure Stream Analytics
- en: There are other eventing services, such as Azure Event Grid, that are not covered
    in this chapter; however, they are extensively covered in *Chapter 10, Azure Integration
    Services with Azure functions (Durable Functions and Proxy functions)*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他事件服务，例如 Azure Event Grid，本章未涉及；然而，它们在*第10章，使用 Azure 函数的 Azure 集成服务（耐久函数和代理函数）*中有广泛介绍。
- en: Introducing events
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入事件
- en: Events are important constructs in both Azure and Azure application architecture.
    Events are everywhere within the software ecosystem. Generally, any action that
    is taken results in an event that can be trapped, and then further action can
    be taken. To take this discussion forward, it is important to first understand
    the basics of events.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 事件是 Azure 和 Azure 应用架构中的重要构件。事件遍布整个软件生态系统。一般来说，任何采取的行动都会导致一个事件的发生，该事件可以被捕捉，然后采取进一步的行动。为了推动这个讨论，首先需要理解事件的基本概念。
- en: 'Events help in capturing the new state of a target resource. A message is a
    lightweight notification of a condition or a state change. Events are different
    than messages. Messages are related to business functionality, such as sending
    order details to another system. They contain raw data and can be large in size.
    In comparison, events are different; for instance, a virtual machine being stopped
    is an event. *Figure 12.1* demonstrates this transition from the current state
    to the target state:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 事件有助于捕获目标资源的新状态。消息是条件或状态变化的轻量级通知。事件与消息不同。消息与业务功能相关，例如将订单详情发送到另一个系统。它们包含原始数据，并且可能很大。相比之下，事件则不同；例如，虚拟机被停止就是一个事件。*图12.1*演示了从当前状态到目标状态的转变：
- en: '![Change of state of a work item due to an event](img/B15432_12_01.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![由于事件引起的工作项状态变化](img/B15432_12_01.jpg)'
- en: 'Figure 12.1: Transition of a state due to an event'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12.1：由于事件引起的状态转变
- en: Events can be stored in durable storage as historical data and events can also
    be used to find patterns that are emerging on an ongoing basis. Events can be
    thought of as data being streamed constantly. To capture, ingest, and perform
    analysis on a stream of data, special infrastructure components that can read
    a small window of data and provide insights are needed, and that is where the
    Stream Analytics service comes into the picture.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 事件可以作为历史数据存储在持久存储中，事件还可以用来发现持续出现的模式。事件可以被视为不断流动的数据。为了捕获、摄取和分析一连串的数据，需要特定的基础设施组件，这些组件能够读取一小段数据并提供洞察力，这就是Stream
    Analytics服务的作用所在。
- en: Event streaming
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 事件流处理
- en: Processing events as they are ingested and streamed over a time window provides
    real-time insights about data. The time window could 15 minutes or an hour—the
    window is defined by the user and depends on the insights that are to be extracted
    from data. Take credit card swipes, for instance—millions of credit card swipes
    happen every minute, and fraud detection can be done over streamed events for
    a time window of one or two minutes.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据流式传输过程中对事件进行处理，能够提供有关数据的实时洞察。时间窗口可以是15分钟或一小时——窗口由用户定义，并取决于要从数据中提取的洞察。例如，信用卡刷卡，每分钟会有成千上万次刷卡事件，欺诈检测可以在流式事件上进行，时间窗口为一到两分钟。
- en: Event streaming refers to services that can accept data as and when it arises,
    rather than accepting it periodically. For example, event streams should be capable
    of accepting temperature information from devices as and when they send it, rather
    than making the data wait in a queue or a staging environment.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 事件流处理指的是能够随时接受数据的服务，而不是按周期性接收数据。例如，事件流应能够随时接收设备发送的温度信息，而不是让数据在队列或暂存环境中等待。
- en: Event streaming also has the capability of querying data while in transit. This
    is temporal data that is stored for a while, and the queries occur on the moving
    data; therefore, the data is not stationary. This capability is not available
    on other data platforms, which can only query stored data and not temporal data
    that has just been ingested.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 事件流处理还具有在数据传输过程中查询数据的能力。这是暂时存储的数据，查询发生在移动数据上；因此，数据不是静态的。其他数据平台无法实现这一功能，它们只能查询已存储的数据，而无法查询刚刚摄取的临时数据。
- en: Event streaming services should be able to scale easily to accept millions or
    even billions of events. They should be highly available such that sources can
    send events and data to them at any time. Real-time data ingestion and being able
    to work on that data, rather than data that's stored in a different location,
    is the key to event streaming.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 事件流服务应能够轻松扩展，接受数百万甚至数十亿个事件。它们应具有高度可用性，使得源可以随时向其发送事件和数据。实时数据摄取并能够在这些数据上工作，而不是在其他位置存储的数据，是事件流处理的关键。
- en: 'But when we already have so many data platforms with advanced query execution
    capabilities, why do we need event steaming? One of the main advantages of event
    streaming is that it provides real-time insights and information whose usefulness
    is time-dependent. The same information found after a few minutes or hours might
    not be that useful. Let''s consider some scenarios in which working on incoming
    data is quite important. These scenarios can''t be effectively and efficiently
    solved by existing data platforms:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 但当我们已经有了许多具备高级查询执行能力的数据平台时，为什么还需要事件流处理呢？事件流处理的主要优势之一是它提供实时洞察和信息，而这些信息的价值依赖于时间。几分钟或几小时后得到的相同信息可能就不那么有用了。让我们考虑一些处理传入数据非常重要的场景。这些场景是现有数据平台无法有效且高效解决的：
- en: '**Credit card fraud detection**: This should happen as and when a fraudulent
    transaction happens.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信用卡欺诈检测**：应该在发生欺诈交易时及时进行。'
- en: '**Telemetry information from sensors**: In the case of IoT devices sending
    vital information about their environments, the user should be notified as and when an
    anomaly is detected.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**来自传感器的遥测信息**：对于发送环境关键信息的物联网设备，用户应在发现异常时及时收到通知。'
- en: '**Live dashboards**: Event streaming is needed to create dashboards that show
    live information.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时仪表盘**：事件流处理用于创建显示实时信息的仪表盘。'
- en: '**Datacenter environment telemetry**: This will let the user know about any
    intrusions, security breaches, failures of components, and more.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据中心环境遥测**：这将让用户知道是否有入侵、安保漏洞、组件故障等问题发生。'
- en: There are many possibilities for applying event streaming within an enterprise,
    and its importance cannot be stressed enough.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 企业内应用事件流处理的可能性非常多，它的重要性无法过分强调。
- en: Event Hubs
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Event Hubs
- en: Azure Event Hubs is a streaming platform that provides functionality related
    to the ingestion and storage of streaming-related events.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Event Hubs 是一个流处理平台，提供与流式事件的获取和存储相关的功能。
- en: 'It can ingest data from a variety of sources; these sources could be IoT sensors
    or any applications using the Event Hubs **Software Development Kit** (**SDK**).
    It supports multiple protocols for ingesting and storing data. These protocols
    are industry standard, and they include the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以从多种来源获取数据；这些来源可以是物联网传感器或任何使用 Event Hubs **软件开发工具包** (**SDK**) 的应用程序。它支持多种协议来获取和存储数据。这些协议是行业标准，包括以下几种：
- en: '**HTTP**: This is a stateless option and does not require an active session.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HTTP**：这是一种无状态的选项，不需要活动会话。'
- en: '**Advanced Messaging Queuing Protocol** (**AMQP**): This requires an active
    session (that is, an established connection using sockets) and works with **Transport
    Layer Security** (**TLS**) and **Secure Socket Layer** (**SSL**).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级消息队列协议** (**AMQP**)：这需要一个活动会话（即通过套接字建立的连接），并与 **传输层安全性** (**TLS**) 和 **安全套接字层**
    (**SSL**) 一起使用。'
- en: '**Apache Kafka**: This is a distributed streaming platform similar to Stream
    Analytics. However, Stream Analytics is designed to run real-time analytics on
    multiple streams of data from various sources, such as IoT sensors and websites.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Kafka**：这是一种类似于流分析的分布式流处理平台。然而，流分析旨在对来自多个数据源（如物联网传感器和网站）的数据流进行实时分析。'
- en: Event Hubs is an event ingestion service. It can't query a request and output
    query results to another location. That is the responsibility of Stream Analytics,
    which is covered in the next section.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Event Hubs 是一个事件获取服务。它不能查询请求并将查询结果输出到其他位置。这是 Stream Analytics 的责任，后文将介绍。
- en: 'To create an Event Hubs instance from the portal, search for Event Hubs in
    Marketplace and click on **Create**. Select a subscription and an existing resource
    group (or create a new one). Provide a name for the Event Hubs namespace, the
    preferred Azure region to host it in, the pricing tier (Basic or Standard, explained
    later), and the number of throughput units (explained later):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要从门户创建 Event Hubs 实例，请在市场中搜索 Event Hubs 并点击 **创建**。选择一个订阅和现有的资源组（或创建一个新的）。为
    Event Hubs 命名空间提供名称，选择首选的 Azure 区域进行托管，定价层级（后文将解释 Basic 或 Standard），以及吞吐量单元的数量（后文将解释）：
- en: '![Creating an Event Hub namespace in the Azure portal](img/B15432_12_02.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![在 Azure 门户中创建 Event Hub 命名空间](img/B15432_12_02.jpg)'
- en: 'Figure 12.2: Creating an Event Hubs namespace'
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12.2：创建 Event Hubs 命名空间
- en: Event Hubs, being a PaaS service, is highly distributed, highly available, and
    highly scalable.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Event Hubs 作为一个 PaaS 服务，具有高度分布式、高可用性和高度可扩展性。
- en: 'Event Hubs comes with the following two SKUs or pricing tiers:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '**Basic**: This comes with one consumer group and can retain messages for 1
    day. It can have a maximum of 100 brokered connections.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standard**: This comes with a maximum of 20 consumer groups and can retain
    messages for 1 day with additional storage for 7 days. It can have a maximum of
    1,000 brokered connections. It is also possible to define policies in this SKU.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 12.3* shows the different SKUs available while creating a new Event
    Hubs namespace. It provides an option to choose an appropriate pricing tier, along
    with other important details:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparing Basic and Standard SKU features](img/B15432_12_03.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.3: Event Hubs SKUs'
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Throughput can also be configured at the namespace level. Namespaces are containers
    that consist of multiple event hubs in the same subscription and region. The throughput
    is calculated as **throughput units** (**TUs**). Each TU provides:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Up to 1 MB per second of ingress or a maximum of 1,000 ingress events and management
    operations per second.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Up to 2 MB per second of egress or a maximum of 4,096 events and management
    operations per second.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Up to 84 GB of storage.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TUs can range from 1 to 20 and they are billed on an hourly basis.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the SKU cannot be changed after provisioning an
    Event Hubs namespace. Due consideration and planning should be undertaken before
    selecting an SKU. The planning process should include planning the number of consumer
    groups required and the number of applications interested in reading events from
    the event hub.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Also, the Standard SKU is not available in every region. It should be checked
    for availability at the time of the design and implementation of the event hub.
    The URL for checking region availability is [https://azure.microsoft.com/global-infrastructure/services/?products=event-hubs](https://azure.microsoft.com/global-infrastructure/services/?products=event-hubs).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Event Hubs architecture
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are three main components of the Event Hubs architecture: The **Event
    Producers**, the **Event Hub**, and the **Event Consumer**, as shown in the following diagram:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![A basic Event Hubs architecture](img/B15432_12_04.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.4: Event Hubs architecture'
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Event Producers** generate events and send them to the **Event Hub**. The **Event
    Hub** stores the ingested events and provides that data to the **Event Consumer**.
    The **Event Consumer** is whatever is interested in those events, and it connects
    to the **Event Hub** to fetch the data.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Event hubs cannot be created without an Event Hubs namespace. The Event Hubs
    namespace acts as a container and can host multiple event hubs. Each Event Hubs
    namespace provides a unique REST-based endpoint that is consumed by clients to
    send data to Event Hubs. This namespace is the same namespace that is needed for
    Service Bus artifacts, such as topics and queues.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'The connection string of an Event Hubs namespace is composed of its URL, policy
    name, and key. A sample connection string is shown in the following code block:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 事件中心命名空间的连接字符串由其 URL、策略名称和密钥组成。以下代码块展示了一个示例连接字符串：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This connection string can be found in the **Shared Access Signature** (**SAS**)
    menu item of the namespace. There can be multiple policies defined for a namespace,
    each having different levels of access to the namespace. The three levels of access
    are as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个连接字符串可以在命名空间的**共享访问签名**（**SAS**）菜单项中找到。命名空间可以定义多个策略，每个策略具有不同的访问级别。访问级别如下：
- en: '**Manage**: This can manage the event hub from an administrative perspective.
    It also has rights for sending and listening to events.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管理**：此选项可以从管理角度管理事件中心。它还具有发送和监听事件的权限。'
- en: '**Send**: This can write events to Event Hubs.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**发送**：此选项可以将事件写入事件中心。'
- en: '**Listen**: This can read events from Event Hubs.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监听**：此选项可以从事件中心读取事件。'
- en: 'By default, the `RootManageSharedAccessKey` policy is created when creating
    an event hub, as shown in *Figure 12.5*. Policies help in creating granular access control on
    Event Hubs. The key associated with each policy is used by consumers to determine
    their identity; additional policies can also be created with any combination of
    the three previously mentioned access levels:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，在创建事件中心时会创建`RootManageSharedAccessKey`策略，如*图12.5*所示。策略有助于在事件中心上创建细粒度的访问控制。与每个策略相关联的密钥由消费者用来确定其身份；还可以创建附加策略，并使用之前提到的三种访问级别的任何组合：
- en: '![A list of shared access policies in Event Hubs](img/B15432_12_05.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![事件中心中的共享访问策略列表](img/B15432_12_05.jpg)'
- en: 'Figure 12.5: Shared access policies in Event Hubs'
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12.5：事件中心中的共享访问策略
- en: 'Event hubs can be created from the Event Hubs namespace service by performing
    the following actions:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过执行以下操作，从事件中心命名空间服务中创建事件中心：
- en: Click on **Event Hubs** in the left-hand menu and click on **+ Event Hub** in
    the resultant screen:![Creating an Event Hub from the Azure portal](img/B15432_12_06.jpg)
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击左侧菜单中的**事件中心**，然后在结果屏幕中点击**+ 事件中心**：![从 Azure 门户创建事件中心](img/B15432_12_06.jpg)
- en: 'Figure 12.6: Creating an event hub from the Azure portal'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12.6：从 Azure 门户创建事件中心
- en: 'Next, provide values for the **Partition Count** and **Message Retention** fields,
    along with the name of your choice. Then, select **Off** for **Capture**, as demonstrated
    in *Figure 12.7*:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，提供**分区数**和**消息保留**字段的值，以及你选择的名称。然后，选择**关闭**作为**捕获**选项，如*图12.7*所示：
- en: '![Providing the Event Hub parameters](img/B15432_12_07.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![提供事件中心参数](img/B15432_12_07.jpg)'
- en: 'Figure 12.7: Creating a new event hub'
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12.7：创建新的事件中心
- en: 'After the event hub is created, you will see it in the list of event hubs,
    as shown in *Figure 12.8*:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 创建事件中心后，你将在事件中心列表中看到它，如*图12.8*所示：
- en: '![List of created event hubs in Azure](img/B15432_12_08.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![Azure 中创建的事件中心列表](img/B15432_12_08.jpg)'
- en: 'Figure 12.8: List of created event hubs'
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12.8：已创建的事件中心列表
- en: Event Hubs also allows the storage of events to a storage account or data lake
    directly using a feature known as Capture.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 事件中心还允许通过名为“捕获”的功能直接将事件存储到存储账户或数据湖中。
- en: 'Capture helps in the automatic storage of ingested data to either an Azure
    storage account or an Azure data lake. This feature ensures that the ingestion
    and storage of events happens in a single step, rather than transferring data
    into storage being a separate activity:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 捕获功能帮助将摄取的数据自动存储到 Azure 存储账户或 Azure 数据湖中。此功能确保数据的摄取和存储在一个步骤中完成，而不是将数据转移到存储中作为单独的活动：
- en: '![Capture feature options](img/B15432_12_09.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![捕获功能选项](img/B15432_12_09.jpg)'
- en: 'Figure 12.9: Capture feature options'
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12.9：捕获功能选项
- en: Separate policies can be assigned to each event hub by adding a new policy at
    the event hub level.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在事件中心级别添加新策略，将单独的策略分配给每个事件中心。
- en: After creating the policy, the connection string is available from the **Secure
    Access Signature** left-menu item in the Azure portal.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 创建策略后，可以从 Azure 门户中**安全访问签名**左侧菜单项获取连接字符串。
- en: 'Since a namespace can consist of multiple event hubs, the connection string
    for an individual event hub will be similar to the following code block. The difference
    here is in the key value and the addition of `EntityPath` with the name of the
    event hub:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一个命名空间可以包含多个事件中心，单个事件中心的连接字符串将类似于以下代码块。这里的区别在于密钥值和添加了`EntityPath`，它指向事件中心的名称：
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We had to keep the **Capture** option set to **Off** while creating the event
    hub, and it can be switched back on after creating the event hub. It helps to
    save events to Azure Blob storage or an Azure Data Lake storage account automatically.
    The configuration for the size and time interval is shown in *Figure 12.10*:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '![Selecting size and time intervals for capturing events](img/B15432_12_10.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.10: Selecting the size and time interval for capturing events'
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We did not cover the concepts of partitions and message retention options while
    creating event hubs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning is an important concept related to the scalability of any data
    store. Events are retained within event hubs for a specific period of time. If
    all events are stored within the same data store, then it becomes extremely difficult
    to scale that data store. Every event producer will connect to the same data store
    and send their events to it. Compare this with a data store that can partition
    the same data into multiple smaller data stores, each being uniquely identified
    with a value.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: The smaller data store is called a **partition**, and the value that defines
    the partition is known as the **partition key**. This partition key is part of
    the event data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Now the event producers can connect to the event hub, and based on the value
    of the partition key, the event hub will store the data in an appropriate partition.
    This will allow the event hub to ingest multiple events at the same time in parallel.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'Deciding on the number of partitions is a crucial aspect of the scalability
    of an event hub. *Figure 12.11* shows that ingested data is stored in the appropriate
    partition internally by Event Hubs using the partition key:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![The partitioning concept in Event Hub](img/B15432_12_11.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.11: Partitioning in an event hub'
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is important to understand that one partition might have multiple keys. The
    user decides how many partitions are required, and the event hub internally decides
    the best way to allocate the partition keys between them. Each partition stores
    data in an orderly way using a timestamp, and newer events are appended toward
    the end of the partition.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that it is not possible to change the number of partitions
    once the event hub is created.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to remember that partitions also help in bringing parallelism
    and concurrency for applications reading the events. For example, if there are
    10 partitions, 10 parallel readers can read the events without any degradation
    in performance.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Message retention refers to the time period for which events should be stored.
    After the expiry of the retention period, the events are discarded.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Consumer groups
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consumers are applications that read events from an event hub. Consumer groups
    are created for consumers to connect to in order to read the events. There can
    be multiple consumer groups for an event hub, and each consumer group has access
    to all the partitions within an event hub. Each consumer group forms a query on
    the events in events hubs. Applications can use consumer groups and each application
    will get a different view of the event hub events. A default `$default` consumer
    group is created when creating an event hub. It is good practice for one consumer
    to be associated with one consumer group for optimal performance. However, it
    is possible to have five readers on each partition in a consumer group:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '![Event receivers in a consumer group](img/B15432_12_12.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.12: Event receivers in a consumer group'
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that you understand consumer groups, it is time to go deeper into the concept
    of Event Hubs throughput.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Throughput
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Partitions help with scalability, while throughput helps with capacity per second.
    So, what is capacity in terms of Event Hubs? It is the amount of data that can
    be handled per second.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'In Event Hubs, a single TU allows the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 1 MB of ingestion data per second or 1,000 events per second (whichever happens
    first)
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2 MB of egress data per second or 4,096 events per second (whichever happens
    first)
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The auto-inflate option helps in increasing the throughput automatically if
    the number of incoming/outgoing events or the incoming/outgoing total size crosses
    a threshold. Instead of throttling, the throughput will scale up and down. The
    configuration of throughput at the time of the creation of the namespace is shown
    in *Figure 12.13*. Again, careful thought should go into deciding the TUs:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '![Selecting the throughput units and enabling auto-inflate](img/B15432_12_13.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.13: Selecting the TUs along with auto-inflate'
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A primer on Stream Analytics
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Event Hubs is a highly scalable data streaming platform, so we need another
    service that can process these events as a stream rather than just as stored data.
    Stream Analytics helps in processing and examining a stream of big data, and Stream
    Analytics jobs help to execute the processing of events.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Stream Analytics can process millions of events per second and it is quite easy
    to get started with it. Azure Stream Analytics is a PaaS that is completely managed
    by Azure. Customers of Stream Analytics do not have to manage the underlying hardware
    and platform.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Each job comprises multiple inputs, outputs, and a query, which transforms
    the incoming data into new output. The whole architecture of Stream Analytics
    is shown in *Figure 12.14*:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![Azure Stream Analytics architecture](img/B15432_12_14.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.14: Azure Stream Analytics architecture'
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In *Figure 12.14*, the event sources are displayed on the extreme left. These
    are the sources that produce the events. They could be IoT devices, custom applications
    written in any programming language, or events coming from other Azure platforms,
    such as Log Analytics or Application Insights.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: These events must first be ingested into the system, and there are numerous
    Azure services that can help to ingest this data. We've already looked at Event
    Hubs and how they help in ingesting data. There are other services, such as IoT
    Hub, that also help in ingesting device-specific and sensor-specific data. IoT
    Hub and ingestion are covered in detail in *Chapter 11, Designing IoT Solutions*.
    This ingested data undergoes processing as it arrives in a stream, and this processing
    is done using Stream Analytics. The output from Stream Analytics could be fed
    to a presentation platform, such as Power BI, to show real-time data to stakeholders,
    or a storage platform such as Cosmos DB, Data Lake Storage, or Azure Storage,
    from which the data can be read and actioned later by Azure Functions and Service
    Bus queues.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Stream Analytics helps in gathering insights from real-time ingested data within
    a time window frame and helps in identifying patterns.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'It does so through three different tasks:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**: The data should be ingested within the analytics process. The data
    can originate from Event Hubs, IoT Hub, or Azure Blob storage. Multiple separate
    reference inputs using a storage account and SQL Database can be used for lookup
    data within queries.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Query**: This is where Stream Analytics does the core job of analyzing the
    ingested data and extracting meaningful insights and patterns. It does so with
    the help of JavaScript user-defined functions, JavaScript user-defined aggregates,
    Azure Machine Learning, and Azure Machine Learning studio.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output**: The result of the queries can be sent to multiple different types
    of destinations, and prominent among them are Cosmos DB, Power BI, Synapse Analytics,
    Data Lake Storage, and Functions:'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Stream Analytics process](img/B15432_12_15.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.15: Stream Analytics process'
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Stream Analytics is capable of ingesting millions of events per second and can
    execute queries on top of them.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'Input data is supported in any of the three following formats:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '**JavaScript Object Notation** (**JSON**): This is a lightweight, plaintext-based
    format that is human readable. It consists of name-value pairs; an example of
    a JSON event follows:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Comma-Separated Values** (**CSV**): These are also plaintext values, which
    are separated by commas. An example of CSV is shown in *Figure 12.16*. The first
    row is the header, containing three fields, followed by two rows of data:'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Input in the CSV datatype](img/B15432_12_16.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.16: Plaintext values'
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Avro**: This format is similar to JSON; however, it is stored in a binary
    format rather than a text format:'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: However, this does not mean that Stream Analytics can only ingest data using
    these three formats. It can also create custom .NET-based deserializers, using
    which any format of data can be ingested, depending upon the deserializers' implementation.
    The steps you can follow to write a custom deserializer are available at [https://docs.microsoft.com/azure/stream-analytics/custom-deserializer-examples](https://docs.microsoft.com/azure/stream-analytics/custom-deserializer-examples).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Not only can Stream Analytics receive events, but it also provides advanced
    query capability for the data that it receives. The queries can extract important
    insights from the temporal data streams and output them.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 12.17*, there is an input dataset and an output dataset;
    the query moves the events from the input to the output. The `INTO` clause refers
    to the output location, and the `FROM` clause refers to the input location. The
    queries are very similar to SQL queries, so the learning curve is not too steep
    for SQL programmers:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![Stream Analytics query for receiving Twitter data](img/B15432_12_17.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.17: Stream Analytics query for receiving Twitter data'
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Event Hubs provides mechanisms for sending outputs from queries to target destinations.
    At the time of writing, Stream Analytics supports multiple destinations for events
    and query outputs, as shown before.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to define custom functions that can be reused within queries.
    There are four options provided to define custom functions.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Azure Machine Learning
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JavaScript user-defined functions
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JavaScript user-defined aggregates
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Machine Learning studio
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hosting environment
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Stream Analytics jobs can run on hosts that are running on the cloud, or they
    can run on IoT edge devices. IoT edge devices are devices that are near to IoT
    sensors, rather than on the cloud. *Figure 12.18* shows the **New Stream Analytics
    job** pane:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a new Stream Analytics job](img/B15432_12_18.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.18: Creating a new Stream Analytics job'
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let's check out streaming units in detail.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Streaming units
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From *Figure 12.18*, you can see that the only configuration that is unique to
    Stream Analytics is streaming units. Streaming units refers to the resources (that
    is, CPU and memory) that are assigned for running a Stream Analytics job. The
    minimum and maximum streaming units are 1 and 120, respectively.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Streaming units must be pre-allocated according to the amount of data and the
    number of queries executed on that data; otherwise, the job will fail.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to scale streaming units up and down from the Azure portal.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: A sample application using Event Hubs and Stream Analytics
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will be creating a sample application comprising multiple
    Azure services, including Azure Logic Apps, Azure Event Hubs, Azure Storage, and
    Azure Stream Analytics.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: In this sample application, we will be reading all tweets containing the word
    "Azure" and storing them in an Azure storage account.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: To create this solution, we first need to provision all the necessary resources.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning a new resource group
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Navigate to the Azure portal, log in using valid credentials, click on **+
    Create a resource**, and search for **Resource group**. Select **Resource group** from
    the search results and create a new resource group. Then, provide a name and choose
    an appropriate location. Note that all resources should be hosted in the same
    resource group and location so that it is easy to delete them:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![Provisioning a new resource group in the Azure portal](img/B15432_12_19.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.19: Provisioning a new resource group in the Azure portal'
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Next, we will create an Event Hubs namespace.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Creating an Event Hubs namespace
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Click on **+ Create a resource** and search for **Event Hubs**. Select **Event
    Hubs** from the search results and create a new event hub. Then, provide a name
    and location, and select a subscription based on the resource group that was created
    earlier. Select **Standard** as the pricing tier and also select **Enable Auto-inflate**,
    as shown in *Figure 12.20*:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating an Event Hubs namespace](img/B15432_12_20.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.20: Creating an Event Hubs namespace'
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By now, an Event Hubs namespace should have been created. It is a pre-requisite
    to have a namespace before an event hub can be created. The next step is to provision
    an event hub.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Creating an event hub
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From the Event Hubs namespace service, click on **Events Hubs** in the left-hand
    menu, and then click on **+ Event hubs** to create a new event hub. Name it **azuretwitterdata** and
    provide an optimal number of partitions and a **Message Retention** value:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating an event hub with the desired credentials](img/B15432_12_21.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.21: Creating the azuretwitterdata event hub'
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: After this step, you will have an event hub that can be used to send event data,
    which is stored in durable storage such as a data lake or an Azure Storage account,
    to be used by downstream services.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning a logic app
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After the resource group is provisioned, click on **+ Create a resource** and
    search for **Logic Apps**. Select **Logic Apps** from the search results and create
    a new logic app. Then, provide a name and location, and select a subscription
    based on the resource group created earlier. It is good practice to enable **Log
    Analytics**. Logic Apps is covered in more detail in *Chapter 11, Azure Solutions
    using Azure Logic Apps, Event Grid, and Functions*. The logic app is responsible
    for connecting to Twitter using an account and fetching all the tweets with **Azure** in
    them:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a logic app with the desired credentials](img/B15432_12_22.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.22: Creating a logic app'
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'After the logic app is created, select the **When a new tweet is posted** trigger
    on the design surface, sign in, and then configure it as shown in *Figure 12.23*.
    You will need a valid Twitter account before configuring this trigger:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![Configuring the frequency of incoming tweets](img/B15432_12_23.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.23: Configuring the frequency of incoming tweets'
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, drop a **Send event** action on the designer surface; this action is responsible for
    sending tweets to the event hub:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![Adding an action to send tweets to the event hub](img/B15432_12_24.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.24: Adding an action to send tweets to the event hub'
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Select the name of the event hub that was created in an earlier step.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'The value specified in the content textbox is an expression that has been dynamically
    composed using Logic Apps–provided functions and Twitter data. Clicking on **Add
    dynamic content** provides a dialog through which the expression can be composed:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![Configuring Logic Apps activity using dynamic expressions](img/B15432_12_25.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.25: Configuring Logic Apps activity using dynamic expressions'
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The value of the expression is as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the next section, we will provision the storage account.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning the storage account
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Click on **+ Create a resource** and search for **Storage Account**. Select **Storage
    Account** from the search results and create a new storage account. Then, provide
    a name and location, and select a subscription based on the resource group that
    was created earlier. Finally, select **StorageV2** for **Account Kind**, **Standard** for **Performance**,
    and **Locally-redundant storage** (**LRS**) for the **Replication** field.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will create a Blob storage container to store the data coming out of
    Stream Analytics.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Creating a storage container
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Stream Analytics will output the data as files, which will be stored within
    a Blob storage container. A container named **twitter** will be created within
    Blob storage, as shown in *Figure 12.26*:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a storage container for Twitter data](img/B15432_12_26.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.26: Creating a storage container'
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let's create a new Stream Analytics job with a hosting environment on the cloud
    and set the streaming units to the default settings.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Creating Stream Analytics jobs
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The input for this Stream Analytics job comes from the event hub, and so we
    need to configure this from the **Inputs** menu:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating an input Stream Analytics job](img/B15432_12_27.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.27: Creating an input Stream Analytics job'
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The output for the Stream Analytics job is a Blob storage account, so you need
    to configure the output accordingly. Provide a path pattern that is suitable for
    this exercise; for example, **{datetime:ss}** is the path pattern that we are
    using for this exercise:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a Blob Storage account as output](img/B15432_12_28.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.28: Creating a Blob storage account as output'
  id: totrans-203
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The query is quite simple; you are just copying the data from the input to
    the output:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![Query for copying Twitter feeds](img/B15432_12_29.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.29: Query for copying Twitter feeds'
  id: totrans-206
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While this example just involves copying data, there can be more complex queries
    for performing transformation before loading data into a destination.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: This concludes all the steps for the application; now you should be able to
    run it.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Running the application
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The logic app should be enabled and Stream Analytics should be running. Now,
    run the logic app; it will create a job to run all the activities within it, as
    shown in *Figure 12.30*:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '![Overview of the GetAzureTwitterData application](img/B15432_12_30.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.30: Overview of the GetAzureTwitterData application'
  id: totrans-212
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The **Storage Account** container should get data, as shown in *Figure 12.31*:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![Checking the storage account container data](img/B15432_12_31.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.31: Checking the Storage Account container data'
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As an exercise, you can extend this sample solution and evaluate the sentiment
    of the tweets every three minutes. The Logic Apps workflow for such an exercise would
    be as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![Flowchart for analyzing tweet sentiments](img/B15432_12_32.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.32: Flowchart for analyzing tweet sentiment'
  id: totrans-218
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To detect sentiment, you'll need to use the Text Analytics API, which should be
    configured before being used in Logic Apps.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter focused on topics related to the streaming and storage of events.
    Events have become an important consideration in overall solution architecture.
    We covered important resources, such as Event Hubs and Stream Analytics, and foundational
    concepts, such as consumer groups and throughputs, as well as creating an end-to-end
    solution using them along with Logic Apps. You learned that events are raised
    from multiple sources, and in order to get insights in real time about activities
    and their related events, services such as Event Hubs and Stream Analytics play
    a significant role. In the next chapter, we will learn about integrating Azure
    DevOps and Jenkins and implementing some of the industry's best practices while
    developing solutions.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
