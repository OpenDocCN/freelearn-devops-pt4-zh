<html><head></head><body>
		<div>
			<div id="_idContainer370" class="Content">
			</div>
		</div>
		<div id="_idContainer371" class="Content">
			<h1 id="_idParaDest-343">14. <a id="_idTextAnchor357"/>Architecting Azure Kubernetes solutions</h1>
		</div>
		<div id="_idContainer392" class="Content">
			<p>Containers are one of the most talked-about infrastructure components of the last decade. Containers are not a new technology; they have been around for quite some time. They have been prevalent in the Linux world for more than two decades. Containers were not well known in the developer community due to their complexity and the fact that there was not much documentation regarding them. However, around the beginning of this decade, in 2013, a company was launched known as Docker that changed the perception and adoption of containers within the developer world.</p>
			<p>Docker wrote a robust API wrapper on top of existing Linux LXC containers and made it easy for developers to create, manage, and destroy containers from the command-line interface. When containerizing applications, the number of containers we have can increase drastically over time, and we can reach a point where we need to manage hundreds or even thousands of containers. This is where container orchestrators play a role, and Kubernetes is one of them. Using Kubernetes, we can automate the deployment, scaling, networking, and management of containers.</p>
			<p>In this chapter, we will look at:</p>
			<ul>
				<li>The introductory concepts of containers</li>
				<li>The concepts of Kubernetes</li>
				<li>The important elements that make Kubernetes work</li>
				<li>Architecting solutions using Azure Kubernetes Service</li>
			</ul>
			<p>Now that you know what Kubernetes is used for, let's start from scratch and discuss what containers are, how they are orchestrated using Kubernetes, and more.</p>
			<h2 id="_idParaDest-344"><a id="_idTextAnchor358"/>Introduction to containers</h2>
			<p>Containers are referred to as operating system–level virtualization systems. They are hosted on an operating system running either on a physical server or a virtual server. The nature of the implementation depends on the host operating system. For example, Linux containers are inspired by cgroups; on the other hand, Windows containers are almost lightweight virtual machines with a small footprint.</p>
			<p>Containers are truly cross-platform. Containerized applications can run on any platform, such as Linux, Windows, or Mac, uniformly without any changes being needed, which makes them highly portable. This makes them a perfect technology for organizations to adopt as they are platform-agnostic.</p>
			<p>In addition, containers can run in any cloud environment or on-premises environment without changes being needed. This means that organizations are also not tied to a single cloud provider if they implement containers as their hosting platform on the cloud. They can move their environment from on-premises and lift and shift to the cloud.</p>
			<p>Containers provide all the benefits that are typically available with virtual machines. They have their own IP addresses, DNS names, identities, networking stacks, filesystems, and other components that give users the impression of using a pristine new operating system environment. Under the hood, the Docker runtime virtualizes multiple operating system kernel–level components to provide that impression.</p>
			<p>All these benefits provide immense benefits for organizations adopting container technology, and Docker is one of the forerunners in this regard. There are other container runtime options available, such as CoreOS Rkt (pronounced as Rocket, out of production), Mesos Containerizer, and LXC containers. Organizations can adopt the technology that they feel comfortable with.</p>
			<p>Containers were previously not available in the Windows world, only becoming available for Windows 10 and Windows Server 2016. However, containers are now first-class citizens in the Windows world.</p>
			<p>As mentioned in the introduction, containers should be monitored, governed, and managed well, just like any other infrastructural component within an ecosystem. It's necessary to deploy an orchestrator, such as Kubernetes, that can help you to do so easily. In the next section, you will learn about the fundamentals of Kubernetes, including what its advantages are.</p>
			<h2 id="_idParaDest-345"><a id="_idTextAnchor359"/>Kubernetes fundamentals</h2>
			<p>Many organizations still ask, "<em class="italics">Do we need Kubernetes, or indeed any container orchestrator?</em>" When we think about container management on a large scale, we need to think about several points, such as scaling, load balancing, life cycle management, continuous delivery, logging and monitoring, and more.</p>
			<p>You might ask, "<em class="italics">Aren't containers supposed to do all that?</em>" The answer is that containers are only a low-level piece of the puzzle. The real benefits are gained through the tools that sit on top of the containers. At the end of the day, we need something to help us with orchestration.</p>
			<p>Kubernetes is a Greek word, κυβερνήτης, which means "helmsman" or "captain of the ship." Keeping the maritime theme of Docker containers, Kubernetes is the captain of the ship. Kubernetes is often denoted as K8s, where 8 represents the eight letters between "K" and "s" in the word "Kubernetes."</p>
			<p>As mentioned before, containers are more agile than virtual machines. They can be created within seconds and destroyed equally quickly. They have a similar life cycle to virtual machines; however, they need to be monitored, governed, and managed actively within an environment.</p>
			<p>It is possible to manage them using your existing toolset; even so, specialized tools, such as Kubernetes, can provide valuable benefits:</p>
			<ul>
				<li>Kubernetes is self-healing in nature. When a Pod (read as "container" for now) goes down within a Kubernetes environment, Kubernetes will ensure that a new Pod is created elsewhere either on the same node or on another node, to respond to requests on behalf of the application.</li>
				<li>Kubernetes also eases the process of upgrading an application. It provides out-of-the-box features to help you perform multiple types of upgrades with the original configuration.</li>
				<li>It helps to enable blue-green deployments. In this type of deployment, Kubernetes will deploy the new version of the application alongside the old one, and once it is confirmed that the new application works as expected, a DNS switch will be made to switch to the new version of the application. The old application deployment can continue to exist for rollback purposes.</li>
				<li>Kubernetes also helps to implement a rolling-upgrade deployment strategy. Here, Kubernetes will deploy the new version of the application one server at a time, and tear down the old deployment one server at a time. It will carry on this activity until there are no more servers left from the old deployment.</li>
				<li>Kubernetes can be deployed on an on-premises data center or on the cloud using the <strong class="bold">infrastructure as a service</strong> (<strong class="bold">IaaS</strong>) paradigm. This means that developers first create a group of virtual machines and deploy Kubernetes on top of it. There is also the alternative approach of using Kubernetes as a <strong class="bold">platform as a service</strong> (<strong class="bold">PaaS</strong>) offering. Azure provides a PaaS service known as <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>), which provides an out-of-the-box Kubernetes environment to developers.</li>
			</ul>
			<p>When it comes to Deployment, Kubernetes can be deployed in two ways:</p>
			<ul>
				<li><strong class="bold">Unmanaged clusters</strong>: Unmanaged clusters can be created by installing Kubernetes and any other relevant packages on a bare–metal machine or a virtual machine. In an unmanaged cluster, there will be master and worker nodes, formerly known as minions. The master and worker nodes work hand–in–hand to orchestrate the containers. If you are wondering how this is achieved, later in this chapter, we will be exploring the complete architecture of Kubernetes. Right now, just know that there are master and worker nodes.</li>
				<li><strong class="bold">Managed clusters</strong>: Managed clusters are normally provided by the cloud provider; the cloud provider manages the infrastructure for you. In Azure, this service is called AKS. Azure will provide active support regarding patching and managing the infrastructure. With IaaS, organizations have to ensure the availability and scalability of the nodes and the infrastructure on their own. In the case of AKS, the master component will not be visible as it is managed by Azure. However, the worker nodes (minions) will be visible and will be deployed to a separate resource group, so you can access the nodes if needed.</li>
			</ul>
			<p>Some of the key benefits of using AKS over unmanaged clusters are:</p>
			<ul>
				<li>If you are using unmanaged clusters, you need to work to make the solution highly available and scalable. In addition to that, you need to have proper update management in place to install updates and patches. On the other hand, in AKS, Azure manages this completely, enabling developers to save time and be more productive.</li>
				<li>Native integration with other services, such as Azure Container Registry to store your container images securely, Azure DevOps to integrate CI/CD pipelines, Azure Monitor for logging, and Azure Active Directory for security.</li>
				<li>Scalability and faster startup speed. </li>
				<li>Support for virtual machine scale sets.</li>
			</ul>
			<p>While there is no difference in terms of the basic functionality of these two deployments, the IaaS form of deployment provides the flexibility to add new plugins and configuration immediately that might take some time for the Azure team to make available with AKS. Also, newer versions of Kubernetes are available within AKS quite quickly, without much delay. </p>
			<p>We have covered the basics of Kubernetes. At this point, you might be wondering how Kubernetes achieves all this. In the next section, we will be looking at the components of Kubernetes and how they work hand–in–hand.</p>
			<h2 id="_idParaDest-346"><a id="_idTextAnchor360"/>Kubernetes architecture</h2>
			<p>The first step in understanding Kubernetes is understanding its architecture. We will go into the details of each component in the next section, but getting a high-level overview of the architecture will help you to understand the interaction between the components.</p>
			<h3 id="_idParaDest-347"><a id="_idTextAnchor361"/>Kubernetes clusters</h3>
			<p>Kubernetes needs physical or virtual nodes for installing two types of components:</p>
			<ul>
				<li>Kubernetes control plane components, or master components</li>
				<li>Kubernetes worker nodes (minions), or non-master components</li>
			</ul>
			<p><em class="italics">Figure 14.1</em> is a diagram that offers a high-level overview of Kubernetes<a id="_idTextAnchor362"/>' architecture. We will get into the components in more detail later on:</p>
			<div>
				<div id="_idContainer372" class="IMG---Figure">
					<img src="image/B15432_14_01.jpg" alt="A block diagram giving an overview of the Kubernetes cluster and showing the relationship between the Kubernetes Master, Nodes, and Pods."/>
				</div>
			</div>
			<h6>Figure 14.1: Kubernetes cluster overview</h6>
			<p>The control plane components are responsible for managing and governing the Kubernetes environment and Kubernetes minions.</p>
			<p>All nodes together—the master as well as the minions—form the cluster. A cluster, in other words, is a collection of nodes. They are virtual or physical, connected to each other, and reachable using the TCP networking stack. The outside world will have no clue about the size or capability of your cluster, or even the names of the worker nodes. The only thing the nodes are aware of is the address of the API server through which they interact with the cluster. For them, the cluster is one large computer that runs their applications.</p>
			<p>It is Kubernetes that internally decides an appropriate strategy, using controllers, to choose a valid, healthy node that can run the application smoothly.</p>
			<p>The control plane components can be installed in a high-availability configuration. So far, we have discussed clusters and how they work. In the next section, we will be taking a look at the components of a cluster.</p>
			<h3 id="_idParaDest-348"><a id="_idTextAnchor363"/>Kubernetes components</h3>
			<p>Kubernetes components are divided into two categories: master components and node components. The master components are also known as the control plane of the cluster. The control plane is responsible for managing the worker nodes and the Pods in the cluster. The decision-making authority of a cluster is the control plane, and it also takes care of detection and responses related to cluster events. <em class="italics">Figure 14.2</em> describes the complete architecture of a Kubernetes cluster:</p>
			<div>
				<div id="_idContainer373" class="IMG---Figure">
					<img src="image/B15432_14_02.jpg" alt="The Kubernetes architecture showing the connection between the Master components and the Node components, where the Master components consist of the Master Node while the Node components consist of the Worker nodes."/>
				</div>
			</div>
			<h6>Figure 14.2: Kubernetes architecture</h6>
			<p>You need to understand each of these components to administer a cluster correctly. Let's go ahead and discuss what the master components are:</p>
			<ul>
				<li><strong class="bold">API server</strong>: The API server is undoubtedly the brain of Kubernetes. It is the central component that enables all activities within Kubernetes. Every client request, with few exceptions, ends up with the API server, which decides the flow for the request. It is solely responsible for interacting with the etcd server. </li>
				<li><strong class="bold">etcd</strong>: etcd is the data store for Kubernetes. Only the API server is allowed to communicate with etcd, and the API server can perform <strong class="bold">Create</strong>, <strong class="bold">Read</strong>, <strong class="bold">Update</strong> and <strong class="bold">Delete</strong> (<strong class="bold">CRUD</strong>) activities on etcd. When a request ends up with the API server, after validation, the API server can perform any CRUD operations, depending on the etcd request. etcd is a distributed, highly available data store. There can be multiple installations of etcd, each with a copy of the data, and any of them can serve the requests from the API server. In <em class="italics">Figure 14.3</em>, you can see that there are multiple instances running in the control plane to provide high availability:<div id="_idContainer374" class="IMG---Figure"><img src="image/B15432_14_03.jpg" alt="A block diagram showing how the load balancer works with the worker nodes, and the control plane nodes to improve the availability."/></div></li>
			</ul>
			<h6>Figure 14.3: Making the control plane highly available</h6>
			<ul>
				<li><strong class="bold">Controller manager</strong>: The controller manager is the workhorse of Kubernetes. While the API server receives the requests, the actual work in Kubernetes is done by the controller manager. The controller manager, as the name suggests, is the manager of the controllers. There are multiple controllers in a Kubernetes master node, and each is responsible for managing a single controller.<p>The main responsibility of a controller is managing a single resource in a Kubernetes environment. For example, there is a replication controller manager for managing replication controller resources, and a ReplicaSet controller to manage ReplicaSets in a Kubernetes environment. The controller keeps a watch on the API server, and when it receives a request for a resource managed by it, the controller performs its job.</p><p>One of the main responsibilities of controllers is to keep running in a loop and ensure that Kubernetes is in the desired state. If there is any deviation from the desired state, the controllers should bring it back to the desired state. A deployment controller watches for any new deployment resources created by the API server. If a new deployment resource is found, the deployment controller creates a new ReplicaSet resource and ensures that the ReplicaSet is always in the desired state. A replication controller keeps running in a loop and checks whether the actual number of Pods in the environment matches the desired number of Pods. If a Pod dies for any reason, the replication controller will find that the actual count has gone down by one and it will schedule a new Pod in the same or another node.</p></li>
				<li><strong class="bold">Scheduler</strong>: The job of a scheduler is to schedule the Pods on Kubernetes minion nodes. It is not responsible for creating Pods. It is purely responsible for assigning Pods to Kubernetes minion nodes. It does so by taking into account the current state of nodes, how busy they are, their available resources, and also the definition of the Pod. A Pod might have a preference regarding a specific node, and the scheduler will keep these requests in consideration while scheduling Pods to nodes.</li>
			</ul>
			<p>We will now explore the node components that are deployed in each of the worker nodes in the cluster:</p>
			<ul>
				<li><strong class="bold">Kubelet</strong>: While the API server, scheduler, controllers, and etcd are deployed on master nodes, kubelets are deployed on minion nodes. They act as agents for the Kubernetes master components and are responsible for managing Pods locally on the nodes. There is one kubelet on each node. A kubelet takes commands from the master components and also provides health, monitoring, and update information about nodes and Pods to the master components, such as the API server and the controller manager. They are the conduit for administrative communication between the master and minion nodes.</li>
				<li><strong class="bold">kube-proxy</strong>: kube-proxy, just like kubelets, is deployed on minion nodes. It is responsible for monitoring Pods and Services, as well as updating the local iptables and netfilter firewall rules with any change in the availability of Pods and Services. This ensures that the routing information on nodes is updated as and when new Pods and Services are created or existing Pods and Services are deleted.</li>
				<li><strong class="bold">Container runtime</strong>: There are many container vendors and providers in the ecosystem today. Docker is the most famous of them all, though others are also gaining popularity. That's why, in our architecture, we denoted the container runtime with the Docker logo. Kubernetes is a generic container orchestrator. It cannot be tightly coupled with any single container vendor, such as Docker. It should be possible to use any container runtime on the minion nodes to manage the life cycle of containers.</li>
			</ul>
			<p>To run containers in Pods, an industry-based standard known as a <strong class="bold">container runtime interface</strong> (<strong class="bold">CRI</strong>) has been developed and is used by all leading companies. The standard provides rules that should be followed to achieve interoperability with orchestrators such as Kubernetes. Kubelets do not know which container binaries are installed on the nodes. They could be Docker binaries or any other binaries.</p>
			<p>As these container runtimes are developed with a common industry-based standard, irrespective of which runtime you are using, kubelets will be able to communicate with the container runtime. This decouples container management from Kubernetes cluster management. The responsibilities of the container runtime include the creation of containers, managing the networking stack of the containers, and managing the bridge network. Since the container management is separate from the cluster management, Kubernetes will not interfere in the responsibilities of the container runtime.</p>
			<p>The components we discussed are applicable to both unmanaged as well as managed AKS clusters. However, the master components are not exposed to the end user, as Azure manages all that in the case of AKS. Later in this chapter, we will cover the architecture of AKS. You will learn about unmanaged clusters and come to understand the differences between these systems more clearly.</p>
			<p>Next, you will learn about some of the most important Kubernetes resources, also known as the primitives, knowledge that is applicable to both unmanaged and AKS clusters.</p>
			<h2 id="_idParaDest-349"><a id="_idTextAnchor364"/>Kubernetes primitives</h2>
			<p>You have learned that Kubernetes is an orchestration system used to deploy and manage containers. Kubernetes defines a set of building blocks, which are also known as primitives. These primitives together can help us to deploy, maintain, and scale containerized applications. Let's take a look at each of the primitives and understand their roles.</p>
			<h3 id="_idParaDest-350"><a id="_idTextAnchor365"/>Pod</h3>
			<p>Pods are the most basic unit of Deployment in Kubernetes. The immediate question that arises to a curious mind is how is a Pod different to a container? Pods are wrappers on top of containers. In other words, containers are contained within Pods. There can be multiple containers within a Pod; however, best practice is to have a one-Pod-one-container relationship. This does not mean we cannot have more than one container in a Pod. Multiple containers in a Pod is also fine, as long as there is one main container and the rest are ancillary containers. There are also patterns, such as sidecar patterns, that can be implemented with multi-container Pods.</p>
			<p>Each Pod has its own IP address and networking stack. All containers share the network interface and the stack. All containers within a Pod can be reached locally using the hostname.</p>
			<p>A simple Pod definition in YAML format is shown in the following lines of code:</p>
			<p class="snippet">---</p>
			<p class="snippet">apiVersion: v1</p>
			<p class="snippet">kind: Pod</p>
			<p class="snippet">metadata:</p>
			<p class="snippet">  name: tappdeployment</p>
			<p class="snippet">  labels:</p>
			<p class="snippet">    appname: tapp</p>
			<p class="snippet">    ostype: linux</p>
			<p class="snippet">spec:</p>
			<p class="snippet">  containers:</p>
			<p class="snippet">  - name: mynewcontainer</p>
			<p class="snippet">    image: "tacracr.azurecr.io/tapp:latest"</p>
			<p class="snippet">    ports:</p>
			<p class="snippet">    - containerPort: 80</p>
			<p class="snippet">      protocol: TCP</p>
			<p class="snippet">      name: http</p>
			<p>The Pod definition shown has a name and defines a few labels, which can be used by the Service resource to expose to other Pods, nodes and external custom resources. It also defines a single container based on a custom image stored in Azure Container Registry and opens port <strong class="inline">80</strong> for the container.</p>
			<h3 id="_idParaDest-351"><a id="_idTextAnchor366"/>Services</h3>
			<p>Kubernetes allows creating Pods with multiple instances. These Pods should be reachable from any Pod or node within a cluster. It is possible to use the IP address of a Pod directly and access the Pod. However, this is far from ideal. Pods are ephemeral and they might get a new IP address if the previous Pod has gone down. In such cases, the application will break easily. Kubernetes provides Services, which decouple Pod instances from their clients. Pods may get created and torn down, but the IP address of a Kubernetes Service remains constant and stable. Clients can connect to the Service IP address, which in turn has one endpoint for each Pod it can send requests to. If there are multiple Pod instances, each of their IP addresses will be available to the Service as an endpoint object. When a Pod goes down, the endpoints are updated to reflect the current Pod instances along with their IP addresses.</p>
			<p>Services are highly decoupled with Pods. The main intention of Services is to queue for Pods that have labels in their Service selector definitions. A Service defines label selectors, and based on label selectors, Pod IP addresses are added to the Service resource. Pods and Services can be managed independently of each other.</p>
			<p>A Service provides multiple types of IP address schemes. There are four types of Services: ClusterIP, NodePort, LoadBalancer, and Ingress Controller using Application Gateway.</p>
			<p>The most fundamental scheme is known as ClusterIP, and it is an internal IP address that can be reached only from within the cluster. The ClusterIP scheme is shown in <em class="italics">Figure 14.4</em>:</p>
			<div>
				<div id="_idContainer375" class="IMG---Figure">
					<img src="image/B15432_14_04.jpg" alt="A block diagram showing the workings of the ClusterIP, where the internal traffic is routed to the ClsuterIP and then it is sent to the respective Pods."/>
				</div>
			</div>
			<h6>Figure 14.4: The workings of ClusterIP</h6>
			<p>ClusterIP also allows the creation of NodePort, using which it gets a ClusterIP. However, it can also open a port on each of the nodes within a cluster. The Pods can be reached using ClusterIP addresses as well as by using a combination of the node IP and node port:</p>
			<div>
				<div id="_idContainer376" class="IMG---Figure">
					<img src="image/B15432_14_05.jpg" alt="The workings of NodePort, where traffic is routed to the AKS node, NodePort, and finally to the Pods."/>
				</div>
			</div>
			<h6>Figure 14.5: The workings of NodePort</h6>
			<p>Services can refer not only to Pods but to external endpoints as well. Finally, Services also allow the creation of a load balancer–based service that is capable of receiving requests externally and redirecting them to a Pod instance using ClusterIP and NodePort internally:</p>
			<div>
				<div id="_idContainer377" class="IMG---Figure">
					<img src="image/B15432_14_06.jpg" alt="A block diagram showing the workings of the LoadBalancer. Here, the incoming traffic is routed through the load balancer and then reaches the Pods through the AKS nodes."/>
				</div>
			</div>
			<h6>Figure 14.6: The workings of Load Balancer</h6>
			<p>There is one final type of service known as Ingress Controller, which provides advanced functionalities such as URL-based routing, as shown in <em class="italics">Figure 14.7</em>:</p>
			<div>
				<div id="_idContainer378" class="IMG---Figure">
					<img src="image/B15432_14_07.jpg" alt="A block diagram showing the workings of the Ingress Controller; the incoming traffic is routed through Ingress, and the Blog service, and finally, it reaches the Pods."/>
				</div>
			</div>
			<h6>Figure 14.7: The workings of Ingress Controller</h6>
			<p>A service definition in YAML format is shown here:</p>
			<p class="snippet">apiVersion: v1</p>
			<p class="snippet">kind: Service</p>
			<p class="snippet">metadata:</p>
			<p class="snippet">  name: tappservice</p>
			<p class="snippet">  labels:</p>
			<p class="snippet">    appname: tapp</p>
			<p class="snippet">    ostype: linux</p>
			<p class="snippet">spec:</p>
			<p class="snippet">  type: LoadBalancer</p>
			<p class="snippet">  selector:</p>
			<p class="snippet">    appname: myappnew</p>
			<p class="snippet">  ports:</p>
			<p class="snippet">  - name: http</p>
			<p class="snippet">    port: 8080</p>
			<p class="snippet">    targetPort: 80</p>
			<p class="snippet">    protocol: TCP</p>
			<p>This service definition creates a load balancer–based service using label selectors.</p>
			<h3 id="_idParaDest-352"><a id="_idTextAnchor367"/>Deployments</h3>
			<p>Kubernetes Deployments are higher-level resources in comparison to ReplicaSets and Pods. Deployments provide functionality related to the upgrading and release of an application. Deployment resources create a ReplicaSet, and the ReplicaSet manages the Pod. It is important to understand the need for deployment resources when ReplicaSets already exist.</p>
			<p>Deployments play a significant role in upgrading applications. If an application is already in production and a new version of the application needs to be deployed, there are a few choices for you:</p>
			<ol>
				<li>Delete existing Pods and create new Pods – in this method, there is downtime for the application, so this method should only be used if downtime is acceptable. There is a risk of increased downtime if the Deployment contains bugs and you have to roll back to a previous version.</li>
				<li>Blue-green deployment – In this method, the existing Pods continue to run and a new set of Pods is created with the new version of the application. The new Pods are not reachable externally. Once the tests have successfully completed, Kubernetes starts pointing to the new set of Pods. The old Pods can stay as-is or can be subsequently deleted.</li>
				<li>Rolling upgrades – In this method, existing Pods are deleted one at a time while new Pods for the new application version are created one at a time. The new Pods are incrementally deployed while the old Pods are incrementally reduced, until they reach a count of zero.</li>
			</ol>
			<p>All these approaches would have to be carried out manually without a Deployment resource. A Deployment resource automates the entire release and upgrade process. It can also help to automatically roll back to a previous version if there are any issues with the current Deployment.</p>
			<p>A Deployment definition is shown in the following code listing:</p>
			<p class="snippet">---</p>
			<p class="snippet">apiVersion: apps/v1</p>
			<p class="snippet">kind: Deployment</p>
			<p class="snippet">metadata:</p>
			<p class="snippet">  name: tappdeployment</p>
			<p class="snippet">  labels:</p>
			<p class="snippet">    appname: tapp</p>
			<p class="snippet">    ostype: linux</p>
			<p class="snippet">spec:</p>
			<p class="snippet">  replicas: 3</p>
			<p class="snippet">  selector:</p>
			<p class="snippet">    matchLabels:</p>
			<p class="snippet">      appname: myappnew</p>
			<p class="snippet">  strategy:</p>
			<p class="snippet">    type: RollingUpdate</p>
			<p class="snippet">    rollingUpdate:</p>
			<p class="snippet">       maxSurge: 1</p>
			<p class="snippet">       maxUnavailable: 1  </p>
			<p class="snippet">  template:</p>
			<p class="snippet">    metadata:</p>
			<p class="snippet">      name: mypod</p>
			<p class="snippet">      labels:</p>
			<p class="snippet">        appname: myappnew</p>
			<p class="snippet">    spec:</p>
			<p class="snippet">      containers:</p>
			<p class="snippet">      - name: mynewcontainer</p>
			<p class="snippet">        image: "tacracr.azurecr.io/tapp:latest"</p>
			<p class="snippet">        ports:</p>
			<p class="snippet">        - containerPort: 80</p>
			<p class="snippet">          protocol: TCP</p>
			<p class="snippet">          name: http</p>
			<p>It is important to note that a Deployment has a <strong class="inline">strategy</strong> property, which determines whether the <strong class="inline">recreate</strong> or <strong class="inline">RollingUpdate</strong> strategy is used. <strong class="inline">recreate</strong> will delete all existing Pods and create new Pods. It also contains configuration details related to <strong class="inline">RollingUpdate</strong> by providing the maximum number of Pods that can be created and destroyed in a single execution.</p>
			<h3 id="_idParaDest-353"><a id="_idTextAnchor368"/>Replication controller and ReplicaSet</h3>
			<p>Kubernetes' replication controller resource ensures that a specified desired number of Pod instances are always running within a cluster. Any deviation from the desired state is watched for by the replication controller, and it creates new Pod instances to meet the desired state.</p>
			<p>ReplicaSets are the new version of the replication controller. ReplicaSets provide the same functionality as that of replication controllers, with a few advanced functionalities. The main one among these is the rich capability for defining the selectors associated with Pods. With ReplicaSets, it is possible to define the dynamic expressions that were missing with replication controllers.</p>
			<p>It is recommended to use ReplicaSets rather than replication controllers.</p>
			<p>The next code listing shows an example of defining a <strong class="inline">ReplicaSet</strong> resource:</p>
			<p class="snippet">---</p>
			<p class="snippet">apiVersion: apps/v1</p>
			<p class="snippet">kind: ReplicaSet</p>
			<p class="snippet">metadata:</p>
			<p class="snippet">  name: tappdeployment</p>
			<p class="snippet">  labels:</p>
			<p class="snippet">    appname: tapp</p>
			<p class="snippet">    ostype: linux</p>
			<p class="snippet">spec:</p>
			<p class="snippet">  replicas: 3</p>
			<p class="snippet">  selector:</p>
			<p class="snippet">    matchLabels:</p>
			<p class="snippet">      appname: myappnew</p>
			<p class="snippet">  template:</p>
			<p class="snippet">    metadata:</p>
			<p class="snippet">      name: mypod</p>
			<p class="snippet">      labels:</p>
			<p class="snippet">        appname: myappnew</p>
			<p class="snippet">    spec:</p>
			<p class="snippet">      containers:</p>
			<p class="snippet">      - name: mynewcontainer</p>
			<p class="snippet">        image: "tacracr.azurecr.io/tapp:latest"</p>
			<p class="snippet">        ports:</p>
			<p class="snippet">        - containerPort: 80</p>
			<p class="snippet">          protocol: TCP</p>
			<p class="snippet">          name: http</p>
			<p>It is important to note that ReplicaSets have a <strong class="inline">replicas</strong> property, which determines the count of Pod instances, a <strong class="inline">selector</strong> property, which defines the Pods that should be managed by ReplicaSet, and finally the <strong class="inline">template</strong> property, which defines the Pod itself.</p>
			<h3 id="_idParaDest-354"><a id="_idTextAnchor369"/>ConfigMaps and Secrets</h3>
			<p>Kubernetes provides two important resources to store configuration data. ConfigMaps are used to store general configuration data that is not security-sensitive. Generic application configuration data, such as folder names, volume names, and DNS names, can be stored in ConfigMaps. On the other hand, sensitive data, such as credentials, certificates, and secrets, should be stored within Secrets resources. This Secrets data is encrypted and stored within the Kubernetes etcd data store.</p>
			<p>Both ConfigMaps and Secrets data can be made available as environment variables or volumes within Pods.</p>
			<p>The definition of the Pod that wants to consume these resources should include a reference to them. We have now covered the Kubernetes primitives and the roles of each of the building blocks. Next, you will be learning about the architecture of AKS.</p>
			<h2 id="_idParaDest-355"><a id="_idTextAnchor370"/>AKS architecture</h2>
			<p>In the previous section, we discussed the architecture of an unmanaged cluster. Now, we will be exploring the architecture of AKS. When you have read this section, you will be able to point out the major differences between the architecture of unmanaged and managed (AKS, in this case) clusters.</p>
			<p>When an AKS instance is created, the worker nodes only are created. The master components are managed by Azure. The master components are the API server, the scheduler, etcd, and the controller manager, which we discussed earlier. The kubelets and kube-proxy are deployed on the worker nodes. Communication between the nodes and master components happens using kubelets, which act as agents for the Kubernetes clusters for the node:</p>
			<div>
				<div id="_idContainer379" class="IMG---Figure">
					<img src="image/B15432_14_08.jpg" alt="The AKS architecture consisting of the Control plane, which is Azure-managed, and the Node, which is Customer-managed."/>
				</div>
			</div>
			<h6>Figure 14.8: AKS architecture</h6>
			<p>When a user requests a Pod instance, the user request lands with the API server. The API server checks and validates the request details and stores in etcd (the data store for the cluster) and also creates the deployment resource (if the Pod request is wrapped around a deployment resource). The deployment controller keeps a watch on the creation of any new deployment resources. If it sees one, it creates a ReplicaSet resource based on the definition provided in the user request.</p>
			<p>The ReplicaSet controller keeps a watch on the creation of any new ReplicaSet resources, and upon seeing a resource being created, it asks the scheduler to schedule the Pods. The scheduler has its own procedure and rules for finding an appropriate node for hosting the Pods. The scheduler informs the kubelet of the node and the kubelet then fetches the definition for the Pod and creates the Pods using the container runtime installed on the nodes. The Pod finally creates the containers within its definition.</p>
			<p>kube-proxy helps in maintaining the list of IP addresses of Pod and Service information on local nodes, as well as updating the local firewall and routing rules. To do a quick recap of what we have discussed so far, we started off with the Kubernetes architecture and then moved on to primitives, followed by the architecture of AKS. Since you are clear on the concepts, let's go ahead and create an AKS cluster in the next section.</p>
			<h2 id="_idParaDest-356"><a id="_idTextAnchor371"/>Deploying an AKS cluster</h2>
			<p>AKS can be provisioned using the Azure portal, the Azure <strong class="bold">CLI</strong> (<strong class="bold">command-line interface</strong>), Azure PowerShell cmdlets, ARM templates, <strong class="bold">SDKs</strong> (<strong class="bold">software development kits</strong>) for supported languages, and even Azure ARM REST APIs.</p>
			<p>The Azure portal is the simplest way of creating an AKS instance; however, to enable DevOps, it is better to create an AKS instance using ARM templates, the CLI, or PowerShell.</p>
			<h3 id="_idParaDest-357"><a id="_idTextAnchor372"/>Creating an AKS cluster</h3>
			<p>Let's create a resource group to deploy our AKS cluster. From the Azure CLI, use the <strong class="inline">az group create</strong> command:</p>
			<p class="snippet">az group create -n AzureForArchitects -l southeastasia</p>
			<p>Here, <strong class="inline">-n</strong> denotes the name of the resource group and <strong class="inline">-l</strong> denotes the location. If the request was successful, you will see a similar response to this:</p>
			<div>
				<div id="_idContainer380" class="IMG---Figure">
					<img src="image/B15432_14_09.jpg" alt="The output of the az group create command showing that the resource group has been created."/>
				</div>
			</div>
			<h6>Figure 14.9: Resource group creation</h6>
			<p>Now that we have the resource group ready, we will go ahead and create the AKS cluster using the <strong class="inline">az aks create</strong> command. The following command will create a cluster named <strong class="inline">AzureForArchitects-AKS</strong> in the <strong class="inline">AzureForArchitects</strong> resource group with a node count of <strong class="inline">2</strong>. The <strong class="inline">--generate-ssh-keys</strong> parameter will allow the creation of <strong class="bold">RSA</strong> (<strong class="bold">Rivest–Shamir–Adleman</strong>) key pairs, a public-key cryptosystem:</p>
			<p class="snippet">az aks create --resource-group AzureForArchitects \</p>
			<p class="snippet">--name AzureForArchitects-AKS \</p>
			<p class="snippet">--node-count 2 \</p>
			<p class="snippet">--generate-ssh-keys</p>
			<p>If the command succeeded, you will be able to see a similar output to this:</p>
			<div>
				<div id="_idContainer381" class="IMG---Figure">
					<img src="image/B15432_14_10.jpg" alt="The output of the az aks create command showing that the cluster has been created."/>
				</div>
			</div>
			<h6>Figure 14.10: Creating the cluster</h6>
			<p>Going through the cluster, you will see a line item that says <strong class="inline">"nodeResourceGroup": "MC_AzureForArchitects_AzureForArchitects-AKS_southeastasia"</strong>. When creating an AKS cluster, a second resource is automatically created to store the node resources.</p>
			<p>Our cluster is provisioned. Now we need to connect to the cluster and interact with it. To control the Kubernetes cluster manager, we will be using kubectl. In the next section, we will take a quick look at kubectl.</p>
			<h3 id="_idParaDest-358"><a id="_idTextAnchor373"/>Kubectl</h3>
			<p>Kubectl is the main component through which developers and infrastructure consultants can interact with AKS. Kubectl helps in creating a REST request containing the HTTP header and body, and submitting it to the API server. The header contains the authentication details, such as a token or username/password combination. The body contains the actual payload in JSON format.</p>
			<p>The kubectl command provides rich log details when used along with the verbose switch. The switch takes an integer input that can range from 0 to 9, which can be viewed from the details logs.</p>
			<h3 id="_idParaDest-359"><a id="_idTextAnchor374"/>Connecting to the cluster</h3>
			<p>To connect to the cluster locally, we need to install kubectl. Azure Cloud Shell has kubectl already installed. If you want to connect locally, use <strong class="inline">az aks install-cli</strong> to install kubectl.</p>
			<p>In order to configure kubectl to connect to our Kubernetes cluster, we need to download the credentials and configure the CLI with them. This can be done using the <strong class="inline">az aks get-credentials</strong> command. Use the command as shown here:</p>
			<p class="snippet">az aks get-credentials \</p>
			<p class="snippet">--resource-group AzureForArchitects \</p>
			<p class="snippet">--name AzureForArchitects-AKS</p>
			<p>Now, we need to verify whether we're connected to the cluster. As mentioned earlier, we'll be using kubectl to communicate with the cluster, and <strong class="inline">kubectl get nodes</strong> will show a list of nodes in the cluster. During creation, we set the node count to <strong class="inline">2</strong>, so the output should have two nodes. Also, we need to make sure that the status of the node is <strong class="inline">Ready</strong>. The output should be something like <em class="italics">Figure 14.11</em>:</p>
			<div>
				<div id="_idContainer382" class="IMG---Figure">
					<img src="image/B15432_14_11.jpg" alt="The output of the kubectl get nodes command showing the Name, Status, Roles, Age, and Version of the nodes."/>
				</div>
			</div>
			<h6>Figure 14.11: Getting the list of nodes</h6>
			<p>Since our node is in the <strong class="inline">Ready</strong> state, let's go ahead and create a Pod. There are two ways in which you can create resources in Kubernetes. They are:</p>
			<ul>
				<li><strong class="bold">Imperative</strong>: In this method, we use the <strong class="inline">kubectl run</strong> and <strong class="inline">kubectl expose</strong> commands to create the resources.</li>
				<li><strong class="bold">Declarative</strong>: We describe the state of the resource via JSON or a YAML file. While we were discussing Kubernetes primitives, you saw a lot of YAML files for each of the building blocks. We will pass the file to the <strong class="inline">kubectl apply</strong> command to create the resources, and the resources declared in the file will be created.</li>
			</ul>
			<p>Let's take the imperative approach first, to create a Pod with the name <strong class="inline">webserver</strong>, running an NGINX container with port <strong class="inline">80</strong> exposed:</p>
			<p class="snippet">kubectl run webserver --restart=Never --image nginx --port 80</p>
			<p>Upon successful completion of the command, the CLI will let you know the status:</p>
			<div>
				<div id="_idContainer383" class="IMG---Figure">
					<img src="image/B15432_14_12.jpg" alt="Creating a webserver with the kubectl run webserver command."/>
				</div>
			</div>
			<h6>Figure 14.12: Creating a Pod </h6>
			<p>Now that we have tried the imperative method, let's follow the declarative method. You can use the structure of the YAML file we discussed in the <em class="italics">Pod</em> subsection of the <em class="italics">Kubernetes primitives</em> section and modify it as per your requirements.</p>
			<p>We will be using the NGINX image, and the Pod will be named <strong class="inline">webserver-2</strong>.</p>
			<p>You can use any text editor and create the file. The final file will look similar to this:</p>
			<p class="snippet">apiVersion: v1</p>
			<p class="snippet">kind: Pod</p>
			<p class="snippet">metadata:</p>
			<p class="snippet">  name: webserver-2</p>
			<p class="snippet">  labels:</p>
			<p class="snippet">    appname: nginx</p>
			<p class="snippet">    ostype: linux</p>
			<p class="snippet">spec:</p>
			<p class="snippet">  containers:</p>
			<p class="snippet">  - name: wenserver-2-container</p>
			<p class="snippet">    image: nginx</p>
			<p class="snippet">    ports:</p>
			<p class="snippet">    - containerPort: 80</p>
			<p class="snippet">      protocol: TCP</p>
			<p class="snippet">      name: http</p>
			<p>In the <strong class="inline">kubectl apply</strong> command, we will pass the filename to the <strong class="inline">-f</strong> parameter, as shown in <em class="italics">Figure 14.13</em>, and you can see that the Pod has been created:</p>
			<div>
				<div id="_idContainer384" class="IMG---Figure">
					<img src="image/B15432_14_13.jpg" alt="Executing the command kubectl apply –f nginx.yaml to create a Pod with the declarative method."/>
				</div>
			</div>
			<h6>Figure 14.13: Creating a Pod using the declarative method.</h6>
			<p>Since we have created the Pods, we can use the <strong class="inline">kubectl get pods</strong> command to list all the Pods. Kubernetes uses the concept of namespaces for the logical isolation of resources. By default, all commands are pointing to the <strong class="inline">default</strong> namespace. If you want to perform an action on a specific namespace, you can pass the namespace name via the <strong class="inline">-n</strong> parameter. In <em class="italics">Figure 14.14</em>, you can see that <strong class="inline">kubectl get pods</strong> returns the Pods we created in the previous example, which reside in the default namespace. Also, when we use <strong class="inline">--all-namespaces</strong>, the output returns pods in all namespaces:</p>
			<div>
				<div id="_idContainer385" class="IMG---Figure">
					<img src="image/B15432_14_14.jpg" alt="The output of the kubectl get pods command displaying the details of all the Pods in the container."/>
				</div>
			</div>
			<h6>Figure 14.14: Listing all Pods</h6>
			<p>Now we will create a simple Deployment that runs NGINX and with a load balancer that exposes it to the internet. The <strong class="inline">YAML</strong> file will look like this:</p>
			<p class="snippet">#Creating a deployment that runs six replicas of nginx </p>
			<p class="snippet">apiVersion: apps/v1</p>
			<p class="snippet">kind: Deployment</p>
			<p class="snippet">metadata:</p>
			<p class="snippet">  name: nginx-server</p>
			<p class="snippet">spec:</p>
			<p class="snippet">  replicas: 6</p>
			<p class="snippet">  selector:</p>
			<p class="snippet">    matchLabels:</p>
			<p class="snippet">      app: nginx-server</p>
			<p class="snippet">  template:</p>
			<p class="snippet">    metadata:</p>
			<p class="snippet">      labels:</p>
			<p class="snippet">        app: nginx-server</p>
			<p class="snippet">    spec:</p>
			<p class="snippet">      containers:</p>
			<p class="snippet">      - name: nginx-server</p>
			<p class="snippet">        image: nginx</p>
			<p class="snippet">        ports:</p>
			<p class="snippet">        - containerPort: 80</p>
			<p class="snippet">          name: http</p>
			<p class="snippet">---</p>
			<p class="snippet">#Creating Service</p>
			<p class="snippet">apiVersion: v1</p>
			<p class="snippet">kind: Service</p>
			<p class="snippet">metadata:</p>
			<p class="snippet">  name: nginx-service</p>
			<p class="snippet">spec:</p>
			<p class="snippet">  ports:</p>
			<p class="snippet">  - port: 80</p>
			<p class="snippet">  selector:</p>
			<p class="snippet">    app: nginx-server</p>
			<p class="snippet">---</p>
			<p class="snippet">apiVersion: v1</p>
			<p class="snippet">kind: Service</p>
			<p class="snippet">metadata:</p>
			<p class="snippet">  name: nginx-lb</p>
			<p class="snippet">spec:</p>
			<p class="snippet">  type: LoadBalancer</p>
			<p class="snippet">  ports:</p>
			<p class="snippet">  - port: 80</p>
			<p class="snippet">  selector:</p>
			<p class="snippet">    app: nginx-server</p>
			<p>We'll be using the <strong class="inline">kubectl apply</strong> command and passing the YAML file to the <strong class="inline">-f</strong> parameter.</p>
			<p>Upon success, all three Services will be created, and if you execute the <strong class="inline">kubectl get deployment nginx-server</strong> command, you will see six replicas running, as shown in <em class="italics">Figure 14.15</em>, to make the Service highly available:</p>
			<div>
				<div id="_idContainer386" class="IMG---Figure">
					<img src="image/B15432_14_15.jpg" alt="Executing the kubectl get deployment nginx-server command to check the details of all the deployments."/>
				</div>
			</div>
			<h6>Figure 14.15: Checking the deployment</h6>
			<p>Since our Deployment is provisioned, we need to check what the public IP of the load balancer that we created is. We can use the <strong class="inline">kubectl get service nginx-lb --watch</strong> command. When the load balancer is initializing, <strong class="inline">EXTERNAL-IP</strong> will show as <strong class="inline">&lt;pending&gt;</strong>, the <strong class="inline">--wait</strong> parameter will let the command run in the foreground, and when the public IP is allocated, we will be able to see a new line, as shown here:</p>
			<div>
				<div id="_idContainer387" class="IMG---Figure">
					<img src="image/B15432_14_16.jpg" alt="Running the kubectl get service nginx-lb --watch command to get the Public IP of the load balancer."/>
				</div>
			</div>
			<h6>Figure 14.16: Finding public IP of the load balancer</h6>
			<p>Now that we have the public IP, we can go to the browser and should see the NGINX landing page, as shown in <em class="italics">Figure 14.17</em>:</p>
			<div>
				<div id="_idContainer388" class="IMG---Figure">
					<img src="image/B15432_14_17.jpg" alt="The browser pointing at the URL 52.187.70.177 and displaying the NGINX landing page."/>
				</div>
			</div>
			<h6>Figure 14.17: NGINX landing page</h6>
			<p>Similarly, you can use the <strong class="inline">YAML</strong> files we discussed in the <em class="italics">Kubernetes primitives</em> section to create different types of resources.</p>
			<p>There are a lot of commands, such as <strong class="inline">logs</strong>, <strong class="inline">describe</strong>, <strong class="inline">exec</strong>, and <strong class="inline">delete</strong>, that administrators need to use with the <strong class="inline">kubectl</strong> command. The objective of this section was to enable you to create an AKS cluster, connect to the cluster, and deploy a simple web application.</p>
			<p>In the next section, we will be discussing AKS networking.</p>
			<h2 id="_idParaDest-360"><a id="_idTextAnchor375"/>AKS networking</h2>
			<p>Networking forms a core component within a Kubernetes cluster. The master components should be able to reach the minion nodes and the Pods running on top of them, while the worker nodes should be able to communicate among themselves as well as with the master components.</p>
			<p>It might come as a surprise that core Kubernetes does not manage the networking stack at all. It is the job of the container runtime on the nodes.</p>
			<p>Kubernetes has prescribed three important tenets that any container runtime should adhere to. These are as follows:</p>
			<ul>
				<li>Pods should be able to communicate with other Pods without any transformation in their source or destination addresses, something that is performed using <strong class="bold">network address translation</strong> (<strong class="bold">NAT</strong>).</li>
				<li>Agents such as kubelets should be able to communicate with Pods directly on the nodes.</li>
				<li>Pods that are directly hosted on the host network still should be able to communicate with all Pods in the cluster.</li>
			</ul>
			<p>Every Pod gets a unique IP address within the Kubernetes cluster, along with a complete network stack, similar to virtual machines. They all are connected to the local bridge network created by the <strong class="bold">Container Networking Interface</strong> (<strong class="bold">CNI</strong>) component. The CNI component also creates the networking stack of the Pod. The bridge network then talks to the host network and becomes the conduit for the flow of traffic from Pods to network and vice versa.</p>
			<p>CNI is a standard managed and maintained by the <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>), and there are many providers that provide their own implementation of the interface. Docker is one of these providers. There are others, such as rkt (read as rocket), weave, calico, and many more. Each has its own capabilities and independently decides the network capabilities, while ensuring that the main tenets of Kubernetes networking are followed completely.</p>
			<p>AKS provides two distinct networking models:</p>
			<ul>
				<li>Kubenet</li>
				<li>Azure CNI</li>
			</ul>
			<h3 id="_idParaDest-361"><a id="_idTextAnchor376"/>Kubenet</h3>
			<p>Kubenet is the default networking framework in AKS. Under Kubenet, each node gets an IP address from the subnet of the virtual network they are connected with. The Pods do not get IP addresses from the subnet. Instead, a separate addressing scheme is used to provide IP addresses to Pods and Kubernetes Services. While creating an AKS instance, it is important to set the IP address range for Pods and Services. Since Pods are not on the same network as that of nodes, requests from Pods and to Pods are always NATed/routed to replace the source Pod IP with the node IP address and vice versa.</p>
			<p>In user-defined routing, Azure can support up to 400 routes, and you also cannot have a cluster larger than 400 nodes. <em class="italics">Figure 14.18</em> shows how the AKS node receives an IP address from the virtual network, but not the Pods created in the node:</p>
			<div>
				<div id="_idContainer389" class="IMG---Figure">
					<img src="image/B15432_14_18.jpg" alt="An architectural diagram illustrating the networking in AKS."/>
				</div>
			</div>
			<h6>Figure 14.18: Networking in AKS</h6>
			<p>By default, this Kubenet is configured with 110 Pods per node. This means there can be a maximum of 110 * 400 Pods in a Kubernetes cluster by default. The maximum number of Pods per node is 250.</p>
			<p>This scheme should be used when IP address availability and having user-defined routing are not a constraint.</p>
			<p>In the Azure CLI, you can execute the following command to create an AKS instance using this networking stack:</p>
			<p class="snippet">az aks create \</p>
			<p class="snippet">    --resource-group myResourceGroup \</p>
			<p class="snippet">    --name myAKSCluster \</p>
			<p class="snippet">    --node-count 3 \</p>
			<p class="snippet">    --network-plugin kubenet \</p>
			<p class="snippet">    --service-cidr 10.0.0.0/16 \</p>
			<p class="snippet">    --dns-service-ip 10.0.0.10 \</p>
			<p class="snippet">    --pod-cidr 10.244.0.0/16 \</p>
			<p class="snippet">    --docker-bridge-address 172.17.0.1/16 \</p>
			<p class="snippet">    --vnet-subnet-id $SUBNET_ID \</p>
			<p class="snippet">    --service-principal &lt;appId&gt; \</p>
			<p class="snippet">    --client-secret &lt;password&gt;</p>
			<p>Notice how all the IP addresses are explicitly provided for Service resources, Pods, nodes, and Docker bridges. These are non-overlapping IP address ranges. Also notice that Kubenet is used as a network plugin.</p>
			<h3 id="_idParaDest-362"><a id="_idTextAnchor377"/>Azure CNI (advanced networking)</h3>
			<p>With Azure CNI, each node and Pod gets an IP address assigned from the network subnet directly. This means there can be as many Pods as there are unique IP addresses available on a subnet. This makes IP address range planning much more important under this networking strategy.</p>
			<p>It is important to note that Windows hosting is only possible using the Azure CNI networking stack. Moreover, some of AKS components, such as virtual nodes and virtual kubelets, are also dependent on the Azure CNI stack. There is a need to reserve IP addresses in advance, depending on the number of Pods that will be created. There should always be extra IP addresses available on the subnet, to avoid exhaustion of IP addresses or to avoid the need to rebuild the cluster for a larger subnet due to application demand.</p>
			<p>By default, this networking stack is configured for 30 Pods per node and it can be configured with 250 Pods as the maximum number of Pods per node.</p>
			<p>The command to execute to create an AKS instance using this networking stack is shown here:</p>
			<p class="snippet">az aks create \</p>
			<p class="snippet">    --resource-group myResourceGroup \</p>
			<p class="snippet">    --name myAKSCluster \</p>
			<p class="snippet">    --network-plugin azure \</p>
			<p class="snippet">    --vnet-subnet-id &lt;subnet-id&gt; \</p>
			<p class="snippet">    --docker-bridge-address 172.17.0.1/16 \</p>
			<p class="snippet">    --dns-service-ip 10.2.0.10 \</p>
			<p class="snippet">    --service-cidr 10.2.0.0/24 \</p>
			<p class="snippet">    --generate-ssh-keys</p>
			<p>Notice how all the IP addresses are explicitly provided for Service resources, Pods, nodes, and Docker bridges. These are non-overlapping IP address ranges. Also, notice that Azure is used as a network plugin.</p>
			<p>So far, you have learned how to deploy a solution and manage the networking of an AKS cluster. Security is another important factor that needs to be addressed. In the next section, we will be focusing on access and identity options for AKS.</p>
			<h2 id="_idParaDest-363"><a id="_idTextAnchor378"/>Access and identity for AKS</h2>
			<p>Kubernetes clusters can be secured in multiple ways.</p>
			<p>The service account is one of the primary user types in Kubernetes. The Kubernetes API manages the service account. Authorized Pods can communicate with the API server using the credentials of service accounts, which are stored as Kubernetes Secrets. Kubernetes does not have any data store or identity provider of its own. It delegates the responsibility of authentication to external software. It provides an authentication plugin that checks for the given credentials and maps them to available groups. If the authentication is successful, the request passes to another set of authorization plugins to check the permission levels of the user on the cluster, as well as the namespace-scoped resources.</p>
			<p>For Azure, the best security integration would be to use Azure AD. Using Azure AD, you can also bring your on-premises identities to AKS to provide centralized management of accounts and security. The basic workflow of Azure AD integration is shown in <em class="italics">Figure 14.19</em>:</p>
			<div>
				<div id="_idContainer390" class="IMG---Figure">
					<img src="image/B15432_14_19.jpg" alt="A block diagram showing the basic workflow of Azure AD integration."/>
				</div>
			</div>
			<h6>Figure 14.19: Basic workflow of Azure AD integration</h6>
			<p>Users or groups can be granted access to resources within a namespace or across a cluster. In the previous section, we used the <strong class="inline">az aks get-credential</strong> command to get the credentials and the kubectl configuration context. When the user tries to interact with kubectl, they are prompted to sign in using their Azure AD credentials. Azure AD validates the credentials and a token is issued for the user. Based on the access level they have, they can access the resources in the cluster or the namespace.</p>
			<p>Additionally, you can leverage Azure <strong class="bold">Role-Based Access Control</strong> (<strong class="bold">RBAC</strong>) to limit access to the resources in the resource group.</p>
			<p>In the next section, we will be discussing virtual kubelet, which is one of the quickest ways to scale out a cluster.</p>
			<h2 id="_idParaDest-364"><a id="_idTextAnchor379"/>Virtual kubelet</h2>
			<p>Virtual kubelet is currently in preview and is managed by the CNCF organization. It is quite an innovative approach that AKS uses for scalability purposes. Virtual kubelet is deployed on the Kubernetes cluster as a Pod. The container running within the Pod uses the Kubernetes SDK to create a new node resource and represents itself to the entire cluster as a node. The cluster components, including the API server, scheduler, and controllers, think of it and treat it as a node and schedule Pods on it.</p>
			<p>However, when a Pod is scheduled on this node that is masquerading as a node, it communicates to its backend components, known as providers, to create, delete, and update the Pods. One of the main providers on Azure is Azure Container Instances. Azure Batch can also be used as a provider. This means the containers are actually created on Container Instances or Azure Batch rather than on the cluster itself; however, they are managed by the cluster. The architecture of virtual kubelet is shown in <em class="italics">Figure 14.20</em>:</p>
			<div>
				<div id="_idContainer391" class="IMG---Figure">
					<img src="image/B15432_14_20.jpg" alt="An architectural diagram of Virtual Kubelet showing how it is integrated with the Kubernetes API, nodes, and kubelets."/>
				</div>
			</div>
			<h6>Figure 14.20: Virtual kubelet architecture</h6>
			<p>Notice that virtual kubelet is represented as a node within the cluster and can help in hosting and managing Pods, just like a normal kubelet would. However, virtual kubelet has one limitation; this is what we are going to discuss in the next section. </p>
			<h2 id="_idParaDest-365"><a id="_idTextAnchor380"/>Virtual nodes</h2>
			<p>One of the limitations of virtual kubelet is that the Pods deployed on virtual kubelet providers are isolated and do not communicate with other Pods in the cluster. If there is a need for the Pods on these providers to talk to other Pods and nodes in the cluster and vice versa, then virtual nodes should be created. Virtual nodes are created on a different subnet on the same virtual network that is hosting Kubernetes cluster nodes, which can enable communication between Pods. Only the Linux operating system is supported, at the time of writing, for working with virtual nodes.</p>
			<p>Virtual nodes give a perception of a node; however, the node does not exist. Anything scheduled on such a node actually gets created in Azure Container Instances. Virtual nodes are based on virtual kubelet but have the extra functionality of seamless to-and-fro communication between the cluster and Azure Container Instances.</p>
			<p>While deploying Pods on virtual nodes, the Pod definition should contain an appropriate node selector to refer to virtual nodes, and also tolerations, as shown in next code snippet:</p>
			<p class="snippet">      nodeSelector:</p>
			<p class="snippet">        kubernetes.io/role: agent</p>
			<p class="snippet">        beta.kubernetes.io/os: linux</p>
			<p class="snippet">        type: virtual-kubelet</p>
			<p class="snippet">      tolerations:</p>
			<p class="snippet">      - key: virtual-kubelet.io/provider</p>
			<p class="snippet">        operator: Exists</p>
			<p class="snippet">      - key: azure.com/aci</p>
			<p class="snippet">        effect: NoSchedule</p>
			<p>Here, the node selector is using the <strong class="inline">type</strong> property to refer to virtual kubelet and the <strong class="inline">tolerations</strong> property to inform Kubernetes that nodes with taints, <strong class="inline">virtual-kubelet.io/provider</strong>,should allow the Deployment of these Pods on them.</p>
			<h2 id="_idParaDest-366"><a id="_idTextAnchor381"/>Summary</h2>
			<p>Kubernetes is the most widely used container orchestrator and works with different container and network runtimes. In this chapter, you learned about the basics of Kubernetes, its architecture, and some of the important infrastructure components, such as etcd, the API server, controller managers, and the scheduler, along with their purpose. Plus, we looked at important resources that can be deployed to manage applications, such as Pods, replication controllers, ReplicaSets, Deployments, and Services.</p>
			<p>AKS provides a couple of different networking stacks—Azure CNI and Kubenet. They provide different strategies for assigning IP addresses to Pods. While Azure CNI provides IP addresses to Pods from the underlying subnet, Kubenet uses virtual IP addresses only.</p>
			<p>We also covered some of the features provided exclusively by Azure, such as virtual nodes, and concepts around virtual kubelet. In the next chapter, we will learn about the provisioning and configuring resources with ARM templates.</p>
		</div>
	</body></html>