- en: 14\. Architecting Azure Kubernetes solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Containers are one of the most talked-about infrastructure components of the
    last decade. Containers are not a new technology; they have been around for quite
    some time. They have been prevalent in the Linux world for more than two decades.
    Containers were not well known in the developer community due to their complexity
    and the fact that there was not much documentation regarding them. However, around
    the beginning of this decade, in 2013, a company was launched known as Docker
    that changed the perception and adoption of containers within the developer world.
  prefs: []
  type: TYPE_NORMAL
- en: Docker wrote a robust API wrapper on top of existing Linux LXC containers and
    made it easy for developers to create, manage, and destroy containers from the
    command-line interface. When containerizing applications, the number of containers
    we have can increase drastically over time, and we can reach a point where we
    need to manage hundreds or even thousands of containers. This is where container
    orchestrators play a role, and Kubernetes is one of them. Using Kubernetes, we
    can automate the deployment, scaling, networking, and management of containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at:'
  prefs: []
  type: TYPE_NORMAL
- en: The introductory concepts of containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concepts of Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The important elements that make Kubernetes work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecting solutions using Azure Kubernetes Service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you know what Kubernetes is used for, let's start from scratch and
    discuss what containers are, how they are orchestrated using Kubernetes, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Containers are referred to as operating system–level virtualization systems.
    They are hosted on an operating system running either on a physical server or
    a virtual server. The nature of the implementation depends on the host operating
    system. For example, Linux containers are inspired by cgroups; on the other hand,
    Windows containers are almost lightweight virtual machines with a small footprint.
  prefs: []
  type: TYPE_NORMAL
- en: Containers are truly cross-platform. Containerized applications can run on any
    platform, such as Linux, Windows, or Mac, uniformly without any changes being
    needed, which makes them highly portable. This makes them a perfect technology
    for organizations to adopt as they are platform-agnostic.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, containers can run in any cloud environment or on-premises environment
    without changes being needed. This means that organizations are also not tied
    to a single cloud provider if they implement containers as their hosting platform
    on the cloud. They can move their environment from on-premises and lift and shift
    to the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Containers provide all the benefits that are typically available with virtual
    machines. They have their own IP addresses, DNS names, identities, networking
    stacks, filesystems, and other components that give users the impression of using
    a pristine new operating system environment. Under the hood, the Docker runtime
    virtualizes multiple operating system kernel–level components to provide that
    impression.
  prefs: []
  type: TYPE_NORMAL
- en: All these benefits provide immense benefits for organizations adopting container
    technology, and Docker is one of the forerunners in this regard. There are other
    container runtime options available, such as CoreOS Rkt (pronounced as Rocket,
    out of production), Mesos Containerizer, and LXC containers. Organizations can
    adopt the technology that they feel comfortable with.
  prefs: []
  type: TYPE_NORMAL
- en: Containers were previously not available in the Windows world, only becoming
    available for Windows 10 and Windows Server 2016\. However, containers are now
    first-class citizens in the Windows world.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in the introduction, containers should be monitored, governed,
    and managed well, just like any other infrastructural component within an ecosystem.
    It's necessary to deploy an orchestrator, such as Kubernetes, that can help you
    to do so easily. In the next section, you will learn about the fundamentals of
    Kubernetes, including what its advantages are.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes fundamentals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many organizations still ask, "*Do we need Kubernetes, or indeed any container
    orchestrator?*" When we think about container management on a large scale, we
    need to think about several points, such as scaling, load balancing, life cycle
    management, continuous delivery, logging and monitoring, and more.
  prefs: []
  type: TYPE_NORMAL
- en: You might ask, "*Aren't containers supposed to do all that?*" The answer is
    that containers are only a low-level piece of the puzzle. The real benefits are
    gained through the tools that sit on top of the containers. At the end of the
    day, we need something to help us with orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is a Greek word, κυβερνήτης, which means "helmsman" or "captain of
    the ship." Keeping the maritime theme of Docker containers, Kubernetes is the
    captain of the ship. Kubernetes is often denoted as K8s, where 8 represents the
    eight letters between "K" and "s" in the word "Kubernetes."
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, containers are more agile than virtual machines. They can
    be created within seconds and destroyed equally quickly. They have a similar life
    cycle to virtual machines; however, they need to be monitored, governed, and managed
    actively within an environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to manage them using your existing toolset; even so, specialized
    tools, such as Kubernetes, can provide valuable benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is self-healing in nature. When a Pod (read as "container" for now)
    goes down within a Kubernetes environment, Kubernetes will ensure that a new Pod
    is created elsewhere either on the same node or on another node, to respond to
    requests on behalf of the application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes also eases the process of upgrading an application. It provides out-of-the-box
    features to help you perform multiple types of upgrades with the original configuration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It helps to enable blue-green deployments. In this type of deployment, Kubernetes
    will deploy the new version of the application alongside the old one, and once
    it is confirmed that the new application works as expected, a DNS switch will
    be made to switch to the new version of the application. The old application deployment
    can continue to exist for rollback purposes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes also helps to implement a rolling-upgrade deployment strategy. Here,
    Kubernetes will deploy the new version of the application one server at a time,
    and tear down the old deployment one server at a time. It will carry on this activity
    until there are no more servers left from the old deployment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes can be deployed on an on-premises data center or on the cloud using
    the **infrastructure as a service** (**IaaS**) paradigm. This means that developers
    first create a group of virtual machines and deploy Kubernetes on top of it. There
    is also the alternative approach of using Kubernetes as a **platform as a service**
    (**PaaS**) offering. Azure provides a PaaS service known as **Azure Kubernetes
    Service** (**AKS**), which provides an out-of-the-box Kubernetes environment to
    developers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When it comes to Deployment, Kubernetes can be deployed in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unmanaged clusters**: Unmanaged clusters can be created by installing Kubernetes
    and any other relevant packages on a bare–metal machine or a virtual machine.
    In an unmanaged cluster, there will be master and worker nodes, formerly known
    as minions. The master and worker nodes work hand–in–hand to orchestrate the containers.
    If you are wondering how this is achieved, later in this chapter, we will be exploring
    the complete architecture of Kubernetes. Right now, just know that there are master
    and worker nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Managed clusters**: Managed clusters are normally provided by the cloud provider;
    the cloud provider manages the infrastructure for you. In Azure, this service
    is called AKS. Azure will provide active support regarding patching and managing
    the infrastructure. With IaaS, organizations have to ensure the availability and
    scalability of the nodes and the infrastructure on their own. In the case of AKS,
    the master component will not be visible as it is managed by Azure. However, the
    worker nodes (minions) will be visible and will be deployed to a separate resource
    group, so you can access the nodes if needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some of the key benefits of using AKS over unmanaged clusters are:'
  prefs: []
  type: TYPE_NORMAL
- en: If you are using unmanaged clusters, you need to work to make the solution highly
    available and scalable. In addition to that, you need to have proper update management
    in place to install updates and patches. On the other hand, in AKS, Azure manages
    this completely, enabling developers to save time and be more productive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Native integration with other services, such as Azure Container Registry to
    store your container images securely, Azure DevOps to integrate CI/CD pipelines,
    Azure Monitor for logging, and Azure Active Directory for security.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalability and faster startup speed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for virtual machine scale sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While there is no difference in terms of the basic functionality of these two
    deployments, the IaaS form of deployment provides the flexibility to add new plugins
    and configuration immediately that might take some time for the Azure team to
    make available with AKS. Also, newer versions of Kubernetes are available within
    AKS quite quickly, without much delay.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered the basics of Kubernetes. At this point, you might be wondering
    how Kubernetes achieves all this. In the next section, we will be looking at the
    components of Kubernetes and how they work hand–in–hand.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step in understanding Kubernetes is understanding its architecture.
    We will go into the details of each component in the next section, but getting
    a high-level overview of the architecture will help you to understand the interaction
    between the components.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Kubernetes needs physical or virtual nodes for installing two types of components:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes control plane components, or master components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes worker nodes (minions), or non-master components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 14.1* is a diagram that offers a high-level overview of Kubernetes''
    architecture. We will get into the components in more detail later on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A block diagram giving an overview of the Kubernetes cluster and showing
    the relationship between the Kubernetes Master, Nodes, and Pods.](img/B15432_14_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.1: Kubernetes cluster overview'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The control plane components are responsible for managing and governing the
    Kubernetes environment and Kubernetes minions.
  prefs: []
  type: TYPE_NORMAL
- en: All nodes together—the master as well as the minions—form the cluster. A cluster,
    in other words, is a collection of nodes. They are virtual or physical, connected
    to each other, and reachable using the TCP networking stack. The outside world
    will have no clue about the size or capability of your cluster, or even the names
    of the worker nodes. The only thing the nodes are aware of is the address of the
    API server through which they interact with the cluster. For them, the cluster
    is one large computer that runs their applications.
  prefs: []
  type: TYPE_NORMAL
- en: It is Kubernetes that internally decides an appropriate strategy, using controllers,
    to choose a valid, healthy node that can run the application smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: The control plane components can be installed in a high-availability configuration.
    So far, we have discussed clusters and how they work. In the next section, we
    will be taking a look at the components of a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Kubernetes components are divided into two categories: master components and
    node components. The master components are also known as the control plane of
    the cluster. The control plane is responsible for managing the worker nodes and
    the Pods in the cluster. The decision-making authority of a cluster is the control
    plane, and it also takes care of detection and responses related to cluster events.
    *Figure 14.2* describes the complete architecture of a Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Kubernetes architecture showing the connection between the Master components
    and the Node components, where the Master components consist of the Master Node
    while the Node components consist of the Worker nodes.](img/B15432_14_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.2: Kubernetes architecture'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You need to understand each of these components to administer a cluster correctly.
    Let''s go ahead and discuss what the master components are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**API server**: The API server is undoubtedly the brain of Kubernetes. It is
    the central component that enables all activities within Kubernetes. Every client
    request, with few exceptions, ends up with the API server, which decides the flow
    for the request. It is solely responsible for interacting with the etcd server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**etcd**: etcd is the data store for Kubernetes. Only the API server is allowed
    to communicate with etcd, and the API server can perform **Create**, **Read**,
    **Update** and **Delete** (**CRUD**) activities on etcd. When a request ends up
    with the API server, after validation, the API server can perform any CRUD operations,
    depending on the etcd request. etcd is a distributed, highly available data store.
    There can be multiple installations of etcd, each with a copy of the data, and
    any of them can serve the requests from the API server. In *Figure 14.3*, you
    can see that there are multiple instances running in the control plane to provide
    high availability:![A block diagram showing how the load balancer works with the
    worker nodes, and the control plane nodes to improve the availability.](img/B15432_14_03.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 14.3: Making the control plane highly available'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Controller manager**: The controller manager is the workhorse of Kubernetes.
    While the API server receives the requests, the actual work in Kubernetes is done
    by the controller manager. The controller manager, as the name suggests, is the
    manager of the controllers. There are multiple controllers in a Kubernetes master
    node, and each is responsible for managing a single controller.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main responsibility of a controller is managing a single resource in a Kubernetes
    environment. For example, there is a replication controller manager for managing
    replication controller resources, and a ReplicaSet controller to manage ReplicaSets
    in a Kubernetes environment. The controller keeps a watch on the API server, and
    when it receives a request for a resource managed by it, the controller performs
    its job.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: One of the main responsibilities of controllers is to keep running in a loop
    and ensure that Kubernetes is in the desired state. If there is any deviation
    from the desired state, the controllers should bring it back to the desired state.
    A deployment controller watches for any new deployment resources created by the
    API server. If a new deployment resource is found, the deployment controller creates
    a new ReplicaSet resource and ensures that the ReplicaSet is always in the desired
    state. A replication controller keeps running in a loop and checks whether the
    actual number of Pods in the environment matches the desired number of Pods. If
    a Pod dies for any reason, the replication controller will find that the actual
    count has gone down by one and it will schedule a new Pod in the same or another
    node.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Scheduler**: The job of a scheduler is to schedule the Pods on Kubernetes
    minion nodes. It is not responsible for creating Pods. It is purely responsible
    for assigning Pods to Kubernetes minion nodes. It does so by taking into account
    the current state of nodes, how busy they are, their available resources, and
    also the definition of the Pod. A Pod might have a preference regarding a specific
    node, and the scheduler will keep these requests in consideration while scheduling
    Pods to nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will now explore the node components that are deployed in each of the worker
    nodes in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubelet**: While the API server, scheduler, controllers, and etcd are deployed
    on master nodes, kubelets are deployed on minion nodes. They act as agents for
    the Kubernetes master components and are responsible for managing Pods locally
    on the nodes. There is one kubelet on each node. A kubelet takes commands from
    the master components and also provides health, monitoring, and update information
    about nodes and Pods to the master components, such as the API server and the
    controller manager. They are the conduit for administrative communication between
    the master and minion nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kube-proxy**: kube-proxy, just like kubelets, is deployed on minion nodes.
    It is responsible for monitoring Pods and Services, as well as updating the local
    iptables and netfilter firewall rules with any change in the availability of Pods
    and Services. This ensures that the routing information on nodes is updated as
    and when new Pods and Services are created or existing Pods and Services are deleted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Container runtime**: There are many container vendors and providers in the
    ecosystem today. Docker is the most famous of them all, though others are also
    gaining popularity. That''s why, in our architecture, we denoted the container
    runtime with the Docker logo. Kubernetes is a generic container orchestrator.
    It cannot be tightly coupled with any single container vendor, such as Docker.
    It should be possible to use any container runtime on the minion nodes to manage
    the life cycle of containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To run containers in Pods, an industry-based standard known as a **container
    runtime interface** (**CRI**) has been developed and is used by all leading companies.
    The standard provides rules that should be followed to achieve interoperability
    with orchestrators such as Kubernetes. Kubelets do not know which container binaries
    are installed on the nodes. They could be Docker binaries or any other binaries.
  prefs: []
  type: TYPE_NORMAL
- en: As these container runtimes are developed with a common industry-based standard,
    irrespective of which runtime you are using, kubelets will be able to communicate
    with the container runtime. This decouples container management from Kubernetes
    cluster management. The responsibilities of the container runtime include the
    creation of containers, managing the networking stack of the containers, and managing
    the bridge network. Since the container management is separate from the cluster
    management, Kubernetes will not interfere in the responsibilities of the container
    runtime.
  prefs: []
  type: TYPE_NORMAL
- en: The components we discussed are applicable to both unmanaged as well as managed
    AKS clusters. However, the master components are not exposed to the end user,
    as Azure manages all that in the case of AKS. Later in this chapter, we will cover
    the architecture of AKS. You will learn about unmanaged clusters and come to understand
    the differences between these systems more clearly.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will learn about some of the most important Kubernetes resources,
    also known as the primitives, knowledge that is applicable to both unmanaged and
    AKS clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes primitives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have learned that Kubernetes is an orchestration system used to deploy and
    manage containers. Kubernetes defines a set of building blocks, which are also
    known as primitives. These primitives together can help us to deploy, maintain,
    and scale containerized applications. Let's take a look at each of the primitives
    and understand their roles.
  prefs: []
  type: TYPE_NORMAL
- en: Pod
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pods are the most basic unit of Deployment in Kubernetes. The immediate question
    that arises to a curious mind is how is a Pod different to a container? Pods are
    wrappers on top of containers. In other words, containers are contained within
    Pods. There can be multiple containers within a Pod; however, best practice is
    to have a one-Pod-one-container relationship. This does not mean we cannot have
    more than one container in a Pod. Multiple containers in a Pod is also fine, as
    long as there is one main container and the rest are ancillary containers. There
    are also patterns, such as sidecar patterns, that can be implemented with multi-container
    Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Each Pod has its own IP address and networking stack. All containers share the
    network interface and the stack. All containers within a Pod can be reached locally
    using the hostname.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple Pod definition in YAML format is shown in the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The Pod definition shown has a name and defines a few labels, which can be used
    by the Service resource to expose to other Pods, nodes and external custom resources.
    It also defines a single container based on a custom image stored in Azure Container
    Registry and opens port `80` for the container.
  prefs: []
  type: TYPE_NORMAL
- en: Services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes allows creating Pods with multiple instances. These Pods should be
    reachable from any Pod or node within a cluster. It is possible to use the IP
    address of a Pod directly and access the Pod. However, this is far from ideal.
    Pods are ephemeral and they might get a new IP address if the previous Pod has
    gone down. In such cases, the application will break easily. Kubernetes provides
    Services, which decouple Pod instances from their clients. Pods may get created
    and torn down, but the IP address of a Kubernetes Service remains constant and
    stable. Clients can connect to the Service IP address, which in turn has one endpoint
    for each Pod it can send requests to. If there are multiple Pod instances, each
    of their IP addresses will be available to the Service as an endpoint object.
    When a Pod goes down, the endpoints are updated to reflect the current Pod instances
    along with their IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: Services are highly decoupled with Pods. The main intention of Services is to
    queue for Pods that have labels in their Service selector definitions. A Service
    defines label selectors, and based on label selectors, Pod IP addresses are added
    to the Service resource. Pods and Services can be managed independently of each
    other.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Service provides multiple types of IP address schemes. There are four types
    of Services: ClusterIP, NodePort, LoadBalancer, and Ingress Controller using Application
    Gateway.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most fundamental scheme is known as ClusterIP, and it is an internal IP
    address that can be reached only from within the cluster. The ClusterIP scheme
    is shown in *Figure 14.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A block diagram showing the workings of the ClusterIP, where the internal
    traffic is routed to the ClsuterIP and then it is sent to the respective Pods.](img/B15432_14_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.4: The workings of ClusterIP'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'ClusterIP also allows the creation of NodePort, using which it gets a ClusterIP.
    However, it can also open a port on each of the nodes within a cluster. The Pods
    can be reached using ClusterIP addresses as well as by using a combination of
    the node IP and node port:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The workings of NodePort, where traffic is routed to the AKS node, NodePort,
    and finally to the Pods.](img/B15432_14_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.5: The workings of NodePort'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Services can refer not only to Pods but to external endpoints as well. Finally,
    Services also allow the creation of a load balancer–based service that is capable
    of receiving requests externally and redirecting them to a Pod instance using
    ClusterIP and NodePort internally:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A block diagram showing the workings of the LoadBalancer. Here, the incoming
    traffic is routed through the load balancer and then reaches the Pods through
    the AKS nodes.](img/B15432_14_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.6: The workings of Load Balancer'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There is one final type of service known as Ingress Controller, which provides
    advanced functionalities such as URL-based routing, as shown in *Figure 14.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A block diagram showing the workings of the Ingress Controller; the incoming
    traffic is routed through Ingress, and the Blog service, and finally, it reaches
    the Pods.](img/B15432_14_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.7: The workings of Ingress Controller'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A service definition in YAML format is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This service definition creates a load balancer–based service using label selectors.
  prefs: []
  type: TYPE_NORMAL
- en: Deployments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes Deployments are higher-level resources in comparison to ReplicaSets
    and Pods. Deployments provide functionality related to the upgrading and release
    of an application. Deployment resources create a ReplicaSet, and the ReplicaSet
    manages the Pod. It is important to understand the need for deployment resources
    when ReplicaSets already exist.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deployments play a significant role in upgrading applications. If an application
    is already in production and a new version of the application needs to be deployed,
    there are a few choices for you:'
  prefs: []
  type: TYPE_NORMAL
- en: Delete existing Pods and create new Pods – in this method, there is downtime
    for the application, so this method should only be used if downtime is acceptable.
    There is a risk of increased downtime if the Deployment contains bugs and you
    have to roll back to a previous version.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Blue-green deployment – In this method, the existing Pods continue to run and
    a new set of Pods is created with the new version of the application. The new
    Pods are not reachable externally. Once the tests have successfully completed,
    Kubernetes starts pointing to the new set of Pods. The old Pods can stay as-is
    or can be subsequently deleted.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rolling upgrades – In this method, existing Pods are deleted one at a time while
    new Pods for the new application version are created one at a time. The new Pods
    are incrementally deployed while the old Pods are incrementally reduced, until
    they reach a count of zero.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All these approaches would have to be carried out manually without a Deployment
    resource. A Deployment resource automates the entire release and upgrade process.
    It can also help to automatically roll back to a previous version if there are
    any issues with the current Deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Deployment definition is shown in the following code listing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: It is important to note that a Deployment has a `strategy` property, which determines
    whether the `recreate` or `RollingUpdate` strategy is used. `recreate` will delete
    all existing Pods and create new Pods. It also contains configuration details
    related to `RollingUpdate` by providing the maximum number of Pods that can be
    created and destroyed in a single execution.
  prefs: []
  type: TYPE_NORMAL
- en: Replication controller and ReplicaSet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes' replication controller resource ensures that a specified desired
    number of Pod instances are always running within a cluster. Any deviation from
    the desired state is watched for by the replication controller, and it creates
    new Pod instances to meet the desired state.
  prefs: []
  type: TYPE_NORMAL
- en: ReplicaSets are the new version of the replication controller. ReplicaSets provide
    the same functionality as that of replication controllers, with a few advanced
    functionalities. The main one among these is the rich capability for defining
    the selectors associated with Pods. With ReplicaSets, it is possible to define
    the dynamic expressions that were missing with replication controllers.
  prefs: []
  type: TYPE_NORMAL
- en: It is recommended to use ReplicaSets rather than replication controllers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next code listing shows an example of defining a `ReplicaSet` resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: It is important to note that ReplicaSets have a `replicas` property, which determines
    the count of Pod instances, a `selector` property, which defines the Pods that
    should be managed by ReplicaSet, and finally the `template` property, which defines
    the Pod itself.
  prefs: []
  type: TYPE_NORMAL
- en: ConfigMaps and Secrets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes provides two important resources to store configuration data. ConfigMaps
    are used to store general configuration data that is not security-sensitive. Generic
    application configuration data, such as folder names, volume names, and DNS names,
    can be stored in ConfigMaps. On the other hand, sensitive data, such as credentials,
    certificates, and secrets, should be stored within Secrets resources. This Secrets
    data is encrypted and stored within the Kubernetes etcd data store.
  prefs: []
  type: TYPE_NORMAL
- en: Both ConfigMaps and Secrets data can be made available as environment variables
    or volumes within Pods.
  prefs: []
  type: TYPE_NORMAL
- en: The definition of the Pod that wants to consume these resources should include
    a reference to them. We have now covered the Kubernetes primitives and the roles
    of each of the building blocks. Next, you will be learning about the architecture
    of AKS.
  prefs: []
  type: TYPE_NORMAL
- en: AKS architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we discussed the architecture of an unmanaged cluster.
    Now, we will be exploring the architecture of AKS. When you have read this section,
    you will be able to point out the major differences between the architecture of
    unmanaged and managed (AKS, in this case) clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'When an AKS instance is created, the worker nodes only are created. The master
    components are managed by Azure. The master components are the API server, the
    scheduler, etcd, and the controller manager, which we discussed earlier. The kubelets
    and kube-proxy are deployed on the worker nodes. Communication between the nodes
    and master components happens using kubelets, which act as agents for the Kubernetes
    clusters for the node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The AKS architecture consisting of the Control plane, which is Azure-managed,
    and the Node, which is Customer-managed.](img/B15432_14_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.8: AKS architecture'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When a user requests a Pod instance, the user request lands with the API server.
    The API server checks and validates the request details and stores in etcd (the
    data store for the cluster) and also creates the deployment resource (if the Pod
    request is wrapped around a deployment resource). The deployment controller keeps
    a watch on the creation of any new deployment resources. If it sees one, it creates
    a ReplicaSet resource based on the definition provided in the user request.
  prefs: []
  type: TYPE_NORMAL
- en: The ReplicaSet controller keeps a watch on the creation of any new ReplicaSet
    resources, and upon seeing a resource being created, it asks the scheduler to
    schedule the Pods. The scheduler has its own procedure and rules for finding an
    appropriate node for hosting the Pods. The scheduler informs the kubelet of the
    node and the kubelet then fetches the definition for the Pod and creates the Pods
    using the container runtime installed on the nodes. The Pod finally creates the
    containers within its definition.
  prefs: []
  type: TYPE_NORMAL
- en: kube-proxy helps in maintaining the list of IP addresses of Pod and Service
    information on local nodes, as well as updating the local firewall and routing
    rules. To do a quick recap of what we have discussed so far, we started off with
    the Kubernetes architecture and then moved on to primitives, followed by the architecture
    of AKS. Since you are clear on the concepts, let's go ahead and create an AKS
    cluster in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying an AKS cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AKS can be provisioned using the Azure portal, the Azure **CLI** (**command-line
    interface**), Azure PowerShell cmdlets, ARM templates, **SDKs** (**software development
    kits**) for supported languages, and even Azure ARM REST APIs.
  prefs: []
  type: TYPE_NORMAL
- en: The Azure portal is the simplest way of creating an AKS instance; however, to
    enable DevOps, it is better to create an AKS instance using ARM templates, the
    CLI, or PowerShell.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an AKS cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s create a resource group to deploy our AKS cluster. From the Azure CLI,
    use the `az group create` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `-n` denotes the name of the resource group and `-l` denotes the location.
    If the request was successful, you will see a similar response to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The output of the az group create command showing that the resource group
    has been created.](img/B15432_14_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.9: Resource group creation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now that we have the resource group ready, we will go ahead and create the
    AKS cluster using the `az aks create` command. The following command will create
    a cluster named `AzureForArchitects-AKS` in the `AzureForArchitects` resource
    group with a node count of `2`. The `--generate-ssh-keys` parameter will allow
    the creation of **RSA** (**Rivest–Shamir–Adleman**) key pairs, a public-key cryptosystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If the command succeeded, you will be able to see a similar output to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The output of the az aks create command showing that the cluster has been
    created.](img/B15432_14_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.10: Creating the cluster'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Going through the cluster, you will see a line item that says `"nodeResourceGroup":
    "MC_AzureForArchitects_AzureForArchitects-AKS_southeastasia"`. When creating an
    AKS cluster, a second resource is automatically created to store the node resources.'
  prefs: []
  type: TYPE_NORMAL
- en: Our cluster is provisioned. Now we need to connect to the cluster and interact
    with it. To control the Kubernetes cluster manager, we will be using kubectl.
    In the next section, we will take a quick look at kubectl.
  prefs: []
  type: TYPE_NORMAL
- en: Kubectl
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubectl is the main component through which developers and infrastructure consultants
    can interact with AKS. Kubectl helps in creating a REST request containing the
    HTTP header and body, and submitting it to the API server. The header contains
    the authentication details, such as a token or username/password combination.
    The body contains the actual payload in JSON format.
  prefs: []
  type: TYPE_NORMAL
- en: The kubectl command provides rich log details when used along with the verbose
    switch. The switch takes an integer input that can range from 0 to 9, which can
    be viewed from the details logs.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to the cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To connect to the cluster locally, we need to install kubectl. Azure Cloud Shell
    has kubectl already installed. If you want to connect locally, use `az aks install-cli`
    to install kubectl.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to configure kubectl to connect to our Kubernetes cluster, we need
    to download the credentials and configure the CLI with them. This can be done
    using the `az aks get-credentials` command. Use the command as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to verify whether we''re connected to the cluster. As mentioned
    earlier, we''ll be using kubectl to communicate with the cluster, and `kubectl
    get nodes` will show a list of nodes in the cluster. During creation, we set the
    node count to `2`, so the output should have two nodes. Also, we need to make
    sure that the status of the node is `Ready`. The output should be something like
    *Figure 14.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The output of the kubectl get nodes command showing the Name, Status, Roles,
    Age, and Version of the nodes.](img/B15432_14_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.11: Getting the list of nodes'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Since our node is in the `Ready` state, let''s go ahead and create a Pod. There
    are two ways in which you can create resources in Kubernetes. They are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl run` and `kubectl expose` commands to create the resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl apply` command to create the resources, and the resources declared
    in the file will be created.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s take the imperative approach first, to create a Pod with the name `webserver`,
    running an NGINX container with port `80` exposed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon successful completion of the command, the CLI will let you know the status:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a webserver with the kubectl run webserver command.](img/B15432_14_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.12: Creating a Pod'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that we have tried the imperative method, let's follow the declarative method.
    You can use the structure of the YAML file we discussed in the *Pod* subsection
    of the *Kubernetes primitives* section and modify it as per your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using the NGINX image, and the Pod will be named `webserver-2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use any text editor and create the file. The final file will look similar
    to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `kubectl apply` command, we will pass the filename to the `-f` parameter,
    as shown in *Figure 14.13*, and you can see that the Pod has been created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Executing the command kubectl apply –f nginx.yaml to create a Pod with the
    declarative method.](img/B15432_14_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.13: Creating a Pod using the declarative method.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Since we have created the Pods, we can use the `kubectl get pods` command to
    list all the Pods. Kubernetes uses the concept of namespaces for the logical isolation
    of resources. By default, all commands are pointing to the `default` namespace.
    If you want to perform an action on a specific namespace, you can pass the namespace
    name via the `-n` parameter. In *Figure 14.14*, you can see that `kubectl get
    pods` returns the Pods we created in the previous example, which reside in the
    default namespace. Also, when we use `--all-namespaces`, the output returns pods
    in all namespaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The output of the kubectl get pods command displaying the details of all
    the Pods in the container.](img/B15432_14_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.14: Listing all Pods'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now we will create a simple Deployment that runs NGINX and with a load balancer
    that exposes it to the internet. The `YAML` file will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We'll be using the `kubectl apply` command and passing the YAML file to the
    `-f` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon success, all three Services will be created, and if you execute the `kubectl
    get deployment nginx-server` command, you will see six replicas running, as shown
    in *Figure 14.15*, to make the Service highly available:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Executing the kubectl get deployment nginx-server command to check the details
    of all the deployments.](img/B15432_14_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.15: Checking the deployment'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Since our Deployment is provisioned, we need to check what the public IP of
    the load balancer that we created is. We can use the `kubectl get service nginx-lb
    --watch` command. When the load balancer is initializing, `EXTERNAL-IP` will show
    as `<pending>`, the `--wait` parameter will let the command run in the foreground,
    and when the public IP is allocated, we will be able to see a new line, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Running the kubectl get service nginx-lb --watch command to get the Public
    IP of the load balancer.](img/B15432_14_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.16: Finding public IP of the load balancer'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now that we have the public IP, we can go to the browser and should see the
    NGINX landing page, as shown in *Figure 14.17*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The browser pointing at the URL 52.187.70.177 and displaying the NGINX landing
    page.](img/B15432_14_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.17: NGINX landing page'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Similarly, you can use the `YAML` files we discussed in the *Kubernetes primitives*
    section to create different types of resources.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of commands, such as `logs`, `describe`, `exec`, and `delete`,
    that administrators need to use with the `kubectl` command. The objective of this
    section was to enable you to create an AKS cluster, connect to the cluster, and
    deploy a simple web application.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will be discussing AKS networking.
  prefs: []
  type: TYPE_NORMAL
- en: AKS networking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Networking forms a core component within a Kubernetes cluster. The master components
    should be able to reach the minion nodes and the Pods running on top of them,
    while the worker nodes should be able to communicate among themselves as well
    as with the master components.
  prefs: []
  type: TYPE_NORMAL
- en: It might come as a surprise that core Kubernetes does not manage the networking
    stack at all. It is the job of the container runtime on the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes has prescribed three important tenets that any container runtime
    should adhere to. These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Pods should be able to communicate with other Pods without any transformation
    in their source or destination addresses, something that is performed using **network
    address translation** (**NAT**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agents such as kubelets should be able to communicate with Pods directly on
    the nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pods that are directly hosted on the host network still should be able to communicate
    with all Pods in the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every Pod gets a unique IP address within the Kubernetes cluster, along with
    a complete network stack, similar to virtual machines. They all are connected
    to the local bridge network created by the **Container Networking Interface**
    (**CNI**) component. The CNI component also creates the networking stack of the
    Pod. The bridge network then talks to the host network and becomes the conduit
    for the flow of traffic from Pods to network and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: CNI is a standard managed and maintained by the **Cloud Native Computing Foundation**
    (**CNCF**), and there are many providers that provide their own implementation
    of the interface. Docker is one of these providers. There are others, such as
    rkt (read as rocket), weave, calico, and many more. Each has its own capabilities
    and independently decides the network capabilities, while ensuring that the main
    tenets of Kubernetes networking are followed completely.
  prefs: []
  type: TYPE_NORMAL
- en: 'AKS provides two distinct networking models:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubenet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure CNI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubenet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubenet is the default networking framework in AKS. Under Kubenet, each node
    gets an IP address from the subnet of the virtual network they are connected with.
    The Pods do not get IP addresses from the subnet. Instead, a separate addressing
    scheme is used to provide IP addresses to Pods and Kubernetes Services. While
    creating an AKS instance, it is important to set the IP address range for Pods
    and Services. Since Pods are not on the same network as that of nodes, requests
    from Pods and to Pods are always NATed/routed to replace the source Pod IP with
    the node IP address and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'In user-defined routing, Azure can support up to 400 routes, and you also cannot
    have a cluster larger than 400 nodes. *Figure 14.18* shows how the AKS node receives
    an IP address from the virtual network, but not the Pods created in the node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![An architectural diagram illustrating the networking in AKS.](img/B15432_14_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.18: Networking in AKS'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By default, this Kubenet is configured with 110 Pods per node. This means there
    can be a maximum of 110 * 400 Pods in a Kubernetes cluster by default. The maximum
    number of Pods per node is 250.
  prefs: []
  type: TYPE_NORMAL
- en: This scheme should be used when IP address availability and having user-defined
    routing are not a constraint.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Azure CLI, you can execute the following command to create an AKS instance
    using this networking stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Notice how all the IP addresses are explicitly provided for Service resources,
    Pods, nodes, and Docker bridges. These are non-overlapping IP address ranges.
    Also notice that Kubenet is used as a network plugin.
  prefs: []
  type: TYPE_NORMAL
- en: Azure CNI (advanced networking)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With Azure CNI, each node and Pod gets an IP address assigned from the network
    subnet directly. This means there can be as many Pods as there are unique IP addresses
    available on a subnet. This makes IP address range planning much more important
    under this networking strategy.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that Windows hosting is only possible using the Azure
    CNI networking stack. Moreover, some of AKS components, such as virtual nodes
    and virtual kubelets, are also dependent on the Azure CNI stack. There is a need
    to reserve IP addresses in advance, depending on the number of Pods that will
    be created. There should always be extra IP addresses available on the subnet,
    to avoid exhaustion of IP addresses or to avoid the need to rebuild the cluster
    for a larger subnet due to application demand.
  prefs: []
  type: TYPE_NORMAL
- en: By default, this networking stack is configured for 30 Pods per node and it
    can be configured with 250 Pods as the maximum number of Pods per node.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command to execute to create an AKS instance using this networking stack
    is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Notice how all the IP addresses are explicitly provided for Service resources,
    Pods, nodes, and Docker bridges. These are non-overlapping IP address ranges.
    Also, notice that Azure is used as a network plugin.
  prefs: []
  type: TYPE_NORMAL
- en: So far, you have learned how to deploy a solution and manage the networking
    of an AKS cluster. Security is another important factor that needs to be addressed.
    In the next section, we will be focusing on access and identity options for AKS.
  prefs: []
  type: TYPE_NORMAL
- en: Access and identity for AKS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes clusters can be secured in multiple ways.
  prefs: []
  type: TYPE_NORMAL
- en: The service account is one of the primary user types in Kubernetes. The Kubernetes
    API manages the service account. Authorized Pods can communicate with the API
    server using the credentials of service accounts, which are stored as Kubernetes
    Secrets. Kubernetes does not have any data store or identity provider of its own.
    It delegates the responsibility of authentication to external software. It provides
    an authentication plugin that checks for the given credentials and maps them to
    available groups. If the authentication is successful, the request passes to another
    set of authorization plugins to check the permission levels of the user on the
    cluster, as well as the namespace-scoped resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Azure, the best security integration would be to use Azure AD. Using Azure
    AD, you can also bring your on-premises identities to AKS to provide centralized
    management of accounts and security. The basic workflow of Azure AD integration
    is shown in *Figure 14.19*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A block diagram showing the basic workflow of Azure AD integration.](img/B15432_14_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.19: Basic workflow of Azure AD integration'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Users or groups can be granted access to resources within a namespace or across
    a cluster. In the previous section, we used the `az aks get-credential` command
    to get the credentials and the kubectl configuration context. When the user tries
    to interact with kubectl, they are prompted to sign in using their Azure AD credentials.
    Azure AD validates the credentials and a token is issued for the user. Based on
    the access level they have, they can access the resources in the cluster or the
    namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you can leverage Azure **Role-Based Access Control** (**RBAC**)
    to limit access to the resources in the resource group.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will be discussing virtual kubelet, which is one of
    the quickest ways to scale out a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual kubelet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Virtual kubelet is currently in preview and is managed by the CNCF organization.
    It is quite an innovative approach that AKS uses for scalability purposes. Virtual
    kubelet is deployed on the Kubernetes cluster as a Pod. The container running
    within the Pod uses the Kubernetes SDK to create a new node resource and represents
    itself to the entire cluster as a node. The cluster components, including the
    API server, scheduler, and controllers, think of it and treat it as a node and
    schedule Pods on it.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, when a Pod is scheduled on this node that is masquerading as a node,
    it communicates to its backend components, known as providers, to create, delete,
    and update the Pods. One of the main providers on Azure is Azure Container Instances.
    Azure Batch can also be used as a provider. This means the containers are actually
    created on Container Instances or Azure Batch rather than on the cluster itself;
    however, they are managed by the cluster. The architecture of virtual kubelet
    is shown in *Figure 14.20*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![An architectural diagram of Virtual Kubelet showing how it is integrated
    with the Kubernetes API, nodes, and kubelets.](img/B15432_14_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.20: Virtual kubelet architecture'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that virtual kubelet is represented as a node within the cluster and
    can help in hosting and managing Pods, just like a normal kubelet would. However,
    virtual kubelet has one limitation; this is what we are going to discuss in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the limitations of virtual kubelet is that the Pods deployed on virtual
    kubelet providers are isolated and do not communicate with other Pods in the cluster.
    If there is a need for the Pods on these providers to talk to other Pods and nodes
    in the cluster and vice versa, then virtual nodes should be created. Virtual nodes
    are created on a different subnet on the same virtual network that is hosting
    Kubernetes cluster nodes, which can enable communication between Pods. Only the
    Linux operating system is supported, at the time of writing, for working with
    virtual nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual nodes give a perception of a node; however, the node does not exist.
    Anything scheduled on such a node actually gets created in Azure Container Instances.
    Virtual nodes are based on virtual kubelet but have the extra functionality of
    seamless to-and-fro communication between the cluster and Azure Container Instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'While deploying Pods on virtual nodes, the Pod definition should contain an
    appropriate node selector to refer to virtual nodes, and also tolerations, as
    shown in next code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, the node selector is using the `type` property to refer to virtual kubelet
    and the `tolerations` property to inform Kubernetes that nodes with taints, `virtual-kubelet.io/provider`,should
    allow the Deployment of these Pods on them.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes is the most widely used container orchestrator and works with different
    container and network runtimes. In this chapter, you learned about the basics
    of Kubernetes, its architecture, and some of the important infrastructure components,
    such as etcd, the API server, controller managers, and the scheduler, along with
    their purpose. Plus, we looked at important resources that can be deployed to
    manage applications, such as Pods, replication controllers, ReplicaSets, Deployments,
    and Services.
  prefs: []
  type: TYPE_NORMAL
- en: AKS provides a couple of different networking stacks—Azure CNI and Kubenet.
    They provide different strategies for assigning IP addresses to Pods. While Azure
    CNI provides IP addresses to Pods from the underlying subnet, Kubenet uses virtual
    IP addresses only.
  prefs: []
  type: TYPE_NORMAL
- en: We also covered some of the features provided exclusively by Azure, such as
    virtual nodes, and concepts around virtual kubelet. In the next chapter, we will
    learn about the provisioning and configuring resources with ARM templates.
  prefs: []
  type: TYPE_NORMAL
