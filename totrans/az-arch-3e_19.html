<html><head></head><body>
		<div>
			<div id="_idContainer445" class="Content">
			</div>
		</div>
		<div id="_idContainer446" class="Content">
			<h1 id="_idParaDest-464">19. <a id="_idTextAnchor489"/>Architecting intelligent solutions</h1>
		</div>
		<div id="_idContainer456" class="Content">
			<p>Cloud technology has changed a lot of things, including the creation of intelligent applications in an agile, scalable, and pay-as-you-go way. Applications prior to the rise of cloud technology generally did not incorporate intelligence within themselves, primarily because:</p>
			<ul>
				<li>It was time-consuming and error-prone.</li>
				<li>It was difficult to write, test, and experiment with algorithms on an ongoing basis.</li>
				<li>There was a lack of sufficient data.</li>
				<li>It was immensely costly.</li>
			</ul>
			<p>Over the last decade, two things have changed that have led to the creation of significantly more intelligent applications than in the past. These two things are the cost-effective, on-demand unlimited scalability of the cloud along with the availability of data in terms of volume, variety, and velocity.</p>
			<p>In this chapter, we will look at architectures that can help build intelligent applications with Azure. Some of the topics covered in this chapter are:</p>
			<ul>
				<li>The evolution of AI</li>
				<li>Azure AI processes</li>
				<li>Azure Cognitive Services</li>
				<li>Building an optical character recognition service</li>
				<li>Building a visual features service using the Cognitive Search .NET SDK</li>
			</ul>
			<h2 id="_idParaDest-465"><a id="_idTextAnchor490"/>The evolution of AI </h2>
			<p>AI is not a new field of knowledge. In fact, the technology is a result of decades of innovation and research. However, its implementation in previous decades was a challenge for the following reasons:</p>
			<ol>
				<li><strong class="bold">Cost</strong>: AI experiments were costly in nature and there was no cloud technology. All the infrastructure was either purchased or hired from a third party. Experiments were also time-consuming to set up and immense skills were needed to get started. A large amount of storage and compute power was also required, which was generally missing in the community at large and held in the hands of just a few.</li>
				<li><strong class="bold">Lack of data</strong>: There were hardly any smart handheld devices and sensors available generating data. Data was limited in nature and had to be procured, which again made AI applications costly. Data was also less reliable and there was a general lack of confidence in the data itself.</li>
				<li><strong class="bold">Difficulty</strong>: AI algorithms were not documented enough and were primarily in the realms of mathematicians and statisticians. They were difficult to create and utilize within applications. Just imagine the creation of an <strong class="bold">optical character recognition</strong> (<strong class="bold">OCR</strong>) system 15 years ago. There were hardly any libraries, data, processing power, or the necessary skills to develop applications using OCR.</li>
			</ol>
			<p>Although the influx of data increased with time, there was still a lack of tools for making sense of the data in a way that added business value. In addition, good AI models are based on sufficiently accurate data and trained with algorithms to be capable of resolving real-life problems. Both cloud technology and the large number of sensors and handheld devices have redefined this landscape.</p>
			<p>With cloud technology, it is possible to provision on-demand storage and compute resources for AI-based applications. Cloud infrastructure provides lots of resources for data migration, storage, processing, and computation, as well as generating insights and eventually providing reports and dashboards. It does all this at a minimal cost in a faster way since there is nothing physical involved. Let's dive into understanding what goes on behind building an AI-based application.</p>
			<h2 id="_idParaDest-466"><a id="_idTextAnchor491"/>Azure AI processes</h2>
			<p>Every AI-based project is required to go through a certain set of steps before being operational. Let's explore these seven phases:</p>
			<h3 id="_idParaDest-467"><a id="_idTextAnchor492"/>Data ingestion</h3>
			<p>In this phase, data is captured from various sources and stored such that it can be consumed in the next phase. The data is cleaned before being stored and any deviations from the norm are disregarded. This is part of the preparation of data. The data could have different velocity, variety, and volume. It can be structured similarly to relational databases, semi-structured like JSON documents, or unstructured like images, Word documents, and so on.</p>
			<h3 id="_idParaDest-468"><a id="_idTextAnchor493"/>Data transformation</h3>
			<p>The data ingested is transformed into another format as it might not be consumable in its current format. The data transformation typically includes the cleaning and filtering of data, removing bias from the data, augmenting data by joining it with other datasets, creating additional data from existing data, and more. This is also part of the preparation of the data.</p>
			<h3 id="_idParaDest-469"><a id="_idTextAnchor494"/>Analysis</h3>
			<p>The data from the last phase is reused for analysis. The analysis phase contains activities related to finding patterns within data, conducting exploratory data analysis, and generating further insights from it. These insights are then stored along with existing data for consumption in the next phase. This is part of the model packaging process.</p>
			<h3 id="_idParaDest-470"><a id="_idTextAnchor495"/>Data modeling</h3>
			<p>Once the data is augmented and cleaned, appropriate and necessary data is made available to the AI algorithms to generate a model that is conducive to achieving the overall aim. It is an iterative process known as experimentation by using various combinations of data (feature engineering) to ensure that the data model is robust. This is also part of the model packaging process.</p>
			<p>The data is fed into learning algorithms to identify patterns. This process is known as training the model. Later, test data is used on the model to check its effectiveness and efficiency.</p>
			<h3 id="_idParaDest-471"><a id="_idTextAnchor496"/>Validating the model</h3>
			<p>Once the model is created, a set of test data is used to find its effectiveness. If the analysis obtained from the test data is reflective of reality, then the model is sound and usable. Testing is an important aspect of the AI process.</p>
			<h3 id="_idParaDest-472"><a id="_idTextAnchor497"/>Deployment</h3>
			<p>The model is deployed to production so that real-time data can be fed into it to get the predicted output. This output can then be used within applications.</p>
			<h3 id="_idParaDest-473"><a id="_idTextAnchor498"/>Monitoring</h3>
			<p>The model deployed to production is monitored on an ongoing basis for the future analysis of all incoming data and to retrain and improve the effectiveness models.</p>
			<p>The AI stages and processes, by nature, are time-consuming and iterative. Thus, applications based on them have an inherent risk of being long-running, experimental, and resource-intensive, along with getting delayed with cost overruns and having low chances of success.</p>
			<p>Keeping these things in mind, there should be out-of-the-box AI-based solutions that developers can use in their applications to make them intelligent. These AI solutions should be easily consumable from applications and should have the following features:</p>
			<ul>
				<li><strong class="bold">Cross-platform</strong>: Developers using any platform should be able to consume these services. They should be deployed and consumed on Linux, Windows, or Mac without any compatibility problems.</li>
				<li><strong class="bold">Cross-language</strong>: Developers should be able to use any language to consume these solutions. Not only will the developers encounter a shorter learning curve but they also won't need to change their preferred choice of language to consume these solutions.</li>
			</ul>
			<p>These solutions should be deployed as services using industry standards and protocols. Generally, these services are available as HTTP REST endpoints that can be invoked using any programming language and platform.</p>
			<p>There are many such types of service that can be modeled and deployed for developer consumption. Some examples include:</p>
			<ul>
				<li><strong class="bold">Language translation</strong>: In such services, the user provides text in one language and gets corresponding text in a different language as output.</li>
				<li><strong class="bold">Character recognition</strong>: These services accept images and return the text present in them.</li>
				<li><strong class="bold">Speech-to-text conversion</strong>: These services can convert input speech to text.</li>
			</ul>
			<p>Now that we have gone through the details of building an AI/ML-based project, let's dive into the applications of various cognitive services offered by Azure.</p>
			<h2 id="_idParaDest-474"><a id="_idTextAnchor499"/>Azure Cognitive Services</h2>
			<p>Azure provides an umbrella service known as Azure Cognitive Services. Azure Cognitive Services is a set of services that developers can consume within their applications to turn them into intelligent applications.</p>
			<div>
				<div id="_idContainer447" class="IMG---Figure">
					<img src="image/Table_19.1.jpg" alt="Set of Azure Cognitive Services"/>
				</div>
			</div>
			<h6>Table 19.1: Azure Cognitive Services</h6>
			<p>The services have been divided into five main categories depending on their nature. These five categories are as follows:</p>
			<h3 id="_idParaDest-475"><a id="_idTextAnchor500"/>Vision</h3>
			<p>This API provides algorithms for image classification and helps in image processing by providing meaningful information. Computer vision can provide a variety of information from images on different objects, people, characters, emotions, and more.</p>
			<h3 id="_idParaDest-476"><a id="_idTextAnchor501"/>Search</h3>
			<p>These APIs help in search-related applications. They help with search based on text, images, video, and providing custom search options.</p>
			<h3 id="_idParaDest-477"><a id="_idTextAnchor502"/>Language</h3>
			<p>These APIs are based on natural language processing and help extract information about the intent of user-submitted text along with entity detection. They also help in text analytics and translation to different languages.</p>
			<h3 id="_idParaDest-478"><a id="_idTextAnchor503"/>Speech</h3>
			<p>These APIs help in translating speech to text, text to speech, and in speech translation. They can be used to ingest audio files and take actions based on the content on behalf of users. Cortana is an example that uses similar services to take actions for users based on speech.</p>
			<h3 id="_idParaDest-479"><a id="_idTextAnchor504"/>Decision</h3>
			<p>These APIs help in anomaly detection and content moderation. They can check for content within images, videos, and text and find out patterns that should be highlighted. An example of such an application is displaying a warning about adult content.</p>
			<p>Now that you have an understanding of the core of Cognitive Services, let's discuss how they work in detail.</p>
			<h2 id="_idParaDest-480"><a id="_idTextAnchor505"/>Understanding Cognitive Services</h2>
			<p>Azure Cognitive Services consists of HTTP endpoints that accept requests and send responses back to the caller. Almost all requests are HTTP POST requests and consist of both a header and a body.</p>
			<p>The provisioning of Cognitive Services generates two important artifacts that help a caller invoke an endpoint successfully. It generates an endpoint URL and a unique key. </p>
			<p>The format of the URL is <strong class="inline">https://{azure location}.api.cognitive.microsoft.com/{cognitive type}/{version}/{sub type of service}?{query parameters}</strong>. An example URL is:</p>
			<p><strong class="inline">https://eastus.api.cognitive.microsoft.com/vision/v2.0/ocr?language=en&amp;detectOrientation=true</strong></p>
			<p>Cognitive Service is provisioned in the East US Azure region. The type of service is computer vision using version 2 and the subtype is OCR. There are generally a few subtypes for each top-level category. Lastly, there are a few query string parameters, such as <strong class="inline">language</strong> and <strong class="inline">detectOrientation</strong>. These query parameters are different for each service category and subcategory.</p>
			<p>Either the header or the query parameters should provide the key value for the endpoint invocation to be successful.</p>
			<p>The key value should be assigned to the <strong class="inline">Ocp-Apim-Subscription-Key</strong> header key with the request.</p>
			<p>The content of the request body can be a simple string, a binary, or a combination of both. Depending on the value, the appropriate content-type header should be set in the request.</p>
			<p>The possible header values are:</p>
			<ul>
				<li><strong class="inline">Application/octet-stream</strong></li>
				<li><strong class="inline">multipart/form-data</strong></li>
				<li><strong class="inline">application/json</strong></li>
			</ul>
			<p>Use <strong class="inline">octet-stream</strong> when sending binary data and <strong class="inline">json</strong> for sending string values. <strong class="inline">form-data</strong> can be used for sending multiple combination values of binary and text.</p>
			<p>The key is a unique string used to validate whether the caller has been given permission to invoke the URL. This key must be protected such that others who should not be able to invoke the endpoints do not get access to it. Later in the chapter, you will see ways to safeguard these keys.</p>
			<h3 id="_idParaDest-481"><a id="_idTextAnchor506"/>Consuming Cognitive Services</h3>
			<p>There are two ways to consume Cognitive Services:</p>
			<ul>
				<li><strong class="bold">Using an HTTP endpoint directly</strong>: In this case, the endpoint is invoked directly by crafting both the header and body with appropriate values. The return value is then parsed and data is extracted out of it. All the AI services in Cognitive Services are REST APIs. They accept HTTP requests in JSON, as well as other formats, and replies in JSON format.</li>
				<li><strong class="bold">Using an SDK</strong>: Azure provides multiple <strong class="bold">software development kits</strong> (<strong class="bold">SDKs</strong>). There are SDKs available for the .NET, Python, Node.js, Java, and Go languages.</li>
			</ul>
			<p>In the following section, we will look into the utilization of one of the Cognitive Services using both ways. Let's explore this by building some AI services using HTTP endpoints.</p>
			<h2 id="_idParaDest-482"><a id="_idTextAnchor507"/>Building an OCR service</h2>
			<p>In this section, we will be using some of the AI services using C# as well as PowerShell to show their usage using the HTTP endpoint directly. The next section will concentrate on doing the same using a .NET SDK.</p>
			<p>Before getting into building a project using Cognitive Services, the first step is to provision the API itself.</p>
			<p>Optical character recognition is available as a Vision API and can be provisioned using the Azure portal, as shown next. Create a vision API by navigating to <strong class="bold">Cognitive Services &gt; Compute Vision &gt; Create</strong>, as shown in <em class="italics">Figure 19.1</em>:</p>
			<div>
				<div id="_idContainer448" class="IMG---Figure">
					<img src="image/Figure_19.2.jpg" alt="Creating a Vision API"/>
				</div>
			</div>
			<h6>Figure 19.1: Create a Vision API</h6>
			<p>Once the API is provisioned, the overview page provides all the details for consuming the API. It provides the base URL and the key information. Make a note of the key as it will be used later: </p>
			<div>
				<div id="_idContainer449" class="IMG---Figure">
					<img src="image/Figure_19.3.jpg" alt="Details of the API in the Overview section"/>
				</div>
			</div>
			<h6>Figure 19.2: Overview page</h6>
			<p>It also provides an API console to quickly test the API. Clicking on it opens a new window that has all the endpoints related to this service available. Clicking on <strong class="bold">OCR</strong> will provide a form that can be filled in with appropriate data and execute the service endpoints. It also provides a complete response. This is shown in <em class="italics">Figure 19.3</em>. The URL is available as a request URL, and the request is a typical HTTP request with a <strong class="inline">POST</strong> method. The URL points to the endpoint in the East US Azure region. It is also related to the Vision group of APIs, version 2, and the OCR endpoint. </p>
			<p>The subscription key is passed in the header with the name <strong class="inline">ocp-apim-subscription-key</strong>. The header also contains the content-type key with <strong class="inline">app<a id="_idTextAnchor508"/>lication/json</strong> as a value. This is because the body of the request contains a JSON string. The body is in the form of JSON with the URL of the image from which text should be extracted:</p>
			<div>
				<div id="_idContainer450" class="IMG---Figure">
					<img src="image/Figure_19.4.jpg" alt="Details of the HTTP request"/>
				</div>
			</div>
			<h6>Figure 19.3: Request URL</h6>
			<p>The request can be sent to the endpoint by clicking on the <strong class="bold">Send</strong> button. It will result in an HTTP response 200 OK, as shown next, if everything goes right. If there is an error in the request values, the response will be an error HTTP code:</p>
			<div>
				<div id="_idContainer451" class="IMG---Figure">
					<img src="image/Figure_19.5.jpg" alt="HTTP response 200 OK"/>
				</div>
			</div>
			<h6>Figure 19.4: HTTP response 200 OK</h6>
			<p>The response consists of details related to billing usage, an internal request ID generated by the endpoint, the content length, the response content type (being JSON), and the data and time of the response. The content of the response consists of a JSON payload with the coordinates of the text and the actual text itself.</p>
			<h3 id="_idParaDest-483"><a id="_idTextAnchor509"/>Using PowerShell</h3>
			<p>The same request can be created using PowerShell. The following PowerShell code can be executed using the PowerShell ISE.</p>
			<p>The code uses the <strong class="inline">Invoke-WebRequest</strong> cmdlet to invoke the Cognitive Services endpoint by passing the URL to the <strong class="inline">Uri</strong> parameter using the <strong class="inline">POST</strong> method, and adds both the appropriate headers as discussed in the last section, and finally, the body consisting of data in JSON format. The data is converted into JSON using the <strong class="inline">ConvertTo-Json</strong> cmdlet:</p>
			<p class="snippet">$ret = Invoke-WebRequest -Uri "https://eastus.api.cognitive.microsoft.com/vision/v2.0/ocr?language=en&amp;detectOrientation=true"  -Method Post -Headers @{"Ocp-Apim-Subscription-Key"="ff0cd61f27d8452bbadad36942570c48"; "Content-type"="application/json"} -Body $(ConvertTo-Json -InputObject  @{"url"="https://ichef.bbci.co.uk/news/320/cpsprodpb/F944/production/_109321836_oomzonlz.jpg"}) </p>
			<p class="snippet">$val = Convertfrom-Json $ret.content</p>
			<p class="snippet">foreach ($region in $val.regions) {</p>
			<p class="snippet">    foreach($line in $region.lines) {</p>
			<p class="snippet">        foreach($word in $line.words) {</p>
			<p class="snippet">            $word.text</p>
			<p class="snippet">        }</p>
			<p class="snippet">    }</p>
			<p class="snippet">} </p>
			<p>The response from the cmdlet is saved in a variable that also consists of data in JSON format. The data is converted into a PowerShell object using the <strong class="inline">Convertfrom-Json</strong> cmdlet and looped over to find the words in the text.</p>
			<h3 id="_idParaDest-484"><a id="_idTextAnchor510"/>Using C#</h3>
			<p>In this section, we will build a service that should accept requests from users, extract the URL of the image, construct the HTTP request, and send it to the Cognitive Services endpoint. The Cognitive Services endpoint returns a JSON response. The appropriate text content is extracted from the response and returned to the user.</p>
			<p><strong class="bold">Architecture and design</strong></p>
			<p>An intelligent application is an ASP.NET Core MVC application. An MVC application is built by a developer on a developer machine, goes through the continuous integration and delivery pipeline, generates a Docker image, and uploads the Docker image to Azure Container Registry. Here, the major components of the application are explained, along with their usage:</p>
			<div>
				<div id="_idContainer452" class="IMG---Figure">
					<img src="image/19.6.jpg" alt="ASP.NET Core MVC application architecture"/>
				</div>
			</div>
			<h6>Figure 19.5: Workflow of an intelligent application</h6>
			<p><strong class="bold">Docker</strong></p>
			<p>Docker is one of the major players within container technologies and is available cross-platform, including Linux, Windows, and Mac. Developing applications and services with containerization in mind provides the flexibility to deploy them across clouds and locations, as well as on-premises. It also removes any dependencies on the host platform, which again allows less reliance on platform as a service. Docker helps with the creation of custom images, and containers can be created out of these images. The images contain all the dependencies, binaries, and frameworks needed to make the application or service work, and they are completely self-reliant. This makes them a great deployment target for services such as microservices.</p>
			<p><strong class="bold">Azure Container Registry</strong></p>
			<p>Azure Container Registry is a registry that's similar to Docker Hub for the storage of container images in a repository. It is possible to create multiple repositories and upload multiple images in them. An image has a name and a version number, together forming a fully qualified name used to refer to them in a Kubernetes Pod definition. These images can be accessed and downloaded by any Kubernetes ecosystem. A prerequisite of this is that appropriate secrets for pulling the image should already be created beforehand. It need not be on the same network as Kubernetes nodes and, in fact, there is no need for a network to create and use Azure Container Registry.</p>
			<p><strong class="bold">Azure Kubernetes Service</strong></p>
			<p>The intelligent application that accepts the URL of an image to retrieve the text in it can be hosted on vanilla virtual machines or even within Azure App Service. However, deploying in Azure Kubernetes Service offers lots of advantages, which was covered in <em class="italics">Chapter 8, Architecting Azure Kubernetes Solutions</em>. For now, it is important to know that these applications are self-healing in nature and a minimum number of instances is automatically maintained by the Kubernetes master along with providing the flexibility to update them in a multitude of ways, including blue-green deployments and canary updates.</p>
			<p><strong class="bold">Pods, replica sets, and deployments</strong></p>
			<p>The developer also creates a Kubernetes deployment-related YAML file that references the images within the Pod specification and also provides a specification for the replica set. It provides its own specification related to the update strategy.</p>
			<p><strong class="bold">Runtime design</strong></p>
			<p>The architecture and design remain the same as in the previous section; however, when the application or service is already live and up and running, it has already downloaded the images from Azure Container Registry and created Pods running containers in them. When a user provides an image URL for decoding the text it contains, the application in the Pod invokes the Azure Cognitive Services Computer Vision API and passes the URL to it and waits for a response from the service:</p>
			<div>
				<div id="_idContainer453" class="IMG---Figure">
					<img src="image/19.7.jpg" alt="ASP.NET Core MVC application architecture using Azure Cognitive Services"/>
				</div>
			</div>
			<h6>Figure 19.6 Workflow of an intelligent application</h6>
			<p>Once it receives the JSON response from the services, it can retrieve the information and return it to the user.</p>
			<h3 id="_idParaDest-485"><a id="_idTextAnchor511"/>The development process</h3>
			<p>The development environment can be Windows or Linux. It will work with both Windows 10 and the Windows 2016/19 server. When using Windows, it is useful to deploy Docker for Windows so that it will create both a Linux and a Windows Docker environment. </p>
			<p>When creating an ASP.NET Core web application project using Visual Studio 2019, the <strong class="bold">Docker support</strong> option should be selected with either <strong class="bold">Windows</strong> or <strong class="bold">Linux</strong> as values. Depending on the chosen value, appropriate content will be generated in <strong class="inline">Dockerfile</strong>. The main difference in <strong class="inline">Dockerfile</strong> is the base image names. It uses different images for Linux compared to Windows.</p>
			<p>When installing Docker for Windows, it also installs a Linux virtual machine, and so it is important to turn on the Hyper-V hypervisor.</p>
			<p>In this example, instead of sending the data as a JSON string, the image is downloaded, and binary data is sent to the Cognitive Services endpoint.</p>
			<p>It has a function that accepts a string input for URL values. It then invokes Cognitive Services with appropriate header values and a body containing the URL. The header values should contain the key provided by Cognitive Services while provisioning the service. The value in the body can contain vanilla string values in the form of JSON or it can contain binary image data itself. The content-type header property should be set accordingly.</p>
			<p>The code declares the URL and the key related to the Cognitive Services. This is shown for demonstration purposes only. The URL and key should be placed in configuration files.</p>
			<p>Using the <strong class="inline">HttpClient</strong> object, the image corresponding to the URL supplied by the user is downloaded and stored within the <strong class="inline">responseMessage</strong> variable. Another <strong class="inline">HttpClient</strong> object is instantiated and its headers are filled with <strong class="inline">Ocp-Apim-Subscription-Key</strong> and <strong class="inline">content-type keys</strong>. The value of the content-type header is <strong class="inline">application/octet-stream</strong> since binary data is being passed to the endpoint.</p>
			<p>A post request is made after extracting the content from the <strong class="inline">responseMessage</strong> variable and passing it as the body of a request to the cognitive service endpoint. </p>
			<p>The code for the controller action is shown next: </p>
			<p class="snippet">        [HttpPost]</p>
			<p class="snippet">        public async Task&lt;string&gt; Post([FromBody] string value)</p>
			<p class="snippet">        {</p>
			<p class="snippet">            string myurl = " https://eastus.api.cognitive.microsoft.com/vision/v2.0/ocr?language=en&amp;detectOrientation=true</p>
			<p class="snippet">            string token = "…………………………";</p>
			<p class="snippet">            using (HttpClient httpClient = new HttpClient())</p>
			<p class="snippet">            {</p>
			<p class="snippet">                var responseMessage = await httpClient.GetAsync(value);</p>
			<p class="snippet">                 using (var httpClient1 = new HttpClient())</p>
			<p class="snippet">                {</p>
			<p class="snippet">                     httpClient1.BaseAddress = new Uri(myurl);</p>
			<p class="snippet">                   httpClient1.DefaultRequestHeaders.Add("Ocp-Apim-Subscription-Key", token);</p>
			<p class="snippet">                    HttpContent content = responseMessage.Content;</p>
			<p class="snippet">                    content.Headers.ContentType = new MediaTypeWithQualityHeaderValue("application/octet-stream");</p>
			<p class="snippet">                    var response = await httpClient1.PostAsync(myurl, content);</p>
			<p class="snippet">                    var responseContent = await response.Content.ReadAsByteArrayAsync();</p>
			<p class="snippet">                    string ret = Encoding.ASCII.GetString(responseContent, 0, responseContent.Length);</p>
			<p class="snippet">                    dynamic image = JsonConvert.DeserializeObject&lt;object&gt;(ret);</p>
			<p class="snippet">                    string temp = "";</p>
			<p class="snippet">                    foreach (var regs in image.regions)</p>
			<p class="snippet">                    {</p>
			<p class="snippet">                        foreach (var lns in regs.lines)</p>
			<p class="snippet">                        {</p>
			<p class="snippet">                            foreach (var wds in lns.words)</p>
			<p class="snippet">                            {</p>
			<p class="snippet">                                temp += wds.text + " ";</p>
			<p class="snippet">                            }</p>
			<p class="snippet">                        }</p>
			<p class="snippet">                    }</p>
			<p class="snippet">                    return temp;</p>
			<p class="snippet">                }</p>
			<p class="snippet">            }</p>
			<p class="snippet">        }</p>
			<p>After the endpoint finishes its processing, it returns the response with a JSON payload. The context is extracted and deserialized into .NET objects. Multiple loops are coded to extract the text from the response.</p>
			<p>In this section, we created a simple application that uses Cognitive Services to provide word extractions from features using the OCR API and deployed it within Kubernetes Pods. This process and architecture can be used within any application that wants to consume Cognitive Services APIs. Next, we will take a look at another Cognitive Services API, known as visual features.</p>
			<h2 id="_idParaDest-486"><a id="_idTextAnchor512"/>Building a visual features service using the Cognitive Search .NET SDK</h2>
			<p>The last section was about creating a service that uses an OCR cognitive endpoint to return text within images. In this section, a new service will be created that will return visual features within an image, such as descriptions, tags, and objects.</p>
			<h3 id="_idParaDest-487"><a id="_idTextAnchor513"/>Using PowerShell</h3>
			<p>The code in PowerShell is similar to the previous OCR example, so it is not repeated here. The URL is different from the previous code example:</p>
			<div>
				<div id="_idContainer454" class="IMG---Figure">
					<img src="image/Figure_19.8.jpg" alt="HTTP request URL when using PowerShell"/>
				</div>
			</div>
			<h6>Figure 19.7: Request URL</h6>
			<p>The request is made using a <strong class="inline">POST</strong> method, and the URL points to the endpoint in the East US Azure region. It also uses version 2 and consumes the Vision API.</p>
			<p>The Cognitive Services access key is part of the HTTP header named <strong class="inline">ocp-apim-subscription-key</strong>. The header also contains the header content-type with <strong class="inline">application/json</strong> as the value. This is because the body of the request contains a JSON value. The body has the URL of the image from which text should be extracted.</p>
			<p>The response will be in JSON format containing the image content and a description.</p>
			<h3 id="_idParaDest-488"><a id="_idTextAnchor514"/>Using .NET</h3>
			<p>This example is again an ASP.NET Core MVC application and has the <strong class="inline">Microsoft.Azure.CognitiveServices.Vision.ComputerVision</strong> NuGet package installed in it: </p>
			<div>
				<div id="_idContainer455" class="IMG---Figure">
					<img src="image/Figure_19.9.jpg" alt="ASP.NET Core MVC application with the NuGet package"/>
				</div>
			</div>
			<h6>Figure 19.8: ASP.NET Core MVC application with the Microsoft.Azure.CognitiveServices.Vision.ComputerVision NuGet package</h6>
			<p>The code for the controller action is shown next. In this code, the cognitive service and key are declared. It also declares variables for the <strong class="inline">ComputerVisionClient</strong> and <strong class="inline">VisionType</strong> objects. It creates an instance of the <strong class="inline">ComputerVisionClient</strong> type, providing it the URL and the key.</p>
			<p>The <strong class="inline">VisionTypes</strong> list consists of multiple types of data sought from the image—tags, descriptions, and objects are added. Only these parameters will be extracted from the image.</p>
			<p>An <strong class="inline">HttpClient</strong> object is instantiated to download the image using the URL provided by the user and sends this binary data to the Cognitive Services endpoint using the <strong class="inline">AnalyzeImageInStreamAsync</strong> function of type <strong class="inline">ComputerVisionClient</strong>:</p>
			<p class="snippet">[HttpPost]</p>
			<p class="snippet">        public string Post([FromBody] string value)</p>
			<p class="snippet">        {</p>
			<p class="snippet">        private string visionapiurl = " https://eastus.api.cognitive.microsoft.com/vision/v2.0/analyze?visualFeaure=tags,description,objects&amp;language=en";</p>
			<p class="snippet">        private string apikey = "e55d36ac228f4d718d365f1fcddc0851";</p>
			<p class="snippet">        private ComputerVisionClient client;</p>
			<p class="snippet">        private List&lt;VisualFeatureTypes&gt; visionType =  new List&lt;VisualFeatureTypes&gt;();</p>
			<p class="snippet">client = new ComputerVisionClient(new ApiKeyServiceClientCredentials(apikey)) { </p>
			<p class="snippet">                Endpoint = visionapiurl</p>
			<p class="snippet">            };</p>
			<p class="snippet">            visionType.Add(VisualFeatureTypes.Description);</p>
			<p class="snippet">            visionType.Add(VisualFeatureTypes.Tags);</p>
			<p class="snippet">            visionType.Add(VisualFeatureTypes.Objects);</p>
			<p class="snippet">            </p>
			<p class="snippet">            string tags = "";</p>
			<p class="snippet">            string descrip   = "";</p>
			<p class="snippet">            string objprop = "";</p>
			<p class="snippet">            using (HttpClient hc = new HttpClient()) {</p>
			<p class="snippet">                var responseMessage =  hc.GetAsync(value).GetAwaiter().GetResult();</p>
			<p class="snippet">                Stream streamData = responseMessage.Content.ReadAsStreamAsync().GetAwaiter().GetResult();</p>
			<p class="snippet">                var result = client.AnalyzeImageInStreamAsync(streamData, visionType).GetAwaiter().GetResult();</p>
			<p class="snippet">                foreach (var tag in result.Tags) {</p>
			<p class="snippet">                    tags += tag.Name + " ";</p>
			<p class="snippet">                }</p>
			<p class="snippet">                foreach (var caption in result.Description.Captions)</p>
			<p class="snippet">                {</p>
			<p class="snippet">                    descrip += caption.Text + " ";</p>
			<p class="snippet">                }</p>
			<p class="snippet">                foreach (var obj in result.Objects)</p>
			<p class="snippet">                {</p>
			<p class="snippet">                    objprop += obj.ObjectProperty + " ";</p>
			<p class="snippet">                }</p>
			<p class="snippet">            }</p>
			<p class="snippet">	return tags;</p>
			<p class="snippet">	// return descrip or objprop</p>
			<p class="snippet">            </p>
			<p class="snippet">        }</p>
			<p> </p>
			<p>The results are looped through and tags are returned to the user. Similarly, descriptions and object properties can also be returned to the user. Now let's check out the ways we can safeguard the exposure of service keys.</p>
			<h2 id="_idParaDest-489"><a id="_idTextAnchor515"/>Safeguarding the Cognitive Services key</h2>
			<p>There are multiple ways to safeguard the exposure of keys to other actors. This can be done using the API Management resource in Azure. It can also be done using Azure Functions Proxies.</p>
			<h3 id="_idParaDest-490"><a id="_idTextAnchor516"/>Using Azure Functions Proxies</h3>
			<p>Azure Functions Proxies can refer to any URL, whether internal or external. When a request reaches Azure Functions Proxies, it will use the URL of the cognitive service along with the key to invoke the cognitive endpoint, and it will also override the request parameters and add the incoming image URL and append it to the cognitive endpoint URL as POST data. When a response comes back from the service, it will override the response, remove the headers, and pass JSON data back to the user.</p>
			<h2 id="_idParaDest-491"><a id="_idTextAnchor517"/>Consuming Cognitive Services</h2>
			<p>Consuming Cognitive Services follows a consistent pattern. Each cognitive service is available as a REST API, with each API expecting different sets of parameters to work on. Clients invoking these URLs should check out the documentation for associate parameters and provide values for them. Consuming URLs is a relatively raw method of using Cognitive Services. Azure provides SDKs for each service and for multiple languages. Clients can use these SDKs to work with Cognitive Services.</p>
			<p>The <strong class="bold">LUIS (Language Understanding Intelligence Service)</strong> authoring API is available at <strong class="inline">https://{luis resource name}-authoring.cognitiveservices.azure.com/</strong> and the production API is available at </p>
			<p><strong class="inline">https://{azure region}.api.cognitive.microsoft.com/luis/prediction/v3.0/apps/{application id}/slots/production/predict?subscription-key={cognitive key} &amp;verbose=true&amp;show-all-intents=true&amp;log=true&amp;query=YOUR_QUERY_HERE</strong>.</p>
			<p>Similarly, the Face API is available at <strong class="inline">https://{endpoint}/face/v1.0/detect[?returnFaceId][&amp;returnFaceLandmarks][&amp;returnFaceAttributes][&amp;recognitionModel][&amp;returnRecognitionModel][&amp;detectionModel]</strong>.</p>
			<p>There are many Cognitive Services APIs, with each having multiple flavors in terms of URLs, and the best way to know about these URLs is to use the Azure documentation.</p>
			<h2 id="_idParaDest-492"><a id="_idTextAnchor518"/>Summary</h2>
			<p>In this chapter, you gained an understanding of the deployment architecture and application architecture for creating intelligent applications in Azure. Azure provides Cognitive Services with numerous endpoints—each endpoint is responsible for executing an AI-related algorithm and providing outputs. Almost all Cognitive Services endpoints work in a similar manner with regard to HTTP requests and responses. These endpoints can also be invoked using SDKs provided by Azure for different languages, and you saw an example of obtaining visual features using them. There are more than 50 different endpoints, and you are advised to get an understanding of the nature of endpoints using the API console feature provided by Azure.</p>
		</div>
	</body></html>