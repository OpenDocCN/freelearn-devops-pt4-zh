<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Approaches, Tools, and Techniques</h1>
                </header>
            
            <article>
                
<p>The last chapter focused on getting a goal, vision, and dedicated team together (or not, as the case may be) to help with the implementation and adoption of CD and DevOps within your business. Over the next couple of chapters, we will go through the steps of executing the plan to ultimately deliver the goal you have defined.</p>
<p>Throughout <a href="17779905-1394-4db1-995e-04c6af9a5125.xhtml"/><a href="17779905-1394-4db1-995e-04c6af9a5125.xhtml">Chapter 3</a>, <em>Culture and Behaviors are the Cornerstones to Success</em>, we focused on the human side of what needs to be in place for CD and DevOps adoption. <a href="a19ac942-68bd-48a6-b59e-cd67ced91b65.xhtml">Chapter 4</a>, <em>Planning for Success</em>, then looked at how to build the plan and some of the building blocks that need to be put into place to make the adoption successful. We will now apply focus on the technical side of the execution<span>—</span>the tools, techniques, approaches, and processes you and the team should be looking to implement and/or refine as part of the plan.</p>
<p>There will be quite a lot of things to cover and take in, some of which you will need, some of which you may already have in place, and some of which you may want to consider implementing later down the line. I would recommend you read through everything, just in case there are some small chunks of wisdom or information that you can adapt or adopt to better fit your requirements.</p>
<p>Quite a bit of this chapter is focused on software engineering (that is, the Dev side of the DevOps partnership), and more CD than DevOps, but bear with me as some of the points covered are as relevant to system operations as they are to software engineering<span>—</span>this is, after all, what DevOps is really all about.</p>
<p>It is worth pointing out that the tools and processes mentioned are not mutually exclusive<span>—</span>it is not a case of all or nothing; you just need to pick what works for you. That said, there is a logical order and dependency to some of the things covered over the next chapter or two, but it's down to you to decide what is viable.</p>
<p>One other very important thing to take into account is that there are a plethora of other books, websites, blogs, and such that go into far more detail than I will. I will endeavor to provide a flavor and overview of what you'll need to drive the CD and DevOps adoption forward. It's down to you and the team to dig a little deeper.</p>
<p>Throughout this chapter, I'll be referring to tools and/or software solutions that you should consider to reduce the burden and ease the CD and DevOps adoption. As with any investment, I would recommend that you don't just chose the first one that appears in your favorite search engine or the one that an existing vendor is pushing. The CD and DevOps tooling market is very competitive; therefore, you should have more than one or two options. Understand what problem you need to solve based upon your specific needs and apply due diligence to the selection. If you need to trial a few different tools, you should do so. The effectiveness of your CD and DevOps adoption may rely upon these tools, so choose carefully.</p>
<p>Now that's out of the way, let's start with some engineering best practices.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Engineering best practices</h1>
                </header>
            
            <article>
                
<p>For those of you who are not software engineers, nor from a software engineering background, your knowledge and/or interest in how software is developed may be extremely minimal. Why, I hear you ask, do I need to know how a developer does their job? Surely, developers know this stuff better than I do? I doubt I even understand 10 percent of it anyway!</p>
<p>To some extent, this is very true; developers do (and should) know their stuff, and having you stick your nose in might not be welcome. However, it does help if you at least have an understanding or appreciation of how software is created, as it can help to identify where potential issues could reside.</p>
<p>Let's put it another way: I have an understanding and appreciation of how an internal combustion engine is put together and how it works, but I am no mechanic<span>—f</span>ar from it, in fact. However, I do know enough to be able to question why a mechanic has replaced my entire exhaust system and rear axle when I took my car in for a fuel-injector problem<span>—</span>in fact, I think I would vigorously question why.</p>
<p>It is the same with software development and the process that surrounds it. If you're not technical in the slightest and haven't done your homework to understand how software should be written, you leave yourself open to decisions being made by (or at the very least, noise emitting from) individuals who prefer to deflect by using technobabble rather than be open, honest, and willing and able to work with you. You will no doubt have come across such individuals during the elephant exposure, and I would wager that they have avoided getting involved with this pipe dream of shipping software quickly without everything going to pot<span>—</span>at least that's what they think. You and the team will need try to and work on the same level as them, so having some idea of what you're talking about will help with those discussions.</p>
<p>Let's start with the basics: CD is based upon a premise that quality software can be developed, built, tested, and shipped many times in quick succession (that's the continuous bit)<span>—</span>ideally, we're talking hours or days at the most. When you consider this list and apply it to a traditional waterfall-style development project, you'll no doubt have found that every step takes time and effort, and contains waste. You would also no doubt have found that it's the shipping part that is the most painful, costly, and risky. When applied to a modern agile development project, you'll normally find that the first three items on the list are more honed, efficient, and effective (although not without some waste and time lag<span>—</span>depending on the maturity of the team), whereas the shipping part is still painful and takes a vast amount of time and effort. We will focus on the shipping (or to be more accurate, the delivery) section later.</p>
<p>From this point forward, I'm going to assume you know what the differences between waterfall and agile development are (if not, I suggest you stop here and do some homework) and move swiftly on.</p>
<p>Let's go back to basics and cover some fundamentals in terms of modern agile software engineering:</p>
<ul>
<li>All code, config, and related metadata are stored in a modern source/version-control solution</li>
<li>Small and complete code changes are committed to the source-control repository frequently</li>
<li>Unit tests are included by default and sit with the source-code repository</li>
<li>Refactoring code happens on a regular basis</li>
<li>Code should not be overly complex and documented</li>
<li>Branches are short-lived, and merges are frequent</li>
</ul>
<ul>
<li>Automated tests sit alongside the code within the source-control repository and are run very frequently</li>
<li>Pair programming, code reviews, or pull requests are used consistently</li>
<li>Build and automated tests are orchestrated and controlled by a <strong>Continuous Integration</strong> (<strong>CI</strong>) solution</li>
<li>Failing tests are not the end of the world; nor is having others find fault in your code</li>
</ul>
<div class="packt_infobox">I may have lost some of you, but before you skip this chapter, please read on a little more as I'll be going through some of these concepts soon.</div>
<p>The preceding list is pretty simplistic and, as stated previously, most software engineers who work on modern agile software development projects will see this as common sense and common practice.</p>
<p>The reference to modern agile software development is purposeful as there are still some (in some industries, that should read <em>many</em>) old-school code cutters who believe that they are exempt from this due to the fact that they have been delivering code without of all of this newfangled hipster stuff for many years without any issues. That may be true; however, there's next to no chance of successfully adopting CD and DevOps without changing the way that software is written and delivered. No doubt these individuals would have been in the disengaged contributors group during the elephant exposure.</p>
<p>What is more worrying is when these individuals are actively discouraging the software engineers who do wish to follow modern agile software engineering best practice from doing so. Whatever the situation, these old dogs will have to learn new tricks.</p>
<p>Ultimately, modern agile software engineering is based on the simple premise of finding software problems as early as possible. Without this approach, these software problems <strong>will</strong> be found later down the line, they <strong>will</strong> slow everything down, and they <strong>will</strong> negatively impact the adoption and the perception of how successful the adoption is.</p>
<p>To put it another way, if you are continuously developing small incremental changes, which are being built, integrated, and tested, the ease of continuous delivery will be vastly increased.</p>
<p>Let's see what our personas can do to help:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 51.6302%">
<p><strong>Good approach</strong></p>
</td>
<td style="width: 47.7884%">
<p><strong>Not-so-good approach</strong></p>
</td>
</tr>
<tr>
<td style="width: 51.6302%">
<p>Victoria (the Veep) should not ignore this as simply "what developers do" and ensure she is aware of the effort needed to successfully embed best practice within the engineering teams and be willing to supply budget and executive sponsorship. </p>
</td>
<td style="width: 47.7884%">
<p>Victoria (the Veep) sees this as more expense which may well slow things down and / or a low priority skunkworks project off of the side of the main product delivery process</p>
</td>
</tr>
<tr>
<td style="width: 51.6302%">
<p>Stan (the manager) should ensure that relative importance is front and center with leadership, peer group and team(s) alike. He should also ensure the correct resources are assigned and aligned across the organization</p>
</td>
<td style="width: 47.7884%">
<p>Stan (the manager) ignores the benefits that engineering best practice will bring and sees adoption as additional workload that will distract the team(s)</p>
</td>
</tr>
<tr>
<td style="width: 51.6302%">
<p>Devina (the developer) and Oscar (the Ops guy) should spend time understanding and fully embrace engineering best practice and lead by example throughout their peer groups.</p>
</td>
<td style="width: 47.7884%">
<p>Devina (the developer) and Oscar (the Ops guy) keep their heads down and leave the leadership to argue about the merits of engineering best practice which they continue to struggle to deliver</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>For those of you whose eyes may have glazed over, or those of you who need a refresher, let's break these down a little further, starting with source-control.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Source-control</h1>
                </header>
            
            <article>
                
<p>There are many different flavors, versions, and solutions available for source-control (sometimes referred to as SCM or version-control systems), both commercial (not free) and open source (free). Most of these tools can be self-hosted (if that's something you need to do), or offered as a <strong>PaaS</strong> model (which isn't free but still relatively cheap). Taking this into account, there are no excuses not to use source-control. None!</p>
<p>If <strong>all</strong> of your code is in source-control, it is versioned (that is, there is a history of every change that has been made from the year dot), it is available to anyone and everyone who has access to the source-control system, it is secure, and it is (should be) backed up so you won't lose any of it.</p>
<p>Some of the more modern solutions can actually help you control the full life cycle of software delivery via inbuilt tools, workflows, and triggers. This can save you a lot of time, complexity, and cost. However, you should not be swayed by this too much. What you need is a solution that best suits your organization and the ways of working (now and in the future), and helps you to deliver quality software continuously.</p>
<p>Some of you may have heard the urban myth that a source-control solution is only useful for software source code. Just like all urban myths, this had some truth way back in the mists of time, but is now bunk. Source-control should not be restricted to software source code. Anything that can, could, and will be changed should be versioned and stored in source-control. I've already mentioned a few examples, so let's expand on this:</p>
<ul>
<li>Unit tests</li>
<li>Test cases</li>
<li>Automated test scripts</li>
<li>Software configuration/metadata</li>
<li>SQL scripts/SPROCS</li>
<li>Documentation</li>
<li>Environmental configurations</li>
<li>Server configuration</li>
<li>Anything and everything than can be changed, edited, or saved</li>
</ul>
<p>The normal bone of contention is environmental/server configurations and other collections of artifacts such as start-up scripts and network routing config, which some may see as exempt from source-control as this is in the land of Ops rather than Dev. However, as you're moving to DevOps, this no longer makes any sense and is not applicable. The rule of thumb should be: if it can be changed, it should be in source-control and versioned.</p>
<p>The DevOps community refers to the approach of representing a given environment via configuration files that can (should) be stored in source-control as configuration as code. It should be pointed out that this approach has grown from the open source community, and therefore some areas of this approach may not be wholly applicable initially<span>—</span>for example, administering Windows servers is more point-and-click than a set of configuration files that would be used to administer a Linux cluster. However, you can also administer Windows via PowerShell scripts, so there is an option. The bottom line is that you should strive toward having every element of a given environment/server/switch/router/firewall represented as configuration files that can (and should) be stored and versioned within your source-control system. That way, you can create an exact clone of a given environment at a given point in time with relative ease (something we'll come to soon).</p>
<p>One thing that may become a blocker is security and access to the contents of the source within the source-control solution. For example, if you're storing environmental configuration as code, you ideally don't want the development team having access to the production database connection strings or API tokens. There are proven and well-documented ways and means to do this (masking, encryption, restricting access to certain repositories, and so on), so it shouldn't be a blocker if you plan for it upfront.</p>
<div class="packt_infobox">There are books and reference materials aplenty regarding source-control that cover this subject in much more depth and detail, so I will not dwell on it here. Suffice it to say, if you do not have a source-control solution, implement one. Now!</div>
<p>As you'll no doubt gather, a source-control solution is a very valuable tool for CD and DevOps adoption. Along with having a central place to securely store your source code, it's also important to apply the same approach to your binary objects and artifacts.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The binary repository</h1>
                </header>
            
            <article>
                
<p>As the name implies, a binary repository is somewhere to store your binary objects and artifacts. Binary objects/artifacts are, in software engineering terms, the runnable software that is created when the source code is successfully compiled.</p>
<p>Binary repositories function in much the same way as a source-control solution, but, as you would expect, are better suited to storing binary objects. Some solutions also provide mechanisms to version, and even package up the binaries for later installation on a target environment.</p>
<p>We'll cover the importance of binary repositories later in the chapter. For now, let's move on to the valuable practice of keeping changes small and frequent.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Small, frequent, and simple changes</h1>
                </header>
            
            <article>
                
<p>Keeping changes small means the impact of the change—sometimes referred to as the blast radius—should also be small, the risks reduced, and the opportunities for change increased. It sounds overly simplistic, but it is also very true. If you consider how many changes to software a typical software engineering team makes in a day and then extrapolate that out to the number of teams you have making said changes, you'll soon find that this adds up. If you then take this number and multiply it by the number of days between releases, you'll find the volume of changes is not insignificant— and nor is the risk of those changes.</p>
<p>In terms of risk, let's assume we have a team of five software engineers who, on average, make 10 code changes each per day<span>—</span>that's 50 changes. Let's assume we have 10 teams all doing the same<span>—</span>that's 500 code changes per day. Let's now assume we're releasing every 12 weeks (or 60 working days); we're now talking 30,000 changes that need to go live. Even if we have industry-leading test coverage<span>—</span>let's say 99.9% coverage<span>—</span>there's still a chance something nasty could slip through. In this case, that's 30 changes not covered. In simple terms, there's a risk that 30 defects may be created every 12 weeks. OK, this is a very simplistic approach, but hopefully it illustrates the point that clumping together a large number of code changes is far from ideal.</p>
<p>One thing that may not be obvious is what happens if a simple defect is spotted the day after a release that can be fixed by a single-line code change. If we follow the preceding example, that defect will stay in production for another 11 weeks and 6 days (assuming we don't have emergency patch releases available to us). The same is true of any change made on day one of the 12-week release cycle<span>—</span>including customer feature requests.</p>
<p>If we were to break this down into smaller more frequent releases<span>—</span>say, every two weeks<span>—</span>and apply the same numbers, we would be looking at something like the following:</p>
<p><em>500 changes * 10 days = 5,000 changes released with a risk of five defects slipping through.</em></p>
<p>Now, let's again assume that if one escaped defect is spotted and fixed the day after the release, then that change will be live in nine days. Again, if a customer feature request change was made on day one of the release cycle, it could be live in 10 days. I think you'll agree that sounds slightly better than the first example.</p>
<p>The following diagram goes some way to illustrate what this could look like:</p>
<p><img src="assets/c28ef67f-8ae6-4d09-953c-72232aac5b62.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Large changes versus small incremental changes</div>
<p>Now, I will admit that the preceding example is very simplistic and may not reflect reality, and you might not currently have the luxury of shipping your code very frequently due to external factors (maybe your customers don't want<span>—</span>or can't accept<span>—</span>frequent releases, or your existing ops processes need time to allow for this); however, that is no excuse for not adopting the concepts now. If your software engineering teams become used to releasing in small and potentially shippable chunks, they form the habit of delivering continuously.</p>
<p>Another way of putting this is that once you have fully adopted CD and DevOps, they will have to work in this mode, so why not start getting used to it?</p>
<p>Continuously delivering small and frequent changes can also help in other areas; namely, reducing complexity, increasing code maintainability, and increasing quality. If an engineer only has to change a small amount of code then they have a far greater chance of refactoring the surrounding code to reduce complexity and overall maintainability of the codebase, including adding in additional unit tests. Another less obvious benefit of small and frequent changes is reducing the overhead of code reviews, pull requests, and merging, which can happen more frequently and become more of an everyday thing than a chore.</p>
<div class="packt_infobox">This practice should not be restricted to software engineering; it is just as relevant to changes in the system operations area. For example, making a small, isolated tweak to the server configuration (such as memory allocation to a virtual server) is much safer and easier to control and monitor than making sweeping changes all at once. If you make small changes, you have a much better chance of seeing whether the change had an impact (positive or negative) on the overall operation of the platform.</div>
<p>Working with small, incremental changes is a very beneficial practice to follow. However, this is going to be pretty difficult to manage unless you have some tools to help automate the building of your software.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Automated builds</h1>
                </header>
            
            <article>
                
<p>One of the common themes with CD and DevOps adoption is how automation is used. As mentioned previously, without some sort of automated tooling or solution, it will be very difficult to deliver on a frequent basis. You may be reading this and thinking, "Well, that's pretty obvious." However, even in this modern technological age there are software engineering teams who do everything manually using manual steps and/or handcrafted scripts<span>—</span>some of which may be older than the engineer running them. Luckily, this is very much a minority nowadays, although I'll cover some aspects of what automation is and why it's key to CD and DevOps adoption, just in case you're in the minority.</p>
<p class="mce-root"/>
<p>Every engineer that makes a change<span>—</span>be they a software or ops engineer<span>—</span>needs feedback as to whether a change they have made works (or not, as the case may be). The sooner they get this feedback, the sooner they can rectify any issues or move on to the next change. From a software engineering perspective, it is also very helpful to know whether the code they have written actually builds and/or compiles cleanly and consistently so that it can be tested.</p>
<p>This validation could be done via a manual process (or processes or scripts), but this can be cumbersome, inconsistent, prone to error, slow, and not always fully repeatable. Without consistency and repeatability, there's additional risk.</p>
<p>Implementing automation will help speed things up and keep things consistent, reliable, and repeatable, and, above, all provide confidence. If you are running the same steps over and over again and getting the same results, it's a strong bet that the process works and that you can trust it. It is therefore plausible that if you change one thing within your software, configuration, or environment, and the previously working process fails, there is a very good chance that the change has broken something.</p>
<p>There are plenty of tools available for building/compiling code<span>—</span>depending on the development language you are using<span>—</span>and all of them do pretty much the same thing: ensure the code is written correctly, the language syntax is as expected, ensure all external references are available, and<span>—</span>if all is as it should be<span>—</span>create a binary that can be run. This is overly simplistic, but hopefully conveys the point. There are a number of different ways to trigger this process:<span> </span>manually from the command line, manually via a script, or from within the developer's IDE itself. Whichever process you use, you should seriously consider automating the process so that you gain consistency and repeatability.</p>
<p>Another tool to consider including within the automation scripts/process is linting. Linting tools are there to help scan and check your source code for syntax issues. This can be a very useful addition as, if used before you build/compile code, it can vastly reduce the time taken to find issues<span>—e</span>specially when you have quite a convoluted codebase, which means the build time is minutes rather than seconds. Again, there are plenty of options to consider, depending on the code language you're using.</p>
<p>Hopefully, you now have some insight into why automating the building of your software components is important. Let's now focus on test-automation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Test-automation</h1>
                </header>
            
            <article>
                
<p>A traditional software-delivery process will normally include an element of testing. However, depending on the organization and age of the software, running the test cases themselves is normally a manual process. That being said, test-automation has been around for a while<span>—</span>for as long as agile software development<span>. H</span>owever, it's not as prevalent as one would hope. I should point out that testing approaches and the automation of such is a massive subject, and I will not be covering everything here. If you need more information, I suggest you do some research and pick up some good books on the subject. What we'll cover here is pretty basic, but should give you enough information to understand how test-automation fits into CD and DevOps adoption.</p>
<p>There are principally three types of tests:</p>
<ul>
<li>Unit tests are normally written in the coding language of the software and are used to exercise <em>code</em> and <em>logic paths</em> within the code base itself. They do not normally align to any particular use case or area of functionality.</li>
<li>Integration tests traditionally exercise the way in which one part of the software system/platform interacts with another (for example, to ensure the login page calls the authentication service correctly).</li>
<li>End-to-end tests are normally focused on the real-world use cases that an end user would initiate (for example, when logged in successfully, the welcome page is presented and the text displayed is in the correct language).</li>
</ul>
<p>This is an overly simplistic view, but hopefully elucidates the different types of tests.</p>
<p>In terms of tooling and technologies you can use to create, maintain, and run automated tests, there are a vast number of different flavors and solutions available, and the selection that best fits your needs can be hard to make. At a basic level, these tools pretty much do the same thing: they orchestrate the running of test scripts and capture the results. The choice of test-automation tooling is something you should not rush into, and my recommendation would be to give this as much thought as you did selecting the development language you use.</p>
<div class="packt_infobox">You will at times hear the word framework being used<span>—</span>especially when researching how to include unit tests. These are basically predefined approaches that are (mostly) industry standards. This means that the tools themselves may be different, but the standards they adhere to are similar.</div>
<p>When choosing a tool, try to consider future-proofing in terms of the testing language used for creating and maintaining the tests themselves. Standardizing on something such as Cucumber would be a good start, and this is something quite a few tools all use. It helps should you wish to adopt a <strong>TDD</strong> and/or <strong>BDD</strong> approach for your integration and end-to-end testing.</p>
<p class="CDPAlignLeft CDPAlign">Ultimately, what you need is to work toward what is widely referred to as "inverting the testing triangle." In essence, traditional testing approaches mostly rely on manually-executed tests, with automated and unit tests being less prevalent. For your CD and DevOps adoption to be successful, you need to change the ratio and vastly reduce the reliance on manual testing and increase automation. There are many documented reasons for this, but in relation to CD and DevOps, the main advantages are speed, reliability, repeatability, and consistency: </p>
<div style="padding-left: 30px" class="CDPAlignLeft CDPAlign"><img src="assets/6822106b-3ec5-4820-801f-895bb104f62f.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Inverted testing triangle</div>
<p>One thing you may notice in relation to the <strong>Agile</strong> triangle compared to the <strong>Traditional</strong> triangle is the relative size of the <strong>Unit Tests</strong> layer. This is the ideal situation to be in, as the more unit tests you have checking the code and logic flows within the code, the greater confidence you will have in the underlying code. This in turn should build greater confidence in higher-level tests. One less obvious advantage is cost<span>—</span>it's far cheaper to write unit tests than it is to write integration and/or full end-to-end tests.</p>
<div class="packt_tip">Agile software engineering approaches, such as TDD and <strong>eXtreme Programming</strong> (<strong>XP</strong>), follow the premise that unit tests are always written and must pass before you progress to the next level of testing.</div>
<p>Staying with automated testing, there is one thing that does add confusion and can put people off: the fact that adopting test-automation can be very daunting<span>— and </span>that's putting it lightly. There are quite a few things to consider when you go down this route: How much of the codebase do you cover with tests? How do you replicate actual users and usage in the real world? Where do you start?</p>
<p>Unfortunately, there are no straightforward or generic answers. This becomes more challenging when you start to look at the reams of online materials, books, and information regarding this very subject. To make matters worse, some of it will be contradictory to others. The only advice I can give is to follow the keep-it-simple (KISS) approach. For example, you may want to start by mapping out some of the primary and most-executed use cases (for example, login/authentication, users navigating from a list to an item detail in a shopping cart, or users making a purchase), and trial a couple of tools by creating automated tests to cover those. As long as you gain the ability to run the tests and the results are consistent, reliable, and repeatable, you should be on the right path.</p>
<div class="packt_tip">To use the KISS approach, e<span>ven one automated test that validates some small part of the code base is better than nothing</span>.</div>
<p>Once you have gained some confidence and trust in the overarching automated testing process, you can move onto the next use cases<span>—</span>or try another tool until you're happy.</p>
<p>I would also recommend the KISS approach for coverage<span>—</span>if you can cover 100% of the code base and use cases, then that's the figure you should chose. If you can't, then find the figure that is viable and increase it as you go along<span>. </span>What I mean by this is do not let the % decrease as new code and features are added. It may help to set a milestone date and realistic percentage goal so that a sense of urgency/focus isn't lost along the way.</p>
<p>There is another set of tools that can help with ascertaining your test coverage by inspecting/analyzing your codebase and source repository (which will, of course, include all of your automated tests) and providing useful information and dashboards for you to review. These can also give you a historical view so that you can measure increases (or decreases) in your coverage.</p>
<p>Another place to apply the KISS approach in is something that normally trips people up when adopting test-automation: the gnarly subject of test data. Test data can be a massive issue, and it can cause more problems than it solves<span>—</span>and quite a few arguments to boot. A good rule of thumb here would be to have the test data you need to run your test(s) created as part of the automated process and<span>—</span>more importantly<span>—</span>removed as a final step. I've seen far too many examples of this KISS approach not being followed, which means you will end up with stale data that may well become out of date quite quickly. This stale data can cause tests that previously ran without issue to start failing or, worse still, other people come along and base their tests on this very same data (which means you can't get rid of it even if you wanted to). It also compromises your ability to ensure your tests are consistent and repeatable.</p>
<p>Let's see what our personas can do to help:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 52.4065%">
<p><strong>Good approach</strong></p>
</td>
<td style="width: 47.0121%">
<p><strong>Not-so-good approach</strong></p>
</td>
</tr>
<tr>
<td style="width: 52.4065%">
<p>Victoria (the Veep) should take an active interest in how test automation can vastly reduce overall cost and effectiveness of quality assurance and be willing to supply budget and executive sponsorship. </p>
</td>
<td style="width: 47.0121%">
<p>Victoria (the Veep) sees this as a head count / cost reduction solution and skimps on the budget to get the best deal rather than the best solutions which the team(s) need</p>
</td>
</tr>
<tr>
<td style="width: 52.4065%">
<p>Stan (the manager) should ensure that relative importance is front and center with leadership, peer group and team(s) alike. He should also ensure the most relevant resources are assigned and aligned across the organization.</p>
</td>
<td style="width: 47.0121%">
<p>Stan (the manager) doesn't bother to understand the advantages test automation brings - he sees this as post-development QA stuff and has no interest in it</p>
</td>
</tr>
<tr>
<td style="width: 52.4065%">
<p>Devina (the developer) and Oscar (the Ops guy) should spend time understanding, investigating and embracing test automation as a day to day activity that sits side by side with software development.</p>
</td>
<td style="width: 47.0121%">
<p>Devina (the developer) and Oscar (the Ops guy) ignore test automation as there's already a team testing software once it's built so what's the point?</p>
</td>
</tr>
</tbody>
</table>
<p>One thing we have not covered thus far is the tooling that can run, manage, and control this automation. That is where continuous integration solutions come into play.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Continuous integration</h1>
                </header>
            
            <article>
                
<p>Continuous integration, or CI as it's more commonly known, is a tried-and-tested method of ensuring the software asset that is being developed builds correctly and plays nicely with the rest of the codebase. The keyword here is <em>continuous</em>, which, as the name implies, is as frequent as possible (ideally on each commit and/or merge to the source-control system). The simplest way to look at a CI solution is to think of it as a tool to orchestrate your build and test-automation tools<span>—</span>the two things we've just covered.</p>
<p>Yet again there are, as you may have guessed,<span> </span>a vast number of mature CI solutions available, both commercial and open source, so there are no excuses for not selecting one and using CI.</p>
<p>As mentioned, CI solutions are a very basic-level, software solution that orchestrates the execution of your build and test-automation. The execution is controlled by what many refer to as <em>CI jobs</em>, which are invoked when certain events occur; for example, when code is committed to and/or merged to the source-control repository, or on a timed schedule, or when another automation tool triggers the CI, and so on. These jobs contain a list of activities (commonly referred to as <em>steps</em>) that need to be run in quick succession; for example, get the latest version of source from source-control, compile to an executable, deploy the binary to a test environment, get the automated tests from source-control, and run them.</p>
<p class="CDPAlignLeft CDPAlign">If all is well, the CI job completes and reports a success. If it fails, it reports this fact and provides detailed feedback as to why it failed. Most tools also let you drill down into the failing step and see what went wrong. Each time you run a given CI job, a complete audit trail is written for you to go back and compare results and/or trends over time, as shown:</p>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/e785ddf0-c97f-4ad9-b7a5-01331153ac00.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span class="MsoSubtleReference">A typical CI process</span></div>
<p>CI tools can be quite powerful, and you can build in simple logic to control the process. For example, if all of the automated tests pass, you can then automatically move the executable (which could have the build version number baked in) to your binary repository, or if something fails, you could email the results to the engineering team. You can even build dashboards or information radiators so provide an instant and easy-to-understand visual representation of what's happening, and the results.</p>
<div class="packt_infobox">CI solutions are a must for CD. If you are building and testing your software changes on a frequent basis, you can ship frequently.</div>
<p>The advantages of CI for traditional systems-operations changes are not as obvious, but they can help a great deal in terms of trying out changes without impacting the production platform. For example, let's presume that you have a CI solution that is running many overnight automated tests against an isolated test environment. The tests have been successfully passing (more commonly referred to as green) for a few days, so you are confident that everything is as it should be. You then make a server configuration change and re-run the CI suite, which then fails. The only change has been the server configuration; therefore, it must have had an adverse impact. This is a good example of the DevOps approach being applied.</p>
<p>Implementing CI is no small feat<span>—</span>especially if you have nothing in terms of automation to start with. However, CI is a very powerful tool and vastly reduces that overhead and risk of using manual methods for building and testing system changes. For all intents and purposes, trying to implement CD without CI is going to be a very hard slog, and therefore my recommendation would be to bite the bullet and implement CI.</p>
<p>Throughout this section, we have been talking about how to automate builds and tests to ensure the software can be validated and delivered. We also refer to results that, overall, should be positive<span>—</span>the automated build has to have completed and the automated tests need to have passed before they can progress to the next stage. In other words, if tests fail, that's a bad thing. On the whole, that is correct. However, failure can be a good thing, as long as the failures are found early in the process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fail fast and often</h1>
                </header>
            
            <article>
                
<p>Failing fast and often may seem counter-intuitive, but it's a very good ethos to work to. If a defect is created but it is not spotted until it has gone live, the cost of rectifying said defect is high (it could be a completely new release), not to mention the impact it could have on your customers, reputation, and possibly your revenue. Finding defects early on is a must.</p>
<p>Agile engineering approaches such as <strong>TDD</strong> or <strong>BDD</strong> are based upon the principle of finding and catering for faults within software very early on in the process, the simple premise being that before code-development begins, tests are written to cover some/all of the use cases the software has to cater to<span>—</span>the proportion is normally referred to as coverage. As the code is being written, these tests can be run again and again as part of the CI process to spot gaps. If the test cases fail at this point, this is a good thing, as the only person impacted is the software engineer writing the code, rather than your customers.</p>
<div class="packt_infobox">This may sound strange<span>—</span>especially for the managers out there<span>—</span>but if defects are found early on, you should not make a big thing of it and you should not chastise people. Think back to what we learned about blame versus learning behaviors. What you want is to find the problem, figure out why it happened, fix it, learn from it, and move on.</div>
<p>One of the things that can sometimes scupper implementing engineering practices such as TDD is the size and complexity of the software platform itself. It may be a very daunting task to retrospectively implement a test suite for a software system that was not built around these principles. If this is the case, it may be wise to start small and build up.</p>
<p>We'll now move away from software building and test-automation and onto the challenges you will face when adopting CD and DevOps in relation to how your software system/platform is designed and architected.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Architectural approaches</h1>
                </header>
            
            <article>
                
<p>The majority of businesses<span>—</span>despite what the tech press and jeans-and-t-shirt conference speakers would have you believe<span>—</span>are not running modern software architecture. The reality is that a vast number of software platforms and systems in the world on which we are reliant have evolved over many years, and some (most) are rather complex and cumbersome to maintain or advance. Even the young and hipster tech sector companies are running and maintaining what would be classed as <em>legacy</em> solutions and platforms that comprise a small number of large executables all built and tested together before getting delivered. That isn't to say CD and DevOps principles aren't being used or can't be adopted; it just means that it takes a little more work.</p>
<p>The immediate reaction may well be to spend vast amounts of time, effort, and money transforming your entire software platform to a new reference architecture model that will allow for seamless adoption of CD and DevOps. If you're lucky enough to have senior leadership who have fully bought into this and have deep pockets, then good luck. Most of us are not that lucky, and therefore need to be creative in our approach.</p>
<p>Something that is also a little daunting, if you were to research the best breed of reference architecture, is that you will find that there are many views (often differing) on what's the best approach. Not to mention the many and varied ways one would go about adopting and implementing said architecture. If you're lucky enough to have a high-flying visionary who knows instinctively what to do, you are off to a great start. In reality, what you will end up with is a target architecture and a plan to get there through what's referred to as legacy strangulation<span>—</span>that being an approach to systematically replace parts of the legacy platform with software components designed and built using a more modern approach and focused on particular functional (and non-functional) areas.</p>
<p>Although legacy solutions are a pain, they are not the end of the world when it comes to CD and DevOps adoption. Take into account the limitations that come from having to build, test, and ship the entire platform each time changes are made, and also the overall duration for this to complete, which can be many minutes (sometime hours) depending on the size and complexity of the platform itself.</p>
<p>This is where creativity comes into play. Let's assume that your <em>legacy</em> platform takes 30 minutes to build and another 90 mins for the automated test suite to complete. That's two hours to wait for <em>each</em> change you make and want to test. Scale that out to the number of engineers making changes. And that's what can only be described as unworkable. Most will overcome this by only triggering the CI job at certain times<span>—</span>for example, at the end of the working day<span>—</span>so that the time taken doesn't leach into the working day. This does help in some ways, but also adds the risk that an entire build could fail due to one simple mistake, defect, or typo, and then hold up all the other changes and engineers who want to move onto their next task.</p>
<p>You could overcome this by looking at some small tweaks to the process to make things (a little more) workable. For example:</p>
<ul>
<li>Split the testing suite into smaller, discrete test packs; for example, use a subset of the tests to run when the build completes (sometimes referred to as a smoke test) and a full set overnight</li>
<li>Add more horsepower to your build and/or automated test servers</li>
<li>Implement a clustered CI solution</li>
<li>Parallelize the CI jobs (you'll need the additional horsepower/clustering)</li>
<li>Alter the way the software is built so that only changes that have changed since the last build are built again (that is, only build deltas rather the entire platform every time)</li>
</ul>
<p>Ultimately, you want to reduce the time taken to build, test, and ship your legacy software. The more you can achieve in this area, the more time you can buy yourself while you look at breaking down the legacy platform into smaller independent software components that can be independently built, tested, and shipped. Even the most integrated and closely-coupled software platform is made up of many small components all talking to each other.</p>
<p><span>If you take a step back and look at your legacy platform, you'll probably find you could actually split it (or at least most of it) into small, manageable chunks (shared libraries, different layers of technology, packaging solutions, and so on) that can be built and tested independently and quickly, and, more importantly, can be delivered frequently.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Component-based architecture</h1>
                </header>
            
            <article>
                
<p>As previously mentioned, if you are lucky enough to have the opportunity to re-engineer your <em>legacy</em> platform<span>—</span>as did ACME systems<span>—</span>then you should take time to consider the best approach for your needs. Ideally, you should look at a technology or an architectural approach that allows the platform to be broken down into small, discrete modules or components that are loosely coupled. By this I mean that each component can be developed, built, tested, and shipped independently of all other components.</p>
<p class="mce-root"/>
<p>This approach has had many names over the years<span>—</span>web services architecture, <strong>Service Orientated Architecture</strong> (<strong>SOA</strong>), or micro services architecture<span>—</span>but at a basic level they are pretty much the same thing: an architectural approach that allows loosely-coupled software components that are self-contained and coexist to provide functionality that would normally have been delivered as a complete monolithic platform shown as follows:</p>
<p><img src="assets/bad8ee9d-8636-4d4c-965b-18075b0bd0c7.png" style="width:77.92em;height:50.17em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">A typical architectural comparison</div>
<p>By going down this route, you have the advantage of small, discreet software components that can be developed and tested, and, more importantly, released independently. This goes a long way to realizing the benefits of CD.</p>
<p>Another advantage of this approach not directly attributable to CD or DevOps is cost saving. Not only does a component-based architecture allow for small and frequent changes to be released, it can also remove the need for large and costly IT infrastructure. For example, if you currently have one or two huge bits of code, you will have to have one or two hulking servers to run them. You then have one or two hulking DB servers and<span>—</span>to allow for Disaster Recovery<span>—y</span>ou'll have another set sitting and waiting. Just think how much that costs to acquire and keep running. With many small components, you can consider more cost-effective hosting approaches<span>—</span>something we'll look at later on.</p>
<p>There is a mountain of options and information available to determine the best approach for your current and future needs. Suffice to say, if you can move toward a component-based architecture, the pain and overhead of releasing will be a thing of the past.</p>
<p>One important thing to note here, should you adopt a component-based architecture (which you should do, by the way, just in case it wasn't clear), is how you release the components. There may be a temptation to use the same <em>clump it all into one big release</em> approach as used for the legacy platform, but that will simply lead to a world of pain and give you no advantage whatsoever. We'll be looking at CD tooling later, so please keep reading.</p>
<p>Let's have a look at another possible solution that may help with legacy strangulation and ease you toward the component-based architecture utopia.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Layers of abstraction</h1>
                </header>
            
            <article>
                
<p>If you have quite complex dependencies throughout your platform, it may help to try and separate your software assets by using some form of abstraction. This technique should assist in removing, or at least reducing, hard dependencies within your platform, and will help move you toward a component-based architecture which, in turn, will give you the opportunity to adopt CD.</p>
<p>Let's say, for example, you have two software components that have to be deployed together, as they have been hardwired in such a way that deploying one without the other would cause a platform outage. Then you're going to struggle to follow the <em>small incremental changes</em> method<span>—</span>not to mention the fact that you will be hard-pressed to release without downtime.</p>
<p>There are plenty of mature and proven design patterns available that can give some good ways of achieving this, but at the very least, it is a good practice to remove close dependencies wherever possible so that you don't end up with clumps of assets that need to be deployed together.</p>
<p class="mce-root"/>
<p>One common area for close coupling is between software and databases. This means that a change to one may mean both need to be tested and shipped. Adding abstraction here could be as complex as adding a data-access layer proxy between the two, or as simple as using SQL views. Another problem area is UI and business logic code being bundled together, which again can be separated out by following a standard design pattern. Whatever the approach, the goal is the same: to be able to build, test, and ship software components independently.</p>
<p>Just to add to your homework, you should spend some time looking at and analyzing areas of the existing platform to find components that are closely coupled, and then see how you can add an abstraction layer to allow each to be worked on without impacting the other. You could also look at fast-moving versus slow-moving areas (for example, which software components are updated on a regular basis and which very rarely), as this may help you to pinpoint which components to separate first.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Never break your consumer</h1>
                </header>
            
            <article>
                
<p>Your software platform will probably be complex and have quite a few dependencies<span>—</span>this is nothing to be ashamed of, and is quite normal. These dependencies can be classified as relationships between consumers and providers. The providers can be anything from shared libraries or core code modules to a database. The consumers will call/execute/send requests to the providers in a specific way as per some predefined interface spec (sometimes called a service contract).</p>
<p>A simple example would be a web page that utilizes a shared library to return content to render and display a user's address details. In this scenario, the web page is the consumer and the shared library is the provider. If the shared library originally returned four pieces of data but was changed to provide three, the consumer may not know how to handle this and may throw an error or, worse still, simply crash. You could, of course, add some defensive code, but in reality, this is just adding more complexity due to lazy change-management.</p>
<p>Most software platforms have many dependencies, which means it is sometimes very difficult to spot which <em>provider</em> has changed and is causing one of the many consumers to fail<span>—</span>especially when you consider that a consumer may also be a provider to another consumer higher up the stack (that is, a shared library, which consumes from a database and then <em>provides</em> said data to a web page, which then consumes it and provides that data to a JavaScript client, and so on).</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To understand how prevalent this situation is, you'll need to do some impact analysis that will help you map this out. Be forewarned that unless you can map out your entire platform into one easy-to-understand format that is consistently up to date, it's going to be a difficult task. Luckily, there are many mature and established patterns that cover these sorts of problems, as well as tools that will help with the analysis.</p>
<p>What can also help is to establish some rules around how changes are approached going forward. In simple terms, if you are making a change to software, config, or a database which will be consumed by another part of the platform, it's the responsibility of the person making the change to validate that the change has not broken anything up/downstream. If you have CI and test-automation in place, then that can help spot issues early. However, simply adding some diligence to the code-review/pull-request process is cheap and easy, and can help cement good behaviors.</p>
<div class="packt_infobox">Within the system operations area, the "never break your consumer" rule should also apply. For example, the software platform could be classed as a consumer of the server operating system (the provider); therefore, if you change or upgrade the operating system, you must ensure that there are no breaking changes that will cause the consumer to fail.</div>
<p>Sometimes breaking changes cannot be avoided (for example, the service contract between components has to change to accommodate new functionality). However, this should be the exception rather than the rule, and you should have a strategy planned to cater for this. An example strategy would be to accommodate side-by-side versioning, which will allow you to run more than one version of a software asset at the same time<span>—</span>something we'll cover later.</p>
<p>There may be times when the consumer/provider relationship fails as the person or team working on the provider is unaware of the relationship. This can be very true of providers within the system operations area. To overcome this, or at least minimize the risk, use open and honest peer-working practices.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Open and honest peer-working practices</h1>
                </header>
            
            <article>
                
<p>There are many different agile software-delivery methodologies in use today, but all of them revolve around some form of highly-collaborative ways of working and free-flowing communication. Agile approaches such as <strong>XP</strong>, pairing, or a simple code-review process, all depend on engineers working closely together.</p>
<p>I cannot stress enough the importance of sharing your work with others. Even the best software engineers (or system admins) on the planet are human and they <strong>will</strong> make mistakes. If you think your code is precious and don't want to share it with anyone else, you <strong>will</strong> create defects and it <strong>will</strong> take longer to overcome small mistakes, which can cause hours of head-scratching, or worse, have an adverse impact on your customers.</p>
<p>If you are confident that your code is of the highest quality and can stand up to scrutiny, then do not hide it away<span>—</span>put your money where your mouth is and share your work. If you are not that confident, sharing with your peer group will help to build that confidence. One thing to point out here<span>—</span>in terms of software engineers<span>—</span>is that the peer group should not be exclusively made up of other software engineers; the operations team can (and should) also be included in this process. It may seem strange, as they may not be able to actually read your code (although you may be surprised how many system admins can read code), but they know how the live platform operates and may be able to provide some valuable input and/or ask some pertinent questions (for example, what the code will do if there's a network blip, how long-lived threads will be, or why connection-polling isn't being used). This also encourages the DevOps mindset and approach. The same rule should apply to changes made by the Ops team.</p>
<p>All things considered, the majority of the world's highest-quality software is built in a highly-collaborative way, so there are no excuses for you not to be doing the same. Some purists may scoff at this approach, but consider this: most of these types will most probably sing the praises of Linux-based operating systems, which, if they actually thought about it, is, like most open source software, written using highly-collaborative approaches that have been part of the development process from day one.</p>
<div class="packt_infobox">Having an open, honest, and transparent peer-review process is as important within an Operations team as it is within a Development team. Changes made to any part of the platform run a risk, and having more than one pair of eyes to review will help reduce this risk. As with software code, there is no reason not to share system configuration changes.</div>
<p>One normally unforeseen advantage of working in this way is the fact that if your code (or configuration change) fails to get through peer review, the impact on the production system is negated. It's all about failing fast rather than waiting to put out something live to find it fails.</p>
<p>Let's assume you have seen the light and have decided to move to a loosely-coupled component-based architecture that has been written using best practices and you're ready to move to the next stage in your software engineering evolution.</p>
<p>Now you have everyone working together and playing nicely, we'll move on to the next challenge: how the expectations of the wider business need to be realigned in terms of release and feature delivery.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Incremental delivery of features</h1>
                </header>
            
            <article>
                
<p>Earlier, we looked at breaking work down into small incremental chunks so that you can deliver and release quickly. You also need to consider how you deal with features. What I'm referring to here is the business-driven deliverables that turn into revenue. Typically, you'll have a year-long business plan that is represented by a number of key initiatives that need to be delivered within that year, and these are further broken down into a selection of features, which is what will be marketed and sold to customers. This is pretty normal in terms of business process.</p>
<div class="packt_infobox">Please note that terms used within your business may differ and you may use terms such as epics, or themes, or goals, or MVP. In essence, we'll focus on the relationship between delivering a thing your business can make money from and the point in time when that thing becomes available to customers. To keep things simple, I'll refer to the thing as a feature and the point in time as a release.</div>
<p>How the adoption of CD and DevOps can impact this does depend on the current release cadence, but I would hazard a guess that you'll be looking at a cadence in months or quarters. Once CD and DevOps are embedded, you'll be looking at weeks, days, or hours between each release. This can only be a good thing, but let's take a moment to consider the wider impact:</p>
<ul>
<li>The wider business would currently be expecting a feature to be delivered in its entirety within a release cycle</li>
<li>Business functions, such as sales, marketing, legal, and support, will have processes in place to cater for this</li>
<li>You will be vastly reducing the release cycle and incrementally delivering changes</li>
</ul>
<p>How should the wider business cater for this? When will the feature be ready? When should they start the wheels turning? What should they say to customers?</p>
<p class="mce-root"/>
<p>What you need to do is work with these business areas and come to an agreement in terms of how features can be incrementally delivered over a number of releases. There are a few approaches you should consider and discuss:</p>
<ul>
<li>Deliver the end-to-end experience in slices and build up the richness of the functionality over a number of releases until the feature is complete</li>
<li>Focus on an area of functionality through to completion, then move onto to the next, then the next, until the feature is complete</li>
<li>Incrementally build the feature over a number of releases but keep it hidden until it's completed</li>
</ul>
<p>Something to consider is approaches such as the first and second could open up avenues such as alpha/beta releases, which means that you start to get customer feedback early on, whereas approaches such as the last one mean you don't get early feedback but the go-live is relatively painless (you've already shipped the code so go-live is really switch-it-on). Whatever approach you choose<span>—</span>and you will need to choose one<span>—</span>you need to ensure that those expecting "release equals feature delivery" are educated and their expectations are realigned.</p>
<p>We'll now move back into a more technical area: ensuring you are deploying the same software throughout your environments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the same binary across all environments</h1>
                </header>
            
            <article>
                
<p>Before a software asset can be used in a given environment, it has to have been built/compiled into an executable or a binary. This binary is important, as it is the version of the software that will be executed at runtime within your environments. Think of it as a snapshot in time. Some would say that the source code is more important than the binary object itself as the binary is simply a byproduct and can be recreated time and time again, although that's not strictly true.</p>
<p>You will not be running functional, regression, performance, or load tests against the source code. You will be doing so against the binary. It's therefore important that the resulting binary is treated with as much reverence as the source code from which it was created. This becomes more important if you're looking at side-by-side versioning and/or baking in versions during the CI process. For example, if your CI solution creates version 1.2.0.1 of the binary, then it's version 1.2.0.1 you should be using and testing.</p>
<p class="mce-root"/>
<p>The ideal, and recommended, approach is that the binary is only built once for a given release/deployment and that the self-same unchanged binary is used in <strong>all</strong> environments, including production. This may sound like common sense, but sometimes this is overlooked or simply cannot be done due to software design and/or tooling, or, more worryingly, it's not seen as important.</p>
<p>One example of tooling/software design limitations would come in the form of software tokens or config related to the environment (sometimes referred to as secrets). Let's take the credentials for a database server, for instance. Some would say that because this data is very sensitive<span>—</span>especially in higher environments such as production<span>—</span>it should be hidden away from all but a select few. One way around this is to <em>bake</em> this information into the binary itself at compile-time, which makes it secure. This is all well and good, but we only want to build it once, and therefore you would have to have the same credentials set up in all environments, including the completely open development environment<span>—</span>far from secure, I think you'll agree. Another drawback to this approach is the fact that someone could reverse-engineer the binary and get hold of the credentials without you knowing. Also, how would you change said credentials should they get leaked and need changing?</p>
<p>You could always build multiple copies of the binary (one for each environment); however, you're back to testing different versions of the software.</p>
<p>There are many industry-standard approaches to this problem, but the simple approach (which seems to work well for a vast amount of businesses) is to have this kind of data held in a startup script or system configuration file (which, of course, is under version-control) and have the software load it up at runtime. If you restrict access to these configs files/scripts, you have a good chance of keeping them <em>secret.</em> Whatever approach you choose, you should ensure that it allows you to use the same binary.</p>
<p>As previously mentioned, use of a binary repository will also allow you to store multiple versions of a given binary, which means a rollback to the previous version is pretty painless.</p>
<p>Now that we've looked at how to deliver the software to each environment, let's see how many environments you need.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How many environments is enough?</h1>
                </header>
            
            <article>
                
<p>This question has been around since software development became a thing. Unfortunately, there is no simple answer, although there are some common-sense and tried-tested-and-trusted approaches that have worked over the years. When I talk about environments, I'm not just referring to servers here; I'm referring to the servers, infrastructure, network, firewalls, first- and third-party integration, and so on. In essence, everything you need to run one copy of your software platform.</p>
<p>Going back to the question at hand, the (rather underwhelming) answer is: the number of environments you <strong>need</strong> depends on your ways of working, your engineering setup, and, of course, the architecture of your platform. There's also another factor to consider: the overhead to manage and look after multiple environments along with the cost of keeping them running and healthy. Suffice it to say that you should not go overboard; try to work to a <em>less-is-more</em> approach where you can.</p>
<p>There may also be a temptation to have environments set up for different scenarios: development, functional testing, regression testing, user acceptance testing, performance testing, and load testing. If you are able to ensure all the environments can be kept up to date (including the all-important test data), can easily deploy to them, and, more importantly, <strong>need</strong> all of them, then this may be viable. The reality is that having too many environments can actually be counterproductive and can cause far too much noise and overhead.</p>
<p>The ideal number of environments is two:</p>
<ul>
<li>One for development</li>
<li>One for production</li>
</ul>
<p>This may sound like an accident waiting to happen, but if you think about it, many small businesses and start-ups manage fine with such a setup. What you'll find is that as a business grows, so does the need to be risk-averse, and hence the potential for multiple environments.</p>
<p>When ACME systems started out, two environments were sufficient. As they grew, so did the need for more environments, and they ended up with multiple development environments (one for each engineering team), an integration environment, a performance-testing environment, a load-testing environment, a pre-live deployment staging environment, and, of course, production environments. They also ended up with an entire team of people whose job was to keep these all running<span>—</span>actually, they ended up with two: one to look after the engineering and testing environments, and one to look after the production environments. Far from ideal or effective.</p>
<p class="mce-root"/>
<p>You may think that with virtualization technologies (including cloud-based) now in a highly-mature state and used by anyone and everyone, setting up and running hundreds of servers, is not as much as an overhead as it once was. There is truth in that thinking, but it's the challenge of keeping everything in line that is massive<span>—</span>versions of software, O/S patch levels, network configuration, firewall configurations, and so on. Therefore, virtualization can help in some ways, but the <em>how many environments</em> question still remains.</p>
<p>Whatever you decide, there may be a fly in the ointment: what if your production environments are locked away in a highly-secure datacenter to which you have little or no access, or, worse still, fully managed by a third party? This can have a massive impact on your <em>less-is-more</em> approach. If this is the case, then you really need to get those managing said environments closely looped into what you're trying to do<span>—</span>if you don't, it can derail your DevOps adoption.</p>
<p>Let's move on to a real-world example and see how ACME systems approached this. When they reviewed the environments <strong>needed</strong> for CD and DevOps, they settled on the following as being sufficient for their needs:</p>
<ul>
<li><strong>Development environments</strong>: Cut-down versions of the platform with only a few other platform components that were needed for local testing</li>
<li><strong>CI environment</strong>: The place where the software is built and all automated tests are run on a regular basis</li>
<li><strong>Pre-production environment</strong>: Used for the occasional spot check/UAT (occasional being the operative word)</li>
<li><strong>Production environment</strong>: This is where all the action takes place</li>
</ul>
<p>The following diagram depicts the environments used:</p>
<p><img src="assets/94632199-b701-4c63-bfb6-2a128075a02c.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">ACME systems 3.0 environment setup</div>
<p>As you can see, this follows the less-is-more approach and allows for enough quality gates to ensure a given change was sufficient. When combined with all of the aforementioned engineering best practices (high levels of test coverage, build automation, CI tooling, and so on), the speed at which a given change could be delivered was minutes.</p>
<p class="mce-root"/>
<p>OK, so this is a bit of a utopia and you may be quite some distance from this now, but hopefully you can see how simple it can be, and hopefully you're slightly closer to answering the <em>how many environments</em> question.</p>
<p>Let's now have a look at another possible environment-related solution that can help speed up your delivery capabilities.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Developing against a like-live environment</h1>
                </header>
            
            <article>
                
<p>There are many ways to ensure one version of a software binary works, or integrates, with other parts of your platform, but by far the easiest is to actually develop against an environment that contains live versions of your platform.</p>
<p>On paper, this may look like a strange statement, but if you think about it, you're making a change to one part of your overall platform and<span>—</span>as is the CD way<span>—</span>you want to validate and ship that change as soon as possible. What this also gives you is the ability to ensure that the dependencies you expect to be available within the production environment are actually there and function as you expect, along with the configuration and infrastructure.</p>
<p>This approach will give the most value when used in conjunction with component-based architecture, but some aspects will apply to legacy platforms as well.</p>
<p>The simplest approach would be to develop against the production environment, but this is very risky and the possibility that you could cause outage<span>—</span>albeit inadvertently<span>—</span>is quite high. There's also the security/access issues. The next best thing is, therefore, to have another like-live environment set up, which contains the versions of code that are running in the production environment.</p>
<p>You may be thinking that developing against a <em>like-live</em> environment is somewhat overkill, and you may be wondering why not simply develop against the versions of software that reside in the CI environment. There is a simple answer: you have no firm idea which of the changed binaries in the CI environment will be live before you. For example, if you are developing and testing against version 1.2.0.3 of the authentication component (to pick a name out of the air), and when your binary hits production and starts to talk to version 1.2.0.1, you may experience issues that you didn't see during the development/testing phase.</p>
<p>This is especially true if someone is testing out a breaking change where you need to ensure that you have covered all scenarios <strong>before</strong> you release it to production.</p>
<p>This like-live environment only needs to be <em>like-live</em> in terms of software (and infrastructure) versions. If you can populate it with live data, that would be good, but the reality is that you would need something as big as production in terms of storage and so on, which is costly. Saying nothing of the risk of exposing confidential data and breaching data protection rules and regulation, such as GDPR<span>—</span>unless you have a way to redact confidential data (which is a whole different challenge).</p>
<p>To give you a flavor of how this could work, the following diagram gives an overview of how ACME systems implemented such a setup:</p>
<p><img src="assets/f4958468-5227-4273-9e42-4c0fa496f940.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The like-live environment used by ACME systems 3.0</div>
<p>As you can see, the like-live environment is tagged onto the end of the deployment pipeline. It is on the end for a reason: you only want to deploy to this environment once the deployment to production is successful.</p>
<p>It should be noted that when we talk about a like-live environment, this need not be a physical set of servers. You could look at virtualization (cloud or desktop-based), whereby you can pretty much spin up a copy of your production environment on a developer's workstation (on the presumption that there's enough horse power and storage available).</p>
<p>Now that you are starting to get all of the building blocks in place to realize your goal of adopting CD and DevOps, we have a few additional blocks you'll need, those being how you actually take the fully-built and tested software component through the environments in a controlled, reliable, and repeatable way. This is where CD tooling comes into play.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CD and DevOps tooling</h1>
                </header>
            
            <article>
                
<p>There is another collection of tools that may not be as readily available to you as the aforementioned tools (automated build and testing, CI, and so on). These are the tools that you will use to control and orchestrate the entire software-delivery life cycle from building the binaries (via CI), deploying said binaries to the various test environments, and if all goes well, pushing the same binary to production. At a very simple level, these tools act as workflow engines wherein each step is defined to do a specific action, and then the flow moves on to the next tasks. They also have basic logic built in to catch exceptions during the flow (for example, if tests fail, then don't go any further and send a notification). This workflow analogy is normally referred to as the CD pipeline, delivery pipeline, or just pipeline.</p>
<p>Over the past few years, the CD and DevOps tooling market has grown from almost nothing to a full-blown multi-million-dollar global business. There are now a plethora of tools and vendors wanting to sell tools to you, and it's become quite difficult to choose the one (or two) that will fit your needs. Just like any tool or technology you use within your software development life cycle, you need CD and DevOps tools that will be reliable, help more than hinder, and will grow with you as your adoption matures. I would also hazard a guess that you will already have some tooling that manages your software delivery/deployment, so you may need to look at something that will either integrate with the existing tooling or replace it completely.</p>
<p>The tooling you choose will be used day in and day out and will be heavily relied upon, so you had better make sure it fits your needs and will be there when it's needed. To assist in this, you could use something similar to the following, which could assist during your tool/vendor selection and due diligence.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>The following are some example questions you should ask of your CD and DevOps tooling/vendor:</p>
<ul>
<li><span>Can it deploy the same binary to multiple environments?</span></li>
<li><span>Can it access the binary and source repositories we're using?<br/>
Can it remotely invoke and control the installation process on the server it's been deployed to?<br/></span></li>
<li><span>Is there functionality to allow it to orchestrate my current tooling?</span></li>
<li><span>Is it capable of deploying database changes?</span></li>
<li><span>Does it have functionality to allow for the queuing up of releases?</span></li>
<li><span>Can it run parallel pipelines?</span></li>
<li><span>Does it contain an audit of what has been deployed, when, and by whom?</span></li>
<li><span>Is it secure?</span></li>
<li><span>How is it hosted (SaaS, PaaS, on-premise, and so on)?</span></li>
<li><span>Can it interact with the infrastructure to allow for no-downtime deployments?</span></li>
<li><span>Can it/could it orchestrate automated infrastructure-provisioning?</span></li>
<li><span>Can it interact with other systems and solutions, such as email, collaboration tools, change-management, issue-management, and project-management solutions?</span></li>
<li><span>Does it have simple and easy-to-understand dashboards that can be displayed on big screens around the office?</span></li>
<li><span>Can it interact with and/or orchestrate CI solutions (our current solution and other industry leaders)?</span></li>
<li><span>Will it grow with our needs?</span></li>
<li><span>What skills/experience do we need to run this?</span></li>
<li><span>Is it simple enough for anyone and everyone to use?</span></li>
<li><span>What support/SLA do we get?</span></li>
<li><span>What set up/implementation support do we get included in the price?</span></li>
<li><span>What about HA/failover?</span></li>
<li><span>What is the process for upgrading the tooling itself?</span></li>
</ul>
<p> </p>
<p>At this point, it would be very helpful if I just listed most of the market-leading tools and gave you an idea of their pros and cons. However, depending on when you read this, that information could be very out of date. This is really something you need to be doing yourself anyway<span>—</span>you know your needs, problems to solve, and budget better than I. It should go without saying, but I'll mention it just in case: the tooling choice should be done in true DevOps style, with both Dev and Ops heavily and equally involved.</p>
<p>There are a few things you should take into account when selecting CD and DevOps tooling: don't skimp on budget as the tooling will become the backbone of your delivery pipeline; don't just stick with the big boys in the marketplace, as they may be too constraining in the long run; and if there are gaps you can fill with bespoke solutions, you should seriously consider that without creating another legacy to look after.</p>
<p>You may have noticed that one of the considerations noted here is automated provisioning. Let's now look into what this means.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Automated provisioning</h1>
                </header>
            
            <article>
                
<p>The norm over the past few years has been to move from traditional tin-and-string physical servers and infrastructure to virtualized equivalents, be that on-premise, datacenter-hosted, or cloud-based. I won't dwell too much on the advantages of one over the other<span>—</span>again, there's plenty of rich information available should you wish to read up<span>—</span>but I will focus on one element that is not always front and center when planning to move to a virtualized environment solution: the ability to use automated provisioning as part of your CD and DevOps pipeline.</p>
<p>Provisioning is nothing new; as long as cloud providers have been a thing, they have been providing their customers with cloud-based virtualized servers that can be provisioned pretty much as and when needed. One also has the freedom in defining the configuration and setup of the servers in terms of horsepower, storage, networking, operating system, and location/region. In addition to this, when the servers are no longer needed, they can then be deleted (sometimes referred to as teared down).</p>
<p>Now consider how useful it would be to have automated provisioning as a step within your CD pipeline. You would then have the ability to not only control and orchestrate the software-delivery life cycle, but you can also create environments on the fly, install your software, run your tests, and then tear it all down. The massive advantages this gives you are predictability and repeatability. If you can guarantee that <strong>every time</strong> you initiate the CD pipeline you will have exactly the same vanilla environment created from scratch, then you can pretty much eliminate what some like to call environmental issues<span>—</span>something we'll be looking at later<span>—</span>which continually cause noise and false negatives (or positives) within the testing step.</p>
<p>As you would expect, there are many industry buzzwords floating around to complicate this sort of activity<span>—</span>the most common ones being <strong>Infrastructure-as-a-Service</strong> (<strong>IaaS</strong>) and PaaS<span>—</span>but what it boils down to is being able to programmatically interface with a provisioning system, tell it what you want in terms of spec, configuration, and so on, and get it to spit one out the other end. When you're done, you programmatically interface again and get the environment removed.</p>
<p>The list of requirements fed into a provisioning system that defines the server spec, CPUs, GPUs, RAM, storage, and so on is normally referred to as the recipe<span>—</span>there are some variations depending on the tools, but they are all pretty much the same thing.</p>
<p>Something to consider with automated provisioning is the time lag you may encounter within the CD pipeline. For example, if you were to take a binary and deploy it to a prebuilt server, then the time taken is simply the act of deploying. Add automated provisioning into the mix and the CD pipeline will have to wait for the new virtual server(s) to be provisioned before you can deploy your binaries. What you need to weigh up is the importance of quality, repeatability, and predictability over speed and convenience. Just because something is faster doesn't make it better. What you can do to overcome this is to pre-bake some vanilla virtual server images that can be added to the environment via the automated provisioning tooling as part of the CD pipeline. That way, you have a fresh virtual server in a fraction of the time. In fact, this is how many leading cloud providers operate. There is an overhead with this approach; someone needs to keep these vanilla virtual server images fresh and updated, with operating-system patch levels being the pain they are. Again, you need to weigh up the pros and cons.</p>
<p>One massive advantage of automated provisioning is blue/green deployments. Strictly speaking, this approach was around before automated provisioning became mainstream, but automated provisioning has made it far easier to realize. I won't go into too much detail—again that's homework for you. However, in simple terms, blue/green deployments allow you to provision a new server with the latest version of software or configuration changes or database updates offline within the environment, then switch old for new via a small network/routing change. Essentially, you do the hard work and prep upfront, and the release simply becomes the switchover. It's very effective and quick, and allows for near-instant rollback if problems are found (for example, switching back from new to old). I would highly recommend adding this to your reading list.</p>
<p>You may be thinking that the automated provisioning approach will only work for environments that are used for development and testing, but this self-same approach can (and should) be considered for your production environment. After all, if you have set up a CD pipeline, what's the point (or value) of stopping short of the goal. I wouldn't recommend doing this on day one; you need to build up your confidence in the tooling and iron out any kinks before taking the plunge. Based upon experience, I can guarantee that once you have automated provisioning in place, you will not look back.</p>
<p>Another massive but little-mentioned benefit of automated provisioning is the ability to overcome the bane of most IT departments and software houses around the globe: having to take your production system offline to upgrade it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">No-downtime deployments</h1>
                </header>
            
            <article>
                
<p>One of the things that comes with large releases of software (legacy or otherwise) is the unforgivable need to take the production platform offline to upgrade it. Yes, I did say unforgivable, because that's exactly what it is. It is also wholly avoidable. It's the IT equivalent to having an out-of-order sign on the elevator:</p>
<div style="padding-left: 210px"><img src="assets/ff4dd7d1-569e-4098-b05e-3ed44eb0607e.png" style=""/></div>
<p>There are two simple reasons for taking a production system offline to upgrade it: there are far too many unreleased changes that have been bundled up together, or you don't trust the quality of the software being delivered. Both of these can be overcome through CD and DevOps adoption.</p>
<p>Let's consider you are operating a real-time online service and you inform your customers that you have to take the system offline for an upgrade. You can bet a pretty penny that your customers will not take kindly to not being able to access your system (or more importantly, their data) for a few hours so that you can upgrade some parts of it. To minimize the impact, you will no doubt schedule the upgrade out-of-hours, which means you'll need to have people on-call to support the upgrade<span>—but </span>being out-of-hours, I doubt you'll have all of the engineers that made the changes available on the night, so you already have a risk.</p>
<p>One important thing in relation to out-of-hours is this: unless you are running a B2B solution and have customers in the same time zone, you may struggle to find a suitable out-of-hours time window. For example, if your solution and business is B2C, you're pretty much offering a 24/7 solution, so unless you know for sure when your consumers sleep, finding the time window is going to be tough. If your customers are global, you will find it even harder to find a suitable window. You will no doubt have included something in your terms of service and/or contracts to cater for taking the live platform offline, but this amounts to admitting to your customers that your business processes are inadequate.</p>
<p>If you also consider how many news stories are reported on a regular basis regarding major issues following a massive down-time IT upgrade, there is also a very strong possibility that your customers will look upon this big-bang approach with distrust as they'll be pretty sure something will go wrong once the service is up and running again. This distrust will be amplified if you go beyond the time window. From experience, I can say with confidence that something will go wrong, and depending on the severity, you will have to quickly move into damage-limitation mode to keep customers happy, T&amp;Cs and contracts aside. It might even get to the stage where your customers may shop around to find a competitor who does not need planned downtime.</p>
<p>OK, so this is a bit doom and gloom, but that is the stark reality. Customers and consumers don't care about your process problems or complexities in your SDLC; they have become accustomed to having the IT services they use on a daily basis being available when they need them<span>. T</span>ry to remember when one of the major search engines or social media platforms was offline. When release issues happen, it's now extremely difficult to contain the bad news, especially with the prevalence of social media, 24-hour news, and the like. You have to remember that bad news travels faster than anything else known to man, so the last thing you need is bad news generated because of a release.</p>
<p>The ultimate goal for CD and DevOps adoption is to repeatedly deliver value as quickly, consistently, and reliably as possible. Removing the need for down-time deployments completely is a massive bonus for you.</p>
<p>One thing to point out which may not be obvious is that it's not just the production environment that should have maximum uptime. Any environment that you are reliant on for your development, testing, and CD should be treated the same. If the like-live environment is down, how are you going to develop? If your CI environment is down, how are you going to integrate and test? The same rules should apply across the board<span>—</span>without exception.</p>
<p>Previously, we covered open and honest ways of working as part of engineering best practices. Openness and honesty are just as important when it comes to CD. A good way of providing this level of transparency is to monitor everything and have it available to all.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monitor, monitor, monitor</h1>
                </header>
            
            <article>
                
<p>One of the most important ways to ensure whether CD and DevOps is working is to monitor, monitor, and then monitor some more. If all of the environments used within the CD process are constantly being observed, then the impact of any change (big or small) is easy to see<span>—</span>in other words, there should be no hidden surprises. A simple rule of thumb here: if it moves, monitor it.</p>
<p>If you have good coverage in terms of monitoring, you have much more transparency across the board. There is no reason why monitoring should be restricted to the operations teams; everyone in the business should be able to see and understand how any environment<span>—</span>especially the production platform<span>—</span>is performing and what it is doing.</p>
<p>There are plenty of mature and industry-standard monitoring tools available, but it can be quite difficult to get a single view that is consistent and meaningful. For example, some tools are geared up for monitoring infrastructure and servers, whereas others are geared up for collecting application metrics, and still others for measuring application and/or database performance. Unless you can tie this data together into a coherent view, things will look disjointed. Ideally, you should try to aggregate the data from these tools<span>—</span>or at least try to integrate them<span>—</span>and present a unified view of how any given environment and the software/services running within are coping and functioning.</p>
<p>You will be surprised how much very valuable data you can get and how it can direct your engineering work, as the engineers can see exactly how their software or infrastructure is behaving in real time with real users.</p>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/37e01040-01d1-459e-87ad-cc0c13f29f0b.png" style=""/></div>
<div class="packt_figref"><span>If it moves, monitor it. If it doesn't, monitor it just in case</span></div>
<p><span>Monitoring is a must for CD and DevOps to work correctly, as things will be continually changing (software, services, infrastructure, and so on), and both halves of the Dev and Ops relationship will need to see what is going on and assist when/if problems occur.</span></p>
<p>Another, less obvious, positive that monitoring can bring you is proof that CD is not having an adverse impact on the production platform. If you're using some <em>graph-over-time</em> solution, you can get your CD tools to add a <em>spike</em> or a marker to the graph when a deployment takes place. You can then visually see the impact (or not) of the change.</p>
<p>So far, we have mainly focused on technical solutions and tools for the adoption of CD and DevOps. These solutions and tools may help to provide you with much of what you need in your toolbox. However, there is still room for simple manual processes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">When a simple manual process is also an effective tool</h1>
                </header>
            
            <article>
                
<p>Even if you have enough tooling to shake a stick at, you will no doubt have some small and niggling challenges that cannot be overcome with tooling and automation alone. To be honest, tooling and automation can be overkill in some respects, and can actually create barriers between certain parts of the organization you are trying so hard to bring together<span>—</span>here, I am talking about the Dev and Ops partnership that forms DevOps.</p>
<p>If tooling and automation completely negate the need for human interaction and discussion, you may well end up back where you started. You may also find that it is almost impossible to automate your way out of a simple problem.</p>
<p>Let's take, for example, the thorny issue of dependency-management. As a software platform matures, many interdependencies will form. If you are deploying your code using a CD process, these many interdependencies become ever-moving targets wherein components are being developed and deployed at different rates. You can try to capture this within your CI process, but something somewhere might be missed and you could end up inadvertently bringing down the entire platform because component B was deployed before component A.</p>
<p>You can try to map this out and build into the tooling rules to restrict, or at least minimize, these moving targets, but the rules may end up being more complex than the original dependencies. Or you could simply agree on a process whereby only one change happens at any given point in time. To feed into this, you can implement a simple queuing mechanism written on a whiteboard and reviewed regularly by all of the engineering and Operations teams.</p>
<p>This approach worked extremely well for ACME systems. The following is what they did:</p>
<ul>
<li>They obtained a blanket agreement from everyone that only one change would go through to production at any given point in time. They called this a deployment transaction.</li>
<li>To highlight the fact that someone was making a change to production (either a deployment or operational change), that person held the production environment token, which was in the form of a plush toy animal and was given the name the build badger. If you had the build badger, you were changing production.</li>
<li>They implemented a simple prioritized queue system using a whiteboard and a pen. Each morning, whoever wanted to make a deployment would come along to the deployment stand-up and agree with everyone there the order in which deployments (or changes) would be made that day.</li>
<li>Screens were installed throughout the office (not just the Dev and Ops areas), showing a real-time dashboard of what was going on.</li>
</ul>
<p>All very simple, but what this gave ACME systems was a way to overcome dependency hell (for example, if they could only change one thing at a time, there was an implied logical order of which change went before another) and built a sense of collaboration throughout all of the teams involved.</p>
<p>The following diagram should give you some idea of what the deployment transaction covered, in terms of a deployment:</p>
<p><img src="assets/1f685d47-931c-4d08-a318-601aed762973.png"/></p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">The deployment transaction</div>
<p>Eventually, ACME managed to engineer out the dependencies that plagued them in the early days so this manual process could be decommissioned, although it helped them keep moving with their CD and DevOps adoption.</p>
<p>Other very simple manual solutions you can use could include the following:</p>
<ul>
<li>Use collaborative tools for real-time communication between everyone and integrate this into your CD tooling so that deployments are announced and can be followed by all.</li>
<li>If your management is uneasy about having developers deploy to production without involving the Operations team, make sure you have a DevOps team doing the release.</li>
<li>If instant rollback is needed should a deployment fail, look into simple ways of rolling back, such as simply deploying the previous version of the component using the CD tooling.</li>
<li>Consistently inspect and adapt through regular retrospectives to see what is working and what is not.</li>
</ul>
<p>As you can tell, it's not all about technical solutions. If simple manual processes or tweaks to the ways of working are sufficient, then why bother trying to automate them?</p>
<p>And so ends the lesson<span>—</span><span>for now. Let's recap what we have covered.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>As stated at the beginning of this chapter was a lot to cover and a lot to take in. Some of it is relevant to you now and some of it will be relevant for the future.</p>
<p class="mce-root">At this point you should have a greater understanding and appreciation for how agile engineering best practices (including use of source control, CI, incremental delivery, test automation, failing fast) along with modern architectural approaches, delivery methods and in-depth monitoring will ease your CD and DevOps adoption. Above all you should have learned that it's not all about technical tools and techniques, sometimes simple processes can solve problems.</p>
<p class="mce-root">We'll now move on from ways to advance your CD and DevOps adoption to the kinds of issues which will trip you up along the way, how to spot them and how to get passed them.</p>


            </article>

            
        </section>
    </body></html>