- en: Approaches, Tools, and Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last chapter focused on getting a goal, vision, and dedicated team together
    (or not, as the case may be) to help with the implementation and adoption of CD
    and DevOps within your business. Over the next couple of chapters, we will go
    through the steps of executing the plan to ultimately deliver the goal you have
    defined.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Throughout [Chapter 3](17779905-1394-4db1-995e-04c6af9a5125.xhtml), *Culture
    and Behaviors are the Cornerstones to Success*, we focused on the human side of
    what needs to be in place for CD and DevOps adoption. [Chapter 4](a19ac942-68bd-48a6-b59e-cd67ced91b65.xhtml),
    *Planning for Success*, then looked at how to build the plan and some of the building
    blocks that need to be put into place to make the adoption successful. We will
    now apply focus on the technical side of the execution—the tools, techniques,
    approaches, and processes you and the team should be looking to implement and/or
    refine as part of the plan.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: There will be quite a lot of things to cover and take in, some of which you
    will need, some of which you may already have in place, and some of which you
    may want to consider implementing later down the line. I would recommend you read
    through everything, just in case there are some small chunks of wisdom or information
    that you can adapt or adopt to better fit your requirements.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Quite a bit of this chapter is focused on software engineering (that is, the
    Dev side of the DevOps partnership), and more CD than DevOps, but bear with me
    as some of the points covered are as relevant to system operations as they are
    to software engineering—this is, after all, what DevOps is really all about.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: It is worth pointing out that the tools and processes mentioned are not mutually
    exclusive—it is not a case of all or nothing; you just need to pick what works
    for you. That said, there is a logical order and dependency to some of the things
    covered over the next chapter or two, but it's down to you to decide what is viable.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: One other very important thing to take into account is that there are a plethora
    of other books, websites, blogs, and such that go into far more detail than I
    will. I will endeavor to provide a flavor and overview of what you'll need to
    drive the CD and DevOps adoption forward. It's down to you and the team to dig
    a little deeper.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, I'll be referring to tools and/or software solutions
    that you should consider to reduce the burden and ease the CD and DevOps adoption.
    As with any investment, I would recommend that you don't just chose the first
    one that appears in your favorite search engine or the one that an existing vendor
    is pushing. The CD and DevOps tooling market is very competitive; therefore, you
    should have more than one or two options. Understand what problem you need to
    solve based upon your specific needs and apply due diligence to the selection.
    If you need to trial a few different tools, you should do so. The effectiveness
    of your CD and DevOps adoption may rely upon these tools, so choose carefully.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Now that's out of the way, let's start with some engineering best practices.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Engineering best practices
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For those of you who are not software engineers, nor from a software engineering
    background, your knowledge and/or interest in how software is developed may be
    extremely minimal. Why, I hear you ask, do I need to know how a developer does
    their job? Surely, developers know this stuff better than I do? I doubt I even
    understand 10 percent of it anyway!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: To some extent, this is very true; developers do (and should) know their stuff,
    and having you stick your nose in might not be welcome. However, it does help
    if you at least have an understanding or appreciation of how software is created,
    as it can help to identify where potential issues could reside.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s put it another way: I have an understanding and appreciation of how
    an internal combustion engine is put together and how it works, but I am no mechanic—far
    from it, in fact. However, I do know enough to be able to question why a mechanic
    has replaced my entire exhaust system and rear axle when I took my car in for
    a fuel-injector problem—in fact, I think I would vigorously question why.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: It is the same with software development and the process that surrounds it.
    If you're not technical in the slightest and haven't done your homework to understand
    how software should be written, you leave yourself open to decisions being made
    by (or at the very least, noise emitting from) individuals who prefer to deflect
    by using technobabble rather than be open, honest, and willing and able to work
    with you. You will no doubt have come across such individuals during the elephant
    exposure, and I would wager that they have avoided getting involved with this
    pipe dream of shipping software quickly without everything going to pot—at least
    that's what they think. You and the team will need try to and work on the same
    level as them, so having some idea of what you're talking about will help with
    those discussions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the basics: CD is based upon a premise that quality software
    can be developed, built, tested, and shipped many times in quick succession (that''s
    the continuous bit)—ideally, we''re talking hours or days at the most. When you
    consider this list and apply it to a traditional waterfall-style development project,
    you''ll no doubt have found that every step takes time and effort, and contains
    waste. You would also no doubt have found that it''s the shipping part that is
    the most painful, costly, and risky. When applied to a modern agile development
    project, you''ll normally find that the first three items on the list are more
    honed, efficient, and effective (although not without some waste and time lag—depending
    on the maturity of the team), whereas the shipping part is still painful and takes
    a vast amount of time and effort. We will focus on the shipping (or to be more
    accurate, the delivery) section later.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从基础开始：CD（持续交付）基于一个前提，即优质软件可以在短时间内多次开发、构建、测试和发布（这就是“持续”的部分）——理想情况下，我们说的是几小时或几天最多。当你把这个列表应用到传统的瀑布式开发项目时，你无疑会发现每个步骤都需要时间和精力，并且充满浪费。你也无疑会发现，最痛苦、最昂贵、最有风险的部分就是发布（或交付）。而当应用到现代敏捷开发项目时，你通常会发现列表中的前三项更为精炼、高效和有效（尽管仍然存在一些浪费和时间滞后——这取决于团队的成熟度），但发布部分仍然是痛苦的，且需要大量的时间和精力。稍后我们将重点讨论发布（更准确地说，是交付）部分。
- en: From this point forward, I'm going to assume you know what the differences between
    waterfall and agile development are (if not, I suggest you stop here and do some
    homework) and move swiftly on.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一点开始，我假设你已经知道瀑布式开发和敏捷开发之间的区别（如果不知道，我建议你停下来做点功课），然后迅速进入下一部分。
- en: 'Let''s go back to basics and cover some fundamentals in terms of modern agile
    software engineering:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回归基础，讲解一些现代敏捷软件工程的基本概念：
- en: All code, config, and related metadata are stored in a modern source/version-control
    solution
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有代码、配置和相关元数据都存储在现代的源代码/版本控制解决方案中
- en: Small and complete code changes are committed to the source-control repository
    frequently
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小的且完整的代码更改应频繁提交到源代码管理仓库
- en: Unit tests are included by default and sit with the source-code repository
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单元测试默认包含并与源代码仓库一起存放
- en: Refactoring code happens on a regular basis
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码重构会定期进行
- en: Code should not be overly complex and documented
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码不应过于复杂，并且应该有文档化说明
- en: Branches are short-lived, and merges are frequent
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分支生命周期短，合并频繁
- en: Automated tests sit alongside the code within the source-control repository
    and are run very frequently
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化测试与代码一起存放在源代码仓库中，并且会非常频繁地运行
- en: Pair programming, code reviews, or pull requests are used consistently
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一直使用结对编程、代码审查或拉取请求
- en: Build and automated tests are orchestrated and controlled by a **Continuous
    Integration** (**CI**) solution
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和自动化测试由**持续集成**（**CI**）解决方案进行协调和控制
- en: Failing tests are not the end of the world; nor is having others find fault
    in your code
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试失败并不意味着世界末日；让别人指出你代码中的缺陷也不是终结
- en: I may have lost some of you, but before you skip this chapter, please read on
    a little more as I'll be going through some of these concepts soon.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 可能有些人已经失去兴趣，但在跳过本章之前，请再稍微阅读下去，因为我很快会深入讲解其中的一些概念。
- en: The preceding list is pretty simplistic and, as stated previously, most software
    engineers who work on modern agile software development projects will see this
    as common sense and common practice.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 上述列表相当简单，正如之前所说，大多数从事现代敏捷软件开发项目的工程师会把这些视为常识和常规做法。
- en: The reference to modern agile software development is purposeful as there are
    still some (in some industries, that should read *many*) old-school code cutters
    who believe that they are exempt from this due to the fact that they have been
    delivering code without of all of this newfangled hipster stuff for many years
    without any issues. That may be true; however, there's next to no chance of successfully
    adopting CD and DevOps without changing the way that software is written and delivered.
    No doubt these individuals would have been in the disengaged contributors group
    during the elephant exposure.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 提到现代敏捷软件开发是有目的的，因为仍然有一些（在某些行业中，应该说是*很多*）老派的程序员认为，他们由于多年来在没有这些新潮的“hipster”技术的情况下成功交付代码，因此可以免除这一切。也许这是事实；然而，如果不改变软件的编写和交付方式，几乎不可能成功地采用CD和DevOps。这些人无疑会在“参与度低”的贡献者组中。
- en: What is more worrying is when these individuals are actively discouraging the
    software engineers who do wish to follow modern agile software engineering best
    practice from doing so. Whatever the situation, these old dogs will have to learn
    new tricks.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 更令人担忧的是，当这些人积极阻止那些希望遵循现代敏捷软件工程最佳实践的软件工程师时。无论情况如何，这些“老狗”都得学会新把戏。
- en: Ultimately, modern agile software engineering is based on the simple premise
    of finding software problems as early as possible. Without this approach, these
    software problems **will** be found later down the line, they **will** slow everything
    down, and they **will** negatively impact the adoption and the perception of how
    successful the adoption is.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，现代敏捷软件工程的基础是尽早发现软件问题。没有这种方法，这些软件问题**必定**会在后期被发现，它们**必定**会拖慢进度，并且**必定**会对最佳实践的采用和其成功的认知产生负面影响。
- en: To put it another way, if you are continuously developing small incremental
    changes, which are being built, integrated, and tested, the ease of continuous
    delivery will be vastly increased.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果你持续开发小的增量变化，并且这些变化正在构建、集成和测试，那么持续交付的便利性将大大提高。
- en: 'Let''s see what our personas can do to help:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的角色可以做些什么来帮助：
- en: '| **Good approach** | **Not-so-good approach** |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| **良好方法** | **不太好的方法** |'
- en: '| Victoria (the Veep) should not ignore this as simply "what developers do"
    and ensure she is aware of the effort needed to successfully embed best practice
    within the engineering teams and be willing to supply budget and executive sponsorship.
    | Victoria (the Veep) sees this as more expense which may well slow things down
    and / or a low priority skunkworks project off of the side of the main product
    delivery process |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| Victoria（副总裁）不应仅仅将此视为“开发人员的工作”，应确保她意识到在工程团队中成功实施最佳实践所需的努力，并愿意提供预算和高层支持。 |
    Victoria（副总裁）将其视为更多的开支，这可能会拖慢进程和/或成为偏离主要产品交付流程的低优先级秘密项目 |'
- en: '| Stan (the manager) should ensure that relative importance is front and center
    with leadership, peer group and team(s) alike. He should also ensure the correct
    resources are assigned and aligned across the organization | Stan (the manager)
    ignores the benefits that engineering best practice will bring and sees adoption
    as additional workload that will distract the team(s) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| Stan（经理）应确保相对重要性在领导层、同伴群体和团队之间得到充分重视，并确保正确的资源在组织中分配和对齐。 | Stan（经理）忽视工程最佳实践带来的好处，认为其采用会增加额外的工作量，从而分散团队的注意力。
    |'
- en: '| Devina (the developer) and Oscar (the Ops guy) should spend time understanding
    and fully embrace engineering best practice and lead by example throughout their
    peer groups. | Devina (the developer) and Oscar (the Ops guy) keep their heads
    down and leave the leadership to argue about the merits of engineering best practice
    which they continue to struggle to deliver |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| Devina（开发人员）和Oscar（运维人员）应该花时间理解并完全接受工程最佳实践，并在同伴中以身作则。 | Devina（开发人员）和Oscar（运维人员）低调行事，把领导角色交给他人，继续在执行工程最佳实践时遇到困难。
    |'
- en: For those of you whose eyes may have glazed over, or those of you who need a
    refresher, let's break these down a little further, starting with source-control.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些眼神开始涣散的朋友，或者那些需要复习的朋友，让我们从源代码管理开始，进一步解释这些概念。
- en: Source-control
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 源代码管理
- en: There are many different flavors, versions, and solutions available for source-control
    (sometimes referred to as SCM or version-control systems), both commercial (not
    free) and open source (free). Most of these tools can be self-hosted (if that's
    something you need to do), or offered as a **PaaS** model (which isn't free but
    still relatively cheap). Taking this into account, there are no excuses not to
    use source-control. None!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多不同版本、种类和解决方案可供选择用于源代码管理（有时称为SCM或版本控制系统），包括商业（收费）和开源（免费的）。这些工具大多可以自托管（如果需要的话），或者作为**PaaS**模式提供（虽然不是免费的，但仍然相对便宜）。考虑到这一点，不使用源代码管理没有任何借口。绝对没有！
- en: If **all** of your code is in source-control, it is versioned (that is, there
    is a history of every change that has been made from the year dot), it is available
    to anyone and everyone who has access to the source-control system, it is secure,
    and it is (should be) backed up so you won't lose any of it.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果**所有**代码都存储在源代码管理中，并且进行了版本控制（即，每一个变更都有历史记录，从最早的版本开始），它对任何拥有源代码管理访问权限的人都是可用的，是安全的，并且应该有备份，以防丢失任何代码。
- en: Some of the more modern solutions can actually help you control the full life
    cycle of software delivery via inbuilt tools, workflows, and triggers. This can
    save you a lot of time, complexity, and cost. However, you should not be swayed
    by this too much. What you need is a solution that best suits your organization
    and the ways of working (now and in the future), and helps you to deliver quality
    software continuously.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of you may have heard the urban myth that a source-control solution is
    only useful for software source code. Just like all urban myths, this had some
    truth way back in the mists of time, but is now bunk. Source-control should not
    be restricted to software source code. Anything that can, could, and will be changed
    should be versioned and stored in source-control. I''ve already mentioned a few
    examples, so let''s expand on this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test cases
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated test scripts
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software configuration/metadata
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SQL scripts/SPROCS
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environmental configurations
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Server configuration
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anything and everything than can be changed, edited, or saved
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The normal bone of contention is environmental/server configurations and other
    collections of artifacts such as start-up scripts and network routing config,
    which some may see as exempt from source-control as this is in the land of Ops
    rather than Dev. However, as you''re moving to DevOps, this no longer makes any
    sense and is not applicable. The rule of thumb should be: if it can be changed,
    it should be in source-control and versioned.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: The DevOps community refers to the approach of representing a given environment
    via configuration files that can (should) be stored in source-control as configuration
    as code. It should be pointed out that this approach has grown from the open source
    community, and therefore some areas of this approach may not be wholly applicable
    initially—for example, administering Windows servers is more point-and-click than
    a set of configuration files that would be used to administer a Linux cluster.
    However, you can also administer Windows via PowerShell scripts, so there is an
    option. The bottom line is that you should strive toward having every element
    of a given environment/server/switch/router/firewall represented as configuration
    files that can (and should) be stored and versioned within your source-control
    system. That way, you can create an exact clone of a given environment at a given
    point in time with relative ease (something we'll come to soon).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: One thing that may become a blocker is security and access to the contents of
    the source within the source-control solution. For example, if you're storing
    environmental configuration as code, you ideally don't want the development team
    having access to the production database connection strings or API tokens. There
    are proven and well-documented ways and means to do this (masking, encryption,
    restricting access to certain repositories, and so on), so it shouldn't be a blocker
    if you plan for it upfront.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: There are books and reference materials aplenty regarding source-control that
    cover this subject in much more depth and detail, so I will not dwell on it here.
    Suffice it to say, if you do not have a source-control solution, implement one.
    Now!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: As you'll no doubt gather, a source-control solution is a very valuable tool
    for CD and DevOps adoption. Along with having a central place to securely store
    your source code, it's also important to apply the same approach to your binary
    objects and artifacts.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: The binary repository
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the name implies, a binary repository is somewhere to store your binary objects
    and artifacts. Binary objects/artifacts are, in software engineering terms, the
    runnable software that is created when the source code is successfully compiled.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Binary repositories function in much the same way as a source-control solution,
    but, as you would expect, are better suited to storing binary objects. Some solutions
    also provide mechanisms to version, and even package up the binaries for later
    installation on a target environment.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: We'll cover the importance of binary repositories later in the chapter. For
    now, let's move on to the valuable practice of keeping changes small and frequent.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Small, frequent, and simple changes
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keeping changes small means the impact of the change—sometimes referred to as
    the blast radius—should also be small, the risks reduced, and the opportunities
    for change increased. It sounds overly simplistic, but it is also very true. If
    you consider how many changes to software a typical software engineering team
    makes in a day and then extrapolate that out to the number of teams you have making
    said changes, you'll soon find that this adds up. If you then take this number
    and multiply it by the number of days between releases, you'll find the volume
    of changes is not insignificant— and nor is the risk of those changes.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: In terms of risk, let's assume we have a team of five software engineers who,
    on average, make 10 code changes each per day—that's 50 changes. Let's assume
    we have 10 teams all doing the same—that's 500 code changes per day. Let's now
    assume we're releasing every 12 weeks (or 60 working days); we're now talking
    30,000 changes that need to go live. Even if we have industry-leading test coverage—let's
    say 99.9% coverage—there's still a chance something nasty could slip through.
    In this case, that's 30 changes not covered. In simple terms, there's a risk that
    30 defects may be created every 12 weeks. OK, this is a very simplistic approach,
    but hopefully it illustrates the point that clumping together a large number of
    code changes is far from ideal.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: One thing that may not be obvious is what happens if a simple defect is spotted
    the day after a release that can be fixed by a single-line code change. If we
    follow the preceding example, that defect will stay in production for another
    11 weeks and 6 days (assuming we don't have emergency patch releases available
    to us). The same is true of any change made on day one of the 12-week release
    cycle—including customer feature requests.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一件可能不太显而易见的事情是，如果在发布后的第二天发现了一个可以通过单行代码修改来修复的简单缺陷会发生什么。如果我们按照前面的例子，那这个缺陷将在生产环境中再停留11周6天（假设我们无法进行紧急补丁发布）。发布周期的第一天所做的任何变更——包括客户功能请求——也会发生同样的情况。
- en: 'If we were to break this down into smaller more frequent releases—say, every
    two weeks—and apply the same numbers, we would be looking at something like the
    following:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将其拆解成更小且更频繁的发布——例如，每两周一次——并应用相同的数据，我们可能会看到类似以下的情况：
- en: '*500 changes * 10 days = 5,000 changes released with a risk of five defects
    slipping through.*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*500次变更 * 10天 = 5,000次变更发布，可能存在五个缺陷未被发现。*'
- en: Now, let's again assume that if one escaped defect is spotted and fixed the
    day after the release, then that change will be live in nine days. Again, if a
    customer feature request change was made on day one of the release cycle, it could
    be live in 10 days. I think you'll agree that sounds slightly better than the
    first example.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设如果在发布后的第二天发现并修复了一个缺陷，那么这个变更将在九天内上线。再次假设，如果在发布周期的第一天做了一个客户功能请求变更，它将在10天内上线。我想你会同意，这听起来比第一个例子要好一些。
- en: 'The following diagram goes some way to illustrate what this could look like:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示可以某种程度上说明这可能会是什么样子：
- en: '![](img/c28ef67f-8ae6-4d09-953c-72232aac5b62.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c28ef67f-8ae6-4d09-953c-72232aac5b62.png)'
- en: Large changes versus small incremental changes
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 大变更与小的增量变更
- en: Now, I will admit that the preceding example is very simplistic and may not
    reflect reality, and you might not currently have the luxury of shipping your
    code very frequently due to external factors (maybe your customers don't want—or
    can't accept—frequent releases, or your existing ops processes need time to allow
    for this); however, that is no excuse for not adopting the concepts now. If your
    software engineering teams become used to releasing in small and potentially shippable
    chunks, they form the habit of delivering continuously.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我必须承认前面的例子非常简化，可能与现实不符，且由于外部因素，你可能目前没有频繁发布代码的奢侈（也许你的客户不希望——或者不能接受——频繁的发布，或者你现有的运维流程需要时间来支持这一点）；然而，这并不能成为你现在不采用这些理念的借口。如果你的软件工程团队习惯于以小且可发布的模块进行发布，他们就会养成持续交付的习惯。
- en: Another way of putting this is that once you have fully adopted CD and DevOps,
    they will have to work in this mode, so why not start getting used to it?
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种表达方式是，一旦你完全采用了持续交付（CD）和DevOps，它们将必须在这种模式下工作，那么为什么不从现在开始适应它呢？
- en: Continuously delivering small and frequent changes can also help in other areas;
    namely, reducing complexity, increasing code maintainability, and increasing quality.
    If an engineer only has to change a small amount of code then they have a far
    greater chance of refactoring the surrounding code to reduce complexity and overall
    maintainability of the codebase, including adding in additional unit tests. Another
    less obvious benefit of small and frequent changes is reducing the overhead of
    code reviews, pull requests, and merging, which can happen more frequently and
    become more of an everyday thing than a chore.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 持续地进行小范围且频繁的变更也有助于其他方面；即减少复杂性、提高代码的可维护性并提升质量。如果工程师只需要更改少量代码，那么他们就有更大的机会重构周围的代码，减少复杂性并提高代码库的整体可维护性，包括增加额外的单元测试。另一个不太明显的好处是减少代码审查、拉取请求和合并的开销，这些过程可以更加频繁地发生，成为一种日常的事情，而不是繁琐的任务。
- en: This practice should not be restricted to software engineering; it is just as
    relevant to changes in the system operations area. For example, making a small,
    isolated tweak to the server configuration (such as memory allocation to a virtual
    server) is much safer and easier to control and monitor than making sweeping changes
    all at once. If you make small changes, you have a much better chance of seeing
    whether the change had an impact (positive or negative) on the overall operation
    of the platform.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Working with small, incremental changes is a very beneficial practice to follow.
    However, this is going to be pretty difficult to manage unless you have some tools
    to help automate the building of your software.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Automated builds
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the common themes with CD and DevOps adoption is how automation is used.
    As mentioned previously, without some sort of automated tooling or solution, it
    will be very difficult to deliver on a frequent basis. You may be reading this
    and thinking, "Well, that's pretty obvious." However, even in this modern technological
    age there are software engineering teams who do everything manually using manual
    steps and/or handcrafted scripts—some of which may be older than the engineer
    running them. Luckily, this is very much a minority nowadays, although I'll cover
    some aspects of what automation is and why it's key to CD and DevOps adoption,
    just in case you're in the minority.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Every engineer that makes a change—be they a software or ops engineer—needs
    feedback as to whether a change they have made works (or not, as the case may
    be). The sooner they get this feedback, the sooner they can rectify any issues
    or move on to the next change. From a software engineering perspective, it is
    also very helpful to know whether the code they have written actually builds and/or
    compiles cleanly and consistently so that it can be tested.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: This validation could be done via a manual process (or processes or scripts),
    but this can be cumbersome, inconsistent, prone to error, slow, and not always
    fully repeatable. Without consistency and repeatability, there's additional risk.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Implementing automation will help speed things up and keep things consistent,
    reliable, and repeatable, and, above, all provide confidence. If you are running
    the same steps over and over again and getting the same results, it's a strong
    bet that the process works and that you can trust it. It is therefore plausible
    that if you change one thing within your software, configuration, or environment,
    and the previously working process fails, there is a very good chance that the
    change has broken something.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'There are plenty of tools available for building/compiling code—depending on
    the development language you are using—and all of them do pretty much the same
    thing: ensure the code is written correctly, the language syntax is as expected,
    ensure all external references are available, and—if all is as it should be—create
    a binary that can be run. This is overly simplistic, but hopefully conveys the
    point. There are a number of different ways to trigger this process: manually
    from the command line, manually via a script, or from within the developer''s
    IDE itself. Whichever process you use, you should seriously consider automating
    the process so that you gain consistency and repeatability.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Another tool to consider including within the automation scripts/process is
    linting. Linting tools are there to help scan and check your source code for syntax
    issues. This can be a very useful addition as, if used before you build/compile
    code, it can vastly reduce the time taken to find issues—especially when you have
    quite a convoluted codebase, which means the build time is minutes rather than
    seconds. Again, there are plenty of options to consider, depending on the code
    language you're using.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, you now have some insight into why automating the building of your
    software components is important. Let's now focus on test-automation.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Test-automation
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A traditional software-delivery process will normally include an element of
    testing. However, depending on the organization and age of the software, running
    the test cases themselves is normally a manual process. That being said, test-automation
    has been around for a while—for as long as agile software development. However,
    it's not as prevalent as one would hope. I should point out that testing approaches
    and the automation of such is a massive subject, and I will not be covering everything
    here. If you need more information, I suggest you do some research and pick up
    some good books on the subject. What we'll cover here is pretty basic, but should
    give you enough information to understand how test-automation fits into CD and
    DevOps adoption.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'There are principally three types of tests:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests are normally written in the coding language of the software and are
    used to exercise *code* and *logic paths* within the code base itself. They do
    not normally align to any particular use case or area of functionality.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration tests traditionally exercise the way in which one part of the software
    system/platform interacts with another (for example, to ensure the login page
    calls the authentication service correctly).
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: End-to-end tests are normally focused on the real-world use cases that an end
    user would initiate (for example, when logged in successfully, the welcome page
    is presented and the text displayed is in the correct language).
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is an overly simplistic view, but hopefully elucidates the different types
    of tests.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of tooling and technologies you can use to create, maintain, and run
    automated tests, there are a vast number of different flavors and solutions available,
    and the selection that best fits your needs can be hard to make. At a basic level,
    these tools pretty much do the same thing: they orchestrate the running of test
    scripts and capture the results. The choice of test-automation tooling is something
    you should not rush into, and my recommendation would be to give this as much
    thought as you did selecting the development language you use.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: You will at times hear the word framework being used—especially when researching
    how to include unit tests. These are basically predefined approaches that are
    (mostly) industry standards. This means that the tools themselves may be different,
    but the standards they adhere to are similar.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: When choosing a tool, try to consider future-proofing in terms of the testing
    language used for creating and maintaining the tests themselves. Standardizing
    on something such as Cucumber would be a good start, and this is something quite
    a few tools all use. It helps should you wish to adopt a **TDD** and/or **BDD**
    approach for your integration and end-to-end testing.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'Ultimately, what you need is to work toward what is widely referred to as "inverting
    the testing triangle." In essence, traditional testing approaches mostly rely
    on manually-executed tests, with automated and unit tests being less prevalent.
    For your CD and DevOps adoption to be successful, you need to change the ratio
    and vastly reduce the reliance on manual testing and increase automation. There
    are many documented reasons for this, but in relation to CD and DevOps, the main
    advantages are speed, reliability, repeatability, and consistency:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6822106b-3ec5-4820-801f-895bb104f62f.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: Inverted testing triangle
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: One thing you may notice in relation to the **Agile** triangle compared to the
    **Traditional** triangle is the relative size of the **Unit Tests** layer. This
    is the ideal situation to be in, as the more unit tests you have checking the
    code and logic flows within the code, the greater confidence you will have in
    the underlying code. This in turn should build greater confidence in higher-level
    tests. One less obvious advantage is cost—it's far cheaper to write unit tests
    than it is to write integration and/or full end-to-end tests.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Agile software engineering approaches, such as TDD and **eXtreme Programming**
    (**XP**), follow the premise that unit tests are always written and must pass
    before you progress to the next level of testing.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'Staying with automated testing, there is one thing that does add confusion
    and can put people off: the fact that adopting test-automation can be very daunting—
    and that''s putting it lightly. There are quite a few things to consider when
    you go down this route: How much of the codebase do you cover with tests? How
    do you replicate actual users and usage in the real world? Where do you start?'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there are no straightforward or generic answers. This becomes
    more challenging when you start to look at the reams of online materials, books,
    and information regarding this very subject. To make matters worse, some of it
    will be contradictory to others. The only advice I can give is to follow the keep-it-simple
    (KISS) approach. For example, you may want to start by mapping out some of the
    primary and most-executed use cases (for example, login/authentication, users
    navigating from a list to an item detail in a shopping cart, or users making a
    purchase), and trial a couple of tools by creating automated tests to cover those.
    As long as you gain the ability to run the tests and the results are consistent,
    reliable, and repeatable, you should be on the right path.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: To use the KISS approach, even one automated test that validates some small
    part of the code base is better than nothing.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Once you have gained some confidence and trust in the overarching automated
    testing process, you can move onto the next use cases—or try another tool until
    you're happy.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: I would also recommend the KISS approach for coverage—if you can cover 100%
    of the code base and use cases, then that's the figure you should chose. If you
    can't, then find the figure that is viable and increase it as you go along. What
    I mean by this is do not let the % decrease as new code and features are added.
    It may help to set a milestone date and realistic percentage goal so that a sense
    of urgency/focus isn't lost along the way.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: There is another set of tools that can help with ascertaining your test coverage
    by inspecting/analyzing your codebase and source repository (which will, of course,
    include all of your automated tests) and providing useful information and dashboards
    for you to review. These can also give you a historical view so that you can measure
    increases (or decreases) in your coverage.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'Another place to apply the KISS approach in is something that normally trips
    people up when adopting test-automation: the gnarly subject of test data. Test
    data can be a massive issue, and it can cause more problems than it solves—and
    quite a few arguments to boot. A good rule of thumb here would be to have the
    test data you need to run your test(s) created as part of the automated process
    and—more importantly—removed as a final step. I''ve seen far too many examples
    of this KISS approach not being followed, which means you will end up with stale
    data that may well become out of date quite quickly. This stale data can cause
    tests that previously ran without issue to start failing or, worse still, other
    people come along and base their tests on this very same data (which means you
    can''t get rid of it even if you wanted to). It also compromises your ability
    to ensure your tests are consistent and repeatable.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what our personas can do to help:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '| **Good approach** | **Not-so-good approach** |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
- en: '| Victoria (the Veep) should take an active interest in how test automation
    can vastly reduce overall cost and effectiveness of quality assurance and be willing
    to supply budget and executive sponsorship. | Victoria (the Veep) sees this as
    a head count / cost reduction solution and skimps on the budget to get the best
    deal rather than the best solutions which the team(s) need |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
- en: '| Stan (the manager) should ensure that relative importance is front and center
    with leadership, peer group and team(s) alike. He should also ensure the most
    relevant resources are assigned and aligned across the organization. | Stan (the
    manager) doesn''t bother to understand the advantages test automation brings -
    he sees this as post-development QA stuff and has no interest in it |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
- en: '| Devina (the developer) and Oscar (the Ops guy) should spend time understanding,
    investigating and embracing test automation as a day to day activity that sits
    side by side with software development. | Devina (the developer) and Oscar (the
    Ops guy) ignore test automation as there''s already a team testing software once
    it''s built so what''s the point? |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
- en: One thing we have not covered thus far is the tooling that can run, manage,
    and control this automation. That is where continuous integration solutions come
    into play.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Continuous integration
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Continuous integration, or CI as it's more commonly known, is a tried-and-tested
    method of ensuring the software asset that is being developed builds correctly
    and plays nicely with the rest of the codebase. The keyword here is *continuous*,
    which, as the name implies, is as frequent as possible (ideally on each commit
    and/or merge to the source-control system). The simplest way to look at a CI solution
    is to think of it as a tool to orchestrate your build and test-automation tools—the
    two things we've just covered.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Yet again there are, as you may have guessed, a vast number of mature CI solutions
    available, both commercial and open source, so there are no excuses for not selecting
    one and using CI.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, CI solutions are a very basic-level, software solution that orchestrates
    the execution of your build and test-automation. The execution is controlled by
    what many refer to as *CI jobs*, which are invoked when certain events occur;
    for example, when code is committed to and/or merged to the source-control repository,
    or on a timed schedule, or when another automation tool triggers the CI, and so
    on. These jobs contain a list of activities (commonly referred to as *steps*)
    that need to be run in quick succession; for example, get the latest version of
    source from source-control, compile to an executable, deploy the binary to a test
    environment, get the automated tests from source-control, and run them.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'If all is well, the CI job completes and reports a success. If it fails, it
    reports this fact and provides detailed feedback as to why it failed. Most tools
    also let you drill down into the failing step and see what went wrong. Each time
    you run a given CI job, a complete audit trail is written for you to go back and
    compare results and/or trends over time, as shown:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e785ddf0-c97f-4ad9-b7a5-01331153ac00.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: A typical CI process
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: CI tools can be quite powerful, and you can build in simple logic to control
    the process. For example, if all of the automated tests pass, you can then automatically
    move the executable (which could have the build version number baked in) to your
    binary repository, or if something fails, you could email the results to the engineering
    team. You can even build dashboards or information radiators so provide an instant
    and easy-to-understand visual representation of what's happening, and the results.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: CI solutions are a must for CD. If you are building and testing your software
    changes on a frequent basis, you can ship frequently.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: The advantages of CI for traditional systems-operations changes are not as obvious,
    but they can help a great deal in terms of trying out changes without impacting
    the production platform. For example, let's presume that you have a CI solution
    that is running many overnight automated tests against an isolated test environment.
    The tests have been successfully passing (more commonly referred to as green)
    for a few days, so you are confident that everything is as it should be. You then
    make a server configuration change and re-run the CI suite, which then fails.
    The only change has been the server configuration; therefore, it must have had
    an adverse impact. This is a good example of the DevOps approach being applied.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Implementing CI is no small feat—especially if you have nothing in terms of
    automation to start with. However, CI is a very powerful tool and vastly reduces
    that overhead and risk of using manual methods for building and testing system
    changes. For all intents and purposes, trying to implement CD without CI is going
    to be a very hard slog, and therefore my recommendation would be to bite the bullet
    and implement CI.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this section, we have been talking about how to automate builds and
    tests to ensure the software can be validated and delivered. We also refer to
    results that, overall, should be positive—the automated build has to have completed
    and the automated tests need to have passed before they can progress to the next
    stage. In other words, if tests fail, that's a bad thing. On the whole, that is
    correct. However, failure can be a good thing, as long as the failures are found
    early in the process.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Fail fast and often
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Failing fast and often may seem counter-intuitive, but it's a very good ethos
    to work to. If a defect is created but it is not spotted until it has gone live,
    the cost of rectifying said defect is high (it could be a completely new release),
    not to mention the impact it could have on your customers, reputation, and possibly
    your revenue. Finding defects early on is a must.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Agile engineering approaches such as **TDD** or **BDD** are based upon the principle
    of finding and catering for faults within software very early on in the process,
    the simple premise being that before code-development begins, tests are written
    to cover some/all of the use cases the software has to cater to—the proportion
    is normally referred to as coverage. As the code is being written, these tests
    can be run again and again as part of the CI process to spot gaps. If the test
    cases fail at this point, this is a good thing, as the only person impacted is
    the software engineer writing the code, rather than your customers.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: This may sound strange—especially for the managers out there—but if defects
    are found early on, you should not make a big thing of it and you should not chastise
    people. Think back to what we learned about blame versus learning behaviors. What
    you want is to find the problem, figure out why it happened, fix it, learn from
    it, and move on.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: One of the things that can sometimes scupper implementing engineering practices
    such as TDD is the size and complexity of the software platform itself. It may
    be a very daunting task to retrospectively implement a test suite for a software
    system that was not built around these principles. If this is the case, it may
    be wise to start small and build up.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: We'll now move away from software building and test-automation and onto the
    challenges you will face when adopting CD and DevOps in relation to how your software
    system/platform is designed and architected.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Architectural approaches
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The majority of businesses—despite what the tech press and jeans-and-t-shirt
    conference speakers would have you believe—are not running modern software architecture.
    The reality is that a vast number of software platforms and systems in the world
    on which we are reliant have evolved over many years, and some (most) are rather
    complex and cumbersome to maintain or advance. Even the young and hipster tech
    sector companies are running and maintaining what would be classed as *legacy*
    solutions and platforms that comprise a small number of large executables all
    built and tested together before getting delivered. That isn't to say CD and DevOps
    principles aren't being used or can't be adopted; it just means that it takes
    a little more work.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: The immediate reaction may well be to spend vast amounts of time, effort, and
    money transforming your entire software platform to a new reference architecture
    model that will allow for seamless adoption of CD and DevOps. If you're lucky
    enough to have senior leadership who have fully bought into this and have deep
    pockets, then good luck. Most of us are not that lucky, and therefore need to
    be creative in our approach.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Something that is also a little daunting, if you were to research the best breed
    of reference architecture, is that you will find that there are many views (often
    differing) on what's the best approach. Not to mention the many and varied ways
    one would go about adopting and implementing said architecture. If you're lucky
    enough to have a high-flying visionary who knows instinctively what to do, you
    are off to a great start. In reality, what you will end up with is a target architecture
    and a plan to get there through what's referred to as legacy strangulation—that
    being an approach to systematically replace parts of the legacy platform with
    software components designed and built using a more modern approach and focused
    on particular functional (and non-functional) areas.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Although legacy solutions are a pain, they are not the end of the world when
    it comes to CD and DevOps adoption. Take into account the limitations that come
    from having to build, test, and ship the entire platform each time changes are
    made, and also the overall duration for this to complete, which can be many minutes
    (sometime hours) depending on the size and complexity of the platform itself.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: This is where creativity comes into play. Let's assume that your *legacy* platform
    takes 30 minutes to build and another 90 mins for the automated test suite to
    complete. That's two hours to wait for *each* change you make and want to test.
    Scale that out to the number of engineers making changes. And that's what can
    only be described as unworkable. Most will overcome this by only triggering the
    CI job at certain times—for example, at the end of the working day—so that the
    time taken doesn't leach into the working day. This does help in some ways, but
    also adds the risk that an entire build could fail due to one simple mistake,
    defect, or typo, and then hold up all the other changes and engineers who want
    to move onto their next task.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'You could overcome this by looking at some small tweaks to the process to make
    things (a little more) workable. For example:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Split the testing suite into smaller, discrete test packs; for example, use
    a subset of the tests to run when the build completes (sometimes referred to as
    a smoke test) and a full set overnight
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add more horsepower to your build and/or automated test servers
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement a clustered CI solution
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelize the CI jobs (you'll need the additional horsepower/clustering)
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alter the way the software is built so that only changes that have changed since
    the last build are built again (that is, only build deltas rather the entire platform
    every time)
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ultimately, you want to reduce the time taken to build, test, and ship your
    legacy software. The more you can achieve in this area, the more time you can
    buy yourself while you look at breaking down the legacy platform into smaller
    independent software components that can be independently built, tested, and shipped.
    Even the most integrated and closely-coupled software platform is made up of many
    small components all talking to each other.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: If you take a step back and look at your legacy platform, you'll probably find
    you could actually split it (or at least most of it) into small, manageable chunks
    (shared libraries, different layers of technology, packaging solutions, and so
    on) that can be built and tested independently and quickly, and, more importantly,
    can be delivered frequently.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Component-based architecture
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As previously mentioned, if you are lucky enough to have the opportunity to
    re-engineer your *legacy* platform—as did ACME systems—then you should take time
    to consider the best approach for your needs. Ideally, you should look at a technology
    or an architectural approach that allows the platform to be broken down into small,
    discrete modules or components that are loosely coupled. By this I mean that each
    component can be developed, built, tested, and shipped independently of all other
    components.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach has had many names over the years—web services architecture,
    **Service Orientated Architecture** (**SOA**), or micro services architecture—but
    at a basic level they are pretty much the same thing: an architectural approach
    that allows loosely-coupled software components that are self-contained and coexist
    to provide functionality that would normally have been delivered as a complete
    monolithic platform shown as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bad8ee9d-8636-4d4c-965b-18075b0bd0c7.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: A typical architectural comparison
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: By going down this route, you have the advantage of small, discreet software
    components that can be developed and tested, and, more importantly, released independently.
    This goes a long way to realizing the benefits of CD.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage of this approach not directly attributable to CD or DevOps
    is cost saving. Not only does a component-based architecture allow for small and
    frequent changes to be released, it can also remove the need for large and costly
    IT infrastructure. For example, if you currently have one or two huge bits of
    code, you will have to have one or two hulking servers to run them. You then have
    one or two hulking DB servers and—to allow for Disaster Recovery—you'll have another
    set sitting and waiting. Just think how much that costs to acquire and keep running.
    With many small components, you can consider more cost-effective hosting approaches—something
    we'll look at later on.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: There is a mountain of options and information available to determine the best
    approach for your current and future needs. Suffice to say, if you can move toward
    a component-based architecture, the pain and overhead of releasing will be a thing
    of the past.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: One important thing to note here, should you adopt a component-based architecture
    (which you should do, by the way, just in case it wasn't clear), is how you release
    the components. There may be a temptation to use the same *clump it all into one
    big release* approach as used for the legacy platform, but that will simply lead
    to a world of pain and give you no advantage whatsoever. We'll be looking at CD
    tooling later, so please keep reading.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Let's have a look at another possible solution that may help with legacy strangulation
    and ease you toward the component-based architecture utopia.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Layers of abstraction
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have quite complex dependencies throughout your platform, it may help
    to try and separate your software assets by using some form of abstraction. This
    technique should assist in removing, or at least reducing, hard dependencies within
    your platform, and will help move you toward a component-based architecture which,
    in turn, will give you the opportunity to adopt CD.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Let's say, for example, you have two software components that have to be deployed
    together, as they have been hardwired in such a way that deploying one without
    the other would cause a platform outage. Then you're going to struggle to follow
    the *small incremental changes* method—not to mention the fact that you will be
    hard-pressed to release without downtime.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: There are plenty of mature and proven design patterns available that can give
    some good ways of achieving this, but at the very least, it is a good practice
    to remove close dependencies wherever possible so that you don't end up with clumps
    of assets that need to be deployed together.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'One common area for close coupling is between software and databases. This
    means that a change to one may mean both need to be tested and shipped. Adding
    abstraction here could be as complex as adding a data-access layer proxy between
    the two, or as simple as using SQL views. Another problem area is UI and business
    logic code being bundled together, which again can be separated out by following
    a standard design pattern. Whatever the approach, the goal is the same: to be
    able to build, test, and ship software components independently.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Just to add to your homework, you should spend some time looking at and analyzing
    areas of the existing platform to find components that are closely coupled, and
    then see how you can add an abstraction layer to allow each to be worked on without
    impacting the other. You could also look at fast-moving versus slow-moving areas
    (for example, which software components are updated on a regular basis and which
    very rarely), as this may help you to pinpoint which components to separate first.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Never break your consumer
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Your software platform will probably be complex and have quite a few dependencies—this
    is nothing to be ashamed of, and is quite normal. These dependencies can be classified
    as relationships between consumers and providers. The providers can be anything
    from shared libraries or core code modules to a database. The consumers will call/execute/send
    requests to the providers in a specific way as per some predefined interface spec
    (sometimes called a service contract).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: A simple example would be a web page that utilizes a shared library to return
    content to render and display a user's address details. In this scenario, the
    web page is the consumer and the shared library is the provider. If the shared
    library originally returned four pieces of data but was changed to provide three,
    the consumer may not know how to handle this and may throw an error or, worse
    still, simply crash. You could, of course, add some defensive code, but in reality,
    this is just adding more complexity due to lazy change-management.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Most software platforms have many dependencies, which means it is sometimes
    very difficult to spot which *provider* has changed and is causing one of the
    many consumers to fail—especially when you consider that a consumer may also be
    a provider to another consumer higher up the stack (that is, a shared library,
    which consumes from a database and then *provides* said data to a web page, which
    then consumes it and provides that data to a JavaScript client, and so on).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: To understand how prevalent this situation is, you'll need to do some impact
    analysis that will help you map this out. Be forewarned that unless you can map
    out your entire platform into one easy-to-understand format that is consistently
    up to date, it's going to be a difficult task. Luckily, there are many mature
    and established patterns that cover these sorts of problems, as well as tools
    that will help with the analysis.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: What can also help is to establish some rules around how changes are approached
    going forward. In simple terms, if you are making a change to software, config,
    or a database which will be consumed by another part of the platform, it's the
    responsibility of the person making the change to validate that the change has
    not broken anything up/downstream. If you have CI and test-automation in place,
    then that can help spot issues early. However, simply adding some diligence to
    the code-review/pull-request process is cheap and easy, and can help cement good
    behaviors.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Within the system operations area, the "never break your consumer" rule should
    also apply. For example, the software platform could be classed as a consumer
    of the server operating system (the provider); therefore, if you change or upgrade
    the operating system, you must ensure that there are no breaking changes that
    will cause the consumer to fail.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes breaking changes cannot be avoided (for example, the service contract
    between components has to change to accommodate new functionality). However, this
    should be the exception rather than the rule, and you should have a strategy planned
    to cater for this. An example strategy would be to accommodate side-by-side versioning,
    which will allow you to run more than one version of a software asset at the same
    time—something we'll cover later.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: There may be times when the consumer/provider relationship fails as the person
    or team working on the provider is unaware of the relationship. This can be very
    true of providers within the system operations area. To overcome this, or at least
    minimize the risk, use open and honest peer-working practices.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Open and honest peer-working practices
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many different agile software-delivery methodologies in use today,
    but all of them revolve around some form of highly-collaborative ways of working
    and free-flowing communication. Agile approaches such as **XP**, pairing, or a
    simple code-review process, all depend on engineers working closely together.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: I cannot stress enough the importance of sharing your work with others. Even
    the best software engineers (or system admins) on the planet are human and they
    **will** make mistakes. If you think your code is precious and don't want to share
    it with anyone else, you **will** create defects and it **will** take longer to
    overcome small mistakes, which can cause hours of head-scratching, or worse, have
    an adverse impact on your customers.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: If you are confident that your code is of the highest quality and can stand
    up to scrutiny, then do not hide it away—put your money where your mouth is and
    share your work. If you are not that confident, sharing with your peer group will
    help to build that confidence. One thing to point out here—in terms of software
    engineers—is that the peer group should not be exclusively made up of other software
    engineers; the operations team can (and should) also be included in this process.
    It may seem strange, as they may not be able to actually read your code (although
    you may be surprised how many system admins can read code), but they know how
    the live platform operates and may be able to provide some valuable input and/or
    ask some pertinent questions (for example, what the code will do if there's a
    network blip, how long-lived threads will be, or why connection-polling isn't
    being used). This also encourages the DevOps mindset and approach. The same rule
    should apply to changes made by the Ops team.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'All things considered, the majority of the world''s highest-quality software
    is built in a highly-collaborative way, so there are no excuses for you not to
    be doing the same. Some purists may scoff at this approach, but consider this:
    most of these types will most probably sing the praises of Linux-based operating
    systems, which, if they actually thought about it, is, like most open source software,
    written using highly-collaborative approaches that have been part of the development
    process from day one.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Having an open, honest, and transparent peer-review process is as important
    within an Operations team as it is within a Development team. Changes made to
    any part of the platform run a risk, and having more than one pair of eyes to
    review will help reduce this risk. As with software code, there is no reason not
    to share system configuration changes.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: One normally unforeseen advantage of working in this way is the fact that if
    your code (or configuration change) fails to get through peer review, the impact
    on the production system is negated. It's all about failing fast rather than waiting
    to put out something live to find it fails.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume you have seen the light and have decided to move to a loosely-coupled
    component-based architecture that has been written using best practices and you're
    ready to move to the next stage in your software engineering evolution.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you have everyone working together and playing nicely, we''ll move on to
    the next challenge: how the expectations of the wider business need to be realigned
    in terms of release and feature delivery.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Incremental delivery of features
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier, we looked at breaking work down into small incremental chunks so that
    you can deliver and release quickly. You also need to consider how you deal with
    features. What I'm referring to here is the business-driven deliverables that
    turn into revenue. Typically, you'll have a year-long business plan that is represented
    by a number of key initiatives that need to be delivered within that year, and
    these are further broken down into a selection of features, which is what will
    be marketed and sold to customers. This is pretty normal in terms of business
    process.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Please note that terms used within your business may differ and you may use
    terms such as epics, or themes, or goals, or MVP. In essence, we'll focus on the
    relationship between delivering a thing your business can make money from and
    the point in time when that thing becomes available to customers. To keep things
    simple, I'll refer to the thing as a feature and the point in time as a release.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'How the adoption of CD and DevOps can impact this does depend on the current
    release cadence, but I would hazard a guess that you''ll be looking at a cadence
    in months or quarters. Once CD and DevOps are embedded, you''ll be looking at
    weeks, days, or hours between each release. This can only be a good thing, but
    let''s take a moment to consider the wider impact:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: The wider business would currently be expecting a feature to be delivered in
    its entirety within a release cycle
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Business functions, such as sales, marketing, legal, and support, will have
    processes in place to cater for this
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will be vastly reducing the release cycle and incrementally delivering changes
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How should the wider business cater for this? When will the feature be ready?
    When should they start the wheels turning? What should they say to customers?
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'What you need to do is work with these business areas and come to an agreement
    in terms of how features can be incrementally delivered over a number of releases.
    There are a few approaches you should consider and discuss:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Deliver the end-to-end experience in slices and build up the richness of the
    functionality over a number of releases until the feature is complete
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focus on an area of functionality through to completion, then move onto to the
    next, then the next, until the feature is complete
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incrementally build the feature over a number of releases but keep it hidden
    until it's completed
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Something to consider is approaches such as the first and second could open
    up avenues such as alpha/beta releases, which means that you start to get customer
    feedback early on, whereas approaches such as the last one mean you don't get
    early feedback but the go-live is relatively painless (you've already shipped
    the code so go-live is really switch-it-on). Whatever approach you choose—and
    you will need to choose one—you need to ensure that those expecting "release equals
    feature delivery" are educated and their expectations are realigned.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll now move back into a more technical area: ensuring you are deploying
    the same software throughout your environments.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Using the same binary across all environments
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before a software asset can be used in a given environment, it has to have been
    built/compiled into an executable or a binary. This binary is important, as it
    is the version of the software that will be executed at runtime within your environments.
    Think of it as a snapshot in time. Some would say that the source code is more
    important than the binary object itself as the binary is simply a byproduct and
    can be recreated time and time again, although that's not strictly true.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: You will not be running functional, regression, performance, or load tests against
    the source code. You will be doing so against the binary. It's therefore important
    that the resulting binary is treated with as much reverence as the source code
    from which it was created. This becomes more important if you're looking at side-by-side
    versioning and/or baking in versions during the CI process. For example, if your
    CI solution creates version 1.2.0.1 of the binary, then it's version 1.2.0.1 you
    should be using and testing.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: The ideal, and recommended, approach is that the binary is only built once for
    a given release/deployment and that the self-same unchanged binary is used in
    **all** environments, including production. This may sound like common sense,
    but sometimes this is overlooked or simply cannot be done due to software design
    and/or tooling, or, more worryingly, it's not seen as important.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: One example of tooling/software design limitations would come in the form of
    software tokens or config related to the environment (sometimes referred to as
    secrets). Let's take the credentials for a database server, for instance. Some
    would say that because this data is very sensitive—especially in higher environments
    such as production—it should be hidden away from all but a select few. One way
    around this is to *bake* this information into the binary itself at compile-time,
    which makes it secure. This is all well and good, but we only want to build it
    once, and therefore you would have to have the same credentials set up in all
    environments, including the completely open development environment—far from secure,
    I think you'll agree. Another drawback to this approach is the fact that someone
    could reverse-engineer the binary and get hold of the credentials without you
    knowing. Also, how would you change said credentials should they get leaked and
    need changing?
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: You could always build multiple copies of the binary (one for each environment);
    however, you're back to testing different versions of the software.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: There are many industry-standard approaches to this problem, but the simple
    approach (which seems to work well for a vast amount of businesses) is to have
    this kind of data held in a startup script or system configuration file (which,
    of course, is under version-control) and have the software load it up at runtime.
    If you restrict access to these configs files/scripts, you have a good chance
    of keeping them *secret.* Whatever approach you choose, you should ensure that
    it allows you to use the same binary.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: As previously mentioned, use of a binary repository will also allow you to store
    multiple versions of a given binary, which means a rollback to the previous version
    is pretty painless.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've looked at how to deliver the software to each environment, let's
    see how many environments you need.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: How many environments is enough?
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This question has been around since software development became a thing. Unfortunately,
    there is no simple answer, although there are some common-sense and tried-tested-and-trusted
    approaches that have worked over the years. When I talk about environments, I'm
    not just referring to servers here; I'm referring to the servers, infrastructure,
    network, firewalls, first- and third-party integration, and so on. In essence,
    everything you need to run one copy of your software platform.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to the question at hand, the (rather underwhelming) answer is: the
    number of environments you **need** depends on your ways of working, your engineering
    setup, and, of course, the architecture of your platform. There''s also another
    factor to consider: the overhead to manage and look after multiple environments
    along with the cost of keeping them running and healthy. Suffice it to say that
    you should not go overboard; try to work to a *less-is-more* approach where you
    can.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'There may also be a temptation to have environments set up for different scenarios:
    development, functional testing, regression testing, user acceptance testing,
    performance testing, and load testing. If you are able to ensure all the environments
    can be kept up to date (including the all-important test data), can easily deploy
    to them, and, more importantly, **need** all of them, then this may be viable.
    The reality is that having too many environments can actually be counterproductive
    and can cause far too much noise and overhead.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'The ideal number of environments is two:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: One for development
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One for production
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This may sound like an accident waiting to happen, but if you think about it,
    many small businesses and start-ups manage fine with such a setup. What you'll
    find is that as a business grows, so does the need to be risk-averse, and hence
    the potential for multiple environments.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'When ACME systems started out, two environments were sufficient. As they grew,
    so did the need for more environments, and they ended up with multiple development
    environments (one for each engineering team), an integration environment, a performance-testing
    environment, a load-testing environment, a pre-live deployment staging environment,
    and, of course, production environments. They also ended up with an entire team
    of people whose job was to keep these all running—actually, they ended up with
    two: one to look after the engineering and testing environments, and one to look
    after the production environments. Far from ideal or effective.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: You may think that with virtualization technologies (including cloud-based)
    now in a highly-mature state and used by anyone and everyone, setting up and running
    hundreds of servers, is not as much as an overhead as it once was. There is truth
    in that thinking, but it's the challenge of keeping everything in line that is
    massive—versions of software, O/S patch levels, network configuration, firewall
    configurations, and so on. Therefore, virtualization can help in some ways, but
    the *how many environments* question still remains.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'Whatever you decide, there may be a fly in the ointment: what if your production
    environments are locked away in a highly-secure datacenter to which you have little
    or no access, or, worse still, fully managed by a third party? This can have a
    massive impact on your *less-is-more* approach. If this is the case, then you
    really need to get those managing said environments closely looped into what you''re
    trying to do—if you don''t, it can derail your DevOps adoption.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s move on to a real-world example and see how ACME systems approached
    this. When they reviewed the environments **needed** for CD and DevOps, they settled
    on the following as being sufficient for their needs:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '**Development environments**: Cut-down versions of the platform with only a
    few other platform components that were needed for local testing'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CI environment**: The place where the software is built and all automated
    tests are run on a regular basis'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pre-production environment**: Used for the occasional spot check/UAT (occasional
    being the operative word)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Production environment**: This is where all the action takes place'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram depicts the environments used:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94632199-b701-4c63-bfb6-2a128075a02c.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: ACME systems 3.0 environment setup
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, this follows the less-is-more approach and allows for enough
    quality gates to ensure a given change was sufficient. When combined with all
    of the aforementioned engineering best practices (high levels of test coverage,
    build automation, CI tooling, and so on), the speed at which a given change could
    be delivered was minutes.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: OK, so this is a bit of a utopia and you may be quite some distance from this
    now, but hopefully you can see how simple it can be, and hopefully you're slightly
    closer to answering the *how many environments* question.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Let's now have a look at another possible environment-related solution that
    can help speed up your delivery capabilities.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Developing against a like-live environment
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many ways to ensure one version of a software binary works, or integrates,
    with other parts of your platform, but by far the easiest is to actually develop
    against an environment that contains live versions of your platform.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: On paper, this may look like a strange statement, but if you think about it,
    you're making a change to one part of your overall platform and—as is the CD way—you
    want to validate and ship that change as soon as possible. What this also gives
    you is the ability to ensure that the dependencies you expect to be available
    within the production environment are actually there and function as you expect,
    along with the configuration and infrastructure.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: This approach will give the most value when used in conjunction with component-based
    architecture, but some aspects will apply to legacy platforms as well.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: The simplest approach would be to develop against the production environment,
    but this is very risky and the possibility that you could cause outage—albeit
    inadvertently—is quite high. There's also the security/access issues. The next
    best thing is, therefore, to have another like-live environment set up, which
    contains the versions of code that are running in the production environment.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'You may be thinking that developing against a *like-live* environment is somewhat
    overkill, and you may be wondering why not simply develop against the versions
    of software that reside in the CI environment. There is a simple answer: you have
    no firm idea which of the changed binaries in the CI environment will be live
    before you. For example, if you are developing and testing against version 1.2.0.3
    of the authentication component (to pick a name out of the air), and when your
    binary hits production and starts to talk to version 1.2.0.1, you may experience
    issues that you didn''t see during the development/testing phase.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: This is especially true if someone is testing out a breaking change where you
    need to ensure that you have covered all scenarios **before** you release it to
    production.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: This like-live environment only needs to be *like-live* in terms of software
    (and infrastructure) versions. If you can populate it with live data, that would
    be good, but the reality is that you would need something as big as production
    in terms of storage and so on, which is costly. Saying nothing of the risk of
    exposing confidential data and breaching data protection rules and regulation,
    such as GDPR—unless you have a way to redact confidential data (which is a whole
    different challenge).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'To give you a flavor of how this could work, the following diagram gives an
    overview of how ACME systems implemented such a setup:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4958468-5227-4273-9e42-4c0fa496f940.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: The like-live environment used by ACME systems 3.0
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the like-live environment is tagged onto the end of the deployment
    pipeline. It is on the end for a reason: you only want to deploy to this environment
    once the deployment to production is successful.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that when we talk about a like-live environment, this need
    not be a physical set of servers. You could look at virtualization (cloud or desktop-based),
    whereby you can pretty much spin up a copy of your production environment on a
    developer's workstation (on the presumption that there's enough horse power and
    storage available).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Now that you are starting to get all of the building blocks in place to realize
    your goal of adopting CD and DevOps, we have a few additional blocks you'll need,
    those being how you actually take the fully-built and tested software component
    through the environments in a controlled, reliable, and repeatable way. This is
    where CD tooling comes into play.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: CD and DevOps tooling
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is another collection of tools that may not be as readily available to
    you as the aforementioned tools (automated build and testing, CI, and so on).
    These are the tools that you will use to control and orchestrate the entire software-delivery
    life cycle from building the binaries (via CI), deploying said binaries to the
    various test environments, and if all goes well, pushing the same binary to production.
    At a very simple level, these tools act as workflow engines wherein each step
    is defined to do a specific action, and then the flow moves on to the next tasks.
    They also have basic logic built in to catch exceptions during the flow (for example,
    if tests fail, then don't go any further and send a notification). This workflow
    analogy is normally referred to as the CD pipeline, delivery pipeline, or just
    pipeline.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Over the past few years, the CD and DevOps tooling market has grown from almost
    nothing to a full-blown multi-million-dollar global business. There are now a
    plethora of tools and vendors wanting to sell tools to you, and it's become quite
    difficult to choose the one (or two) that will fit your needs. Just like any tool
    or technology you use within your software development life cycle, you need CD
    and DevOps tools that will be reliable, help more than hinder, and will grow with
    you as your adoption matures. I would also hazard a guess that you will already
    have some tooling that manages your software delivery/deployment, so you may need
    to look at something that will either integrate with the existing tooling or replace
    it completely.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: The tooling you choose will be used day in and day out and will be heavily relied
    upon, so you had better make sure it fits your needs and will be there when it's
    needed. To assist in this, you could use something similar to the following, which
    could assist during your tool/vendor selection and due diligence.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some example questions you should ask of your CD and DevOps
    tooling/vendor:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Can it deploy the same binary to multiple environments?
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can it access the binary and source repositories we're using?
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can it remotely invoke and control the installation process on the server it's
    been deployed to?
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Is there functionality to allow it to orchestrate my current tooling?
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is it capable of deploying database changes?
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does it have functionality to allow for the queuing up of releases?
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can it run parallel pipelines?
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does it contain an audit of what has been deployed, when, and by whom?
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is it secure?
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How is it hosted (SaaS, PaaS, on-premise, and so on)?
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can it interact with the infrastructure to allow for no-downtime deployments?
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can it/could it orchestrate automated infrastructure-provisioning?
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can it interact with other systems and solutions, such as email, collaboration
    tools, change-management, issue-management, and project-management solutions?
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does it have simple and easy-to-understand dashboards that can be displayed
    on big screens around the office?
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can it interact with and/or orchestrate CI solutions (our current solution and
    other industry leaders)?
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will it grow with our needs?
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What skills/experience do we need to run this?
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is it simple enough for anyone and everyone to use?
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What support/SLA do we get?
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What set up/implementation support do we get included in the price?
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What about HA/failover?
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the process for upgrading the tooling itself?
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At this point, it would be very helpful if I just listed most of the market-leading
    tools and gave you an idea of their pros and cons. However, depending on when
    you read this, that information could be very out of date. This is really something
    you need to be doing yourself anyway—you know your needs, problems to solve, and
    budget better than I. It should go without saying, but I''ll mention it just in
    case: the tooling choice should be done in true DevOps style, with both Dev and
    Ops heavily and equally involved.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few things you should take into account when selecting CD and DevOps
    tooling: don''t skimp on budget as the tooling will become the backbone of your
    delivery pipeline; don''t just stick with the big boys in the marketplace, as
    they may be too constraining in the long run; and if there are gaps you can fill
    with bespoke solutions, you should seriously consider that without creating another
    legacy to look after.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed that one of the considerations noted here is automated
    provisioning. Let's now look into what this means.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Automated provisioning
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The norm over the past few years has been to move from traditional tin-and-string
    physical servers and infrastructure to virtualized equivalents, be that on-premise,
    datacenter-hosted, or cloud-based. I won''t dwell too much on the advantages of
    one over the other—again, there''s plenty of rich information available should
    you wish to read up—but I will focus on one element that is not always front and
    center when planning to move to a virtualized environment solution: the ability
    to use automated provisioning as part of your CD and DevOps pipeline.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning is nothing new; as long as cloud providers have been a thing, they
    have been providing their customers with cloud-based virtualized servers that
    can be provisioned pretty much as and when needed. One also has the freedom in
    defining the configuration and setup of the servers in terms of horsepower, storage,
    networking, operating system, and location/region. In addition to this, when the
    servers are no longer needed, they can then be deleted (sometimes referred to
    as teared down).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Now consider how useful it would be to have automated provisioning as a step
    within your CD pipeline. You would then have the ability to not only control and
    orchestrate the software-delivery life cycle, but you can also create environments
    on the fly, install your software, run your tests, and then tear it all down.
    The massive advantages this gives you are predictability and repeatability. If
    you can guarantee that **every time** you initiate the CD pipeline you will have
    exactly the same vanilla environment created from scratch, then you can pretty
    much eliminate what some like to call environmental issues—something we'll be
    looking at later—which continually cause noise and false negatives (or positives)
    within the testing step.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: As you would expect, there are many industry buzzwords floating around to complicate
    this sort of activity—the most common ones being **Infrastructure-as-a-Service**
    (**IaaS**) and PaaS—but what it boils down to is being able to programmatically
    interface with a provisioning system, tell it what you want in terms of spec,
    configuration, and so on, and get it to spit one out the other end. When you're
    done, you programmatically interface again and get the environment removed.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: The list of requirements fed into a provisioning system that defines the server
    spec, CPUs, GPUs, RAM, storage, and so on is normally referred to as the recipe—there
    are some variations depending on the tools, but they are all pretty much the same
    thing.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Something to consider with automated provisioning is the time lag you may encounter
    within the CD pipeline. For example, if you were to take a binary and deploy it
    to a prebuilt server, then the time taken is simply the act of deploying. Add
    automated provisioning into the mix and the CD pipeline will have to wait for
    the new virtual server(s) to be provisioned before you can deploy your binaries.
    What you need to weigh up is the importance of quality, repeatability, and predictability
    over speed and convenience. Just because something is faster doesn't make it better.
    What you can do to overcome this is to pre-bake some vanilla virtual server images
    that can be added to the environment via the automated provisioning tooling as
    part of the CD pipeline. That way, you have a fresh virtual server in a fraction
    of the time. In fact, this is how many leading cloud providers operate. There
    is an overhead with this approach; someone needs to keep these vanilla virtual
    server images fresh and updated, with operating-system patch levels being the
    pain they are. Again, you need to weigh up the pros and cons.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: One massive advantage of automated provisioning is blue/green deployments. Strictly
    speaking, this approach was around before automated provisioning became mainstream,
    but automated provisioning has made it far easier to realize. I won't go into
    too much detail—again that's homework for you. However, in simple terms, blue/green
    deployments allow you to provision a new server with the latest version of software
    or configuration changes or database updates offline within the environment, then
    switch old for new via a small network/routing change. Essentially, you do the
    hard work and prep upfront, and the release simply becomes the switchover. It's
    very effective and quick, and allows for near-instant rollback if problems are
    found (for example, switching back from new to old). I would highly recommend
    adding this to your reading list.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: You may be thinking that the automated provisioning approach will only work
    for environments that are used for development and testing, but this self-same
    approach can (and should) be considered for your production environment. After
    all, if you have set up a CD pipeline, what's the point (or value) of stopping
    short of the goal. I wouldn't recommend doing this on day one; you need to build
    up your confidence in the tooling and iron out any kinks before taking the plunge.
    Based upon experience, I can guarantee that once you have automated provisioning
    in place, you will not look back.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'Another massive but little-mentioned benefit of automated provisioning is the
    ability to overcome the bane of most IT departments and software houses around
    the globe: having to take your production system offline to upgrade it.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: No-downtime deployments
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the things that comes with large releases of software (legacy or otherwise)
    is the unforgivable need to take the production platform offline to upgrade it.
    Yes, I did say unforgivable, because that''s exactly what it is. It is also wholly
    avoidable. It''s the IT equivalent to having an out-of-order sign on the elevator:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff4dd7d1-569e-4098-b05e-3ed44eb0607e.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
- en: 'There are two simple reasons for taking a production system offline to upgrade
    it: there are far too many unreleased changes that have been bundled up together,
    or you don''t trust the quality of the software being delivered. Both of these
    can be overcome through CD and DevOps adoption.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider you are operating a real-time online service and you inform your
    customers that you have to take the system offline for an upgrade. You can bet
    a pretty penny that your customers will not take kindly to not being able to access
    your system (or more importantly, their data) for a few hours so that you can
    upgrade some parts of it. To minimize the impact, you will no doubt schedule the
    upgrade out-of-hours, which means you'll need to have people on-call to support
    the upgrade—but being out-of-hours, I doubt you'll have all of the engineers that
    made the changes available on the night, so you already have a risk.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'One important thing in relation to out-of-hours is this: unless you are running
    a B2B solution and have customers in the same time zone, you may struggle to find
    a suitable out-of-hours time window. For example, if your solution and business
    is B2C, you''re pretty much offering a 24/7 solution, so unless you know for sure
    when your consumers sleep, finding the time window is going to be tough. If your
    customers are global, you will find it even harder to find a suitable window.
    You will no doubt have included something in your terms of service and/or contracts
    to cater for taking the live platform offline, but this amounts to admitting to
    your customers that your business processes are inadequate.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: If you also consider how many news stories are reported on a regular basis regarding
    major issues following a massive down-time IT upgrade, there is also a very strong
    possibility that your customers will look upon this big-bang approach with distrust
    as they'll be pretty sure something will go wrong once the service is up and running
    again. This distrust will be amplified if you go beyond the time window. From
    experience, I can say with confidence that something will go wrong, and depending
    on the severity, you will have to quickly move into damage-limitation mode to
    keep customers happy, T&Cs and contracts aside. It might even get to the stage
    where your customers may shop around to find a competitor who does not need planned
    downtime.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: OK, so this is a bit doom and gloom, but that is the stark reality. Customers
    and consumers don't care about your process problems or complexities in your SDLC;
    they have become accustomed to having the IT services they use on a daily basis
    being available when they need them. Try to remember when one of the major search
    engines or social media platforms was offline. When release issues happen, it's
    now extremely difficult to contain the bad news, especially with the prevalence
    of social media, 24-hour news, and the like. You have to remember that bad news
    travels faster than anything else known to man, so the last thing you need is
    bad news generated because of a release.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: The ultimate goal for CD and DevOps adoption is to repeatedly deliver value
    as quickly, consistently, and reliably as possible. Removing the need for down-time
    deployments completely is a massive bonus for you.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: One thing to point out which may not be obvious is that it's not just the production
    environment that should have maximum uptime. Any environment that you are reliant
    on for your development, testing, and CD should be treated the same. If the like-live
    environment is down, how are you going to develop? If your CI environment is down,
    how are you going to integrate and test? The same rules should apply across the
    board—without exception.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Previously, we covered open and honest ways of working as part of engineering
    best practices. Openness and honesty are just as important when it comes to CD.
    A good way of providing this level of transparency is to monitor everything and
    have it available to all.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Monitor, monitor, monitor
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most important ways to ensure whether CD and DevOps is working is
    to monitor, monitor, and then monitor some more. If all of the environments used
    within the CD process are constantly being observed, then the impact of any change
    (big or small) is easy to see—in other words, there should be no hidden surprises.
    A simple rule of thumb here: if it moves, monitor it.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: If you have good coverage in terms of monitoring, you have much more transparency
    across the board. There is no reason why monitoring should be restricted to the
    operations teams; everyone in the business should be able to see and understand
    how any environment—especially the production platform—is performing and what
    it is doing.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: There are plenty of mature and industry-standard monitoring tools available,
    but it can be quite difficult to get a single view that is consistent and meaningful.
    For example, some tools are geared up for monitoring infrastructure and servers,
    whereas others are geared up for collecting application metrics, and still others
    for measuring application and/or database performance. Unless you can tie this
    data together into a coherent view, things will look disjointed. Ideally, you
    should try to aggregate the data from these tools—or at least try to integrate
    them—and present a unified view of how any given environment and the software/services
    running within are coping and functioning.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: You will be surprised how much very valuable data you can get and how it can
    direct your engineering work, as the engineers can see exactly how their software
    or infrastructure is behaving in real time with real users.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37e01040-01d1-459e-87ad-cc0c13f29f0b.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
- en: If it moves, monitor it. If it doesn't, monitor it just in case
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring is a must for CD and DevOps to work correctly, as things will be
    continually changing (software, services, infrastructure, and so on), and both
    halves of the Dev and Ops relationship will need to see what is going on and assist
    when/if problems occur.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Another, less obvious, positive that monitoring can bring you is proof that
    CD is not having an adverse impact on the production platform. If you're using
    some *graph-over-time* solution, you can get your CD tools to add a *spike* or
    a marker to the graph when a deployment takes place. You can then visually see
    the impact (or not) of the change.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have mainly focused on technical solutions and tools for the adoption
    of CD and DevOps. These solutions and tools may help to provide you with much
    of what you need in your toolbox. However, there is still room for simple manual
    processes.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: When a simple manual process is also an effective tool
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even if you have enough tooling to shake a stick at, you will no doubt have
    some small and niggling challenges that cannot be overcome with tooling and automation
    alone. To be honest, tooling and automation can be overkill in some respects,
    and can actually create barriers between certain parts of the organization you
    are trying so hard to bring together—here, I am talking about the Dev and Ops
    partnership that forms DevOps.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: If tooling and automation completely negate the need for human interaction and
    discussion, you may well end up back where you started. You may also find that
    it is almost impossible to automate your way out of a simple problem.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Let's take, for example, the thorny issue of dependency-management. As a software
    platform matures, many interdependencies will form. If you are deploying your
    code using a CD process, these many interdependencies become ever-moving targets
    wherein components are being developed and deployed at different rates. You can
    try to capture this within your CI process, but something somewhere might be missed
    and you could end up inadvertently bringing down the entire platform because component
    B was deployed before component A.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: You can try to map this out and build into the tooling rules to restrict, or
    at least minimize, these moving targets, but the rules may end up being more complex
    than the original dependencies. Or you could simply agree on a process whereby
    only one change happens at any given point in time. To feed into this, you can
    implement a simple queuing mechanism written on a whiteboard and reviewed regularly
    by all of the engineering and Operations teams.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach worked extremely well for ACME systems. The following is what
    they did:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: They obtained a blanket agreement from everyone that only one change would go
    through to production at any given point in time. They called this a deployment
    transaction.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To highlight the fact that someone was making a change to production (either
    a deployment or operational change), that person held the production environment
    token, which was in the form of a plush toy animal and was given the name the
    build badger. If you had the build badger, you were changing production.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They implemented a simple prioritized queue system using a whiteboard and a
    pen. Each morning, whoever wanted to make a deployment would come along to the
    deployment stand-up and agree with everyone there the order in which deployments
    (or changes) would be made that day.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Screens were installed throughout the office (not just the Dev and Ops areas),
    showing a real-time dashboard of what was going on.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All very simple, but what this gave ACME systems was a way to overcome dependency
    hell (for example, if they could only change one thing at a time, there was an
    implied logical order of which change went before another) and built a sense of
    collaboration throughout all of the teams involved.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram should give you some idea of what the deployment transaction
    covered, in terms of a deployment:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f685d47-931c-4d08-a318-601aed762973.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
- en: The deployment transaction
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Eventually, ACME managed to engineer out the dependencies that plagued them
    in the early days so this manual process could be decommissioned, although it
    helped them keep moving with their CD and DevOps adoption.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'Other very simple manual solutions you can use could include the following:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Use collaborative tools for real-time communication between everyone and integrate
    this into your CD tooling so that deployments are announced and can be followed
    by all.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your management is uneasy about having developers deploy to production without
    involving the Operations team, make sure you have a DevOps team doing the release.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If instant rollback is needed should a deployment fail, look into simple ways
    of rolling back, such as simply deploying the previous version of the component
    using the CD tooling.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consistently inspect and adapt through regular retrospectives to see what is
    working and what is not.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can tell, it's not all about technical solutions. If simple manual processes
    or tweaks to the ways of working are sufficient, then why bother trying to automate
    them?
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: And so ends the lesson—for now. Let's recap what we have covered.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As stated at the beginning of this chapter was a lot to cover and a lot to take
    in. Some of it is relevant to you now and some of it will be relevant for the
    future.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: At this point you should have a greater understanding and appreciation for how
    agile engineering best practices (including use of source control, CI, incremental
    delivery, test automation, failing fast) along with modern architectural approaches,
    delivery methods and in-depth monitoring will ease your CD and DevOps adoption.
    Above all you should have learned that it's not all about technical tools and
    techniques, sometimes simple processes can solve problems.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: We'll now move on from ways to advance your CD and DevOps adoption to the kinds
    of issues which will trip you up along the way, how to spot them and how to get
    passed them.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
