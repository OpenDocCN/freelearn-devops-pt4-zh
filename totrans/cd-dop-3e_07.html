<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Vital Measurements</h1>
                </header>
            
            <article>
                
<p>Over the previous chapters, we have looked at what tools and techniques you should be considering, the need to acknowledge how change will impact people in different ways, why culture, behaviors, and environment are important, what potential hurdles you'll need to overcome, and how all of this is needed to successfully adopt CD and DevOps. If you are taking this into account, creating plans to cater for and/or address this, you should be in a good shape to make wide strides forward.</p>
<p>We will now look at the important but sometimes overlooked<span>—</span>or simply dismissed<span>—</span>area of monitoring and measuring progress. We did touch on this subject previously, but what we considered was a small slice of the pie<span>, </span>figuratively speaking. What we're looking at now is the capturing, compiling, and sharing of metrics related to the impact that CD and DevOps has on the day-to-day ways of working and the business as a whole.</p>
<p>This, on the face of it, might be seen as something that is only useful to the management types and won't add value to those who will be dealing with the CD and DevOps adoption on a day-to-day basis. In some regards, that is true, but being able to analyze, understand, and share demonstrable progress will definitely add value to you and everyone else who is on the CD and DevOps journey. We're not just talking about simple project management charts, graphs, and PowerPoint fodder here; what we are looking at is measuring as many aspects of the overall process as possible. That way, anyone and everyone can plainly see and understand how far you have collectively come and how far from the ultimate goal you are.</p>
<p>To do this effectively, you'll need to ensure that you start this data capture very early into the CD and DevOps adoption, as it will be very difficult to see a comparison between then and now if you don't have data representing <em>then</em>. You will need to be vigilant and consistent in ensuring that you are continuously capturing these measurements so that you can compare the state of progress at different points in time. Some would consider this anal, but this whole CD and DevOps journey started because the data captured in the elephant exposure pointed to areas of waste<span>—</span>or, at the very least, ineffective processes.</p>
<p>In this chapter, you will learn the following topics:</p>
<ul>
<li>How to measure the effectiveness of your engineering process(es)</li>
<li>How to measure the stability of the various environments you use and rely on</li>
<li>How to measure the impact your adoption of CD and DevOps is having</li>
</ul>
<p>We'll start, as they say, at the beginning and focus initially on engineering metrics.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Measuring effective engineering best practices</h1>
                </header>
            
            <article>
                
<p>This is quite a weird concept to get your head around: How can you measure effective engineering, and more than that, how can you measure best practices? There's another often-asked question: what has this got to do with DevOps or CD? We'll look at the former in a moment, but now let's focus on the latter.</p>
<p>Let's take two scenarios:</p>
<ul>
<li>Your current software-engineering process is very waterfall and you have a vast amount of manual testing to validate your code just before it gets shipped<span>—</span>which happens every 3-6 months<span>—</span>and build in a buffer for bug fixing</li>
<li>Your current software-engineering process is pretty agile and follows (mostly) industry best practices, however as there is plenty of time between releases you can sometimes let technical debt slip (including test automation) as there will be time to go back and mop up just before the next release<span>—</span>which happens every 3-6 months</li>
</ul>
<p>OK, so this is pretty simplistic, but bear with me. As the CD and DevOps adoption starts to gather momentum, the time between releases will decrease. Therefore, the we can do that later window gets smaller and smaller. This can lead to engineers having to start cutting corners simply because they have run out of time to mop up the pre-release tech-debt tasks. The adoption of CD and DevOps ultimately allows you to deliver solutions quickly<span>—</span>there's nothing that categorically states that engineers will be given more time to write and test said solutions.</p>
<p>Let's consider what a large quarterly release looks like in terms of timeline and effort, as shown:</p>
<div class="CDPAlignLeft CDPAlign"><img src="assets/03651732-d728-43e1-aa7c-8725be322b3f.png" style=""/></div>
<p>Now let's compare that to a CD-type release, as follows:</p>
<div class="CDPAlignLeft CDPAlign"><img src="assets/192a5c25-d72b-4737-8cfe-131a500e1422.png" style=""/></div>
<p>These are both very simplistic examples, but they highlight the impact that reducing the time between releases will have on the key players. The we can do that later window goes from days/weeks to hours.</p>
<p>In <a href="6a4f746d-b386-49b3-b82b-154ff1f604e8.xhtml"/><a href="6a4f746d-b386-49b3-b82b-154ff1f604e8.xhtml">Chapter 5</a><em>,</em> <em>Approaches, Tools, and Techniques</em>, we looked at how the wider business perceives the relationship between features and releases. As your CD and DevOps adoption matures, the time between releases will decrease, which means that engineers will have less time to complete features. If the wider business has become accustomed to having features delivered within a given release, they will continue to expect this until things bed in.</p>
<p>Let's go back to the corners. These will normally be related to the non-cutting code, yet still time-consuming activities<span>—</span>skipping the odd unit test here, leaving the odd gap in integration tests there, forgoing documentation, reducing the tendency to refactor old code, and so on. In simple terms, the engineers will be under pressure to deliver and they will no longer have the time to address everything they did previously. This therefore becomes technical debt—which is something every software-engineering team tries to avoid at all costs, as it will come back to bite them later.</p>
<p>Going back to the main subject of measuring effective engineering best practices, it's not as strange or uncommon as you might think. There are a great number of software-based businesses around the globe regularly using tools to capture data and measurements for things such as:</p>
<ul>
<li><span>Overall code quality</span></li>
<li>Adherence to coding rules and standards</li>
<li>Code documentation</li>
<li>Code complexity</li>
<li>Code duplication</li>
<li>Redundant code</li>
<li>Unit-test coverage</li>
<li>Technical debt</li>
<li>Mean time between failures</li>
<li>Mean time to resolution</li>
<li>Bug-escape distance</li>
<li>Fix-bounce rate</li>
</ul>
<p>Measuring each of these in isolation might not bring a vast amount of value; however, when pooled together, you can get a very detailed picture of how things stand. In addition, if you can continuously capture this level of detail over a period of time, you can then start to measure and report on progress. Why this is important to the adoption of CD and DevOps is quite simple: if the quality of your software decreases due to the fact that things are being shipped faster, the laggards will have a field day. If those laggards are in influential and/or decision-making positions, the whole adoption could be derailed.</p>
<p>As stated previously, if you can spot this as it starts to happen, you have a fighting chance of stopping it. There is also another side to this; if your quality currently sucks and you can prove that CD and DevOps adoption helps to increase quality, then that's a massive good news story<span>—</span>we can ship quicker and the quality is vastly better. Take that, laggards!</p>
<p>It all sounds very simple, and to be honest, it can be, but you need to be mindful of the fact that you will need to apply some time, effort, and rigor to ensure that you gain the most value. There will also be a degree of trial and error and tweaking as you go, to ensure you can capture the data in a reliable and repeatable way<span>—</span>more inspecting and adapting<span>—</span>so you need to ensure that you factor this in. Not only will these sorts of measurements help your engineering team(s), they will also help with building trust across the wider business. For example, you'll be able to provide open, honest, and truthful metrics in relation to the quality of your software, which, in turn, will reinforce the trust they have in the team(s) building and looking after the platform.</p>
<p>One thing to seriously consider before you look at measuring things such as software code metrics is how the engineers themselves will feel about this. What Devina is thinking might be a typical reaction:</p>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/1c570634-a501-428f-b725-635fd0fb9e24.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">A typical reaction to this approach</div>
<p>Some engineers will become guarded or defensive, and may see it as questioning their skills and craftsmanship in relation to creating quality code. You need to be careful that you don't get barriers put up between you and the engineering teams or let them slip back into the <em>laggards</em> camp. You should <em>sell</em> these tools as a positive benefit for the engineers. For example, they have a way to definitively prove how good their code actually is; they can use the tools to inspect areas of over-complexity or areas of code that are more at risk of containing bugs; they can highlight redundant code and remove it from the codebase; and they can visually see hard dependencies, which can help when looking at componentization.</p>
<p>If you have vocal laggards, get them actively involved in the setup and configuration of the tools (for example, they could take ownership of defining the threshold of acceptable code coverage or choose the tools to be implemented)</p>
<p><span>If nothing else, you need to ensure that you have the innovators and followers from the engineering community brought in. To add some clarity, let's look at a few items from the</span> preceding list<span>—which, by the way, is not exhaustive</span><span>—</span><span>in a little more detail, and examine why they are potentially important to your CD and DevOps adoption. Let's start with code complexity.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Code complexity</h1>
                </header>
            
            <article>
                
<p>Having complex code is sometimes necessary, especially when you're looking at extremely optimized code where in resources are limited and/or there<span> </span><span>is a real-time UI—basically, where every millisecond counts. When you have something such as an online store, login page, or a finance module, having overly complex code can do more harm than good. Some engineers believe they are</span> special <span>because they can write complex code; however, complexity for complexity's sake is really just showing off.</span></p>
<p>Overly complex code can cause lots of general problems<span>—</span>especially when trying to debug or when you're trying to extend it to cater for additional use cases<span>—</span>which can directly impact the speed at which you can implement even the smallest change. The premise of CD is to deliver small, incremental, quality changes. If your code is too complex to allow for this, you are going to have issues down the line<span>—</span>normally referred to the maintainability, testability, and readability of the code base.</p>
<p>I would recommend that you put some time aside to look into this complex (pun intended) subject in more detail before you dive into implementing any process or tooling. You really need to understand what the underlying principles are and the science behind them; otherwise, this will become messy and confused. Some of the science is explained in the <a href="b212024b-5ba5-4f01-b76d-a8367c6ec41c.xhtml">Appendix A</a>, <em>Some Useful Information</em>.</p>
<div class="packt_infobox"><span class="IntenseQuoteChar">One suggestion would be to take one of the various code-analysis tools available and run a trial to profile your code base, which will help highlight some existing pain points. From this, you can start to formulate a plan.</span></div>
<p>The next thing you could consider is code coverage.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unit-test coverage</h1>
                </header>
            
            <article>
                
<p>Incorporating unit tests within the software-development process is uniformly recognized as best practice<span>—</span><a href="e0c2e609-a018-4a15-aa14-e01fde967902.xhtml">Chapter 6</a>, <em>Avoiding Hurdles</em>. There is a vast amount of information available on this subject, so I won't spend too much time focusing on this here, but I would recommend that you apply some time and effort into investigating this subject and how you can adopt this approach within your SDLC.</p>
<p>So as not to short-change you, I will provide some insight and background into this subject in relation to CD and DevOps.</p>
<p>At a simplistic level, unit tests allow software-engineering teams to exercise and validate code paths and logic at a granular level during the development process; this, in turn, can help spot and eradicate software defects very early on. Incorporating these tests within CI (see the <span><a href="e0c2e609-a018-4a15-aa14-e01fde967902.xhtml">Chapter 6</a>,</span> <em>Avoiding Hurdles</em>, for information on CI) and having them stop the build can help stop defects escaping into downstream phases of the CD pipeline. This can also be used as an early warning for regression; for example, if the unit test that was previously working starts to fail, there is a high probability that regression has been introduced.</p>
<p>The premise of CD is to be able to ship changes frequently. If you have good unit test coverage across the codebase, you will have a greater level of confidence that you can ship that code frequently with reduced risk.</p>
<p>Analyzing the coverage is a good indication as to how much you can rely upon unit tests to spot problems. You can also use this data to map out the areas of risk when it comes to shipping code quickly (for example, if your login page is frequently changed and has a high level of coverage, the risk of shipping this frequently becomes lower).</p>
<p class="mce-root"/>
<p>There is one thing you do need to take into account regarding coverage measurements<span>—</span>that being the mix of legacy versus new code. What you'll usually find is that legacy code<span>—</span>especially that based on older technologies<span>—</span>may have little-to-no unit-test coverage. If this type of code makes up the majority of your code base, the coverage measure will be pretty low. If the wider business gets too hung up on this measure, they may perceive a low score as a major risk. Although this is technically true, you can't really expect to have older code fully covered from day one. You therefore need to ensure you set the context of the data and have a plan for increasing the coverage over time. One approach would be to set a rule that all new code or refactored code should have a high degree of coverage (ideally 100% as long as this is attainable without slipping into the realm of. diminishing returns), and that the overall coverage figure must grow as refactoring of legacy code increases.</p>
<p>Let's now look at the effectiveness of measuring the frequency of commits.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Commit and merge rates</h1>
                </header>
            
            <article>
                
<p>Regular commits to source control is something that should be widely encouraged and deeply embedded within your ways of working. Having source code sitting on people's workstations or laptops for prolonged periods of time is very risky and can sometimes lead to duplication of effort or, worse still, might block the progress of other engineers.</p>
<p>There might be a fear that if engineers commit too frequently, the chance of defects being created increases, especially when you think there's an outside risk that unfinished code could be incorporated into the main code branch. This fear is a fallacy. No engineer worth their salt would seriously consider doing such a thing<span>—</span>why would they? If you have checks and balances in place, such as regular code reviews or a pull-request approval process, the risk of buggy code being merged will be vastly reduced. Add in unit tests and code analysis and you're looking at next-to-no risk.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Opposite to this is the very real risk of delays between commits and code merges. The more code there is to be merged, the greater the risk and the higher the potential for code conflicts, defects, and incomplete functionality to be introduced. The CD approach is based on delivering working software often. This should not be restricted to software binaries; delivering small incremental chunks of source code little and often is also a good practice.</p>
<p>Most source control systems will have tools and or logs that can be analyzed by third-party tools. The sort of data you should be analyzing will include such things as number of commits and merges per engineer per day, time between merges, and which areas of the code base are changed most frequently.</p>
<p>From this data, you can start to see patterns, such as seeing who is playing ball and who isn't, and what areas of the code base carry the most risk. A word of warning: don't use this data to reward or punish engineers, as this can promote the wrong kinds of behaviors and can be as damaging as ignoring engineering best practices.</p>
<p>Next, we'll look at the thorny issue of code violations and adherence to rules.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adherence to coding rules and standards</h1>
                </header>
            
            <article>
                
<p>You may already have coding standards within your software-development teams and/or try to adhere to an externally-documented and recognized best practice. Being able to analyze your code base to see which parts do and don't adhere to the standards is extremely useful as it helps highlight areas of potential risk. If you continue to capture this data over time, you can start to spot trends<span>—</span>especially when these figures start to fall.</p>
<p>There are a number of tools available to help you do this, some of which are listed in <a href="b212024b-5ba5-4f01-b76d-a8367c6ec41c.xhtml">Appendix A</a>, Some <em>Useful Information</em>.</p>
<div class="packt_infobox">This type of analysis will take some setting up, as it is normally based on a set of predefined rules and thresholds (for example, info, minor, major, critical, and blocker), and you'll need to work with the engineering teams to agree on and set these up within your tooling.</div>
<p>Measuring adherence to coding rules and standards goes some way to stopping defects in your code leaking, but software is software and defects will sneak through. What you therefore need to do is analyze what happens when they do.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Quality metrics</h1>
                </header>
            
            <article>
                
<p>Quality is something that everyone involved in writing and delivering software should want to uphold and build into their solutions. The preceding sections included some elements of quality metrics, but you should also consider some specific measurements targeted on time.</p>
<p>The ones that are pertinent to CD and DevOps are <strong>Mean time between failures</strong> (<strong>MTBF</strong>), <strong>Mean time to resolution</strong> (<strong>MTTR</strong>), and defect-escape distance, which are explained as follows:</p>
<ul>
<li><strong>MTBF</strong>: This will help you measure how often problems (or failures) are found by end users<span>—</span>the longer the time between failures, the greater the stability and quality of the overall platform</li>
<li><strong>MTTR</strong>: This will help you measure the time taken between an issue being found and being fixed</li>
<li><strong>Defect escape distance</strong>: This will help you measure when an issue is found and by whom<span>—</span>for example, defects found by the engineering team are close to the source of the defect (for example, one of the team), whereas UAT spotting a defect is farther out from the source</li>
</ul>
<p>The first two give some good indication as to how CD and DevOps adoption is going as they relate to the speed of delivery. For example, one would expect MTBF to go up and MTTR to go down over time if CD and DevOps adoption is working well. If they don't, there's something wrong that needs looking into.</p>
<p>The third of the trio<span>—</span>defect-escape distance<span>—</span>is a good indication of engineering best practices and how well the CD pipeline is picking up issues early. If the engineering team is spotting defects early on in the process<span>—</span>for example, a CI step fails due to a failing unit test<span>—</span>then the distance and impact is small. If a defect escapes to a downstream process<span>—</span>for example, the UAT team<span>—</span>then the distance and impact is larger. If a defect gets all the way to the production environment then ... well, I think you get the gist.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>One way to represent this is to add a $ value to a defect based upon the environment it is found in and the time it took to find it. For example, let's assume we have four environments used as part of the CD pipeline: Dev, QA, UAT, and Prod. We then apply a sliding scale of cost for each environment based upon the distance from the source:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr style="height: 63px">
<td>
<p><strong>Env</strong></p>
</td>
<td>
<p><strong>Cost</strong></p>
</td>
</tr>
<tr style="padding-left: 30px;height: 63px">
<td>
<p>Dev</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr style="padding-left: 30px;height: 63px">
<td>
<p>QA</p>
</td>
<td>
<p>2</p>
</td>
</tr>
<tr style="padding-left: 30px;height: 65.4824px">
<td>
<p>UAT</p>
</td>
<td>
<p>8</p>
</td>
</tr>
<tr style="padding-left: 30px;height: 63px">
<td>
<p>Prod</p>
</td>
<td>
<p>16</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Let's now consider the cost of each defect using a multiplier based upon the lead time between the defect being created and it being spotted. You'll end up with something such as this:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><strong><span>Defect#</span></strong></p>
</td>
<td>
<p><strong><span>Env</span></strong></p>
</td>
<td>
<p><strong><span>Env cost</span></strong></p>
</td>
<td>
<p><strong><span>Lead time (days)</span></strong></p>
</td>
<td>
<p><strong><span>Defect cost</span></strong></p>
</td>
</tr>
<tr>
<td>
<p>DE1</p>
</td>
<td>
<p>Dev</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>2</p>
</td>
</tr>
<tr>
<td>
<p>DE2</p>
</td>
<td>
<p>Dev</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>5</p>
</td>
</tr>
<tr>
<td>
<p>DE3</p>
</td>
<td>
<p>QA</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>10</p>
</td>
<td>
<p>20</p>
</td>
</tr>
<tr>
<td>
<p>DE4</p>
</td>
<td>
<p>Dev</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0.5</p>
</td>
<td>
<p>0.5</p>
</td>
</tr>
<tr>
<td>
<p>DE5</p>
</td>
<td>
<p>Prod</p>
</td>
<td>
<p>16</p>
</td>
<td>
<p>20</p>
</td>
<td>
<p>320</p>
</td>
</tr>
<tr>
<td>
<p>DE6</p>
</td>
<td>
<p>Prod</p>
</td>
<td>
<p>16</p>
</td>
<td>
<p>50</p>
</td>
<td>
<p>800</p>
</td>
</tr>
<tr>
<td>
<p>DE7</p>
</td>
<td>
<p>UAT</p>
</td>
<td>
<p>8</p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>40</p>
</td>
</tr>
<tr>
<td>
<p>DE8</p>
</td>
<td>
<p>QA</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>14</p>
</td>
</tr>
<tr>
<td>
<p>DE9</p>
</td>
<td>
<p>Dev</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>15</p>
</td>
<td>
<p>15</p>
</td>
</tr>
<tr>
<td>
<p>DE10</p>
</td>
<td>
<p>UAT</p>
</td>
<td>
<p>8</p>
</td>
<td>
<p>12</p>
</td>
<td>
<p>96</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>This is a snapshot in time that gives you an indication of the cost of defects. This doesn't mean you should totally eradicate defects<span>—</span>the only way to do that is to stop writing software<span>—</span>but you should focus on eradicating the high-cost defects. After all, the cost of defects found by customers in real life is far greater than a defect found during the SDLC.</p>
<p>We'll now take a look at the meaning of lead (and cycle) times.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cycle and lead times</h1>
                </header>
            
            <article>
                
<p>These are more time-based metrics that are very useful to measure the progress and effectiveness of the changes you make during CD and DevOps adoption. These two metrics are pretty simple to understand:</p>
<ul>
<li><strong>Lead time</strong>: The measurement of time between a requirement being identified and it being delivered to a customer</li>
<li><strong>Cycle time</strong>: The time between someone starting work on a given work item/story/defect and it being delivered to a customer</li>
</ul>
<p>The following diagram should give you a better idea of what this means:</p>
<p><img src="assets/faf0fba6-2d2d-4f74-b195-cd453a7341f4.png" style="width:42.58em;height:14.08em;"/></p>
<p>The observant among you may notice that for defects, the lead time is pretty much the same as for MTTR, which means that one simple data point can be used for two measurements. Two for the price of one is good value.</p>
<p>Regularly taking snapshots of lead and cycle time gives a very good indication of whether things are working well (or not, as the case may be). It should be noted that lead time can be dependent on changes in business priorities and time-based commitments<span>—</span>for example, a feature may be deprioritized when something more urgent comes into the backlog<span>—</span>so there may be some fluctuation in the value over time. What you should be striving toward is an overall reduction in lead time. Cycle time, on the other hand, is more within the control of the engineering team, and therefore reducing that is within their hands. As CD and DevOps adoption takes hold, the act of delivery should be much simpler, so the average cycle time should decrease. If it doesn't, you should be looking at what is causing the pain points. Some of that may be related to quality issues.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Quality gates</h1>
                </header>
            
            <article>
                
<p>Not only does capturing data help build up a picture over time and spot trends, but you can also use the data to stop quality issues from leaking. What I mean by this is that once you have some data captured and analyzed regarding such things as code coverage, adherence to coding standards, code complexity, or code documentation levels, you could set some thresholds within the CD pipeline, which, if exceeded, will stop the pipeline in its tracks. You can also implement quality gates based upon the results of automated tests<span>—</span>again, if the tests fail, the CD pipeline stops.</p>
<p>For example, let's assume that you have decided that any new piece of software must have 100 percent unit-test code coverage and must not contain any documented security vulnerabilities; then you can implement a code analysis/linting tool within the CD pipeline to check each commit or merge. If the tools report that the code in question doesn't pass the checks, the CD pipeline will stop and let the team know.</p>
<p class="mce-root"/>
<div class="packt_tip">When referring to the CD pipeline, I would include the CI solution being part of the whole pipeline—just in case you were thinking they are separate things.</div>
<p>Implementation of such tools will not only ensure your code is up to scratch, it can also help reduce things such as escaping defects and ensuring code that flows through the CD pipeline with minimal interruption. Capturing this data will also give you some historical insight in relation to when quality gates pass/fail, which may correlate with another event<span>—f</span>or example, failures may grow during the frantic period before a major release.</p>
<p>Some of you may be thinking that this all sounds like hard work<span>—</span>on top of all the other hard work<span>—</span>so is it actually worth it? Yes, it is!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Where to start and why bother?</h1>
                </header>
            
            <article>
                
<p>As stated earlier, there are many things that you can and should be measuring, analyzing, and producing metrics for, and there are many tools that can help you do this. You just need to work out what is most important and start from there. The work and effort needed to set up the tools required should be seen as a great opportunity to bring into play some of the good behaviors you want to embed: collaboration, open and honest dialogue, and trust.</p>
<p>I would advise implementing these types of tools early in your CD and DevOps adoption so that you can start to track progress from the get-go. Needless to say, it is not going to be a pretty sight to begin with, and there will no doubt be questions around the validity of doing this when it doesn't directly drive the adoption forward<span>—</span>in fact, things might look pretty awful, especially early on.</p>
<p><span>It might not directly affect the adoption, but it offers some worthwhile additions, which are explained here:</span></p>
<ul>
<li>Having additional data to prove the quality of the software will, in turn, build trust that the code can be shipped quickly and safely</li>
<li>There is a good chance that having a very concise view of the overall code base will help with the re-engineering to componentize the platform</li>
<li>If the engineers have more confidence in the code base, they can focus on new feature development without concerns about opening a can of worms every time they make a change</li>
</ul>
<p>We'll now move our focus from measuring the act of creating software and look at the importance of measuring what happens when it's built.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Measuring the real world</h1>
                </header>
            
            <article>
                
<p>Analyzing and measuring your code and engineering expertise is one thing; however, for CD and DevOps to really work, you also need to keep a close eye on the overall environment, platform, the running software, and the progress of CD and DevOps effectiveness. Let's start with environments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Measuring the stability of the environments</h1>
                </header>
            
            <article>
                
<p>You may have a number of different environments that are used for different purposes throughout the product-delivery process: development, CI, QA, UAT, performance/load testing, <span>and so on</span>. As your release cycle speeds up, your reliance on these various environments will grow<span>—</span>if you're working in a 2-to-3-month release cycle, having an issue within one of the environments for half a day or so will not have, in the grand scheme of things, a vast impact on your release, whereas if you're releasing 10 times per day, a half-a-day downtime is a major impact.</p>
<p>There seems to be a universal vocabulary throughout the IT industry related to this, and the term environmental issue crops up time and time again, as we can see here:</p>
<div style="padding-left: 30px" class="CDPAlignLeft CDPAlign"><img src="assets/274e8de5-84d0-4dd9-9c86-b5a5cd93a7c7.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The universal environmental issue discussion</div>
<p>We've all heard this, and some of us are just as guilty of saying these things ourselves. All in all, it's far from helpful and can be counterproductive in the long run, especially where building good working relationships across the Dev and Ops divide is concerned, as the implication is that the infrastructure (which is looked after by the operations side) is at fault even though there's no concrete proof.</p>
<p>To overcome this attitude and instill some good behaviors, we need to do one of two things:</p>
<ul>
<li>Prove beyond a shadow of a doubt that the software platform is working as expected, and, therefore, any issues encountered must be based on problems within the infrastructure</li>
<li>Prove beyond a shadow of a doubt that the infrastructure is working as expected, and, therefore, any issues encountered must be based on problems within the software</li>
</ul>
<p>When I said quite simple, I actually meant not very simple. Let's look at the options we have.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Incorporating automated tests</h1>
                </header>
            
            <article>
                
<p>We've looked at the merits of using automated tests to help prove the quality of each software component as it is being released, but what if you were to group these tests together and run them continuously against a given environment? This way, you would end up with a vast majority of the platform being tested over and over again<span>—</span>continuously, in fact.</p>
<p>If you were to capture the results of these tests, you can quickly and easily see how healthy the environment is, or, more precisely, you could see whether the software is behaving as expected. If tests start failing, we can look at what has changed since that last successful run and try to pinpoint the root cause.</p>
<p>There are, of course, many caveats to this:</p>
<ul>
<li>You'll need a good coverage of tests to build a high level of confidence</li>
<li>You might have different tests written in different ways using different technologies, which do not play well together</li>
<li>Some tests could conflict with each other, especially if they rely on certain predetermined sets of test data being available</li>
<li>The tests themselves might not be bulletproof and might not show issues, especially when they have mocking or stubbing included</li>
<li>Some of your tests might flap, which is to say they are inconsistent and for one reason or another fail every now and again</li>
<li>It could take many hours to run all of the tests end-to-end (on the assumption that you are running these sequentially)</li>
</ul>
<p>Assuming that you are happy to live with the caveats or you have resources available to bolster up the tests so that they can be run as a group continuously and consistently, you will end up with a solution that will give you a higher level of confidence in the software platform.</p>
<div class="packt_infobox"><span class="IntenseQuoteChar">I would suggest you apply some focus to flapping and/or tests that do not provide the same results after execution, as these will impact confidence. The rule of thumb is that if you can't trust the test, either refactor it or remove it from the suite.</span></div>
<p>If you extend this thinking, you could also use the same approach to build confidence in your environment(s). For example, if you run the same test suite a number of times against the same environment without changing anything in terms of software, configuration, or environment, you should get the same results each time. Therefore, you should be able to spot instability issues within a given environment with relative ease<span>—</span>sort of.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Combining automated tests and system monitoring</h1>
                </header>
            
            <article>
                
<p>Realistically, just running tests will only give you half the story. To get a truer picture, you could combine your automated test results with the output of your monitoring solution (as covered in <a href="6a4f746d-b386-49b3-b82b-154ff1f604e8.xhtml">Chapter 5</a>, <em>Approaches, Tools, and Techniques</em>). Combining the two will give you a more holistic view of the stability<span>—</span>or not, as the case may be<span>—</span>of the environment as a whole. More importantly, should problems occur, you will have a better chance of pinpointing the root cause(s).</p>
<p>OK, so I've made this sound quite simple, and to be honest, the overall objective is simple; the implementation might be somewhat more difficult. As ever, there are many tools available that will allow you do to this, but again, time and effort is required to get them implemented and set up correctly. You should see this as yet another DevOps collaboration opportunity.</p>
<p>There is, however, another caveat that we should add to the previously mentioned list: you might have major issues trying to run some of your automated tests in the production environment</p>
<p>Unless your operations team is happy with test data being generated and torn down within the production database many times per hour/day and they are happy with the extra load that will generate and the possible security implications, this approach might be restricted to non-production environments.</p>
<p><span>This might be enough to begin with, but if you want a well-rounded picture, you need to look at another complementary approach to gain some more in-depth real-time metrics.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Real-time monitoring of the software itself</h1>
                </header>
            
            <article>
                
<p>Combining automated tests and system monitoring will give you useful data, but will realistically only prove two things: the platform is up, and the tests pass. It does not give you an in-depth understanding of how your software platform is behaving or, more importantly, how it is behaving in the production environment being used by many millions of real-world users. To achieve this, you need to go to the next level.</p>
<p>Consider how a Formula One car is developed. We have a test driver sitting in the cockpit who is generating input to make the car do something; their foot is on the accelerator, making the car move forward, and they are steering the car to make it go around corners. You have a fleet of technicians and engineers observing how fast the car goes, and they can observe how the car functions (that is, the car goes faster when the accelerator is pressed and goes around a corner when the steering wheel is turned). This is all well and good, but what is more valuable to the technicians and the engineers is the in-depth metrics and data generated by the myriad of sensors and electronic gubbins deep within the car itself.</p>
<p>This approach can be applied to a software platform as well. You need data and metrics from deep within the bowels of the platform to fully understand what is going on; no amount of testing and observation of the results will give you this. This is not a new concept; it has been around for many years. Just look at any operating system; there are many ways to delve into the depths and pull out useful and meaningful metrics and data. Why not simply apply this concept to software components? In some respects, this is already built in; look at the various log files that your software platform generates (for example, HTTP logs and error logs), so you have a head start; if only you could harvest this data and make use of it.</p>
<p>There are a number of tools available that allow you to trawl through such output and compile them into useful and meaningful reports and graphs. There is a but here: it's very difficult to generate this in real-time, especially when there's a vast amount of data being produced, which will take time to fetch and process.</p>
<p>A cleaner approach would be to build something into the software itself that can produce this kind of low-level data for you in a small, concise, and consistent format that is useful to you<span>—</span>if truth be told, your average HTTP log contains a vast amount of data that is of no value to you at all. I'll cover some examples in <a href="b212024b-5ba5-4f01-b76d-a8367c6ec41c.xhtml">Appendix A</a>, <em>Some Useful Information</em>, but simply put, this approach falls into two categories:</p>
<ul>
<li>Incorporate a health-check function within your software APIs; this will provide low-level metrics data when called periodically by a central data-collection solution</li>
<li>Extend your software platform to push low-level metrics data to a central data-collection solution periodically</li>
</ul>
<p class="mce-root"/>
<p>You will, of course, need something to act as the central data-collection solution, but there are tools available if you shop around and work in a DevOps manner to choose and implement what works best for you.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monitoring utopia</h1>
                </header>
            
            <article>
                
<p>Whatever approach (or combination of approaches) you adopt, you should end up with some very rich and in-depth information. In essence, you'll much have as much data as your average Formula One technician (that being lots and lots of data). You just need to pull it all together into a coherent and easy-to-understand form. This challenge is another one to encourage DevOps behaviors, as the sort of data you want to capture/present is best fleshed out and agreed on between the engineers on both sides.</p>
<div class="packt_infobox">If you're unsure whether you should measure a specific part of the platform or the infrastructure, but feel it might be useful, measure it anyway. You never know whether this data will come in handy later. The rule of thumb is: if it moves, monitor it; if it doesn't move, monitor it just in case.</div>
<p>Ultimately, what you want to be able to do is ensure that the entire environment (infrastructure, configuration, and software platform) is healthy. This way, if someone says it must be an environmental issue, they might actually be correct.</p>
<p>If we pull all of this together, we can now expand up on the preceding list:</p>
<ul>
<li>Prove beyond a shadow of a doubt that the software platform is working as expected, and, therefore, any issues encountered must be based on problems within the infrastructure</li>
<li>Prove beyond a shadow of a doubt that the infrastructure is working as expected, and, therefore, any issues encountered must be based on problems within the software</li>
<li>Agree that problems can occur for whatever reason and that the root cause(s) should be identified and addressed in a collaborative DevOps way</li>
</ul>
<p>We'll now move on from the technical side of measuring and look at the business-focused view.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Effectiveness of CD and DevOps</h1>
                </header>
            
            <article>
                
<p>Implementing CD and DevOps is not cheap. There's quite a lot of effort required, which directly translates into cost. Every business likes to see a return on investment, so there is no reason why you should not provide this sort of information and data. For the majority of this chapter, we've been focusing on the more in-depth, technical side of measuring progress and success. This is very valuable to technical-minded individuals, but your average middle manager might not get the subtleties of what it means, and to be honest, you can't really blame them. Seeing a huge amount of data and charts that contain information, such as <strong>Transactions per second</strong> (<strong>TPS</strong>) counts, response times for a given software component, or how many commits were made, is not awe-inspiring for your average suit. What they like is top-level summary information and data, which represents progress and success.</p>
<p>As far as CD and DevOps is concerned, the main factors that are important are improvements in efficiency and throughput, as these translate directly into how quickly products can be delivered to the market and how quickly the business can start realizing the value. This is what it's all about. CD and DevOps is the catalyst to allow for this to be realized, so why not show this?</p>
<p>With any luck, you will have (or plan to have) some tooling to facilitate and orchestrate the CD process. What you should also have built into this tooling is metrics; the sort of metrics that you should be capturing are:</p>
<ul>
<li>A count of the number of deployments completed</li>
<li>The time taken to take a release candidate to production</li>
<li>The time taken from commit to the working software being in production</li>
<li>A count of the release candidates that have been built</li>
<li>A league table of software components that are released</li>
<li>A list of the unique software components going through the CD pipeline</li>
</ul>
<p>You can then take this data and summarize it for all to see<span>—</span>it must be simple, and it must be easy to understand. An example of the sort of information you could display on screens around the office could be something such as the one shown in the following screenshot:</p>
<div class="CDPAlignLeft CDPAlign"> <img src="assets/98597789-08b6-453f-9e04-017a75f96108.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">An example page summarizing the effectiveness of the CD process</div>
<p>This kind of information is extremely effective, and if it's visible and easily accessible, it also opens up discussions around how well things are progressing and what areas still need some work and optimization.</p>
<p>What would also be valuable, especially to management types, is financial data and information, such as the cost of each release in terms of resources. If you have this data available to you, including it will not only be useful for the management, but it could also help provide focus for the engineering teams, as they will start to understand how much these things cost.</p>
<p>Access to this data and information should not be restricted and should be highly visible so that everyone can see the progress being made and, more importantly, see how far they are from the original goal.</p>
<p><span>We've looked at the effectiveness; let's now look at the real-world impact.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Impact of CD and DevOps</h1>
                </header>
            
            <article>
                
<p>Implementing CD and DevOps will have an impact on your ways of working and business as a whole. This is a fact. What would be good is to understand what this impact actually is. You might already be capturing and reporting against things such as business <strong>key performance indicators</strong> (<strong>KPI</strong>) (number of active users, revenue, page visits, and so on), so why not add these into the overarching metrics and measurements? If CD and DevOps is having a positive impact on customer retention, wouldn't it be nice for everyone to see this?</p>
<p>At a basic level, you want to ensure that you are going in the right direction.</p>
<p><span>Before we move away from measuring and monitoring, let's look at something that, on the face of it, does seem strange: measuring your DevOps culture.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Measuring your culture</h1>
                </header>
            
            <article>
                
<p>I know what you're thinking: measuring software, environments, and processes is hard enough, but how can you measure something as intangible as culture? To be honest, there are no easy answers, and it really depends on what you feel is most valuable. For example, you might feel having developers working with system operators 20 percent of their time is a good indication that DevOps is working and is healthy, or the fact that live issues are resolved by developers and the operations team is a good sign.</p>
<p>Capturing this information can also be tricky, but it doesn't need to be overly complex. What you really need to know is how people feel things are progressing and whether they think things are progressing in the correct way.</p>
<p>The simplest way to capture this is to ask as many people as you can. Of course, you'll want to capture some meaningful data points<span>—</span>simply having a graph with the words it's going OK doesn't really give you much. You could look at using periodical interviews or questionnaires that capture data such as:</p>
<ul>
<li>Do you feel there is an effective level of collaboration between engineers (Dev and Ops)?</li>
<li>How willing are engineers (Dev and Ops) to collaborate to solve production issues?</li>
<li>Do you feel blame is still predominant when issues occur?</li>
<li>Do you feel operations engineers are involved early enough in feature development?</li>
<li>Are there enough opportunities for engineers (Dev and Ops) to improve their ways of working?</li>
</ul>
<ul>
<li>Do you feel you have the tools, skills, and environment to effectively do your job?</li>
<li>Do you feel that CD and DevOps is having a positive impact on our business?</li>
</ul>
<p>There might be other example questions that you can think up; however, don't overdo it and bombard people<span>—</span>KISS (see the <a href="17779905-1394-4db1-995e-04c6af9a5125.xhtml">Chapter 3</a>, <em>Culture and Behaviors are the Cornerstones to Success</em>). If you can use questions that allow for answers in a scale form (for example, 1 being strongly agree, 2 being agree, 3 being disagree, and 4 being strongly disagree), you'll be able to get a clearer picture, which you can then compare over time.</p>
<p>Again, if you pool this data with your technical data, this might provide some insights you were not expecting. For example, maybe you implemented a new process that has reduced the escaped defects by 10 percent, but releases per day have dropped by 5 percent and the majority of the engineering team is unhappy. In such a case, you might have a problem with the process itself or the acceptance of it at a grass-roots level.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Throughout this chapter, you learned that capturing data and measurements is important, as this gives you a clear indication of whether things are working and progressing in the way you planned and hoped for. Whether you're interested in the gains in software quality over time, reduction in bugs, performance of your software platform, or number of environmental issues in the past quarter, you need data. Lots of data. Complementing this with business-focused and real-world data will only add value and provide you with more insight into how things are going.</p>
<p>You are striving to encourage openness and honesty throughout the organization (see the <a href="a19ac942-68bd-48a6-b59e-cd67ced91b65.xhtml">Chapter 4</a>, <em>Culture and Behaviors</em>); therefore, sharing all of the metrics and data you collect during your CD and DevOps implementation will provide a high degree of transparency. At the end of the day, every part of any business turns into data, metrics, and graphs (financial figures, head count, public opinion of your product, and so on), so why should the product-delivery process be any different?</p>
<p>The sooner you start to capture this data, the sooner you can inspect and adapt. You need to extend your mantra from monitor, monitor, and then monitor some more, to monitor and measure continuously and consistently.</p>
<p>Let's now move from measuring everything that can and should be measured to see how things look once your CD and DevOps adoption has matured. In <a href="c2437827-f7ff-49a7-8ee7-5bdd39c5ddbe.xhtml">Chapter 8</a>, <em>You Are Not Finished Just Yet</em>, we'll be covering some of the things you should be considering when CD and DevOps become the norm.</p>


            </article>

            
        </section>
    </body></html>