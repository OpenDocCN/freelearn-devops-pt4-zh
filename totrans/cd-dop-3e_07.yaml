- en: Vital Measurements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the previous chapters, we have looked at what tools and techniques you
    should be considering, the need to acknowledge how change will impact people in
    different ways, why culture, behaviors, and environment are important, what potential
    hurdles you'll need to overcome, and how all of this is needed to successfully
    adopt CD and DevOps. If you are taking this into account, creating plans to cater
    for and/or address this, you should be in a good shape to make wide strides forward.
  prefs: []
  type: TYPE_NORMAL
- en: We will now look at the important but sometimes overlooked—or simply dismissed—area
    of monitoring and measuring progress. We did touch on this subject previously,
    but what we considered was a small slice of the pie, figuratively speaking. What
    we're looking at now is the capturing, compiling, and sharing of metrics related
    to the impact that CD and DevOps has on the day-to-day ways of working and the
    business as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: This, on the face of it, might be seen as something that is only useful to the
    management types and won't add value to those who will be dealing with the CD
    and DevOps adoption on a day-to-day basis. In some regards, that is true, but
    being able to analyze, understand, and share demonstrable progress will definitely
    add value to you and everyone else who is on the CD and DevOps journey. We're
    not just talking about simple project management charts, graphs, and PowerPoint
    fodder here; what we are looking at is measuring as many aspects of the overall
    process as possible. That way, anyone and everyone can plainly see and understand
    how far you have collectively come and how far from the ultimate goal you are.
  prefs: []
  type: TYPE_NORMAL
- en: To do this effectively, you'll need to ensure that you start this data capture
    very early into the CD and DevOps adoption, as it will be very difficult to see
    a comparison between then and now if you don't have data representing *then*.
    You will need to be vigilant and consistent in ensuring that you are continuously
    capturing these measurements so that you can compare the state of progress at
    different points in time. Some would consider this anal, but this whole CD and
    DevOps journey started because the data captured in the elephant exposure pointed
    to areas of waste—or, at the very least, ineffective processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How to measure the effectiveness of your engineering process(es)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to measure the stability of the various environments you use and rely on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to measure the impact your adoption of CD and DevOps is having
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll start, as they say, at the beginning and focus initially on engineering
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring effective engineering best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is quite a weird concept to get your head around: How can you measure
    effective engineering, and more than that, how can you measure best practices?
    There''s another often-asked question: what has this got to do with DevOps or
    CD? We''ll look at the former in a moment, but now let''s focus on the latter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take two scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: Your current software-engineering process is very waterfall and you have a vast
    amount of manual testing to validate your code just before it gets shipped—which
    happens every 3-6 months—and build in a buffer for bug fixing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your current software-engineering process is pretty agile and follows (mostly)
    industry best practices, however as there is plenty of time between releases you
    can sometimes let technical debt slip (including test automation) as there will
    be time to go back and mop up just before the next release—which happens every
    3-6 months
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OK, so this is pretty simplistic, but bear with me. As the CD and DevOps adoption
    starts to gather momentum, the time between releases will decrease. Therefore,
    the we can do that later window gets smaller and smaller. This can lead to engineers
    having to start cutting corners simply because they have run out of time to mop
    up the pre-release tech-debt tasks. The adoption of CD and DevOps ultimately allows
    you to deliver solutions quickly—there's nothing that categorically states that
    engineers will be given more time to write and test said solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider what a large quarterly release looks like in terms of timeline
    and effort, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03651732-d728-43e1-aa7c-8725be322b3f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s compare that to a CD-type release, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/192a5c25-d72b-4737-8cfe-131a500e1422.png)'
  prefs: []
  type: TYPE_IMG
- en: These are both very simplistic examples, but they highlight the impact that
    reducing the time between releases will have on the key players. The we can do
    that later window goes from days/weeks to hours.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 5](6a4f746d-b386-49b3-b82b-154ff1f604e8.xhtml)*,* *Approaches, Tools,
    and Techniques*, we looked at how the wider business perceives the relationship
    between features and releases. As your CD and DevOps adoption matures, the time
    between releases will decrease, which means that engineers will have less time
    to complete features. If the wider business has become accustomed to having features
    delivered within a given release, they will continue to expect this until things
    bed in.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go back to the corners. These will normally be related to the non-cutting
    code, yet still time-consuming activities—skipping the odd unit test here, leaving
    the odd gap in integration tests there, forgoing documentation, reducing the tendency
    to refactor old code, and so on. In simple terms, the engineers will be under
    pressure to deliver and they will no longer have the time to address everything
    they did previously. This therefore becomes technical debt—which is something
    every software-engineering team tries to avoid at all costs, as it will come back
    to bite them later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to the main subject of measuring effective engineering best practices,
    it''s not as strange or uncommon as you might think. There are a great number
    of software-based businesses around the globe regularly using tools to capture
    data and measurements for things such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Overall code quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adherence to coding rules and standards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code documentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code complexity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code duplication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redundant code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unit-test coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical debt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean time between failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean time to resolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bug-escape distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fix-bounce rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Measuring each of these in isolation might not bring a vast amount of value;
    however, when pooled together, you can get a very detailed picture of how things
    stand. In addition, if you can continuously capture this level of detail over
    a period of time, you can then start to measure and report on progress. Why this
    is important to the adoption of CD and DevOps is quite simple: if the quality
    of your software decreases due to the fact that things are being shipped faster,
    the laggards will have a field day. If those laggards are in influential and/or
    decision-making positions, the whole adoption could be derailed.'
  prefs: []
  type: TYPE_NORMAL
- en: As stated previously, if you can spot this as it starts to happen, you have
    a fighting chance of stopping it. There is also another side to this; if your
    quality currently sucks and you can prove that CD and DevOps adoption helps to
    increase quality, then that's a massive good news story—we can ship quicker and
    the quality is vastly better. Take that, laggards!
  prefs: []
  type: TYPE_NORMAL
- en: It all sounds very simple, and to be honest, it can be, but you need to be mindful
    of the fact that you will need to apply some time, effort, and rigor to ensure
    that you gain the most value. There will also be a degree of trial and error and
    tweaking as you go, to ensure you can capture the data in a reliable and repeatable
    way—more inspecting and adapting—so you need to ensure that you factor this in.
    Not only will these sorts of measurements help your engineering team(s), they
    will also help with building trust across the wider business. For example, you'll
    be able to provide open, honest, and truthful metrics in relation to the quality
    of your software, which, in turn, will reinforce the trust they have in the team(s)
    building and looking after the platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing to seriously consider before you look at measuring things such as
    software code metrics is how the engineers themselves will feel about this. What
    Devina is thinking might be a typical reaction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c570634-a501-428f-b725-635fd0fb9e24.png)'
  prefs: []
  type: TYPE_IMG
- en: A typical reaction to this approach
  prefs: []
  type: TYPE_NORMAL
- en: Some engineers will become guarded or defensive, and may see it as questioning
    their skills and craftsmanship in relation to creating quality code. You need
    to be careful that you don't get barriers put up between you and the engineering
    teams or let them slip back into the *laggards* camp. You should *sell* these
    tools as a positive benefit for the engineers. For example, they have a way to
    definitively prove how good their code actually is; they can use the tools to
    inspect areas of over-complexity or areas of code that are more at risk of containing
    bugs; they can highlight redundant code and remove it from the codebase; and they
    can visually see hard dependencies, which can help when looking at componentization.
  prefs: []
  type: TYPE_NORMAL
- en: If you have vocal laggards, get them actively involved in the setup and configuration
    of the tools (for example, they could take ownership of defining the threshold
    of acceptable code coverage or choose the tools to be implemented)
  prefs: []
  type: TYPE_NORMAL
- en: If nothing else, you need to ensure that you have the innovators and followers
    from the engineering community brought in. To add some clarity, let's look at
    a few items from the preceding list—which, by the way, is not exhaustive—in a
    little more detail, and examine why they are potentially important to your CD
    and DevOps adoption. Let's start with code complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Code complexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having complex code is sometimes necessary, especially when you're looking at
    extremely optimized code where in resources are limited and/or there is a real-time
    UI—basically, where every millisecond counts. When you have something such as
    an online store, login page, or a finance module, having overly complex code can
    do more harm than good. Some engineers believe they are special because they can
    write complex code; however, complexity for complexity's sake is really just showing
    off.
  prefs: []
  type: TYPE_NORMAL
- en: Overly complex code can cause lots of general problems—especially when trying
    to debug or when you're trying to extend it to cater for additional use cases—which
    can directly impact the speed at which you can implement even the smallest change.
    The premise of CD is to deliver small, incremental, quality changes. If your code
    is too complex to allow for this, you are going to have issues down the line—normally
    referred to the maintainability, testability, and readability of the code base.
  prefs: []
  type: TYPE_NORMAL
- en: I would recommend that you put some time aside to look into this complex (pun
    intended) subject in more detail before you dive into implementing any process
    or tooling. You really need to understand what the underlying principles are and
    the science behind them; otherwise, this will become messy and confused. Some
    of the science is explained in the [Appendix A](b212024b-5ba5-4f01-b76d-a8367c6ec41c.xhtml),
    *Some Useful Information*.
  prefs: []
  type: TYPE_NORMAL
- en: One suggestion would be to take one of the various code-analysis tools available
    and run a trial to profile your code base, which will help highlight some existing
    pain points. From this, you can start to formulate a plan.
  prefs: []
  type: TYPE_NORMAL
- en: The next thing you could consider is code coverage.
  prefs: []
  type: TYPE_NORMAL
- en: Unit-test coverage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Incorporating unit tests within the software-development process is uniformly
    recognized as best practice—[Chapter 6](e0c2e609-a018-4a15-aa14-e01fde967902.xhtml),
    *Avoiding Hurdles*. There is a vast amount of information available on this subject,
    so I won't spend too much time focusing on this here, but I would recommend that
    you apply some time and effort into investigating this subject and how you can
    adopt this approach within your SDLC.
  prefs: []
  type: TYPE_NORMAL
- en: So as not to short-change you, I will provide some insight and background into
    this subject in relation to CD and DevOps.
  prefs: []
  type: TYPE_NORMAL
- en: At a simplistic level, unit tests allow software-engineering teams to exercise
    and validate code paths and logic at a granular level during the development process;
    this, in turn, can help spot and eradicate software defects very early on. Incorporating
    these tests within CI (see the [Chapter 6](e0c2e609-a018-4a15-aa14-e01fde967902.xhtml),
    *Avoiding Hurdles*, for information on CI) and having them stop the build can
    help stop defects escaping into downstream phases of the CD pipeline. This can
    also be used as an early warning for regression; for example, if the unit test
    that was previously working starts to fail, there is a high probability that regression
    has been introduced.
  prefs: []
  type: TYPE_NORMAL
- en: The premise of CD is to be able to ship changes frequently. If you have good
    unit test coverage across the codebase, you will have a greater level of confidence
    that you can ship that code frequently with reduced risk.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the coverage is a good indication as to how much you can rely upon
    unit tests to spot problems. You can also use this data to map out the areas of
    risk when it comes to shipping code quickly (for example, if your login page is
    frequently changed and has a high level of coverage, the risk of shipping this
    frequently becomes lower).
  prefs: []
  type: TYPE_NORMAL
- en: There is one thing you do need to take into account regarding coverage measurements—that
    being the mix of legacy versus new code. What you'll usually find is that legacy
    code—especially that based on older technologies—may have little-to-no unit-test
    coverage. If this type of code makes up the majority of your code base, the coverage
    measure will be pretty low. If the wider business gets too hung up on this measure,
    they may perceive a low score as a major risk. Although this is technically true,
    you can't really expect to have older code fully covered from day one. You therefore
    need to ensure you set the context of the data and have a plan for increasing
    the coverage over time. One approach would be to set a rule that all new code
    or refactored code should have a high degree of coverage (ideally 100% as long
    as this is attainable without slipping into the realm of. diminishing returns),
    and that the overall coverage figure must grow as refactoring of legacy code increases.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now look at the effectiveness of measuring the frequency of commits.
  prefs: []
  type: TYPE_NORMAL
- en: Commit and merge rates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regular commits to source control is something that should be widely encouraged
    and deeply embedded within your ways of working. Having source code sitting on
    people's workstations or laptops for prolonged periods of time is very risky and
    can sometimes lead to duplication of effort or, worse still, might block the progress
    of other engineers.
  prefs: []
  type: TYPE_NORMAL
- en: There might be a fear that if engineers commit too frequently, the chance of
    defects being created increases, especially when you think there's an outside
    risk that unfinished code could be incorporated into the main code branch. This
    fear is a fallacy. No engineer worth their salt would seriously consider doing
    such a thing—why would they? If you have checks and balances in place, such as
    regular code reviews or a pull-request approval process, the risk of buggy code
    being merged will be vastly reduced. Add in unit tests and code analysis and you're
    looking at next-to-no risk.
  prefs: []
  type: TYPE_NORMAL
- en: Opposite to this is the very real risk of delays between commits and code merges.
    The more code there is to be merged, the greater the risk and the higher the potential
    for code conflicts, defects, and incomplete functionality to be introduced. The
    CD approach is based on delivering working software often. This should not be
    restricted to software binaries; delivering small incremental chunks of source
    code little and often is also a good practice.
  prefs: []
  type: TYPE_NORMAL
- en: Most source control systems will have tools and or logs that can be analyzed
    by third-party tools. The sort of data you should be analyzing will include such
    things as number of commits and merges per engineer per day, time between merges,
    and which areas of the code base are changed most frequently.
  prefs: []
  type: TYPE_NORMAL
- en: 'From this data, you can start to see patterns, such as seeing who is playing
    ball and who isn''t, and what areas of the code base carry the most risk. A word
    of warning: don''t use this data to reward or punish engineers, as this can promote
    the wrong kinds of behaviors and can be as damaging as ignoring engineering best
    practices.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll look at the thorny issue of code violations and adherence to rules.
  prefs: []
  type: TYPE_NORMAL
- en: Adherence to coding rules and standards
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may already have coding standards within your software-development teams
    and/or try to adhere to an externally-documented and recognized best practice.
    Being able to analyze your code base to see which parts do and don't adhere to
    the standards is extremely useful as it helps highlight areas of potential risk.
    If you continue to capture this data over time, you can start to spot trends—especially
    when these figures start to fall.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of tools available to help you do this, some of which are
    listed in [Appendix A](b212024b-5ba5-4f01-b76d-a8367c6ec41c.xhtml), Some *Useful
    Information*.
  prefs: []
  type: TYPE_NORMAL
- en: This type of analysis will take some setting up, as it is normally based on
    a set of predefined rules and thresholds (for example, info, minor, major, critical,
    and blocker), and you'll need to work with the engineering teams to agree on and
    set these up within your tooling.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring adherence to coding rules and standards goes some way to stopping
    defects in your code leaking, but software is software and defects will sneak
    through. What you therefore need to do is analyze what happens when they do.
  prefs: []
  type: TYPE_NORMAL
- en: Quality metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quality is something that everyone involved in writing and delivering software
    should want to uphold and build into their solutions. The preceding sections included
    some elements of quality metrics, but you should also consider some specific measurements
    targeted on time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ones that are pertinent to CD and DevOps are **Mean time between failures**
    (**MTBF**), **Mean time to resolution** (**MTTR**), and defect-escape distance,
    which are explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**MTBF**: This will help you measure how often problems (or failures) are found
    by end users—the longer the time between failures, the greater the stability and
    quality of the overall platform'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MTTR**: This will help you measure the time taken between an issue being
    found and being fixed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Defect escape distance**: This will help you measure when an issue is found
    and by whom—for example, defects found by the engineering team are close to the
    source of the defect (for example, one of the team), whereas UAT spotting a defect
    is farther out from the source'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first two give some good indication as to how CD and DevOps adoption is
    going as they relate to the speed of delivery. For example, one would expect MTBF
    to go up and MTTR to go down over time if CD and DevOps adoption is working well.
    If they don't, there's something wrong that needs looking into.
  prefs: []
  type: TYPE_NORMAL
- en: The third of the trio—defect-escape distance—is a good indication of engineering
    best practices and how well the CD pipeline is picking up issues early. If the
    engineering team is spotting defects early on in the process—for example, a CI
    step fails due to a failing unit test—then the distance and impact is small. If
    a defect escapes to a downstream process—for example, the UAT team—then the distance
    and impact is larger. If a defect gets all the way to the production environment
    then ... well, I think you get the gist.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to represent this is to add a $ value to a defect based upon the environment
    it is found in and the time it took to find it. For example, let''s assume we
    have four environments used as part of the CD pipeline: Dev, QA, UAT, and Prod.
    We then apply a sliding scale of cost for each environment based upon the distance
    from the source:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Env** | **Cost** |'
  prefs: []
  type: TYPE_TB
- en: '| Dev | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| QA | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| UAT | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Prod | 16 |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s now consider the cost of each defect using a multiplier based upon the
    lead time between the defect being created and it being spotted. You''ll end up
    with something such as this:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Defect#** | **Env** | **Env cost** | **Lead time (days)** | **Defect cost**
    |'
  prefs: []
  type: TYPE_TB
- en: '| DE1 | Dev | 1 | 2 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| DE2 | Dev | 1 | 5 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| DE3 | QA | 2 | 10 | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| DE4 | Dev | 1 | 0.5 | 0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| DE5 | Prod | 16 | 20 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '| DE6 | Prod | 16 | 50 | 800 |'
  prefs: []
  type: TYPE_TB
- en: '| DE7 | UAT | 8 | 5 | 40 |'
  prefs: []
  type: TYPE_TB
- en: '| DE8 | QA | 2 | 7 | 14 |'
  prefs: []
  type: TYPE_TB
- en: '| DE9 | Dev | 1 | 15 | 15 |'
  prefs: []
  type: TYPE_TB
- en: '| DE10 | UAT | 8 | 12 | 96 |'
  prefs: []
  type: TYPE_TB
- en: This is a snapshot in time that gives you an indication of the cost of defects.
    This doesn't mean you should totally eradicate defects—the only way to do that
    is to stop writing software—but you should focus on eradicating the high-cost
    defects. After all, the cost of defects found by customers in real life is far
    greater than a defect found during the SDLC.
  prefs: []
  type: TYPE_NORMAL
- en: We'll now take a look at the meaning of lead (and cycle) times.
  prefs: []
  type: TYPE_NORMAL
- en: Cycle and lead times
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These are more time-based metrics that are very useful to measure the progress
    and effectiveness of the changes you make during CD and DevOps adoption. These
    two metrics are pretty simple to understand:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lead time**: The measurement of time between a requirement being identified
    and it being delivered to a customer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cycle time**: The time between someone starting work on a given work item/story/defect
    and it being delivered to a customer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram should give you a better idea of what this means:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/faf0fba6-2d2d-4f74-b195-cd453a7341f4.png)'
  prefs: []
  type: TYPE_IMG
- en: The observant among you may notice that for defects, the lead time is pretty
    much the same as for MTTR, which means that one simple data point can be used
    for two measurements. Two for the price of one is good value.
  prefs: []
  type: TYPE_NORMAL
- en: Regularly taking snapshots of lead and cycle time gives a very good indication
    of whether things are working well (or not, as the case may be). It should be
    noted that lead time can be dependent on changes in business priorities and time-based
    commitments—for example, a feature may be deprioritized when something more urgent
    comes into the backlog—so there may be some fluctuation in the value over time.
    What you should be striving toward is an overall reduction in lead time. Cycle
    time, on the other hand, is more within the control of the engineering team, and
    therefore reducing that is within their hands. As CD and DevOps adoption takes
    hold, the act of delivery should be much simpler, so the average cycle time should
    decrease. If it doesn't, you should be looking at what is causing the pain points.
    Some of that may be related to quality issues.
  prefs: []
  type: TYPE_NORMAL
- en: Quality gates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Not only does capturing data help build up a picture over time and spot trends,
    but you can also use the data to stop quality issues from leaking. What I mean
    by this is that once you have some data captured and analyzed regarding such things
    as code coverage, adherence to coding standards, code complexity, or code documentation
    levels, you could set some thresholds within the CD pipeline, which, if exceeded,
    will stop the pipeline in its tracks. You can also implement quality gates based
    upon the results of automated tests—again, if the tests fail, the CD pipeline
    stops.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let's assume that you have decided that any new piece of software
    must have 100 percent unit-test code coverage and must not contain any documented
    security vulnerabilities; then you can implement a code analysis/linting tool
    within the CD pipeline to check each commit or merge. If the tools report that
    the code in question doesn't pass the checks, the CD pipeline will stop and let
    the team know.
  prefs: []
  type: TYPE_NORMAL
- en: When referring to the CD pipeline, I would include the CI solution being part
    of the whole pipeline—just in case you were thinking they are separate things.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of such tools will not only ensure your code is up to scratch,
    it can also help reduce things such as escaping defects and ensuring code that
    flows through the CD pipeline with minimal interruption. Capturing this data will
    also give you some historical insight in relation to when quality gates pass/fail,
    which may correlate with another event—for example, failures may grow during the
    frantic period before a major release.
  prefs: []
  type: TYPE_NORMAL
- en: Some of you may be thinking that this all sounds like hard work—on top of all
    the other hard work—so is it actually worth it? Yes, it is!
  prefs: []
  type: TYPE_NORMAL
- en: Where to start and why bother?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As stated earlier, there are many things that you can and should be measuring,
    analyzing, and producing metrics for, and there are many tools that can help you
    do this. You just need to work out what is most important and start from there.
    The work and effort needed to set up the tools required should be seen as a great
    opportunity to bring into play some of the good behaviors you want to embed: collaboration,
    open and honest dialogue, and trust.'
  prefs: []
  type: TYPE_NORMAL
- en: I would advise implementing these types of tools early in your CD and DevOps
    adoption so that you can start to track progress from the get-go. Needless to
    say, it is not going to be a pretty sight to begin with, and there will no doubt
    be questions around the validity of doing this when it doesn't directly drive
    the adoption forward—in fact, things might look pretty awful, especially early
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'It might not directly affect the adoption, but it offers some worthwhile additions,
    which are explained here:'
  prefs: []
  type: TYPE_NORMAL
- en: Having additional data to prove the quality of the software will, in turn, build
    trust that the code can be shipped quickly and safely
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a good chance that having a very concise view of the overall code base
    will help with the re-engineering to componentize the platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the engineers have more confidence in the code base, they can focus on new
    feature development without concerns about opening a can of worms every time they
    make a change
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll now move our focus from measuring the act of creating software and look
    at the importance of measuring what happens when it's built.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the real world
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Analyzing and measuring your code and engineering expertise is one thing; however,
    for CD and DevOps to really work, you also need to keep a close eye on the overall
    environment, platform, the running software, and the progress of CD and DevOps
    effectiveness. Let's start with environments.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the stability of the environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You may have a number of different environments that are used for different
    purposes throughout the product-delivery process: development, CI, QA, UAT, performance/load
    testing, and so on. As your release cycle speeds up, your reliance on these various
    environments will grow—if you''re working in a 2-to-3-month release cycle, having
    an issue within one of the environments for half a day or so will not have, in
    the grand scheme of things, a vast impact on your release, whereas if you''re
    releasing 10 times per day, a half-a-day downtime is a major impact.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There seems to be a universal vocabulary throughout the IT industry related
    to this, and the term environmental issue crops up time and time again, as we
    can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/274e8de5-84d0-4dd9-9c86-b5a5cd93a7c7.png)'
  prefs: []
  type: TYPE_IMG
- en: The universal environmental issue discussion
  prefs: []
  type: TYPE_NORMAL
- en: We've all heard this, and some of us are just as guilty of saying these things
    ourselves. All in all, it's far from helpful and can be counterproductive in the
    long run, especially where building good working relationships across the Dev
    and Ops divide is concerned, as the implication is that the infrastructure (which
    is looked after by the operations side) is at fault even though there's no concrete
    proof.
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome this attitude and instill some good behaviors, we need to do one
    of two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Prove beyond a shadow of a doubt that the software platform is working as expected,
    and, therefore, any issues encountered must be based on problems within the infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove beyond a shadow of a doubt that the infrastructure is working as expected,
    and, therefore, any issues encountered must be based on problems within the software
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When I said quite simple, I actually meant not very simple. Let's look at the
    options we have.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating automated tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've looked at the merits of using automated tests to help prove the quality
    of each software component as it is being released, but what if you were to group
    these tests together and run them continuously against a given environment? This
    way, you would end up with a vast majority of the platform being tested over and
    over again—continuously, in fact.
  prefs: []
  type: TYPE_NORMAL
- en: If you were to capture the results of these tests, you can quickly and easily
    see how healthy the environment is, or, more precisely, you could see whether
    the software is behaving as expected. If tests start failing, we can look at what
    has changed since that last successful run and try to pinpoint the root cause.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are, of course, many caveats to this:'
  prefs: []
  type: TYPE_NORMAL
- en: You'll need a good coverage of tests to build a high level of confidence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might have different tests written in different ways using different technologies,
    which do not play well together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some tests could conflict with each other, especially if they rely on certain
    predetermined sets of test data being available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tests themselves might not be bulletproof and might not show issues, especially
    when they have mocking or stubbing included
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of your tests might flap, which is to say they are inconsistent and for
    one reason or another fail every now and again
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It could take many hours to run all of the tests end-to-end (on the assumption
    that you are running these sequentially)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assuming that you are happy to live with the caveats or you have resources available
    to bolster up the tests so that they can be run as a group continuously and consistently,
    you will end up with a solution that will give you a higher level of confidence
    in the software platform.
  prefs: []
  type: TYPE_NORMAL
- en: I would suggest you apply some focus to flapping and/or tests that do not provide
    the same results after execution, as these will impact confidence. The rule of
    thumb is that if you can't trust the test, either refactor it or remove it from
    the suite.
  prefs: []
  type: TYPE_NORMAL
- en: If you extend this thinking, you could also use the same approach to build confidence
    in your environment(s). For example, if you run the same test suite a number of
    times against the same environment without changing anything in terms of software,
    configuration, or environment, you should get the same results each time. Therefore,
    you should be able to spot instability issues within a given environment with
    relative ease—sort of.
  prefs: []
  type: TYPE_NORMAL
- en: Combining automated tests and system monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Realistically, just running tests will only give you half the story. To get
    a truer picture, you could combine your automated test results with the output
    of your monitoring solution (as covered in [Chapter 5](6a4f746d-b386-49b3-b82b-154ff1f604e8.xhtml),
    *Approaches, Tools, and Techniques*). Combining the two will give you a more holistic
    view of the stability—or not, as the case may be—of the environment as a whole.
    More importantly, should problems occur, you will have a better chance of pinpointing
    the root cause(s).
  prefs: []
  type: TYPE_NORMAL
- en: OK, so I've made this sound quite simple, and to be honest, the overall objective
    is simple; the implementation might be somewhat more difficult. As ever, there
    are many tools available that will allow you do to this, but again, time and effort
    is required to get them implemented and set up correctly. You should see this
    as yet another DevOps collaboration opportunity.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is, however, another caveat that we should add to the previously mentioned
    list: you might have major issues trying to run some of your automated tests in
    the production environment'
  prefs: []
  type: TYPE_NORMAL
- en: Unless your operations team is happy with test data being generated and torn
    down within the production database many times per hour/day and they are happy
    with the extra load that will generate and the possible security implications,
    this approach might be restricted to non-production environments.
  prefs: []
  type: TYPE_NORMAL
- en: This might be enough to begin with, but if you want a well-rounded picture,
    you need to look at another complementary approach to gain some more in-depth
    real-time metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time monitoring of the software itself
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Combining automated tests and system monitoring will give you useful data,
    but will realistically only prove two things: the platform is up, and the tests
    pass. It does not give you an in-depth understanding of how your software platform
    is behaving or, more importantly, how it is behaving in the production environment
    being used by many millions of real-world users. To achieve this, you need to
    go to the next level.'
  prefs: []
  type: TYPE_NORMAL
- en: Consider how a Formula One car is developed. We have a test driver sitting in
    the cockpit who is generating input to make the car do something; their foot is
    on the accelerator, making the car move forward, and they are steering the car
    to make it go around corners. You have a fleet of technicians and engineers observing
    how fast the car goes, and they can observe how the car functions (that is, the
    car goes faster when the accelerator is pressed and goes around a corner when
    the steering wheel is turned). This is all well and good, but what is more valuable
    to the technicians and the engineers is the in-depth metrics and data generated
    by the myriad of sensors and electronic gubbins deep within the car itself.
  prefs: []
  type: TYPE_NORMAL
- en: This approach can be applied to a software platform as well. You need data and
    metrics from deep within the bowels of the platform to fully understand what is
    going on; no amount of testing and observation of the results will give you this.
    This is not a new concept; it has been around for many years. Just look at any
    operating system; there are many ways to delve into the depths and pull out useful
    and meaningful metrics and data. Why not simply apply this concept to software
    components? In some respects, this is already built in; look at the various log
    files that your software platform generates (for example, HTTP logs and error
    logs), so you have a head start; if only you could harvest this data and make
    use of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a number of tools available that allow you to trawl through such
    output and compile them into useful and meaningful reports and graphs. There is
    a but here: it''s very difficult to generate this in real-time, especially when
    there''s a vast amount of data being produced, which will take time to fetch and
    process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A cleaner approach would be to build something into the software itself that
    can produce this kind of low-level data for you in a small, concise, and consistent
    format that is useful to you—if truth be told, your average HTTP log contains
    a vast amount of data that is of no value to you at all. I''ll cover some examples
    in [Appendix A](b212024b-5ba5-4f01-b76d-a8367c6ec41c.xhtml), *Some Useful Information*,
    but simply put, this approach falls into two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Incorporate a health-check function within your software APIs; this will provide
    low-level metrics data when called periodically by a central data-collection solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extend your software platform to push low-level metrics data to a central data-collection
    solution periodically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will, of course, need something to act as the central data-collection solution,
    but there are tools available if you shop around and work in a DevOps manner to
    choose and implement what works best for you.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring utopia
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whatever approach (or combination of approaches) you adopt, you should end up
    with some very rich and in-depth information. In essence, you'll much have as
    much data as your average Formula One technician (that being lots and lots of
    data). You just need to pull it all together into a coherent and easy-to-understand
    form. This challenge is another one to encourage DevOps behaviors, as the sort
    of data you want to capture/present is best fleshed out and agreed on between
    the engineers on both sides.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''re unsure whether you should measure a specific part of the platform
    or the infrastructure, but feel it might be useful, measure it anyway. You never
    know whether this data will come in handy later. The rule of thumb is: if it moves,
    monitor it; if it doesn''t move, monitor it just in case.'
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, what you want to be able to do is ensure that the entire environment
    (infrastructure, configuration, and software platform) is healthy. This way, if
    someone says it must be an environmental issue, they might actually be correct.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we pull all of this together, we can now expand up on the preceding list:'
  prefs: []
  type: TYPE_NORMAL
- en: Prove beyond a shadow of a doubt that the software platform is working as expected,
    and, therefore, any issues encountered must be based on problems within the infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove beyond a shadow of a doubt that the infrastructure is working as expected,
    and, therefore, any issues encountered must be based on problems within the software
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agree that problems can occur for whatever reason and that the root cause(s)
    should be identified and addressed in a collaborative DevOps way
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll now move on from the technical side of measuring and look at the business-focused
    view.
  prefs: []
  type: TYPE_NORMAL
- en: Effectiveness of CD and DevOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing CD and DevOps is not cheap. There's quite a lot of effort required,
    which directly translates into cost. Every business likes to see a return on investment,
    so there is no reason why you should not provide this sort of information and
    data. For the majority of this chapter, we've been focusing on the more in-depth,
    technical side of measuring progress and success. This is very valuable to technical-minded
    individuals, but your average middle manager might not get the subtleties of what
    it means, and to be honest, you can't really blame them. Seeing a huge amount
    of data and charts that contain information, such as **Transactions per second**
    (**TPS**) counts, response times for a given software component, or how many commits
    were made, is not awe-inspiring for your average suit. What they like is top-level
    summary information and data, which represents progress and success.
  prefs: []
  type: TYPE_NORMAL
- en: As far as CD and DevOps is concerned, the main factors that are important are
    improvements in efficiency and throughput, as these translate directly into how
    quickly products can be delivered to the market and how quickly the business can
    start realizing the value. This is what it's all about. CD and DevOps is the catalyst
    to allow for this to be realized, so why not show this?
  prefs: []
  type: TYPE_NORMAL
- en: 'With any luck, you will have (or plan to have) some tooling to facilitate and
    orchestrate the CD process. What you should also have built into this tooling
    is metrics; the sort of metrics that you should be capturing are:'
  prefs: []
  type: TYPE_NORMAL
- en: A count of the number of deployments completed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The time taken to take a release candidate to production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The time taken from commit to the working software being in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A count of the release candidates that have been built
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A league table of software components that are released
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of the unique software components going through the CD pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can then take this data and summarize it for all to see—it must be simple,
    and it must be easy to understand. An example of the sort of information you could
    display on screens around the office could be something such as the one shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98597789-08b6-453f-9e04-017a75f96108.png)'
  prefs: []
  type: TYPE_IMG
- en: An example page summarizing the effectiveness of the CD process
  prefs: []
  type: TYPE_NORMAL
- en: This kind of information is extremely effective, and if it's visible and easily
    accessible, it also opens up discussions around how well things are progressing
    and what areas still need some work and optimization.
  prefs: []
  type: TYPE_NORMAL
- en: What would also be valuable, especially to management types, is financial data
    and information, such as the cost of each release in terms of resources. If you
    have this data available to you, including it will not only be useful for the
    management, but it could also help provide focus for the engineering teams, as
    they will start to understand how much these things cost.
  prefs: []
  type: TYPE_NORMAL
- en: Access to this data and information should not be restricted and should be highly
    visible so that everyone can see the progress being made and, more importantly,
    see how far they are from the original goal.
  prefs: []
  type: TYPE_NORMAL
- en: We've looked at the effectiveness; let's now look at the real-world impact.
  prefs: []
  type: TYPE_NORMAL
- en: Impact of CD and DevOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing CD and DevOps will have an impact on your ways of working and business
    as a whole. This is a fact. What would be good is to understand what this impact
    actually is. You might already be capturing and reporting against things such
    as business **key performance indicators** (**KPI**) (number of active users,
    revenue, page visits, and so on), so why not add these into the overarching metrics
    and measurements? If CD and DevOps is having a positive impact on customer retention,
    wouldn't it be nice for everyone to see this?
  prefs: []
  type: TYPE_NORMAL
- en: At a basic level, you want to ensure that you are going in the right direction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move away from measuring and monitoring, let''s look at something
    that, on the face of it, does seem strange: measuring your DevOps culture.'
  prefs: []
  type: TYPE_NORMAL
- en: Measuring your culture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I know what you''re thinking: measuring software, environments, and processes
    is hard enough, but how can you measure something as intangible as culture? To
    be honest, there are no easy answers, and it really depends on what you feel is
    most valuable. For example, you might feel having developers working with system
    operators 20 percent of their time is a good indication that DevOps is working
    and is healthy, or the fact that live issues are resolved by developers and the
    operations team is a good sign.'
  prefs: []
  type: TYPE_NORMAL
- en: Capturing this information can also be tricky, but it doesn't need to be overly
    complex. What you really need to know is how people feel things are progressing
    and whether they think things are progressing in the correct way.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest way to capture this is to ask as many people as you can. Of course,
    you''ll want to capture some meaningful data points—simply having a graph with
    the words it''s going OK doesn''t really give you much. You could look at using
    periodical interviews or questionnaires that capture data such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Do you feel there is an effective level of collaboration between engineers (Dev
    and Ops)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How willing are engineers (Dev and Ops) to collaborate to solve production issues?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you feel blame is still predominant when issues occur?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you feel operations engineers are involved early enough in feature development?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there enough opportunities for engineers (Dev and Ops) to improve their
    ways of working?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you feel you have the tools, skills, and environment to effectively do your
    job?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you feel that CD and DevOps is having a positive impact on our business?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There might be other example questions that you can think up; however, don't
    overdo it and bombard people—KISS (see the [Chapter 3](17779905-1394-4db1-995e-04c6af9a5125.xhtml),
    *Culture and Behaviors are the Cornerstones to Success*). If you can use questions
    that allow for answers in a scale form (for example, 1 being strongly agree, 2
    being agree, 3 being disagree, and 4 being strongly disagree), you'll be able
    to get a clearer picture, which you can then compare over time.
  prefs: []
  type: TYPE_NORMAL
- en: Again, if you pool this data with your technical data, this might provide some
    insights you were not expecting. For example, maybe you implemented a new process
    that has reduced the escaped defects by 10 percent, but releases per day have
    dropped by 5 percent and the majority of the engineering team is unhappy. In such
    a case, you might have a problem with the process itself or the acceptance of
    it at a grass-roots level.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this chapter, you learned that capturing data and measurements is
    important, as this gives you a clear indication of whether things are working
    and progressing in the way you planned and hoped for. Whether you're interested
    in the gains in software quality over time, reduction in bugs, performance of
    your software platform, or number of environmental issues in the past quarter,
    you need data. Lots of data. Complementing this with business-focused and real-world
    data will only add value and provide you with more insight into how things are
    going.
  prefs: []
  type: TYPE_NORMAL
- en: You are striving to encourage openness and honesty throughout the organization
    (see the [Chapter 4](a19ac942-68bd-48a6-b59e-cd67ced91b65.xhtml), *Culture and
    Behaviors*); therefore, sharing all of the metrics and data you collect during
    your CD and DevOps implementation will provide a high degree of transparency.
    At the end of the day, every part of any business turns into data, metrics, and
    graphs (financial figures, head count, public opinion of your product, and so
    on), so why should the product-delivery process be any different?
  prefs: []
  type: TYPE_NORMAL
- en: The sooner you start to capture this data, the sooner you can inspect and adapt.
    You need to extend your mantra from monitor, monitor, and then monitor some more,
    to monitor and measure continuously and consistently.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now move from measuring everything that can and should be measured to
    see how things look once your CD and DevOps adoption has matured. In [Chapter
    8](c2437827-f7ff-49a7-8ee7-5bdd39c5ddbe.xhtml), *You Are Not Finished Just Yet*,
    we'll be covering some of the things you should be considering when CD and DevOps
    become the norm.
  prefs: []
  type: TYPE_NORMAL
