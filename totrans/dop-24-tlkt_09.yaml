- en: Defining Continuous Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We should be able to execute most of the CDP steps from anywhere. Developers
    should be able to run them locally from a Shell. Others might want to integrate
    them into their favorite IDEs. The number of ways all or parts of the CDP steps
    can be executed might be quite huge. Running them as part of every commit is only
    one of those permutations. The way we execute CDP steps should be agnostic to
    the way we define them. If we add the need for very high (if not complete) automation,
    it is clear that the steps must be simple commands or Shell scripts. Adding anything
    else to the mix is likely to result in tight coupling which limits our ability
    to be independent of the tools we’re using to run those steps.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal in this chapter is to define the minimum number of steps a continuous
    deployment process might need. From there on, it would be up to you to extend
    those steps to serve a particular use-case you might be facing in your project.
  prefs: []
  type: TYPE_NORMAL
- en: Once we know what should be done, we’ll proceed and define the commands that
    will get us there. We’ll do our best to create the CDP steps in a way that they
    can be easily ported to other tools. We’ll try to be tools agnostic. There will
    always be some steps that are very specific to the tools we’ll use, but I hope
    that they will be limited to scaffolding, and not the CDP logic.
  prefs: []
  type: TYPE_NORMAL
- en: Whether we’ll manage to reach our goals entirely is yet to be seen. For now,
    we’ll ignore the existence of Jenkins and all the other tools that could be used
    to orchestrate our continuous deployment processes. Instead, we’ll focus purely
    on Shell and the commands we need to execute. We might write a script or two though.
  prefs: []
  type: TYPE_NORMAL
- en: To Continuously Deliver Or To Continuously Deploy?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Everyone wants to implement continuous delivery or deployment. After all, the
    benefits are too significant to be ignored. Increase the speed of delivery, increase
    the quality, decrease the costs, free people to dedicate time to what brings value,
    and so on and so forth. Those improvements are like music to any decision maker,
    especially if that person has a business background. If a tech geek can articulate
    the benefits continuous delivery brings to the table, when he asks a business
    representative for a budget, the response is almost always “Yes! Do it.”
  prefs: []
  type: TYPE_NORMAL
- en: By now, you might be confused with the differences between continuous integration,
    delivery, and deployment, so I’ll do my best to walk you through the primary objectives
    behind each.
  prefs: []
  type: TYPE_NORMAL
- en: You are doing continuous integration (CI) if you have a set of automated processes
    that are executed every time you commit a change to a code repository. What we’re
    trying to accomplish with CI is a state when every commit is validated shortly
    after a commit. We want to know not only whether what we did works, but also whether
    it integrates with the work our colleagues did. That’s why it is crucial that
    everyone merges code to the master branch or, at least, to some other common branch.
    It does not matter much how we name it. What does matter is that not much time
    passes since the moment we fork the code. That can be hours or maybe days. If
    we delay integration for more than that, we are risking spending too much time
    working on something that breaks work of others.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with continuous integration is that the level of automation is not
    high enough. We do not trust the process enough. We feel that it provides benefits,
    but we also require a second opinion. We need humans to confirm the result of
    the process executed by machines.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous delivery (CD) is a superset of continuous integration. It features
    a fully automated process executed on every commit. If none of the steps in the
    process fail, we declare the commit as ready for production.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, continuous deployment (CDP) is almost the same as continuous delivery.
    All the steps in the process are in both cases fully automated. The only difference
    is that the button that says “deploy to production” is gone.
  prefs: []
  type: TYPE_NORMAL
- en: Even though CD and CDP are almost the same from the process perspective, the
    latter might require changes in the way we develop our applications. We might,
    for example, need to start using feature toggles that allow us to disable partially
    finished features. Most of the changes required for CDP are things that should
    be adopted anyways. However, with CDP that need is increased to an even higher
    level.
  prefs: []
  type: TYPE_NORMAL
- en: We won’t go into all the cultural and development changes one would need to
    employ before attempting to reach the stage where CDP is desirable, or even possible.
    That would be a subject for a different book and would require much more space
    than what we have. I am not even going to try to convince you to embrace continuous
    deployment. There are many valid cases when CDP is not a good option and even
    more of those when it is not even possible without substantial cultural and technical
    changes which are outside Kubernetes domain. Statistically speaking, it is likely
    that you are not ready to embrace continuous deployment.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you might be wondering whether it makes sense for you to continue
    reading. Maybe you are indeed not ready for continuous deployment, and maybe thinking
    this is a waste of time. If that’s the case, my message to you is that, it does
    not matter. The fact that you already have some experience with Kubernetes tells
    me that you are not a laggard. You chose to embrace a new way of working. You
    saw the benefits of distributed systems, and you are embracing what surely looked
    like madness when you made your first steps.
  prefs: []
  type: TYPE_NORMAL
- en: If you reached this far, you are ready to learn and practice the processes that
    follow. You might not be ready to do continuous deployment. That’s OK. You can
    fall back to continuous delivery. If that is also too big of a scratch, you can
    start with continuous integration. The reason I’m saying that it does not matter
    lies in the fact that most of the steps are the same in all those cases. No matter
    whether you are planning to do CI, CD, or CDP, you have to build something, you
    have to run some tests, and you have to deploy your applications somewhere.
  prefs: []
  type: TYPE_NORMAL
- en: From the technical perspective, it does not matter whether we deploy to a local
    cluster, to the one dedicated to testing, or to production. A deployment to a
    Kubernetes cluster is (more or less) the same no matter what its purpose is. You
    might choose to have a single cluster for everything. That’s also OK. That’s why
    we have Namespaces. You might not trust your tests. Still, that’s not a problem
    from the start because the way we execute tests is the same no matter how much
    we trust them. I can continue for a while with statements like that. What truly
    matters is that the process is, more or less, the same, no matter how much you
    trust it. Trust is earned with time.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this book is to teach you how to employ continuous deployment into
    a Kubernetes cluster. It’s up to you to decide when are your expertise, culture,
    and code ready for it. The pipeline we’ll build should be the same no matter whether
    you’re planning to use CI, CD, or CDP. Only a few arguments might change.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, the first objective is to define the base set of steps for our continuous
    deployment processes. We’ll worry about executing those steps later.
  prefs: []
  type: TYPE_NORMAL
- en: Defining Continuous Deployment Goals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The continuous deployment process is relatively easy to explain, even though
    implementation might get tricky. We’ll split our requirements into two groups.
    We’ll start with a discussion about the overall goals that should be applied to
    the whole process. To be more precise, we’ll talk about what I consider non-negotiable
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: A Pipeline needs to be secure. Typically, that would not be a problem. In past
    before Kubernetes was born, we would run the pipeline steps on separate servers.
    We’d have one dedicated to building and another for testing. We might have one
    for integration and another for performance tests. Once we adopt container schedulers
    and move into clusters, we lose control of the servers. Even though it is possible
    to run something on a specific server, that is highly discouraged in Kubernetes.
    We should let Kubernetes schedule Pods with as few restraints as possible. That
    means that our builds and tests might run in the production cluster and that might
    prove not to be secure. If we are not careful, a malicious user might exploit
    shared space. Even more likely, our tests might contain an unwanted side-effect
    that could put production applications at risk.
  prefs: []
  type: TYPE_NORMAL
- en: We could create separate clusters. One can be dedicated to the production and
    the other to everything else. While that is indeed an option we should explore,
    Kubernetes already provides the tools we need to make a cluster secure. We have
    RBAC, ServiceAccounts, Namespaces, PodSecurityPolicies, NetworkPolicies, and a
    few other resources at our disposal. So we can share the same cluster and be reasonably
    secure at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Security is not the only requirement. Even when everything is secured, we still
    need to make sure that our pipelines do not affect negatively other applications
    running inside a cluster. If we are not careful, tests might, for example, request
    or use too many resources and, as a result, we might be left with insufficient
    memory for the other applications and processes running inside our cluster. Fortunately,
    Kubernetes has a solution for those problems as well. We can combine Namespaces
    with LimitRanges and ResourceQuotas. While they do not provide a complete guarantee
    that nothing will go wrong (nothing does), they do provide a set of tools that,
    when used correctly, do provide reasonable guarantees that the processes in a
    Namespace will not go “wild”.
  prefs: []
  type: TYPE_NORMAL
- en: Our pipeline should be fast. If it takes too much time for it to execute, we
    might be compelled to start working on a new feature before the execution of the
    pipeline is finished. If it fails, we will have to decide whether to stop working
    on the new feature and incur context switching penalty or to ignore the problem
    until we are free to deal with it. While both scenarios are bad, the latter is
    worst and should be avoided at all costs. A failed pipeline must have the highest
    priority. Otherwise, what’s the point of having automated and continuous processes
    if dealing with issues is eventual?
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that we often cannot accomplish those goals independently. We
    might be forced to make tradeoffs. Security often clashes with speed, and we might
    need to strike a balance between the two.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the primary goal, that one that is above all the others, is that our
    continuous deployment pipeline must be executed on every commit to the master
    branch. That will provide continuous feedback about the readiness of the system,
    and, in a way, it will force people to merge to the master often. When we create
    a branch, it is non-existent until it gets back to the master, or whatever is
    the name of the production-ready branch. The more time passes until the merge,
    the bigger the chance that our code does not integrate with the work of our colleagues.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the high-level objectives straighten out, we should switch
    our focus to the particular steps a pipeline should contain.
  prefs: []
  type: TYPE_NORMAL
- en: Defining Continuous Deployment Steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll try to define a minimum set of steps any continuous deployment pipeline
    should execute. Do not take them literally. Every company is different, and every
    project has something special. You will likely have to extend them to suit your
    particular needs. However, that should not be a problem. Once we get a grip on
    those that are mandatory, extending the process should be relatively straightforward,
    except if you need to interact with tools that do not have a well-defined API
    nor a good CLI. If that’s the case, my recommendation is to drop those tools.
    They’re not worthy of the suffering they often impose.
  prefs: []
  type: TYPE_NORMAL
- en: We can split the pipeline into several stages. We’ll need to *build* the artifacts
    (after running static tests and analysis). We have to run *functional tests* because
    unit testing is not enough. We need to create a *release* and *deploy* it somewhere
    (hopefully to production). No matter how much we trust the earlier stages, we
    do have to run tests to validate that the deployment (to production) was successful.
    Finally, we need to do some cleanup at the end of the process and remove all the
    processes created for the Pipeline. It would be pointless to leave them running
    idle.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, the stages are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Build stage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functional testing stage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Release stage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy stage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Production testing stage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleanup stage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here’s the plan. In the build stage, we’ll build a Docker image and push it
    to a registry (in our case Docker Hub). However, since building untested artifacts
    should be stopped, we are going to run static tests before the actual build. Once
    our Docker image is pushed, we’ll deploy the application and run tests against
    it. If everything works as expected, we’ll make a new release and deploy it to
    production. To be on the safe side, we’ll run another round of tests to validate
    that the deployment was indeed successful in production. Finally, we’ll clean
    up the system by removing everything except the production release.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3-1: The stages of a continuous deployment pipeline](img/00007.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-1: The stages of a continuous deployment pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll discuss the steps of each of those stages later on. For now, we need a
    cluster we’ll use for the hands-on exercises that’ll help us get a better understanding
    of the pipeline we’ll build later. If we are successful with the manually executed
    steps, writing pipeline script should be relatively simple.
  prefs: []
  type: TYPE_NORMAL
- en: Creating A Cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll start the hands-on part by going back to the local copy of the `vfarcic/k8s-specs`
    repository and pulling the latest version.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
