<html><head></head><body>
<div class="calibre6">
<h2 id="leanpub-auto-installing-and-setting-up-jenkins" class="calibre16">Installing and Setting Up Jenkins</h2>

<aside class="tip">
    <p class="calibre3">When used by engineers, UIs are evil. They sidetrack us from repeatability and automation.</p>

</aside>

<p class="calibre3">UIs do have their purpose. They are supposed to provide enough colors and random graphs for CIO, CTO, and other C-level executives and mid-level managers. Management works in multi-color, while engineers should be limited to dual-color terminals, mixed with a slightly increased color pallet of IDEs and editors we use to code. We produce commits, while managers fake interest by looking at UIs.</p>

<p class="calibre3">The above phrase is a bit exaggerated. It’s not true that UIs are useful only to managers nor that they fake interest. At least, that’s not true for all of them. UIs do provide a lot of value but, unfortunately, they are often abused to the level of even postponing or even preventing automation. We’ll try to make an additional effort to remove Jenkins UI for any setup related tasks. We’ll try to automate everything.</p>

<p class="calibre3">We already improved a lot our ability to install Jenkins. A mere switch from custom-made YAML files to Helm Charts is a considerable step forward from the operational perspective. The addition of ServiceAccounts bound to Roles improved security. But, there’s still one big thing left only partly explored. We did not yet reach the point where we can install and fully setup Jenkins from a command line. So far, there were always a few things we had to do manually from its UI. We’ll try to get rid of those steps in the hope that the only command we’ll need to execute is <code class="calibre19">helm install</code>.</p>

<p class="calibre3">As it often goes, we cannot hope to fully automate the setup without going through manual steps first. So, we’ll start by exploring different use-cases. If we hit a road-block, we’ll try to figure out how to overcome it. The chances are that another one will be waiting for us after the first, and another one after that. We’re yet to see which obstacles we’ll encounter and which steps are missing until we make Jenkins fully operational and, at the same time, reasonably secure. We’ll try to automate the process only once we’re confident in the way we set up Jenkins manually.</p>

<h3 id="leanpub-auto-creating-a-cluster-and-retrieving-its-ip-1" class="calibre20">Creating A Cluster And Retrieving Its IP</h3>

<p class="calibre3">You already know what the first steps are. Create a new cluster or reuse the one you dedicated to the exercises.</p>

<p class="calibre3">We’ll start by going to the local copy of the <em class="calibre17">vfarcic/k8s-specs</em> repository and making sure that we have the latest revision.</p>

<aside class="information">
    <p class="calibre3">All the commands from this chapter are available in the <a href="https://gist.github.com/4ea447d106c96cb088bc8d616719f6e8">06-jenkins-setup.sh</a> Gist.</p>

</aside>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">cd</code> k8s-specs
<code class="lineno">2 </code>
<code class="lineno">3 </code>git pull
</pre></div>

</figure>

<p class="calibre3">We’ll need a few files from the <em class="calibre17">go-demo-3</em> repository you cloned in one of the previous chapters. To be on the safe side, please merge it the upstream. If you forgot the commands, they are available in the <a href="https://gist.github.com/171172b69bb75903016f0676a8fe9388">go-demo-3-merge.sh gist</a>.</p>

<p class="calibre3">The requirements are the same as those from the previous chapters. The only difference is that I will assume that you’ll store the IP of the cluster or the external load balancer as the environment variable <code class="calibre19">LB_IP</code>.</p>

<p class="calibre3">For your convenience, the Gists and the specs are available below. Please note that they are the same as those we used in the previous chapter with the addition of the <code class="calibre19">export LB_IP</code> command.</p>

<ul class="calibre21">
  <li class="calibre15">
<a href="https://gist.github.com/66842a54ef167219dc18b03991c26edb">docker4mac-ip.sh</a>: <strong class="calibre18">Docker for Mac</strong> with 3 CPUs, 3 GB RAM, with <strong class="calibre18">nginx Ingress</strong>, with <strong class="calibre18">tiller</strong>, and with <code class="calibre19">LB_IP</code> variable set to <code class="calibre19">127.0.0.1</code>.</li>
  <li class="calibre15">
<a href="https://gist.github.com/df5518b24bc39a8b8cca95cc37617221">minikube-ip.sh</a>: <strong class="calibre18">minikube</strong> with 3 CPUs, 3 GB RAM, with <code class="calibre19">ingress</code>, <code class="calibre19">storage-provisioner</code>, and <code class="calibre19">default-storageclass</code> addons enabled, with <strong class="calibre18">tiller</strong>, and with <code class="calibre19">LB_IP</code> variable set to the VM created by minikube.</li>
  <li class="calibre15">
<a href="https://gist.github.com/7ee11f4dd8a130b51407582505c817cb">kops-ip.sh</a>: <strong class="calibre18">kops in AWS</strong> with 3 t2.small masters and 2 t2.medium nodes spread in three availability zones, with <strong class="calibre18">nginx Ingress</strong>, with <strong class="calibre18">tiller</strong>, and with <code class="calibre19">LB_IP</code> variable set to the IP retrieved by pinging ELB’s hostname. The Gist assumes that the prerequisites are set through <a href="part0018.html#appendix-b">Appendix B</a>.</li>
  <li class="calibre15">
<a href="https://gist.github.com/fa902cc2e2f43dcbe88a60138dd20932">minishift-ip.sh</a>: <strong class="calibre18">minishift</strong> with 3 CPUs, 3 GB RAM, with version 1.16+, with <strong class="calibre18">tiller</strong>, and with <code class="calibre19">LB_IP</code> variable set to the VM created by minishift.</li>
  <li class="calibre15">
<a href="https://gist.github.com/3e53def041591f3c0f61569d49ffd879">gke-ip.sh</a>: <strong class="calibre18">Google Kubernetes Engine (GKE)</strong> with 3 n1-highcpu-2 (2 CPUs, 1.8 GB RAM) nodes (one in each zone), and with <strong class="calibre18">nginx Ingress</strong> controller running on top of the “standard” one that comes with GKE, with <strong class="calibre18">tiller</strong>, and with <code class="calibre19">LB_IP</code> variable set to the IP of the external load balancer created when installing nginx Ingress. We’ll use nginx Ingress for compatibility with other platforms. Feel free to modify the YAML files and Helm Charts if you prefer NOT to install nginx Ingress.</li>
  <li class="calibre15">
<a href="https://gist.github.com/f7f3956cd39c3bc55638529cfeb2ff12">eks-ip.sh</a>: <strong class="calibre18">Elastic Kubernetes Service (EKS)</strong> with 2 t2.medium nodes, with <strong class="calibre18">nginx Ingress</strong> controller, with a <strong class="calibre18">default StorageClass</strong>, with <strong class="calibre18">tiller</strong>, and with <code class="calibre19">LB_IP</code> variable set tot he IP retrieved by pinging ELB’s hostname.</li>
</ul>

<p class="calibre3">Now we’re ready to install Jenkins.</p>

<h3 id="leanpub-auto-running-jenkins" class="calibre20">Running Jenkins</h3>

<p class="calibre3">We’ll need a domain which we’ll use to set Ingress’ hostname and through which we’ll be able to open Jenkins UI. We’ll continue using <em class="calibre17">nip.io</em> service to generate domains. Just as before, remember that this is only a temporary solution and that you should use “real” domains with the IP of your external load balancer instead.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">JENKINS_ADDR</code><code class="o">=</code><code class="s">"jenkins.</code><code class="nv">$LB_IP</code><code class="s">.nip.io"</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code><code class="nb">echo</code> <code class="nv">$JENKINS_ADDR</code>
</pre></div>

</figure>

<p class="calibre3">The output of the latter command should provide a visual confirmation that the address we’ll use for Jenkins looks OK. In my case, it is <code class="calibre19">jenkins.52.15.140.221.nip.io</code>.</p>

<aside class="warning">
    <h3 id="leanpub-auto-a-note-to-minishift-users-24" class="calibre22">A note to minishift users</h3>

  <p class="calibre3">Helm will try to install Jenkins Chart with the process in a container running as user 0. By default, that is not allowed in OpenShift. We’ll skip discussing the best approach to correct the issue, and I’ll assume you already know how to set the permissions on the per-Pod basis. Instead, we’ll do the most straightforward fix. Please execute the command that follows to allow the creation of restricted Pods to run as any user.</p>

  <p class="calibre3"><code class="calibre19">oc patch scc restricted -p '{"runAsUser":{"type": "RunAsAny"}}'</code></p>

</aside>

<p class="calibre3">We’ll start exploring the steps we’ll need to run Jenkins in a Kubernetes cluster by executing the same <code class="calibre19">helm install</code> command we used in the previous chapters. It won’t provide everything we need, but it will be a good start. We’ll improve the process throughout the rest of the chapter with the objective of having a fully automated Jenkins installation process. We might not be able to accomplish our goal 100%. Or, we might conclude that full automation is not worth the trouble. Nevertheless, we’ll use the installation from the <a href="part0012.html#chartmuseum">Packaging Kubernetes Applications</a> as the base and see how far we can go in our quest for full automation.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>helm install stable/jenkins <code class="se">\</code>
<code class="lineno">2 </code>    --name jenkins <code class="se">\</code>
<code class="lineno">3 </code>    --namespace jenkins <code class="se">\</code>
<code class="lineno">4 </code>    --values helm/jenkins-values.yml <code class="se">\</code>
<code class="lineno">5 </code>    --set Master.HostName<code class="o">=</code><code class="nv">$JENKINS_ADDR</code>
</pre></div>

</figure>

<p class="calibre3">Next, we’ll confirm that Jenkins is rolled out.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kubectl -n jenkins <code class="se">\</code>
<code class="lineno">2 </code>    rollout status deployment jenkins
</pre></div>

</figure>

<p class="calibre3">The latter command will wait until <code class="calibre19">jenkins</code> Deployment rolls out. Its output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>Waiting for rollout to finish: 0 of 1 updated replicas are available...
<code class="lineno">2 </code>deployment "jenkins" successfully rolled out
</pre></div>

</figure>

<aside class="warning">
    <h3 id="leanpub-auto-a-note-to-minishift-users-25" class="calibre22">A note to minishift users</h3>

  <p class="calibre3">OpenShift requires Routes to make services accessible outside the cluster. To make things more complicated, they are not part of “standard Kubernetes” so we’ll need to create one using <code class="calibre19">oc</code>. Please execute the command that follows.</p>

  <p class="calibre3"><code class="calibre19">oc -n jenkins create route edge --service jenkins --insecure-policy Allow --hostname $JENKINS_ADDR</code></p>

  <p class="calibre3">That command created an <code class="calibre19">edge</code> Router tied to the <code class="calibre19">jenkins</code> Service. Since we do not have SSL certificates for HTTPS communication, we also specified that it is OK to use insecure policy which will allow us to access Jenkins through plain HTTP. Finally, the last argument defined the address through which we’d like to access Jenkins UI.</p>

</aside>


<figure class="image1">
  <img src="../images/00016.jpeg" alt="Figure 6-1: Jenkins setup operating in a single Namespace" class="calibre25"/>
  <figcaption class="calibre26">Figure 6-1: Jenkins setup operating in a single Namespace</figcaption>
</figure>


<p class="calibre3">Now that Jenkins is up-and-running, we can open it in your favorite browser.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">"</code>
</pre></div>

</figure>

<aside class="tip">
    <h3 id="leanpub-auto-a-note-to-windows-users-3" class="calibre22">A note to Windows users</h3>

  <p class="calibre3">Git Bash might not be able to use the <code class="calibre19">open</code> command. If that’s the case, please replace the <code class="calibre19">open</code> command with <code class="calibre19">echo</code>. As a result, you’ll get the full address that should be opened directly in your browser of choice.</p>

</aside>

<p class="calibre3">Since this is the first time we’re accessing this Jenkins instance, we’ll need to login first. Just as before, the password is stored in the Secret <code class="calibre19">jenkins</code>, under <code class="calibre19">jenkins-admin-password</code>. So, we’ll query the secret to find out the password.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">JENKINS_PASS</code><code class="o">=</code><code class="k">$(</code>kubectl -n jenkins <code class="se">\</code>
<code class="lineno">2 </code>    get secret jenkins <code class="se">\</code>
<code class="lineno">3 </code>    -o <code class="nv">jsonpath</code><code class="o">=</code><code class="s">"{.data.jenkins-admin-password}"</code> <code class="se">\</code>
<code class="lineno">4 </code>    <code class="calibre19">|</code> base64 --decode<code class="calibre19">;</code> <code class="nb">echo</code><code class="k">)</code>
<code class="lineno">5 </code>
<code class="lineno">6 </code><code class="nb">echo</code> <code class="nv">$JENKINS_PASS</code>
</pre></div>

</figure>

<p class="calibre3">The output of the latter command should be a random string. As an example, I got <code class="calibre19">Ucg2tab4FK</code>. Please copy it, return to the Jenkins login screen opened in your browser, and use it to authenticate. We did not retrieve the username since it is hard-coded to <em class="calibre17">admin</em>.</p>

<p class="calibre3">We’ll leave this admin user as-is since we won’t explore authentication methods. When running Jenkins “for real”, you should install a plugin that provides the desired authentication mechanism and configure Jenkins to use it instead. That could be LDAP, Google or GitHub authentication, and many other providers. For now, we’ll continue using <em class="calibre17">admin</em> as the only god-like user.</p>

<p class="calibre3">Now that we got Jenkins up-and-running, we’ll create a Pipeline which can be used to test our setup.</p>

<h3 id="leanpub-auto-using-pods-to-run-tools" class="calibre20">Using Pods to Run Tools</h3>

<p class="calibre3">We won’t explore how to write a continuous deployment pipeline in this chapter. That is reserved for the next one. Right now, we are only concerned whether our Jenkins setup is working as expected. We need to know if Jenkins can interact with Kubernetes, whether we can run the tools we need as Pods, and whether they can be spun across different Namespaces. On top of those, we still need to solve the issue of building container images. Since we already established that it is not a good idea to mount a Docker socket, nor to run containers in privileged mode, we need to find a valid alternative. In parallel to solving those and a few other challenges we’ll encounter, we cannot lose focus from automation. Everything we do has to be converted into automatic setup unless we make a conscious decision that it is not worth the trouble.</p>

<p class="calibre3">I’m jumping ahead of myself by bombing you with too many things. We’ll backtrack a bit and start with a simple requirement. Later on, we’ll build on top of it. So, our first requirement is to run different tools packaged as containers inside a Pod.</p>

<p class="calibre3">Please go back to Jenkins UI and click the <em class="calibre17">New Item</em> link in the left-hand menu. Type <em class="calibre17">my-k8s-job</em> in the <em class="calibre17">item name</em> field, select <em class="calibre17">Pipeline</em> as the job type and click the <em class="calibre17">OK</em> button.</p>

<p class="calibre3">We created a new job which does not yet do anything. Our next step is to write a very simple Pipeline that will validate that we can indeed use Jenkins to spin up a Pod with the containers we need.</p>

<p class="calibre3">Please click the <em class="calibre17">Pipeline</em> tab and you’ll be presented with the <em class="calibre17">Pipeline Script</em> field. Write the script that follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">podTemplate</code><code class="o">(</code>
<code class="lineno"> 2 </code>    <code class="nl">label:</code> <code class="s">"kubernetes"</code><code class="o">,</code>
<code class="lineno"> 3 </code>    <code class="nl">containers:</code> <code class="o">[</code>
<code class="lineno"> 4 </code>        <code class="calibre19">containerTemplate</code><code class="o">(</code><code class="nl">name:</code> <code class="s">"maven"</code><code class="o">,</code> <code class="nl">image:</code> <code class="s">"maven:alpine"</code><code class="o">,</code> <code class="nl">ttyEnabled:</code> <code class="k">true</code><code class="o">,</code> <code class="calibre19">co</code><code class="err">\</code>
<code class="lineno"> 5 </code><code class="nl">mmand:</code> <code class="s">"cat"</code><code class="o">),</code>
<code class="lineno"> 6 </code>        <code class="calibre19">containerTemplate</code><code class="o">(</code><code class="nl">name:</code> <code class="s">"golang"</code><code class="o">,</code> <code class="nl">image:</code> <code class="s">"golang:alpine"</code><code class="o">,</code> <code class="nl">ttyEnabled:</code> <code class="k">true</code><code class="o">,</code> <code class="err">\</code>
<code class="lineno"> 7 </code><code class="nl">command:</code> <code class="s">"cat"</code><code class="o">)</code>
<code class="lineno"> 8 </code>    <code class="o">]</code>
<code class="lineno"> 9 </code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">10 </code>    <code class="calibre19">node</code><code class="o">(</code><code class="s">"kubernetes"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">11 </code>        <code class="calibre19">container</code><code class="o">(</code><code class="s">"maven"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">12 </code>            <code class="calibre19">stage</code><code class="o">(</code><code class="s">"build"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">13 </code>                <code class="calibre19">sh</code> <code class="s">"mvn --version"</code>
<code class="lineno">14 </code>            <code class="o">}</code>
<code class="lineno">15 </code>            <code class="calibre19">stage</code><code class="o">(</code><code class="s">"unit-test"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">16 </code>                <code class="calibre19">sh</code> <code class="s">"java -version"</code>
<code class="lineno">17 </code>            <code class="o">}</code>
<code class="lineno">18 </code>        <code class="o">}</code>
<code class="lineno">19 </code>        <code class="calibre19">container</code><code class="o">(</code><code class="s">"golang"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">20 </code>            <code class="calibre19">stage</code><code class="o">(</code><code class="s">"deploy"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">21 </code>                <code class="calibre19">sh</code> <code class="s">"go version"</code>
<code class="lineno">22 </code>            <code class="o">}</code>
<code class="lineno">23 </code>        <code class="o">}</code>
<code class="lineno">24 </code>    <code class="o">}</code>
<code class="lineno">25 </code><code class="o">}</code>
</pre></div>

</figure>

<aside class="information">
    <p class="calibre3">If you prefer to copy and paste, the job is available in the <a href="https://gist.github.com/2cf872c3a9acac51409fbd5a2789cb02">my-k8s-job.groovy Gist</a>.</p>

</aside>

<p class="calibre3">The script defines a Pod template with two containers. One is based on the <code class="calibre19">maven</code> image and the other on the <code class="calibre19">golang</code> image. Further down, we defined that Jenkins should use that template as the <code class="calibre19">node</code>. Inside it, we are using the <code class="calibre19">maven</code> container to execute two stages. One will return Maven version, and the other will output Java version. Further down, we switch to the <code class="calibre19">golang</code> container only to output Go version.</p>

<p class="calibre3">This job is straightforward, and it does not do anything related to our continuous deployment processes. Nevertheless, it should be enough to provide a necessary validation that we can use Jenkins to create a Pod, that we can switch from one container to another, and that we can execute commands inside them.</p>

<p class="calibre3">Don’t forget to click the <em class="calibre17">Save</em> button before proceeding.</p>

<p class="calibre3">If the job we created looks familiar, that’s because it is the same as the one we used in the <a href="part0010.html#sa">Enabling Process Communication With Kube API Through Service Accounts</a> chapter. Since our goal is to confirm that our current Jenkins setup can create the Pods, that job is as good as any other to validate that claim.</p>

<p class="calibre3">Please click the <em class="calibre17">Open Blue Ocean</em> link from the left-hand menu. You’ll see the <em class="calibre17">Run</em> button in the middle of the screen. Click it. As a result, a row will appear with a new build. Click it to see the details.</p>

<p class="calibre3">The build is running, and we should go back to the terminal window to confirm that the Pod is indeed created.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kubectl -n jenkins get pods
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>NAME              READY STATUS            RESTARTS AGE
<code class="lineno">2 </code>jenkins-...       1/1   Running           0        5m
<code class="lineno">3 </code>jenkins-slave-... 0/3   ContainerCreating 0        16s
</pre></div>

</figure>

<p class="calibre3">We can see that there are two Pods in the <code class="calibre19">jenkins</code> Namespace. One is hosting Jenkins itself, while the other was created when we run the Jenkins build. You’ll notice that even though we defined two containers, we can see three. The additional container was added automatically to the Pod, and it’s used to establish communication with Jenkins.</p>

<p class="calibre3">In your case, the status of the <code class="calibre19">jenkins-slave</code> Pod might be different. Besides <code class="calibre19">ContainerCreating</code>, it could be <code class="calibre19">Running</code>, <code class="calibre19">Terminating</code>, or you might not even see it. It all depends on how much time passed between initiating the build and retrieving the Pods in the <code class="calibre19">jenkins</code> Namespace.</p>

<p class="calibre3">What matters is the process. When we initiated a new build, Jenkins created the Pod in the same Namespace. Once all the containers are up-and-running, Jenkins will execute the steps we defined through the Pipeline script. When finished, the Pod will be removed, freeing resources for other processes.</p>

<p class="calibre3">Please go back to Jenkins UI and wait until the build is finished.</p>


<figure class="image1">
  <img src="../images/00017.jpeg" alt="Figure 6-2: Jenkins spinning an agent (slave) Pod in the same Namespace" class="calibre25"/>
  <figcaption class="calibre26">Figure 6-2: Jenkins spinning an agent (slave) Pod in the same Namespace</figcaption>
</figure>


<p class="calibre3">We proved that we could run a very simple job. We’re yet to discover whether we can do more complicated operations.</p>

<p class="calibre3">On the first look, the script we wrote looks OK. However, I’m not happy with the way we defined <code class="calibre19">podTemplate</code>. Wouldn’t it be better if we could use the same YAML format for defining the template as if we’d define a Pod in Kubernetes? Fortunately, <a href="https://github.com/jenkinsci/kubernetes-plugin">jenkins-kubernetes-plugin</a> recently added that feature. So, we’ll try to rewrite the script to better match Pod definitions.</p>

<p class="calibre3">We’ll use the rewriting opportunity to replace <code class="calibre19">maven</code> with the tools we are more likely to use with a CD pipeline for the <em class="calibre17">go-demo-3</em> application. We still need <code class="calibre19">golang</code>. On top of it, we should be able to run <code class="calibre19">kubectl</code>, <code class="calibre19">helm</code>, and, <code class="calibre19">openshift-client</code>. The latter is required only if you’re using OpenShift, and you are free to remove it if that’s not your case.</p>

<p class="calibre3">Let’s open <code class="calibre19">my-k8s-job</code> configuration screen and modify the job.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/job/my-k8s-job/configure"</code>
</pre></div>

</figure>

<p class="calibre3">Please click the <em class="calibre17">Pipeline</em> tab and replace the script with the one that follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">podTemplate</code><code class="o">(</code><code class="nl">label:</code> <code class="s">"kubernetes"</code><code class="o">,</code> <code class="nl">yaml:</code> <code class="s">"""</code>
<code class="lineno"> 2 </code><code class="s">apiVersion: v1</code>
<code class="lineno"> 3 </code><code class="s">kind: Pod</code>
<code class="lineno"> 4 </code><code class="s">spec:</code>
<code class="lineno"> 5 </code><code class="s">  containers:</code>
<code class="lineno"> 6 </code><code class="s">  - name: kubectl</code>
<code class="lineno"> 7 </code><code class="s">    image: vfarcic/kubectl</code>
<code class="lineno"> 8 </code><code class="s">    command: ["sleep"]</code>
<code class="lineno"> 9 </code><code class="s">    args: ["100000"]</code>
<code class="lineno">10 </code><code class="s">  - name: oc</code>
<code class="lineno">11 </code><code class="s">    image: vfarcic/openshift-client</code>
<code class="lineno">12 </code><code class="s">    command: ["sleep"]</code>
<code class="lineno">13 </code><code class="s">    args: ["100000"]</code>
<code class="lineno">14 </code><code class="s">  - name: golang</code>
<code class="lineno">15 </code><code class="s">    image: golang:1.9</code>
<code class="lineno">16 </code><code class="s">    command: ["sleep"]</code>
<code class="lineno">17 </code><code class="s">    args: ["100000"]</code>
<code class="lineno">18 </code><code class="s">  - name: helm</code>
<code class="lineno">19 </code><code class="s">    image: vfarcic/helm:2.8.2</code>
<code class="lineno">20 </code><code class="s">    command: ["sleep"]</code>
<code class="lineno">21 </code><code class="s">    args: ["100000"]</code>
<code class="lineno">22 </code><code class="s">"""</code>
<code class="lineno">23 </code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">24 </code>    <code class="calibre19">node</code><code class="o">(</code><code class="s">"kubernetes"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">25 </code>        <code class="calibre19">container</code><code class="o">(</code><code class="s">"kubectl"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">26 </code>            <code class="calibre19">stage</code><code class="o">(</code><code class="s">"kubectl"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">27 </code>                <code class="calibre19">sh</code> <code class="s">"kubectl version"</code>
<code class="lineno">28 </code>            <code class="o">}</code>
<code class="lineno">29 </code>        <code class="o">}</code>
<code class="lineno">30 </code>        <code class="calibre19">container</code><code class="o">(</code><code class="s">"oc"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">31 </code>            <code class="calibre19">stage</code><code class="o">(</code><code class="s">"oc"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">32 </code>                <code class="calibre19">sh</code> <code class="s">"oc version"</code>
<code class="lineno">33 </code>            <code class="o">}</code>
<code class="lineno">34 </code>        <code class="o">}</code>
<code class="lineno">35 </code>        <code class="calibre19">container</code><code class="o">(</code><code class="s">"golang"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">36 </code>            <code class="calibre19">stage</code><code class="o">(</code><code class="s">"golang"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">37 </code>                <code class="calibre19">sh</code> <code class="s">"go version"</code>
<code class="lineno">38 </code>            <code class="o">}</code>
<code class="lineno">39 </code>        <code class="o">}</code>
<code class="lineno">40 </code>        <code class="calibre19">container</code><code class="o">(</code><code class="s">"helm"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">41 </code>            <code class="calibre19">stage</code><code class="o">(</code><code class="s">"helm"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">42 </code>                <code class="calibre19">sh</code> <code class="s">"helm version"</code>
<code class="lineno">43 </code>            <code class="o">}</code>
<code class="lineno">44 </code>        <code class="o">}</code>
<code class="lineno">45 </code>    <code class="o">}</code>
<code class="lineno">46 </code><code class="o">}</code>
</pre></div>

</figure>

<aside class="information">
    <p class="calibre3">If you prefer to copy and paste, the job is available in the <a href="https://gist.github.com/a1b3b36c68323aea161d7364b1231de2">my-k8s-job-yaml.groovy Gist</a>.</p>

</aside>

<p class="calibre3">This time, the format of the script is different. Instead of the <code class="calibre19">containers</code> argument inside <code class="calibre19">podTemplate</code>, now we have <code class="calibre19">yaml</code>. Inside it is Kubernetes Pod definition just as if we’d define a standard Kubernetes resource.</p>

<p class="calibre3">The rest of the script follows the same logic as before. The only difference is that this time we are using the tools were are more likely to need in our yet-to-be-defined <em class="calibre17">go-demo-3</em> Pipeline. All we’re doing is churning output of <code class="calibre19">kubectl</code>, <code class="calibre19">oc</code>, <code class="calibre19">go</code>, and <code class="calibre19">helm</code> versions.</p>

<p class="calibre3">Don’t forget to click the <em class="calibre17">Save</em> button.</p>

<p class="calibre3">Next, we’ll run a build of the job with the new script.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/blue/organizations/jenkins/my-k8s-job/activity"</code>
</pre></div>

</figure>

<p class="calibre3">Please click the <em class="calibre17">Run</em> button, followed with a click on the row with the new build.</p>

<p class="calibre3">I have an assignment for you while the build is running. Try to find out what is wrong with our current setup without looking at the results of the build. You have approximately six minutes to complete the task. Proceed only if you know the answer or if you gave up.</p>

<p class="calibre3">Jenkins will create a Pod in the same Namespace. That Pod will have five containers, four of which will host the tools we specified in the <code class="calibre19">podTemplate</code>, and Jenkins will inject the fifth as a way to establish the communication between Jenkins and the Pod. We can confirm that by listing the Pods in the <code class="calibre19">jenkins</code> Namespace.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kubectl -n jenkins get pods
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>NAME              READY STATUS            RESTARTS AGE
<code class="lineno">2 </code>jenkins-...       1/1   Running           0        16m
<code class="lineno">3 </code>jenkins-slave-... 0/5   ContainerCreating 0        19s
</pre></div>

</figure>

<p class="calibre3">So far, everything looks OK. Containers are being created. The <code class="calibre19">jenkins-slave-...</code> Pod will soon change its status to <code class="calibre19">Running</code>, and Jenkins will try to execute all the steps defined in the script.</p>

<p class="calibre3">Let’s take a look at the build from Jenkins’ UI.</p>

<p class="calibre3">After a while, the build will reach the <code class="calibre19">helm</code> stage. Click it, and you’ll see the output similar to the one that follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>[my-k8s-job] Running shell script
<code class="lineno">2 </code>
<code class="lineno">3 </code>+ helm version
<code class="lineno">4 </code>
<code class="lineno">5 </code>Client: &amp;version.Version{SemVer:"v2.8.2", GitCommit:"...", GitTreeState:"clean"}
</pre></div>

</figure>

<p class="calibre3">You’ll notice that the build will hang at this point. After a few minutes, you might think that it will hang forever. It won’t. Approximately five minutes later, the output of the step in the <code class="calibre19">helm</code> stage will change to the one that follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>[my-k8s-job] Running shell script
<code class="lineno">2 </code>
<code class="lineno">3 </code>+ helm version
<code class="lineno">4 </code>
<code class="lineno">5 </code>Client: &amp;version.Version{SemVer:"v2.8.2", GitCommit:"...", GitTreeState:"clean"}
<code class="lineno">6 </code>
<code class="lineno">7 </code>EXITCODE   0Error: cannot connect to Tiller
<code class="lineno">8 </code>
<code class="lineno">9 </code>script returned exit code 1
</pre></div>

</figure>

<aside class="warning">
    <h3 id="leanpub-auto-a-note-to-docker-for-macwindows-users-4" class="calibre22">A note to Docker For Mac/Windows users</h3>

  <p class="calibre3">Even though Docker for Mac/Windows supports RBAC, it allows any internal process inside containers to communicate with Kube API. Unlike with other Kubernetes flavors, you will not see the same error. The build will complete successfully.</p>

</aside>

<p class="calibre3">Our build could not connect to <code class="calibre19">Tiller</code>. Helm kept trying for five minutes. It reached its pre-defined timeout, and it gave up.</p>


<figure class="image1">
  <img src="../images/00018.jpeg" alt="Figure 6-3: Jenkins agent (slave) Pod trying to connect to tiller in a different Namespace" class="calibre25"/>
  <figcaption class="calibre26">Figure 6-3: Jenkins agent (slave) Pod trying to connect to tiller in a different Namespace</figcaption>
</figure>


<p class="calibre3">If what we learned in the <a href="part0010.html#sa">Enabling Process Communication With Kube API Through Service Accounts</a> chapter is still fresh in your mind, that outcome should not be a surprise. We did not set ServiceAccount that would allow Helm running inside a container to communicate with Tiller. It is questionable whether we should even allow Helm running in a container to interact with Tiller running in <code class="calibre19">kube-system</code>. That would be a huge security risk that would allow anyone with access to Jenkins to gain access to any part of the cluster. It would defy one of the big reasons why we’re using Namespaces. We’ll explore this, and a few other problems next. For now, we’ll confirm that Jenkins removed the Pod created by the failed build.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kubectl -n jenkins get pods
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>NAME        READY STATUS  RESTARTS AGE
<code class="lineno">2 </code>jenkins-... 1/1   Running 0        42m
</pre></div>

</figure>

<p class="calibre3">The <code class="calibre19">jenkins-slave-...</code> Pod is gone, and our system is restored to the state before the build started.</p>

<h3 id="leanpub-auto-running-builds-in-different-namespaces" class="calibre20">Running Builds In Different Namespaces</h3>

<p class="calibre3">One of the significant disadvantages of the script we used inside <code class="calibre19">my-k8s-job</code> is that it runs in the same Namespace as Jenkins. We should separate builds from Jenkins and thus ensure that they do not affect its stability.</p>

<p class="calibre3">We can create a system where each application has two namespaces; one for testing and the other for production. We can define quotas, limitations, and other things we are used to defining on the Namespace level. As a result, we can guarantee that testing an application will not affect the production release. With Namespaces we can separate one set of applications from another. At the same time, we’ll reduce the chance that one team will accidentally mess up with the applications of the other. Our end-goal is to be secure without limiting our teams. By giving them freedom in their own Namespace, we can be secure without impacting team’s performance and its ability to move forward without depending on other teams.</p>

<p class="calibre3">Let’s go back to the job configuration screen.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/job/my-k8s-job/configure"</code>
</pre></div>

</figure>

<p class="calibre3">Please click the <em class="calibre17">Pipeline</em> tab and replace the script with the one that follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">podTemplate</code><code class="o">(</code>
<code class="lineno"> 2 </code>    <code class="nl">label:</code> <code class="s">"kubernetes"</code><code class="o">,</code>
<code class="lineno"> 3 </code>    <code class="nl">namespace:</code> <code class="s">"go-demo-3-build"</code><code class="o">,</code>
<code class="lineno"> 4 </code>    <code class="nl">serviceAccount:</code> <code class="s">"build"</code><code class="o">,</code>
<code class="lineno"> 5 </code>    <code class="nl">yaml:</code> <code class="s">"""</code>
<code class="lineno"> 6 </code><code class="s">apiVersion: v1</code>
<code class="lineno"> 7 </code><code class="s">kind: Pod</code>
<code class="lineno"> 8 </code><code class="s">spec:</code>
<code class="lineno"> 9 </code><code class="s">  containers:</code>
<code class="lineno">10 </code><code class="s">  - name: kubectl</code>
<code class="lineno">11 </code><code class="s">    image: vfarcic/kubectl</code>
<code class="lineno">12 </code><code class="s">    command: ["sleep"]</code>
<code class="lineno">13 </code><code class="s">    args: ["100000"]</code>
<code class="lineno">14 </code><code class="s">  - name: oc</code>
<code class="lineno">15 </code><code class="s">    image: vfarcic/openshift-client</code>
<code class="lineno">16 </code><code class="s">    command: ["sleep"]</code>
<code class="lineno">17 </code><code class="s">    args: ["100000"]</code>
<code class="lineno">18 </code><code class="s">  - name: golang</code>
<code class="lineno">19 </code><code class="s">    image: golang:1.9</code>
<code class="lineno">20 </code><code class="s">    command: ["sleep"]</code>
<code class="lineno">21 </code><code class="s">    args: ["100000"]</code>
<code class="lineno">22 </code><code class="s">  - name: helm</code>
<code class="lineno">23 </code><code class="s">    image: vfarcic/helm:2.8.2</code>
<code class="lineno">24 </code><code class="s">    command: ["sleep"]</code>
<code class="lineno">25 </code><code class="s">    args: ["100000"]</code>
<code class="lineno">26 </code><code class="s">"""</code>
<code class="lineno">27 </code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">28 </code>    <code class="calibre19">node</code><code class="o">(</code><code class="s">"kubernetes"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">29 </code>        <code class="calibre19">container</code><code class="o">(</code><code class="s">"kubectl"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">30 </code>            <code class="calibre19">stage</code><code class="o">(</code><code class="s">"kubectl"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">31 </code>                <code class="calibre19">sh</code> <code class="s">"kubectl version"</code>
<code class="lineno">32 </code>            <code class="o">}</code>
<code class="lineno">33 </code>        <code class="o">}</code>
<code class="lineno">34 </code>        <code class="calibre19">container</code><code class="o">(</code><code class="s">"oc"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">35 </code>            <code class="calibre19">stage</code><code class="o">(</code><code class="s">"oc"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">36 </code>                <code class="calibre19">sh</code> <code class="s">"oc version"</code>
<code class="lineno">37 </code>            <code class="o">}</code>
<code class="lineno">38 </code>        <code class="o">}</code>
<code class="lineno">39 </code>        <code class="calibre19">container</code><code class="o">(</code><code class="s">"golang"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">40 </code>            <code class="calibre19">stage</code><code class="o">(</code><code class="s">"golang"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">41 </code>                <code class="calibre19">sh</code> <code class="s">"go version"</code>
<code class="lineno">42 </code>            <code class="o">}</code>
<code class="lineno">43 </code>        <code class="o">}</code>
<code class="lineno">44 </code>        <code class="calibre19">container</code><code class="o">(</code><code class="s">"helm"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">45 </code>            <code class="calibre19">stage</code><code class="o">(</code><code class="s">"helm"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">46 </code>                <code class="calibre19">sh</code> <code class="s">"helm version --tiller-namespace go-demo-3-build"</code>
<code class="lineno">47 </code>            <code class="o">}</code>
<code class="lineno">48 </code>        <code class="o">}</code>
<code class="lineno">49 </code>    <code class="o">}</code>
<code class="lineno">50 </code><code class="o">}</code>
</pre></div>

</figure>

<aside class="information">
    <p class="calibre3">Getting spoiled with Gist and still do not want to type? The job is available in the <a href="https://gist.github.com/ced1806af8e092d202942a79e81d5ba9">my-k8s-job-ns.groovy Gist</a>.</p>

</aside>

<p class="calibre3">The only difference between that job and the one we used before is in <code class="calibre19">podTemplate</code> arguments <code class="calibre19">namespace</code> and <code class="calibre19">serviceAccount</code>. This time we specified that the Pod should be created in the <code class="calibre19">go-demo-3-build</code> Namespace and that it should use the ServiceAccount <code class="calibre19">build</code>. If everything works as expected, the instruction to run Pods in a different Namespace should provide the separation we crave, and the ServiceAccount will give the permissions the Pod might need when interacting with Kube API or other Pods.</p>

<p class="calibre3">Please click the <em class="calibre17">Save</em> button to persist the change of the Job definition.</p>

<p class="calibre3">Next, we’ll open Jenkins’ BlueOcean screen and check whether we can run builds based on the modified Job.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/blue/organizations/jenkins/my-k8s-job/activity"</code>
</pre></div>

</figure>

<p class="calibre3">Please click the <em class="calibre17">Run</em> button, and select the row with the new build. You’ll see the same <code class="calibre19">Waiting for next available executor</code> message we’ve already seen in the past. Jenkins needs to wait until a Pod is created and is fully operational. However, this time the wait will be longer since Jenkins will not be able to create the Pod.</p>

<p class="calibre3">The fact that we defined that the Job should operate in a different Namespace will do us no good if such a Namespace does not exist. Even if we create the Namespace, we specified that it should use the ServiceAccount <code class="calibre19">build</code>. So, we need to create both. However, that’s not where our troubles stop. There are a few other problems we’ll need to solve but, for now, we’ll concentrate on the missing Namespace.</p>

<p class="calibre3">Please click the <em class="calibre17">Stop</em> button in the top-right corner or the build. That will abort the futile attempt to create a Pod, and we can proceed and make the necessary changes that will allow us to run a build of that Job in the <code class="calibre19">go-demo-3-build</code> Namespace.</p>

<p class="calibre3">As a minimum, we’ll have to make sure that the <code class="calibre19">go-demo-3-build</code> Namespace exists and that it has the ServiceAccount <code class="calibre19">build</code> which is bound to a Role with sufficient permissions. While we’re defining the Namespace, we should probably define a LimitRange and a ResourceQuota. Fortunately, we already did all that in the previous chapters, and we already have a YAML file that does just that.</p>

<p class="calibre3">Let’s take a quick look at the <code class="calibre19">build-ns.yml</code> file available in the <em class="calibre17">go-demo-3</em> repository.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>cat ../go-demo-3/k8s/build-ns.yml
</pre></div>

</figure>

<p class="calibre3">We won’t go through the details behind that definition since we already explored it in the previous chapters. Instead, we’ll imagine that we are cluster administrators and that the team in charge of <em class="calibre17">go-demo-3</em> asked us to <code class="calibre19">apply</code> that definition.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kubectl apply <code class="se">\</code>
<code class="lineno">2 </code>    -f ../go-demo-3/k8s/build-ns.yml <code class="se">\</code>
<code class="lineno">3 </code>    --record
</pre></div>

</figure>

<p class="calibre3">The output shows that the resources defined in that YAML were created.</p>

<p class="calibre3">Even though we won’t build a continuous deployment pipeline just yet, we should be prepared for running our application in production. Since it should be separated from the testing Pods and releases under test, we’ll create another Namespace that will be used exclusively for <em class="calibre17">go-demo-3</em> production releases. Just as before, we’ll <code class="calibre19">apply</code> the definition stored in the <em class="calibre17">go-demo-3</em> repository.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>cat ../go-demo-3/k8s/prod-ns.yml
<code class="lineno">2 </code>
<code class="lineno">3 </code>kubectl apply <code class="se">\</code>
<code class="lineno">4 </code>    -f ../go-demo-3/k8s/prod-ns.yml <code class="se">\</code>
<code class="lineno">5 </code>    --record
</pre></div>

</figure>

<p class="calibre3">We’re missing one more thing before the part of the setup related to Kubernetes resources is finished.</p>

<p class="calibre3">So far, we have a RoleBinding inside the <code class="calibre19">jenkins</code> Namespace that provides Jenkins with enough permissions to create Pods in the same Namespace. However, our latest Pipeline wants to create Pods in the <code class="calibre19">go-demo-3-build</code> Namespace. Given that we are not using ClusterRoleBinding that would provide cluster-wide permissions, we’ll need to create a RoleBinding in <code class="calibre19">go-demo-3-build</code> as well. Since that is specific to the application, the definition is in its repository, and it should be executed by the administrator of the cluster, just as the previous two.</p>

<p class="calibre3">Let’s take a quick look at the definition.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>cat ../go-demo-3/k8s/jenkins.yml
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">apiVersion</code><code class="calibre19">:</code> <code class="calibre19">rbac.authorization.k8s.io/v1</code>
<code class="lineno"> 2 </code><code class="calibre19">kind</code><code class="calibre19">:</code> <code class="calibre19">RoleBinding</code>
<code class="lineno"> 3 </code><code class="calibre19">metadata</code><code class="calibre19">:</code>
<code class="lineno"> 4 </code>  <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">jenkins-role-binding</code>
<code class="lineno"> 5 </code>  <code class="calibre19">namespace</code><code class="calibre19">:</code> <code class="calibre19">go-demo-3-build</code>
<code class="lineno"> 6 </code>  <code class="calibre19">labels</code><code class="calibre19">:</code>
<code class="lineno"> 7 </code>    <code class="calibre19">app</code><code class="calibre19">:</code> <code class="calibre19">jenkins</code>
<code class="lineno"> 8 </code><code class="calibre19">roleRef</code><code class="calibre19">:</code>
<code class="lineno"> 9 </code>  <code class="calibre19">apiGroup</code><code class="calibre19">:</code> <code class="calibre19">rbac.authorization.k8s.io</code>
<code class="lineno">10 </code>  <code class="calibre19">kind</code><code class="calibre19">:</code> <code class="calibre19">ClusterRole</code>
<code class="lineno">11 </code>  <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">cluster-admin</code>
<code class="lineno">12 </code><code class="calibre19">subjects</code><code class="calibre19">:</code>
<code class="lineno">13 </code><code class="calibre19">-</code> <code class="calibre19">kind</code><code class="calibre19">:</code> <code class="calibre19">ServiceAccount</code>
<code class="lineno">14 </code>  <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">jenkins</code>
<code class="lineno">15 </code>  <code class="calibre19">namespace</code><code class="calibre19">:</code> <code class="calibre19">jenkins</code>
</pre></div>

</figure>

<p class="calibre3">The binding is relatively straightforward. It will bind the ServiceAccount in the <code class="calibre19">jenkins</code> Namespace with the ClusterRole <code class="calibre19">cluster-admin</code>. We will reduce those permissions in the next chapter. For now, remember that we’re creating a RoleBinding in the <code class="calibre19">go-demo-3-build</code> Namespace and that it’ll give ServiceAccount <code class="calibre19">jenkins</code> in the <code class="calibre19">jenkins</code> Namespace full permissions to do whatever it wants in the <code class="calibre19">go-demo-3-build</code> Namespace.</p>

<p class="calibre3">Let’s <code class="calibre19">apply</code> this last Kubernetes definition before we proceed with changes in Jenkins itself.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kubectl apply <code class="se">\</code>
<code class="lineno">2 </code>    -f ../go-demo-3/k8s/jenkins.yml <code class="se">\</code>
<code class="lineno">3 </code>    --record
</pre></div>

</figure>

<p class="calibre3">The next issue we’ll have to solve is communication between Jenkins and the Pods spun during builds. Let’s take a quick look at the configuration screen.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/configure"</code>
</pre></div>

</figure>

<p class="calibre3">If you scroll down to the <em class="calibre17">Jenkins URL</em> field of the <em class="calibre17">Kubernetes</em> section, you’ll notice that it is set to <em class="calibre17">http://jenkins:8080</em>. Similarly, <em class="calibre17">Jenkins tunnel</em> is <em class="calibre17">jenkins-agent:50000</em>. The two values correspond to the names of the Services through which agent Pods will establish communication with the master and vice versa. As you hopefully already know, using only the name of a Service allows communication between Pods in the same Namespace. If we’d like to extend that communication across different Namespaces, we need to use the <em class="calibre17">[SERVICE_NAME].[NAMESPACE]</em> format. That way, agent Pods will know where to find the Jenkins Pod, no matter where they’re running. Communication will be established even if Jenkins is in the <code class="calibre19">jenkins</code> Namespace and the agent Pods are in <code class="calibre19">go-demo-3-build</code>, or anywhere else.</p>

<p class="calibre3">Let’s change the config.</p>

<p class="calibre3">Please scroll to the <em class="calibre17">Kubernetes</em> section, and change the value of the <em class="calibre17">Jenkins URL</em> field to <em class="calibre17">http://jenkins.jenkins:8080</em>. Similarly, change the <em class="calibre17">Jenkins tunnel</em> field to <em class="calibre17">jenkins-agent.jenkins:50000</em>. Don’t forget to click the <em class="calibre17">Save</em> button.</p>


<figure class="image1">
  <img src="../images/00019.jpeg" alt="Figure 6-4: Jenkins configuration screen with Kubernetes plugin configured for cross-Namespace usage" class="calibre25"/>
  <figcaption class="calibre26">Figure 6-4: Jenkins configuration screen with Kubernetes plugin configured for cross-Namespace usage</figcaption>
</figure>


<p class="calibre3">Our troubles are not yet over. We need to rethink our Helm strategy.</p>

<p class="calibre3">We have Tiller running in the <code class="calibre19">kube-system</code> Namespace. However, our agent Pods running in <code class="calibre19">go-demo-3-build</code> do not have permissions to access it. We could extend the permissions, but that would allow the Pods in that Namespace to gain almost complete control over the whole cluster. Unless your organization is very small, that is often not acceptable. Instead, we’ll deploy another Tiller instance in the <code class="calibre19">go-demo-3-build</code> Namespace and tie it to the ServiceAccount <code class="calibre19">build</code>. That will give the new tiller the same permissions in the <code class="calibre19">go-demo-3</code> and <code class="calibre19">go-demo-3-build</code> Namespaces. It’ll be able to do anything in those, but nothing anywhere else.</p>

<p class="calibre3">That strategy has a downside. It is more expensive to run multiple Tillers than to run one. However, if we organize them per teams in our organization by giving each a separate Tiller instance, we can allow them full freedom within their Namespaces without affecting others. On top of that, remember that Tiller will be removed in Helm v3, so this is only a temporary fix.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>helm init --service-account build <code class="se">\</code>
<code class="lineno">2 </code>    --tiller-namespace go-demo-3-build
</pre></div>

</figure>

<p class="calibre3">The output ends with the <code class="calibre19">Happy Helming!</code> message, letting us know that Tiller resources are installed. To be on the safe side, we’ll wait until it rolls out.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kubectl -n go-demo-3-build <code class="se">\</code>
<code class="lineno">2 </code>    rollout status <code class="se">\</code>
<code class="lineno">3 </code>    deployment tiller-deploy
</pre></div>

</figure>


<figure class="image1">
  <img src="../images/00020.jpeg" alt="Figure 6-5: Jenkins with permissions to operate across multiple Namespaces" class="calibre25"/>
  <figcaption class="calibre26">Figure 6-5: Jenkins with permissions to operate across multiple Namespaces</figcaption>
</figure>


<p class="calibre3">Now we are ready to re-run the job.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/blue/organizations/jenkins/my-k8s-job/activity"</code>
</pre></div>

</figure>

<p class="calibre3">Please click the <em class="calibre17">Run</em> button followed with a click to the row of the new build.</p>

<p class="calibre3">While waiting for the build to start, we’ll go back to the terminal and confirm that a new <code class="calibre19">jenkins-slave-...</code> Pod is created.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kubectl -n go-demo-3-build <code class="se">\</code>
<code class="lineno">2 </code>    get pods
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>NAME              READY STATUS  RESTARTS AGE
<code class="lineno">2 </code>jenkins-slave-... 5/5   Running 0        36s
<code class="lineno">3 </code>tiller-deploy-... 1/1   Running 0        3m
</pre></div>

</figure>

<p class="calibre3">If you do not see the <code class="calibre19">jenkins-slave</code> Pod, you might need to wait for a few moments, and retrieve the Pods again.</p>

<p class="calibre3">Once the state of the <code class="calibre19">jenkins-slave</code> Pod is <code class="calibre19">Running</code>, we can go back to Jenkins UI and observe that it progresses until the end and that it turns to green.</p>


<figure class="image1">
  <img src="../images/00021.jpeg" alt="Figure 6-6: Jenkins job for testing tools" class="calibre25"/>
  <figcaption class="calibre26">Figure 6-6: Jenkins job for testing tools</figcaption>
</figure>


<p class="calibre3">We managed to run the tools in the separate Namespace. However, we still need to solve the issue of building container images.</p>

<h3 id="leanpub-auto-creating-nodes-for-building-container-images" class="calibre20">Creating Nodes For Building Container Images</h3>

<p class="calibre3">We already discussed that mounting a Docker socket is a bad idea due to security risks. Running Docker in Docker would require privileged access, and that is almost as unsafe and Docker socket. On top of that, both options have other downsides. Using Docker socket would introduce processes unknown to Kubernetes and could interfere with it’s scheduling capabilities. Running Docker in Docker could mess up with networking. There are other reasons why both options are not good, so we need to look for an alternative.</p>

<p class="calibre3">Recently, new projects spun up attempting to help with building container images. Good examples are <a href="https://github.com/genuinetools/img">img</a>, <a href="https://github.com/cyphar/orca-build">orca-build</a>, <a href="https://github.com/openSUSE/umoci">umoci</a>, <a href="https://github.com/projectatomic/buildah">buildah</a>, <a href="https://github.com/GoogleCloudPlatform/runtimes-common/tree/master/ftl">FTL</a>, and <a href="https://github.com/bazelbuild/rules_docker">Bazel rules_docker</a>. They all have serious downsides. While they might help, none of them is a good solution which I’d recommend as a replacement for building container images with Docker.</p>

<p class="calibre3"><a href="https://github.com/GoogleContainerTools/kaniko">kaniko</a> is a shiny star that has a potential to become a preferable way for building container images. It does not require Docker nor any other node dependency. It can run as a container, and it is likely to become a valid alternative one day. However, that day is not today (June 2018). It is still green, unstable, and unproven.</p>

<p class="calibre3">All in all, Docker is still our best option for building container images, but not inside a Kubernetes cluster. That means that we need to build our images in a VM outside Kubernetes.</p>

<p class="calibre3">How are we going to create a VM for building container images? Are we going to have a static VM that will be wasting our resources when at rest?</p>

<p class="calibre3">The answer to those questions depends on the hosting provider you’re using. If it allows dynamic creation of VMs, we can create them when we need them, and destroy them when we don’t. If that’s not an option, we need to fall back to a dedicated machine for building images.</p>

<p class="calibre3">I could not describe all the methods for creating VMs, so I limited the scope to three combinations. We’ll explore how to create a static VM in cases when dynamic provisioning is not an option. If you’re using Docker For Mac or Windows, minikube, or minishift, that is your best bet. We’ll use Vagrant, but the same principles can be applied to any other, often on-premise, virtualization technology.</p>

<p class="calibre3">On the other hand, if you’re using a hosting provider that does support dynamic provisioning of VMs, you should leverage that to your benefit to create them when needed, and destroy them when not. I’ll show you the examples of Amazon’s Elastic Compute Cloud (EC2) and Google Cloud Engine (GCE). If you use something else (e.g., Azure, DigitalOcean), the principle will be the same, even though the implementation might vary significantly.</p>

<p class="calibre3">The primary question is whether Jenkins supports your provider. If it does, you can use a plugin that will take care of creating and destroying nodes. Otherwise, you might need to extend your Pipeline scripts to use provider’s API to spin up new nodes. In that case, you might want to evaluate whether such an option is worth the trouble. Remember, if everything else fails, having a static VM dedicated to building container images will always work.</p>

<p class="calibre3">Even if you chose to build your container images differently, it is still a good idea to know how to connect external VMs to Jenkins. There’s often a use-case that cannot (or shouldn’t) be accomplished inside a Kubernetes cluster. You might need to execute some of the steps in Windows nodes. There might be processes that shouldn’t run inside containers. Or, maybe you need to connect Android devices to your Pipelines. No matter the use-case, knowing how to connect external agents to Jenkins is essential. So, building container images is not necessarily the only reason for having external agents (nodes), and I strongly suggest exploring the sections that follow, even if you don’t think it’s useful at this moment.</p>

<p class="calibre3">Before we jump into different ways to create VMs for building and pushing container images, we need to create one thing common to all. We’ll need to create a set of credentials that will allow us to login to Docker Hub.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/credentials/store/system/domain/_/newCredentials"</code>
</pre></div>

</figure>

<p class="calibre3">Please type your Docker Hub <em class="calibre17">Username</em> and <em class="calibre17">Password</em>. Both the <em class="calibre17">ID</em> and the <em class="calibre17">Description</em> should be set to <em class="calibre17">docker</em> since that is the reference we’ll use later. Don’t forget to click the <em class="calibre17">OK</em> button.</p>

<p class="calibre3">Now we are ready to create some VMs. Please choose the section that best fits your use case. Or, even better, try all three of them.</p>

<h4 id="leanpub-auto-creating-a-vm-with-vagrant-and-virtualbox" class="calibre20">Creating a VM with Vagrant and VirtualBox</h4>

<aside class="information">
    <p class="calibre3">This section is appropriate for those using <strong class="calibre18">Docker for Mac or Windows</strong>, <strong class="calibre18">minikube</strong>, <strong class="calibre18">minishift</strong>, or anyone else planning to use static nodes as agents.</p>

</aside>

<p class="calibre3">We’ll use <a href="https://www.vagrantup.com/">Vagrant</a> to create a local VM. Please install it if you do not have it already.</p>

<p class="calibre3">The <em class="calibre17">Vagrantfile</em> we’ll use is already available inside the <a href="https://github.com/vfarcic/k8s-specs">vfarcic/k8s-specs</a>. It’s in the <em class="calibre17">cd/docker-build</em> directory, so let’s go there and take a quick look at the definition.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">cd</code> cd/docker-build
<code class="lineno">2 </code>
<code class="lineno">3 </code>cat Vagrantfile
</pre></div>

</figure>

<p class="calibre3">The output of the latter command is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="c"># vi: set ft=ruby :</code>
<code class="lineno"> 2 </code>
<code class="lineno"> 3 </code><code class="no">Vagrant</code><code class="o">.</code><code class="calibre19">configure</code><code class="calibre19">(</code><code class="s">"2"</code><code class="calibre19">)</code> <code class="k">do</code> <code class="o">|</code><code class="calibre19">config</code><code class="o">|</code>
<code class="lineno"> 4 </code>    <code class="calibre19">config</code><code class="o">.</code><code class="calibre19">vm</code><code class="o">.</code><code class="calibre19">box</code> <code class="o">=</code> <code class="s">"ubuntu/xenial64"</code>
<code class="lineno"> 5 </code>
<code class="lineno"> 6 </code>    <code class="calibre19">config</code><code class="o">.</code><code class="calibre19">vm</code><code class="o">.</code><code class="calibre19">define</code> <code class="s">"docker-build"</code> <code class="k">do</code> <code class="o">|</code><code class="calibre19">node</code><code class="o">|</code>
<code class="lineno"> 7 </code>      <code class="calibre19">node</code><code class="o">.</code><code class="calibre19">vm</code><code class="o">.</code><code class="calibre19">hostname</code> <code class="o">=</code> <code class="s">"docker-build"</code>
<code class="lineno"> 8 </code>      <code class="calibre19">node</code><code class="o">.</code><code class="calibre19">vm</code><code class="o">.</code><code class="calibre19">network</code> <code class="nv">:private_network</code><code class="calibre19">,</code> <code class="nv">ip</code><code class="calibre19">:</code> <code class="s">"10.100.198.200"</code>
<code class="lineno"> 9 </code>      <code class="calibre19">node</code><code class="o">.</code><code class="calibre19">vm</code><code class="o">.</code><code class="calibre19">provision</code> <code class="nv">:shell</code><code class="calibre19">,</code> <code class="nv">inline</code><code class="calibre19">:</code> <code class="s">"apt remove -y docker docker-engine docker.i\</code>
<code class="lineno">10 </code><code class="s">o"</code>
<code class="lineno">11 </code>      <code class="calibre19">node</code><code class="o">.</code><code class="calibre19">vm</code><code class="o">.</code><code class="calibre19">provision</code> <code class="nv">:shell</code><code class="calibre19">,</code> <code class="nv">inline</code><code class="calibre19">:</code> <code class="s">"apt update"</code>
<code class="lineno">12 </code>      <code class="calibre19">node</code><code class="o">.</code><code class="calibre19">vm</code><code class="o">.</code><code class="calibre19">provision</code> <code class="nv">:shell</code><code class="calibre19">,</code> <code class="nv">inline</code><code class="calibre19">:</code> <code class="s">"apt install apt-transport-https ca-certific\</code>
<code class="lineno">13 </code><code class="s">ates curl software-properties-common"</code>
<code class="lineno">14 </code>      <code class="calibre19">node</code><code class="o">.</code><code class="calibre19">vm</code><code class="o">.</code><code class="calibre19">provision</code> <code class="nv">:shell</code><code class="calibre19">,</code> <code class="nv">inline</code><code class="calibre19">:</code> <code class="s">"curl -fsSL https://download.docker.com/linu\</code>
<code class="lineno">15 </code><code class="s">x/ubuntu/gpg | apt-key add -"</code>
<code class="lineno">16 </code>      <code class="calibre19">node</code><code class="o">.</code><code class="calibre19">vm</code><code class="o">.</code><code class="calibre19">provision</code> <code class="nv">:shell</code><code class="calibre19">,</code> <code class="nv">inline</code><code class="calibre19">:</code> <code class="s">"add-apt-repository </code><code class="se">\"</code><code class="s">deb [arch=amd64] https\</code>
<code class="lineno">17 </code><code class="s">://download.docker.com/linux/ubuntu $(lsb_release -cs) stable</code><code class="se">\"</code><code class="s">"</code>
<code class="lineno">18 </code>      <code class="calibre19">node</code><code class="o">.</code><code class="calibre19">vm</code><code class="o">.</code><code class="calibre19">provision</code> <code class="nv">:shell</code><code class="calibre19">,</code> <code class="nv">inline</code><code class="calibre19">:</code> <code class="s">"apt update"</code>
<code class="lineno">19 </code>      <code class="calibre19">node</code><code class="o">.</code><code class="calibre19">vm</code><code class="o">.</code><code class="calibre19">provision</code> <code class="nv">:shell</code><code class="calibre19">,</code> <code class="nv">inline</code><code class="calibre19">:</code> <code class="s">"apt install -y docker-ce"</code>
<code class="lineno">20 </code>      <code class="calibre19">node</code><code class="o">.</code><code class="calibre19">vm</code><code class="o">.</code><code class="calibre19">provision</code> <code class="nv">:shell</code><code class="calibre19">,</code> <code class="nv">inline</code><code class="calibre19">:</code> <code class="s">"sudo apt install -y default-jre"</code>
<code class="lineno">21 </code>    <code class="k">end</code>
<code class="lineno">22 </code><code class="k">end</code>
</pre></div>

</figure>

<p class="calibre3">That Vagrantfile is very simple. Even if you never used Vagrant, you should have no trouble understanding what it does.</p>

<p class="calibre3">We’re defining a VM called <code class="calibre19">docker-build</code>, and we’re assigning it a static IP <code class="calibre19">10.100.198.200</code>. The <code class="calibre19">node.vm.provision</code> will install Docker and JRE. The latter is required for establishing the connection between Jenkins and this soon-to-be VM.</p>

<p class="calibre3">Next, we’ll create a VM based on that Vagrantfile definition.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>vagrant up
</pre></div>

</figure>

<p class="calibre3">Now that the VM is up and running, we can go back to Jenkins and add it as a new agent node.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/computer/new"</code>
</pre></div>

</figure>

<p class="calibre3">Please type <em class="calibre17">docker-build</em> as the <em class="calibre17">Node name</em>, select <em class="calibre17">Permanent Agent</em>, and click the <em class="calibre17">OK</em> button.</p>


<figure class="image1">
  <img src="../images/00022.jpeg" alt="Figure 6-7: Jenkins screen for adding new nodes/agents" class="calibre25"/>
  <figcaption class="calibre26">Figure 6-7: Jenkins screen for adding new nodes/agents</figcaption>
</figure>


<p class="calibre3">You are presented with a node configuration screen.</p>

<p class="calibre3">Please type <em class="calibre17">2</em> as the <em class="calibre17"># of executors</em>. That will allow us to run up to two processes inside this agent. To put it differently, up to two builds will be able to use it in parallel. If there are more than two, the new builds will wait in a queue until one of the executors is released. Depending on the size of your organization, you might want to increase the number of executors or add more nodes. As a rule of thumb, you should have one executor per CPU. In our case, we should be better of with one executor, but we’ll roll with two mostly as a demonstration.</p>

<p class="calibre3">Next, we should set the <em class="calibre17">Remote root directory</em>. That’s the place on the node’s file system where Jenkins will store the state of the builds. Please set it to <em class="calibre17">/tmp</em> or choose any other directory. Just remember that Jenkins will not create it, so the folder must already exist on the system.</p>

<p class="calibre3">We should set labels that define the machine we’re going to use as a Jenkins agent. It is always a good idea to be descriptive, even if we’re sure that we will use only one of the labels. Since that node is based on Ubuntu Linux distribution and it has Docker, our labels will be <em class="calibre17">docker ubuntu linux</em>. Please type the three into the <em class="calibre17">Labels</em> field.</p>

<p class="calibre3">There are a couple of methods we can use to establish the communication between Jenkins and the newly created node. Since it’s Linux, the easiest, and probably the best method is SSH. Please select <em class="calibre17">Launch slave agents via SSH</em> as the <em class="calibre17">Launch Method</em>.</p>

<p class="calibre3">The last piece of information we’ll define, before jumping into credentials, is the <em class="calibre17">Host</em>. Please type <em class="calibre17">10.100.198.200</em>.</p>

<p class="calibre3">We’re almost finished. The only thing left is to create a set of credentials and assign them to this agent.</p>

<p class="calibre3">Please click the <em class="calibre17">Add</em> drop-down next to <em class="calibre17">Credentials</em> and select <em class="calibre17">Jenkins</em>.</p>

<p class="calibre3">Once in the credentials popup screen, select <em class="calibre17">SSH Username with private key</em> as the <em class="calibre17">Kind</em>, type <em class="calibre17">vagrant</em> as the <em class="calibre17">Username</em>, and select <em class="calibre17">Enter directly</em> as the <em class="calibre17">Private Key</em>.</p>

<p class="calibre3">We’ll have to go back to the terminal to retrieve the private key created by Vagrant when it generated the VM.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>cat .vagrant/machines/docker-build/virtualbox/private_key
</pre></div>

</figure>

<p class="calibre3">Please copy the output, go back to Jenkins UI, and paste it into the <em class="calibre17">Key</em> field. Type <em class="calibre17">docker-build</em> as the <em class="calibre17">ID</em>, and click the <em class="calibre17">Add</em> button.</p>

<p class="calibre3">The credentials are generated, and we are back in the agent configuration screen. However, Jenkins did not pick the newly credentials automatically, so we’ll need to select <em class="calibre17">vagrant</em> in the <em class="calibre17">Credentials</em> drop-down list. Finally, since we used the private key, we’ll skip verification by selecting <em class="calibre17">Not verifying Verification Strategy</em> as the <em class="calibre17">Host Key Verification Strategy</em>.</p>


<figure class="image1">
  <img src="../images/00023.jpeg" alt="Figure 6-8: Jenkins node/agent configuration screen" class="calibre25"/>
  <figcaption class="calibre26">Figure 6-8: Jenkins node/agent configuration screen</figcaption>
</figure>


<p class="calibre3">Do not forget to click the <em class="calibre17">Save</em> button to persist the agent information.</p>

<p class="calibre3">You’ll be redirected back to the Nodes screen. Please refresh the screen if the newly created agent is red.</p>


<figure class="image1">
  <img src="../images/00024.jpeg" alt="Figure 6-9: Jenkins nodes/agents screen" class="calibre25"/>
  <figcaption class="calibre26">Figure 6-9: Jenkins nodes/agents screen</figcaption>
</figure>


<p class="calibre3">All that’s left is to go back to the <em class="calibre17">k8s-specs</em> root directory.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">cd</code> ../../
<code class="lineno">2 </code>
<code class="lineno">3 </code><code class="nb">export</code> <code class="nv">DOCKER_VM</code><code class="o">=</code><code class="nb">true</code>
</pre></div>

</figure>

<p class="calibre3">We’ll use the newly created agent soon. Feel free to skip the next two sections if this was the way you’re planning to create agents for building container images.</p>

<h4 id="leanpub-auto-creating-amazon-machine-images-amis" class="calibre20">Creating Amazon Machine Images (AMIs)</h4>

<aside class="information">
    <p class="calibre3">This section is appropriate for those using <strong class="calibre18">AWS</strong>.</p>

</aside>

<p class="calibre3">We’ll use <a href="https://plugins.jenkins.io/ec2">Jenkins EC2 plugin</a> to create agent nodes when needed and destroy them after a period of inactivity. The plugin is already installed. However, we’ll need to configure it to use a specific Amazon Machine Image (AMI), so creating one is our first order of business.</p>

<p class="calibre3">Before we proceed, please make sure that the environment variables <code class="calibre19">AWS_ACCESS_KEY_ID</code>, <code class="calibre19">AWS_SECRET_ACCESS_KEY</code>, and <code class="calibre19">AWS_DEFAULT_REGION</code> are set. If you followed the instructions for setting up the cluster with kops, the environment variables are already defined in <code class="calibre19">source cluster/kops</code>.</p>

<p class="calibre3">We’ll build the image with <a href="https://www.packer.io/">Packer</a>, so please make sure that it is installed in your laptop.</p>

<p class="calibre3">Packer definition we’ll explore soon will require a security group. Please execute the command that follows to create one.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">export</code> <code class="nv">AWS_DEFAULT_REGION</code><code class="o">=</code>us-east-2
<code class="lineno">2 </code>
<code class="lineno">3 </code>aws ec2 create-security-group <code class="se">\</code>
<code class="lineno">4 </code>    --description <code class="s">"For building Docker images"</code> <code class="se">\</code>
<code class="lineno">5 </code>    --group-name docker <code class="se">\</code>
<code class="lineno">6 </code>    <code class="calibre19">|</code> tee cluster/sg.json
</pre></div>

</figure>

<p class="calibre3">For convenience, we’ll parse the output stored in <code class="calibre19">cluster/sg.json</code> to retrieve <code class="calibre19">GroupId</code> and assign it in an environment variable. Please install <a href="https://stedolan.github.io/jq/">jq</a> if you don’t have it already.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">SG_ID</code><code class="o">=</code><code class="k">$(</code>cat cluster/sg.json <code class="se">\</code>
<code class="lineno">2 </code>    <code class="calibre19">|</code> jq -r <code class="s">".GroupId"</code><code class="k">)</code>
<code class="lineno">3 </code>
<code class="lineno">4 </code><code class="nb">echo</code> <code class="nv">$SG_ID</code>
</pre></div>

</figure>

<p class="calibre3">The output of the latter command should be similar to the one that follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>sg-5fe96935
</pre></div>

</figure>

<p class="calibre3">Next, we’ll store the security group <code class="calibre19">export</code> in a file so that we can easily retrieve it in the next chapters.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">echo</code> <code class="s">"export SG_ID=</code><code class="nv">$SG_ID</code><code class="s">"</code> <code class="se">\</code>
<code class="lineno">2 </code>    <code class="calibre19">|</code> tee -a cluster/docker-ec2
</pre></div>

</figure>

<p class="calibre3">The security group we created is useless in its current form. We’ll need to authorize it to allow communication on port <code class="calibre19">22</code> so that Packer can access it and execute provisioning.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>aws ec2 <code class="se">\</code>
<code class="lineno">2 </code>    authorize-security-group-ingress <code class="se">\</code>
<code class="lineno">3 </code>    --group-name docker <code class="se">\</code>
<code class="lineno">4 </code>    --protocol tcp <code class="se">\</code>
<code class="lineno">5 </code>    --port <code class="o">22</code> <code class="se">\</code>
<code class="lineno">6 </code>    --cidr <code class="o">0</code>.0.0.0/0
</pre></div>

</figure>

<p class="calibre3">We’re done with the preparation steps and we can proceed to create the AMI.</p>

<p class="calibre3">Let’s take a quick look at the Package definition we’ll use.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>cat jenkins/docker-ami.json
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">{</code>
<code class="lineno"> 2 </code>  <code class="k">"builders"</code><code class="calibre19">:</code> <code class="calibre19">[{</code>
<code class="lineno"> 3 </code>    <code class="k">"type"</code><code class="calibre19">:</code> <code class="s">"amazon-ebs"</code><code class="calibre19">,</code>
<code class="lineno"> 4 </code>    <code class="k">"region"</code><code class="calibre19">:</code> <code class="s">"us-east-2"</code><code class="calibre19">,</code>
<code class="lineno"> 5 </code>    <code class="k">"source_ami_filter"</code><code class="calibre19">:</code> <code class="calibre19">{</code>
<code class="lineno"> 6 </code>      <code class="k">"filters"</code><code class="calibre19">:</code> <code class="calibre19">{</code>
<code class="lineno"> 7 </code>        <code class="k">"virtualization-type"</code><code class="calibre19">:</code> <code class="s">"hvm"</code><code class="calibre19">,</code>
<code class="lineno"> 8 </code>        <code class="k">"name"</code><code class="calibre19">:</code> <code class="s">"*ubuntu-xenial-16.04-amd64-server-*"</code><code class="calibre19">,</code>
<code class="lineno"> 9 </code>        <code class="k">"root-device-type"</code><code class="calibre19">:</code> <code class="s">"ebs"</code>
<code class="lineno">10 </code>      <code class="calibre19">},</code>
<code class="lineno">11 </code>      <code class="k">"most_recent"</code><code class="calibre19">:</code> <code class="k">true</code>
<code class="lineno">12 </code>    <code class="calibre19">},</code>
<code class="lineno">13 </code>    <code class="k">"instance_type"</code><code class="calibre19">:</code> <code class="s">"t2.micro"</code><code class="calibre19">,</code>
<code class="lineno">14 </code>    <code class="k">"ssh_username"</code><code class="calibre19">:</code> <code class="s">"ubuntu"</code><code class="calibre19">,</code>
<code class="lineno">15 </code>    <code class="k">"ami_name"</code><code class="calibre19">:</code> <code class="s">"docker"</code><code class="calibre19">,</code>
<code class="lineno">16 </code>    <code class="k">"force_deregister"</code><code class="calibre19">:</code> <code class="k">true</code>
<code class="lineno">17 </code>  <code class="calibre19">}],</code>
<code class="lineno">18 </code>  <code class="k">"provisioners"</code><code class="calibre19">:</code> <code class="calibre19">[{</code>
<code class="lineno">19 </code>    <code class="k">"type"</code><code class="calibre19">:</code> <code class="s">"shell"</code><code class="calibre19">,</code>
<code class="lineno">20 </code>    <code class="k">"inline"</code><code class="calibre19">:</code> <code class="calibre19">[</code>
<code class="lineno">21 </code>      <code class="s">"sleep 15"</code><code class="calibre19">,</code>
<code class="lineno">22 </code>      <code class="s">"sudo apt-get clean"</code><code class="calibre19">,</code>
<code class="lineno">23 </code>      <code class="s">"sudo apt-get update"</code><code class="calibre19">,</code>
<code class="lineno">24 </code>      <code class="s">"sudo apt-get install -y apt-transport-https ca-certificates nfs-common"</code><code class="calibre19">,</code>
<code class="lineno">25 </code>      <code class="s">"curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -"</code><code class="calibre19">,</code>
<code class="lineno">26 </code>      <code class="s">"sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/\</code>
<code class="lineno">27 </code><code class="s">ubuntu $(lsb_release -cs) stable\""</code><code class="calibre19">,</code>
<code class="lineno">28 </code>      <code class="s">"sudo add-apt-repository -y ppa:openjdk-r/ppa"</code><code class="calibre19">,</code>
<code class="lineno">29 </code>      <code class="s">"sudo apt-get update"</code><code class="calibre19">,</code>
<code class="lineno">30 </code>      <code class="s">"sudo apt-get install -y docker-ce"</code><code class="calibre19">,</code>
<code class="lineno">31 </code>      <code class="s">"sudo usermod -aG docker ubuntu"</code><code class="calibre19">,</code>
<code class="lineno">32 </code>      <code class="s">"sudo apt-get install -y openjdk-8-jdk"</code>
<code class="lineno">33 </code>    <code class="calibre19">]</code>
<code class="lineno">34 </code>  <code class="calibre19">}]</code>
<code class="lineno">35 </code><code class="calibre19">}</code>
</pre></div>

</figure>

<p class="calibre3">Most of the definition should be self-explanatory. We’ll create an EBS image based on Ubuntu in the <code class="calibre19">us-east-2</code> region and we’ll use the <code class="calibre19">shell</code> <code class="calibre19">provisioner</code> to install Docker and JDK.</p>

<p class="calibre3">Let’s create the AMI.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>packer build -machine-readable <code class="se">\</code>
<code class="lineno">2 </code>    jenkins/docker-ami.json <code class="se">\</code>
<code class="lineno">3 </code>    <code class="calibre19">|</code> tee cluster/docker-ami.log
</pre></div>

</figure>

<p class="calibre3">The last lines of the output are as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>...
<code class="lineno">2 </code>...,amazon-ebs,artifact,0,id,us-east-2:ami-ea053b8f
<code class="lineno">3 </code>...,amazon-ebs,artifact,0,string,AMIs were created:\nus-east-2: ami-ea053b8f\n
<code class="lineno">4 </code>...,amazon-ebs,artifact,0,files-count,0
<code class="lineno">5 </code>...,amazon-ebs,artifact,0,end
<code class="lineno">6 </code>...,,ui,say,--&gt; amazon-ebs: AMIs were created:\nus-east-2: ami-ea053b8f\n
</pre></div>

</figure>

<p class="calibre3">The important line is the one that contains <code class="calibre19">artifact,0,id</code>. The last column in that row contains the ID we’ll need to tell Jenkins about the new AMI. We’ll store it in an environment variable for convenience.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">AMI_ID</code><code class="o">=</code><code class="k">$(</code>grep <code class="s">'artifact,0,id'</code> <code class="se">\</code>
<code class="lineno">2 </code>    cluster/docker-ami.log <code class="se">\</code>
<code class="lineno">3 </code>    <code class="calibre19">|</code> cut -d: -f2<code class="k">)</code>
<code class="lineno">4 </code>
<code class="lineno">5 </code><code class="nb">echo</code> <code class="nv">$AMI_ID</code>
</pre></div>

</figure>

<p class="calibre3">The output of the latter command should be similar to the one that follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ami-ea053b8f
</pre></div>

</figure>

<p class="calibre3">Just as with the security group, we’ll store the <code class="calibre19">AMI_ID</code> <code class="calibre19">export</code> in the <code class="calibre19">docker-ec2</code> file so that we can retrieve it easily in the next chapters.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">echo</code> <code class="s">"export AMI_ID=</code><code class="nv">$AMI_ID</code><code class="s">"</code> <code class="se">\</code>
<code class="lineno">2 </code>    <code class="calibre19">|</code> tee -a cluster/docker-ec2
</pre></div>

</figure>

<p class="calibre3">Now that we have the AMI, we need to move to Jenkins and configure the <em class="calibre17">Amazon EC2</em> plugin.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/configure"</code>
</pre></div>

</figure>

<p class="calibre3">Please scroll to the <em class="calibre17">Cloud</em> section and click the <em class="calibre17">Add a new cloud</em> drop-down list. Choose <em class="calibre17">Amazon EC2</em>. A new form will appear.</p>

<p class="calibre3">Type <em class="calibre17">docker-agents</em> as the <em class="calibre17">Name</em>, and expand the <em class="calibre17">Add</em> drop-down list next to <em class="calibre17">Amazon EC2 Credentials</em>. Choose <em class="calibre17">Jenkins</em>.</p>

<p class="calibre3">From the credentials screen, please choose <em class="calibre17">AWS Credentials</em> as the <em class="calibre17">Kind</em>, and type <em class="calibre17">aws</em> as both the <em class="calibre17">ID</em> and the <em class="calibre17">Description</em>.</p>

<p class="calibre3">Next, we need to return to the terminal to retrieve the AWS access key ID.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">echo</code> <code class="nv">$AWS_ACCESS_KEY_ID</code>
</pre></div>

</figure>

<p class="calibre3">Please copy the output, return to Jenkins UI, and paste it into the <em class="calibre17">Access Key ID</em> field.</p>

<p class="calibre3">We’ll repeat the same process for the AWS secrets access key.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">echo</code> <code class="nv">$AWS_SECRET_ACCESS_KEY</code>
</pre></div>

</figure>

<p class="calibre3">Copy the output, return to Jenkins UI, and paste it into the <em class="calibre17">Secret Access Key</em> field.</p>

<p class="calibre3">With the credentials information filled in, we need to press the <em class="calibre17">Add</em> button to store it and return to the EC2 configuration screen.</p>

<p class="calibre3">Please choose the newly created credentials and select <em class="calibre17">us-east-2</em> as the <em class="calibre17">Region</em>.</p>

<p class="calibre3">We need the private key next. It can be created through the <code class="calibre19">aws ec2</code> command <code class="calibre19">create-key-pair</code>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>aws ec2 create-key-pair <code class="se">\</code>
<code class="lineno">2 </code>    --key-name devops24 <code class="se">\</code>
<code class="lineno">3 </code>    <code class="calibre19">|</code> jq -r <code class="s">'.KeyMaterial'</code> <code class="se">\</code>
<code class="lineno">4 </code>    &gt;cluster/devops24.pem
</pre></div>

</figure>

<p class="calibre3">We created a new key pair, filtered the output so that only the <code class="calibre19">KeyMaterial</code> is returned, and stored it in the <code class="calibre19">devops24.pem</code> file.</p>

<p class="calibre3">For security reasons, we should change the permissions of the <code class="calibre19">devops24.pem</code> file so that only the current user can read it.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>chmod <code class="o">400</code> cluster/devops24.pem
</pre></div>

</figure>

<p class="calibre3">Finally, we’ll output the content of the pem file.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>cat cluster/devops24.pem
</pre></div>

</figure>

<p class="calibre3">Please copy the output, return to Jenkins UI, and paste it into the <em class="calibre17">EC2 Key Pair’s Private Key</em> field.</p>

<p class="calibre3">To be on the safe side, press the <em class="calibre17">Test Connection</em> button, and confirm that the output is <em class="calibre17">Success</em>.</p>

<p class="calibre3">We’re finished with the general <em class="calibre17">Amazon EC2</em> configuration, and we can proceed to add the first and the only AMI.</p>

<p class="calibre3">Please click the <em class="calibre17">Add</em> button next to <em class="calibre17">AMIs</em>, and type <em class="calibre17">docker</em> as the <em class="calibre17">Description</em>.</p>

<p class="calibre3">We need to return to the terminal one more time to retrieve the AMI ID.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">echo</code> <code class="nv">$AMI_ID</code>
</pre></div>

</figure>

<p class="calibre3">Please copy the output, return to Jenkins UI, and paste it into the <em class="calibre17">AMI ID</em> field.</p>

<p class="calibre3">To be on the safe side, please click the <em class="calibre17">Check AMI</em> button, and confirm that the output does not show any error.</p>

<p class="calibre3">We’re almost done.</p>

<p class="calibre3">Select <em class="calibre17">T2Micro</em> as the <em class="calibre17">Instance Type</em>, type <em class="calibre17">docker</em> as the <em class="calibre17">Security group names</em>, and type <em class="calibre17">ubuntu</em> as the <em class="calibre17">Remote user</em>. The <em class="calibre17">Remote ssh port</em> should be set to <em class="calibre17">22</em>. Please write <em class="calibre17">docker ubuntu linux</em> as the labels, and change the <em class="calibre17">Idle termination time</em> to <em class="calibre17">10</em>.</p>

<p class="calibre3">Finally, click the <em class="calibre17">Save</em> button to preserve the changes.</p>

<p class="calibre3">We’ll use the newly created EC2 template soon. Feel free to skip the next section if this was the way you’re planning to create agents for building container images.</p>

<h4 id="leanpub-auto-creating-google-cloud-engine-gce-images" class="calibre20">Creating Google Cloud Engine (GCE) Images</h4>

<aside class="information">
    <p class="calibre3">This section is appropriate for those using <strong class="calibre18">GKE</strong>.</p>

</aside>

<p class="calibre3">If you reached this far, it means that you prefer running your cluster in GKE, or that you are so curious that you prefer trying all three ways to create VMs that will be used to build container images. No matter the reason, we’re about to create a GCE image and configure Jenkins to spin up VMs when needed and destroy them when they’re not in use.</p>

<p class="calibre3">Before we do anything related to GCE, we need to authenticate.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>gcloud auth login
</pre></div>

</figure>

<p class="calibre3">Next, we need to create a service account that can be used by Packer to create GCE images.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>gcloud iam service-accounts <code class="se">\</code>
<code class="lineno">2 </code>    create jenkins
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>Created service account [jenkins].
</pre></div>

</figure>

<p class="calibre3">We’ll also need to know the project you’re planning to use. We’ll assume that it’s the one that is currently active and we’ll retrieve it with the <code class="calibre19">gcloud info</code> command.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">export</code> <code class="nv">G_PROJECT</code><code class="o">=</code><code class="k">$(</code>gcloud info <code class="se">\</code>
<code class="lineno">2 </code>    --format<code class="o">=</code><code class="s">'value(config.project)'</code><code class="k">)</code>
<code class="lineno">3 </code>
<code class="lineno">4 </code><code class="nb">echo</code> <code class="nv">$G_PROJECT</code>
</pre></div>

</figure>

<p class="calibre3">Please note that the output might differ from what I’ve got. In my case, the output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>devops24-book
</pre></div>

</figure>

<p class="calibre3">The last information we need is the email that was generated when we created the service account.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">export</code> <code class="nv">SA_EMAIL</code><code class="o">=</code><code class="k">$(</code>gcloud iam <code class="se">\</code>
<code class="lineno">2 </code>    service-accounts list <code class="se">\</code>
<code class="lineno">3 </code>    --filter<code class="o">=</code><code class="s">"name:jenkins"</code> <code class="se">\</code>
<code class="lineno">4 </code>    --format<code class="o">=</code><code class="s">'value(email)'</code><code class="k">)</code>
<code class="lineno">5 </code>
<code class="lineno">6 </code><code class="nb">echo</code> <code class="nv">$SA_EMAIL</code>
</pre></div>

</figure>

<p class="calibre3">In my case, the output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>jenkins@devops24-book.iam.gserviceaccount.com
</pre></div>

</figure>

<p class="calibre3">Now that we retrieved all the information we need, we can proceed and create a policy binding between the service account and the <code class="calibre19">compute.admin</code> role. That will give us more than sufficient privileges not only to create images but also to instantiate VMs based on them.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>gcloud projects add-iam-policy-binding <code class="se">\</code>
<code class="lineno">2 </code>    --member serviceAccount:<code class="nv">$SA_EMAIL</code> <code class="se">\</code>
<code class="lineno">3 </code>    --role roles/compute.admin <code class="se">\</code>
<code class="lineno">4 </code>    <code class="nv">$G_PROJECT</code>
</pre></div>

</figure>

<p class="calibre3">The output shows all the information related to the binding we created. Instead of going into details, we’ll create another one.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>gcloud projects add-iam-policy-binding <code class="se">\</code>
<code class="lineno">2 </code>    --member serviceAccount:<code class="nv">$SA_EMAIL</code> <code class="se">\</code>
<code class="lineno">3 </code>    --role roles/iam.serviceAccountUser <code class="se">\</code>
<code class="lineno">4 </code>    <code class="nv">$G_PROJECT</code>
</pre></div>

</figure>

<p class="calibre3">Now that our service account is bound both to <code class="calibre19">compute.admin</code> and <code class="calibre19">iam.serviceAccountUser</code> roles, the only thing left before we create a GCE image is to create a set of keys.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>gcloud iam service-accounts <code class="se">\</code>
<code class="lineno">2 </code>    keys create <code class="se">\</code>
<code class="lineno">3 </code>    --iam-account <code class="nv">$SA_EMAIL</code> <code class="se">\</code>
<code class="lineno">4 </code>    cluster/gce-jenkins.json
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>created key [...] of type [json] as [cluster/gce-jenkins.json] for [jenkins@devops24\
<code class="lineno">2 </code>-book.iam.gserviceaccount.com]
</pre></div>

</figure>

<p class="calibre3">We’re finally ready to create an image. We’ll build it with <a href="https://www.packer.io/">Packer</a>, so please make sure that it is installed in your laptop.</p>

<p class="calibre3">The definition of the image we’ll create is stored in the <code class="calibre19">docker-gce.json</code> file. Let’s take a quick look.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>cat jenkins/docker-gce.json
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">{</code>
<code class="lineno"> 2 </code>  <code class="k">"variables"</code><code class="calibre19">:</code> <code class="calibre19">{</code>
<code class="lineno"> 3 </code>    <code class="k">"project_id"</code><code class="calibre19">:</code> <code class="s">""</code>
<code class="lineno"> 4 </code>  <code class="calibre19">},</code>
<code class="lineno"> 5 </code>  <code class="k">"builders"</code><code class="calibre19">:</code> <code class="calibre19">[{</code>
<code class="lineno"> 6 </code>    <code class="k">"type"</code><code class="calibre19">:</code> <code class="s">"googlecompute"</code><code class="calibre19">,</code>
<code class="lineno"> 7 </code>    <code class="k">"account_file"</code><code class="calibre19">:</code> <code class="s">"cluster/gce-jenkins.json"</code><code class="calibre19">,</code>
<code class="lineno"> 8 </code>    <code class="k">"project_id"</code><code class="calibre19">:</code> <code class="s">"{{user `project_id`}}"</code><code class="calibre19">,</code>
<code class="lineno"> 9 </code>    <code class="k">"source_image_project_id"</code><code class="calibre19">:</code> <code class="s">"ubuntu-os-cloud"</code><code class="calibre19">,</code>
<code class="lineno">10 </code>    <code class="k">"source_image_family"</code><code class="calibre19">:</code> <code class="s">"ubuntu-1604-lts"</code><code class="calibre19">,</code>
<code class="lineno">11 </code>    <code class="k">"ssh_username"</code><code class="calibre19">:</code> <code class="s">"ubuntu"</code><code class="calibre19">,</code>
<code class="lineno">12 </code>    <code class="k">"zone"</code><code class="calibre19">:</code> <code class="s">"us-east1-b"</code><code class="calibre19">,</code>
<code class="lineno">13 </code>    <code class="k">"image_name"</code><code class="calibre19">:</code> <code class="s">"docker"</code>
<code class="lineno">14 </code>  <code class="calibre19">}],</code>
<code class="lineno">15 </code>  <code class="k">"provisioners"</code><code class="calibre19">:</code> <code class="calibre19">[{</code>
<code class="lineno">16 </code>    <code class="k">"type"</code><code class="calibre19">:</code> <code class="s">"shell"</code><code class="calibre19">,</code>
<code class="lineno">17 </code>    <code class="k">"inline"</code><code class="calibre19">:</code> <code class="calibre19">[</code>
<code class="lineno">18 </code>      <code class="s">"sleep 15"</code><code class="calibre19">,</code>
<code class="lineno">19 </code>      <code class="s">"sudo apt-get clean"</code><code class="calibre19">,</code>
<code class="lineno">20 </code>      <code class="s">"sudo apt-get update"</code><code class="calibre19">,</code>
<code class="lineno">21 </code>      <code class="s">"sudo apt-get install -y apt-transport-https ca-certificates nfs-common"</code><code class="calibre19">,</code>
<code class="lineno">22 </code>      <code class="s">"curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -"</code><code class="calibre19">,</code>
<code class="lineno">23 </code>      <code class="s">"sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/\</code>
<code class="lineno">24 </code><code class="s">ubuntu $(lsb_release -cs) stable\""</code><code class="calibre19">,</code>
<code class="lineno">25 </code>      <code class="s">"sudo add-apt-repository -y ppa:openjdk-r/ppa"</code><code class="calibre19">,</code>
<code class="lineno">26 </code>      <code class="s">"sudo apt-get update"</code><code class="calibre19">,</code>
<code class="lineno">27 </code>      <code class="s">"sudo apt-get install -y docker-ce"</code><code class="calibre19">,</code>
<code class="lineno">28 </code>      <code class="s">"sudo usermod -aG docker ubuntu"</code><code class="calibre19">,</code>
<code class="lineno">29 </code>      <code class="s">"sudo apt-get install -y openjdk-8-jdk"</code>
<code class="lineno">30 </code>    <code class="calibre19">]</code>
<code class="lineno">31 </code>  <code class="calibre19">}]</code>
<code class="lineno">32 </code><code class="calibre19">}</code>
</pre></div>

</figure>

<p class="calibre3">That Packer definition should be self-explanatory. It containers the <code class="calibre19">builders</code> section that defines the parameters required to build an image in GCE, and the <code class="calibre19">provisioners</code> contain the <code class="calibre19">shell</code> commands that install Docker and JDK. The latter is required for Jenkins to establish the communication with the agent VMs we’ll create from that image.</p>

<p class="calibre3">Feel free to change the zone if you’re running your cluster somewhere other than <code class="calibre19">us-east1</code>.</p>

<p class="calibre3">Next, we’ll execute <code class="calibre19">packer build</code> command that will create the image.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>packer build -machine-readable <code class="se">\</code>
<code class="lineno">2 </code>    --force <code class="se">\</code>
<code class="lineno">3 </code>    -var <code class="s">"project_id=</code><code class="nv">$G_PROJECT</code><code class="s">"</code> <code class="se">\</code>
<code class="lineno">4 </code>    jenkins/docker-gce.json <code class="se">\</code>
<code class="lineno">5 </code>    <code class="calibre19">|</code> tee cluster/docker-gce.log
</pre></div>

</figure>

<p class="calibre3">The output, limited to the last few lines, is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>...
<code class="lineno">2 </code>...,googlecompute,artifact,0,id,docker
<code class="lineno">3 </code>...,googlecompute,artifact,0,string,A disk image was created: docker
<code class="lineno">4 </code>...,googlecompute,artifact,0,files-count,0
<code class="lineno">5 </code>...,googlecompute,artifact,0,end
<code class="lineno">6 </code>...,,ui,say,--&gt; googlecompute: A disk image was created: docker
</pre></div>

</figure>

<p class="calibre3">Now that we have the image, we should turn our attention back to Jenkins and configure <em class="calibre17">Google Compute Engine Cloud</em>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/configure"</code>
</pre></div>

</figure>

<p class="calibre3">The chances are that your Jenkins session expired and that you’ll need to log in again. If that’s the case, please output the password we stored in the environment variable <code class="calibre19">JENKINS_PASS</code> and use it to authenticate.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">echo</code> <code class="nv">$JENKINS_PASS</code>
</pre></div>

</figure>

<p class="calibre3">Once inside the Jenkins configuration screen, please expand the <em class="calibre17">Add a new cloud</em> drop-down list. It is located near the bottom of the screen. Select <em class="calibre17">Google Compute Engine</em>.</p>

<p class="calibre3">A new set of fields will appear. We’ll need to fill them in so that Jenkins knows how to connect to GCE and what to do if we request a new node.</p>

<p class="calibre3">Type <em class="calibre17">docker</em> as the <em class="calibre17">Name</em>.</p>

<p class="calibre3">We’ll need to go back to the terminal and retrieve the Project ID we stored in the environment variable <code class="calibre19">G_PROJECT</code>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">echo</code> <code class="nv">$G_PROJECT</code>
</pre></div>

</figure>

<p class="calibre3">Please copy the output, go back to Jenkins UI, and paste it into the <em class="calibre17">Project ID</em> field.</p>

<p class="calibre3">Next, we’ll create the credentials.</p>

<p class="calibre3">Expand the <em class="calibre17">Add</em> drop-down list next to <em class="calibre17">Service Account Credentials</em> and select <em class="calibre17">Jenkins</em>.</p>

<p class="calibre3">You’ll see a new popup with the form to create credentials.</p>

<p class="calibre3">Select <em class="calibre17">Google Service Account from private key</em> as the <em class="calibre17">Kind</em> and paste the name of the project to the <em class="calibre17">Project Name</em> field (the one you got from <code class="calibre19">G_PROJECT</code> variable).</p>

<p class="calibre3">Click <em class="calibre17">Choose File</em> button in the <em class="calibre17">JSON Key</em> field and select the <em class="calibre17">gce-jenkins.json</em> file we created earlier in the <em class="calibre17">cluster</em> directory.</p>

<p class="calibre3">Click the <em class="calibre17">Add</em> button, and the new credential will be persisted.</p>


<figure class="image1">
  <img src="../images/00025.jpeg" alt="Figure 6-10: Jenkins Google credentials screen" class="calibre25"/>
  <figcaption class="calibre26">Figure 6-10: Jenkins Google credentials screen</figcaption>
</figure>


<p class="calibre3">We’re back in the <em class="calibre17">Google Compute Engine</em> screen, and we need to select the newly created credential before we proceed.</p>

<p class="calibre3">Next, we’ll add a definition of VMs we’d like to create through Jenkins.</p>

<p class="calibre3">Please click the <em class="calibre17">Add</em> button next to <em class="calibre17">Instance Configurations</em>, type <em class="calibre17">docker</em> as the <em class="calibre17">Name Prefix</em>, and type <em class="calibre17">Docker build instances</em> as the <em class="calibre17">Description</em>. Write <em class="calibre17">10</em> as the <em class="calibre17">Node Retention Time</em> and type <em class="calibre17">docker ubuntu linux</em> as the <em class="calibre17">Labels</em>. The retention time defines the period Jenkins will wait until destroying the VM. If in our case no other build needs that VM, it’ll be destroyed after one minute. In “real” Jenkins, we’d need to think carefully what to use as retention. If the value is too low, we’ll save on costs but builds execution will be longer since they’ll need to wait until a new VM is created. On the other hand, if the value is too high, the same VM will be reused more often, but we’ll be paying for compute time we don’t use if there are no pending builds. For our experiments, one minute should do.</p>

<p class="calibre3">If you’re running your cluster in <em class="calibre17">us-east-1</em>, please select it as the <em class="calibre17">Region</em>. Otherwise, switch to whichever region your cluster is running in and, more importantly, the region where the image was created. Similarly, select the appropriate zone. If you’re following the exact steps, it should be <em class="calibre17">us-east1-b</em>. The important part is that the zone must be the same as the one where we built the image.</p>

<p class="calibre3">We’re almost done with <em class="calibre17">Google Compute Engine</em> Jenkins’ configuration.</p>

<p class="calibre3">Select <em class="calibre17">n1-standard-2</em> as the <em class="calibre17">Machine Type</em>, select <em class="calibre17">default</em> as both the <em class="calibre17">Network</em> and the <em class="calibre17">Subnetwork</em> and check both <em class="calibre17">External IP</em> and <em class="calibre17">Internal IP</em> check boxes.</p>

<aside class="information">
    <p class="calibre3">In GCE, you will need to have either an external IP or a NAT gateway setup to download anything from the internet. As we do not want to bother you with the NAT gateway setup, we will configure an <em class="calibre17">External IP</em>. As we also want to have our connection traffic not unnecessarily going through the internet, we also select <em class="calibre17">Internal IP</em> to make sure Jenkins still uses the internal IP.</p>

</aside>

<p class="calibre3">The <em class="calibre17">Image project</em> should be set to the same value as the one we stored in the environment variable <code class="calibre19">G_PROJECT</code>.</p>

<p class="calibre3">Finally, select <em class="calibre17">docker</em> as the <em class="calibre17">Image name</em> and click the <em class="calibre17">Save</em> button.</p>

<h3 id="leanpub-auto-testing-docker-builds-outside-the-cluster" class="calibre20">Testing Docker Builds Outside The Cluster</h3>

<p class="calibre3">No matter whether you choose to use static VMs or you decided to create them dynamically in AWS or GCE, the steps to test them are the same. From Jenkins’ perspective, all that matters is that there are agent nodes with the labels <em class="calibre17">docker</em>.</p>

<p class="calibre3">We’ll modify our Pipeline to use the <code class="calibre19">node</code> labeled <code class="calibre19">docker</code>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/job/my-k8s-job/configure"</code>
</pre></div>

</figure>

<p class="calibre3">Please click the <em class="calibre17">Pipeline</em> tab and replace the script with the one that follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">podTemplate</code><code class="o">(</code>
<code class="lineno"> 2 </code>    <code class="nl">label:</code> <code class="s">"kubernetes"</code><code class="o">,</code>
<code class="lineno"> 3 </code>    <code class="nl">namespace:</code> <code class="s">"go-demo-3-build"</code><code class="o">,</code>
<code class="lineno"> 4 </code>    <code class="nl">serviceAccount:</code> <code class="s">"build"</code><code class="o">,</code>
<code class="lineno"> 5 </code>    <code class="nl">yaml:</code> <code class="s">"""</code>
<code class="lineno"> 6 </code><code class="s">apiVersion: v1</code>
<code class="lineno"> 7 </code><code class="s">kind: Pod</code>
<code class="lineno"> 8 </code><code class="s">spec:</code>
<code class="lineno"> 9 </code><code class="s">  containers:</code>
<code class="lineno">10 </code><code class="s">  - name: kubectl</code>
<code class="lineno">11 </code><code class="s">    image: vfarcic/kubectl</code>
<code class="lineno">12 </code><code class="s">    command: ["sleep"]</code>
<code class="lineno">13 </code><code class="s">    args: ["100000"]</code>
<code class="lineno">14 </code><code class="s">  - name: oc</code>
<code class="lineno">15 </code><code class="s">    image: vfarcic/openshift-client</code>
<code class="lineno">16 </code><code class="s">    command: ["sleep"]</code>
<code class="lineno">17 </code><code class="s">    args: ["100000"]</code>
<code class="lineno">18 </code><code class="s">  - name: golang</code>
<code class="lineno">19 </code><code class="s">    image: golang:1.9</code>
<code class="lineno">20 </code><code class="s">    command: ["sleep"]</code>
<code class="lineno">21 </code><code class="s">    args: ["100000"]</code>
<code class="lineno">22 </code><code class="s">  - name: helm</code>
<code class="lineno">23 </code><code class="s">    image: vfarcic/helm:2.8.2</code>
<code class="lineno">24 </code><code class="s">    command: ["sleep"]</code>
<code class="lineno">25 </code><code class="s">    args: ["100000"]</code>
<code class="lineno">26 </code><code class="s">"""</code>
<code class="lineno">27 </code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">28 </code>    <code class="calibre19">node</code><code class="o">(</code><code class="s">"docker"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">29 </code>        <code class="calibre19">stage</code><code class="o">(</code><code class="s">"docker"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">30 </code>            <code class="calibre19">sh</code> <code class="s">"sudo docker version"</code>
<code class="lineno">31 </code>        <code class="o">}</code>
<code class="lineno">32 </code>    <code class="o">}</code>
<code class="lineno">33 </code>    <code class="calibre19">node</code><code class="o">(</code><code class="s">"kubernetes"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">34 </code>        <code class="calibre19">container</code><code class="o">(</code><code class="s">"kubectl"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">35 </code>            <code class="calibre19">stage</code><code class="o">(</code><code class="s">"kubectl"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">36 </code>                <code class="calibre19">sh</code> <code class="s">"kubectl version"</code>
<code class="lineno">37 </code>            <code class="o">}</code>
<code class="lineno">38 </code>        <code class="o">}</code>
<code class="lineno">39 </code>        <code class="calibre19">container</code><code class="o">(</code><code class="s">"oc"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">40 </code>            <code class="calibre19">stage</code><code class="o">(</code><code class="s">"oc"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">41 </code>                <code class="calibre19">sh</code> <code class="s">"oc version"</code>
<code class="lineno">42 </code>            <code class="o">}</code>
<code class="lineno">43 </code>        <code class="o">}</code>
<code class="lineno">44 </code>        <code class="calibre19">container</code><code class="o">(</code><code class="s">"golang"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">45 </code>            <code class="calibre19">stage</code><code class="o">(</code><code class="s">"golang"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">46 </code>                <code class="calibre19">sh</code> <code class="s">"go version"</code>
<code class="lineno">47 </code>            <code class="o">}</code>
<code class="lineno">48 </code>        <code class="o">}</code>
<code class="lineno">49 </code>        <code class="calibre19">container</code><code class="o">(</code><code class="s">"helm"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">50 </code>            <code class="calibre19">stage</code><code class="o">(</code><code class="s">"helm"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">51 </code>                <code class="calibre19">sh</code> <code class="s">"helm version --tiller-namespace go-demo-3-build"</code>
<code class="lineno">52 </code>            <code class="o">}</code>
<code class="lineno">53 </code>        <code class="o">}</code>
<code class="lineno">54 </code>    <code class="o">}</code>
<code class="lineno">55 </code><code class="o">}</code>
</pre></div>

</figure>

<aside class="information">
    <p class="calibre3">If you prefer to copy and paste, the job is available in the <a href="https://gist.github.com/fbf9fc6611fe400c7950f43cfc89f406">my-k8s-job-docker.groovy Gist</a>.</p>

</aside>

<p class="calibre3">The only notable difference, when compared with the previous version of the job, is that we added the second <code class="calibre19">node</code> segment. Most of the steps will be executed inside the <code class="calibre19">kubernetes</code> node that hosts a few containers. The new <code class="calibre19">node</code> is called <code class="calibre19">docker</code> and will be in charge of the steps that require Docker server. Depending on the path you took, that node might be a static VM, a dynamically created (and destroyed) node in AWS or GCE, or something entirely different. From job’s perspective, it does not matter how is that node created, but that it exists or that it will be created on demand. The job will request nodes <code class="calibre19">docker</code> and <code class="calibre19">kubernetes</code>, and it is up to Jenkins’ internal configuration to figure out how to get them.</p>

<p class="calibre3">Please click the <em class="calibre17">Save</em> button to persist the updated job.</p>

<p class="calibre3">Next, we’ll open the job in BlueOcean and run it as a way to confirm that everything works as expected.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/blue/organizations/jenkins/my-k8s-job/activity"</code>
</pre></div>

</figure>

<p class="calibre3">Press the <em class="calibre17">Run</em> button, followed with a click on the row of the new build. Wait until all the stages are executed.</p>


<figure class="image1">
  <img src="../images/00026.jpeg" alt="Figure 6-11: Jenkins job for testing tools" class="calibre25"/>
  <figcaption class="calibre26">Figure 6-11: Jenkins job for testing tools</figcaption>
</figure>


<p class="calibre3">This time, everything worked, and the build is green. We managed to run the steps in a different Namespace without sacrificing security while keeping <code class="calibre19">docker</code> commands outside the Kubernetes cluster in a separate node.</p>


<figure class="image1">
  <img src="../images/00027.jpeg" alt="Figure 6-12: Jenkins external VMs for building container images" class="calibre25"/>
  <figcaption class="calibre26">Figure 6-12: Jenkins external VMs for building container images</figcaption>
</figure>


<p class="calibre3">Now that we know what we want to accomplish, we’ll switch our attention to full automation of Jenkins installation and setup.</p>

<h3 id="leanpub-auto-automating-jenkins-installation-and-setup" class="calibre20">Automating Jenkins Installation And Setup</h3>

<p class="calibre3">One of the critical parts of Jenkins automation is the management of credentials. Jenkins uses <code class="calibre19">hudson.util.Secret</code> and <code class="calibre19">master.key</code> files to encrypt all the credentials. The two are stored in <em class="calibre17">secrets</em> directory inside Jenkins home directory. The credentials we uploaded or pasted are stored in <code class="calibre19">credentials.yml</code>. On top of those, each plugin (e.g., Google cloud) can add their files with credentials.</p>

<p class="calibre3">We need the credentials as well and the secrets if we are to automate Jenkins setup. One solution could be to generate the secrets, use them to encrypt credentials, and store them as Kubernetes secrets or config maps. However, that is a tedious process. Since we already have a fully configured Jenkins, we might just as well copy the files.</p>

<p class="calibre3">We’ll persist the files we need to the local directories <code class="calibre19">cluster/jenkins</code> and <code class="calibre19">cluster/jenkins/secrets</code>. So, our first step is to create them.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>mkdir -p cluster/jenkins/secrets
</pre></div>

</figure>

<p class="calibre3">Next, we need to copy the files from the existing Jenkins instance. To do that, we need to find out the name of the Pod with Jenkins. We’ll describe <code class="calibre19">jenkins</code> Deployment and check whether Jenkins Pods have labels that uniquely describe them.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kubectl -n jenkins <code class="se">\</code>
<code class="lineno">2 </code>    describe deployment jenkins
</pre></div>

</figure>

<p class="calibre3">The output, limited to the <code class="calibre19">Labels</code> section, is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>...
<code class="lineno">2 </code>Labels: chart=jenkins-0.16.1
<code class="lineno">3 </code>        component=jenkins-jenkins-master
<code class="lineno">4 </code>        heritage=Tiller
<code class="lineno">5 </code>        release=jenkins
<code class="lineno">6 </code>...
</pre></div>

</figure>

<p class="calibre3">The <code class="calibre19">component</code> label seems to be unique, and we can use it to retrieve the Jenkins Pod.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kubectl -n jenkins <code class="se">\</code>
<code class="lineno">2 </code>    get pods <code class="se">\</code>
<code class="lineno">3 </code>    -l <code class="nv">component</code><code class="o">=</code>jenkins-jenkins-master
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>NAME        READY STATUS  RESTARTS AGE
<code class="lineno">2 </code>jenkins-... 1/1   Running 0        3h
</pre></div>

</figure>

<p class="calibre3">We can combine that command with <code class="calibre19">jsonpath</code> output to retrieve only the name of the Pod and store it in an environment variable we’ll use later to copy the files we need.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">JENKINS_POD</code><code class="o">=</code><code class="k">$(</code>kubectl -n jenkins <code class="se">\</code>
<code class="lineno">2 </code>    get pods <code class="se">\</code>
<code class="lineno">3 </code>    -l <code class="nv">component</code><code class="o">=</code>jenkins-jenkins-master <code class="se">\</code>
<code class="lineno">4 </code>    -o <code class="nv">jsonpath</code><code class="o">=</code><code class="s">'{.items[0].metadata.name}'</code><code class="k">)</code>
<code class="lineno">5 </code>
<code class="lineno">6 </code><code class="nb">echo</code> <code class="nv">$JENKINS_POD</code>
</pre></div>

</figure>

<p class="calibre3">The output of the latter command is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>jenkins-c7f7c77b4-cgxx8
</pre></div>

</figure>

<p class="calibre3">Now we can copy the files we need.</p>

<p class="calibre3">As a minimum, we’ll need <code class="calibre19">credentials.xml</code>. That’s where (most of) the credentials are stored. Since Jenkins uses the secrets to encrypt and decrypt credentials, we’ll need them as well. Otherwise, Jenkins would generate new secrets when initializing and it could not decrypt the credentials.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>kubectl -n jenkins cp <code class="se">\</code>
<code class="lineno"> 2 </code>    <code class="nv">$JENKINS_POD</code>:var/jenkins_home/credentials.xml <code class="se">\</code>
<code class="lineno"> 3 </code>    cluster/jenkins
<code class="lineno"> 4 </code>
<code class="lineno"> 5 </code>kubectl -n jenkins cp <code class="se">\</code>
<code class="lineno"> 6 </code>    <code class="nv">$JENKINS_POD</code>:var/jenkins_home/secrets/hudson.util.Secret <code class="se">\</code>
<code class="lineno"> 7 </code>    cluster/jenkins/secrets
<code class="lineno"> 8 </code>
<code class="lineno"> 9 </code>kubectl -n jenkins cp <code class="se">\</code>
<code class="lineno">10 </code>    <code class="nv">$JENKINS_POD</code>:var/jenkins_home/secrets/master.key <code class="se">\</code>
<code class="lineno">11 </code>    cluster/jenkins/secrets
</pre></div>

</figure>

<aside class="warning">
    <h3 id="leanpub-auto-a-note-to-gke-users-13" class="calibre22">A note to GKE users</h3>

  <p class="calibre3">Google cloud plugin stores authentication in a JSON file in the <code class="calibre19">gauth</code> directory. We’ll need to copy the files from that folder as well.</p>

  <p class="calibre3"><code class="calibre19">kubectl -n jenkins cp $JENKINS_POD:var/jenkins_home/gauth/ cluster/jenkins/secrets</code></p>

  <p class="calibre3"><code class="calibre19">G_AUTH_FILE=$(ls cluster/jenkins/secrets/key*json | xargs -n 1 basename)</code></p>

  <p class="calibre3"><code class="calibre19">echo $G_AUTH_FILE</code></p>

  <p class="calibre3">The first command copied all the files from the <code class="calibre19">gauth</code> directory inside Jenkins home, the second stored the name of the JSON file in the environment variable <code class="calibre19">G_AUTH_FILE</code>, and the last command output the name of the file so that we can confirm that it looks OK. In my case, the output of the latter command is <code class="calibre19">key7754885476942296969.json</code>.</p>

</aside>

<p class="calibre3">Now that we stored the secrets and the credentials, we can remove Jenkins altogether and start over. This time, the goal is to have the installation and the setup fully automated or, if that’s not possible, to reduce the number of manual steps to a minimum.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>helm delete jenkins --purge
</pre></div>

</figure>

<p class="calibre3">The output clearly states that the <code class="calibre19">release "jenkins"</code> was <code class="calibre19">deleted</code>.</p>

<p class="calibre3">We’ll use a custom-made Helm chart this time. It is located in <em class="calibre17">helm/jenkins</em> directory inside the <a href="https://github.com/vfarcic/k8s-specs">vfarcic/k8s-specs</a> repository that we already cloned.</p>

<p class="calibre3">Let’s take a look at the files.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ls -1 helm/jenkins
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>Chart.yaml
<code class="lineno">2 </code>requirements.yaml
<code class="lineno">3 </code>templates
<code class="lineno">4 </code>values.yaml
</pre></div>

</figure>

<p class="calibre3">We can see that Chart follows a similar logic as the others. <code class="calibre19">Chart.yaml</code> defines metadata. You already used <code class="calibre19">values.yaml</code> so that should not come as a surprise, even though there is a twist which we’ll experience a bit later. The <code class="calibre19">templates</code> directory contains the templates that form the Chart even though, as you will discover later, it has only one file. What makes this Chart unique, when compared with those we used so far, is the <code class="calibre19">requirements.yaml</code> file. We’ll explore it first since it’ll provide insights into the twists and weirdness we’ll encounter in other files.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>cat helm/jenkins/requirements.yaml
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="calibre19">dependencies</code><code class="calibre19">:</code>
<code class="lineno">2 </code>  <code class="calibre19">-</code> <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">jenkins</code>
<code class="lineno">3 </code>    <code class="calibre19">version</code><code class="calibre19">:</code> <code class="calibre19">0.16.1</code>
<code class="lineno">4 </code>    <code class="calibre19">repository</code><code class="calibre19">:</code>  <code class="calibre19">https://kubernetes-charts.storage.googleapis.com</code>
</pre></div>

</figure>

<p class="calibre3">The <code class="calibre19">requirements.yaml</code> file lists all the dependencies of our Chart. Since all we need is Jenkins, it is the only requirement we specified.</p>

<p class="calibre3">Typically, we’d define our Chart and use <code class="calibre19">requirements.yaml</code> to add the dependencies our application needs. However, this use-case is a bit different. We do not have a Chart or, to be more precise, we did not define even a single YAML file in templates. All we want is to install Jenkins, customized to serve our needs.</p>

<p class="calibre3">At this point, you might be wondering why we do not install <code class="calibre19">stable/jenkins</code> directly with <code class="calibre19">--values</code> argument that will customize it. The reason behind using the requirements approach lies in our need to customize Jenkins <code class="calibre19">config.xml</code> file. README available in <code class="calibre19">stable/jenkins</code> Chart provides additional insight.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>helm inspect readme stable/jenkins
</pre></div>

</figure>

<p class="calibre3">The output, limited to the <code class="calibre19">Custom ConfigMap</code> section, is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>...
<code class="lineno"> 2 </code>## Custom ConfigMap
<code class="lineno"> 3 </code>
<code class="lineno"> 4 </code>When creating a new parent chart with this chart as a dependency, the `CustomConfigM\
<code class="lineno"> 5 </code>ap` parameter can be used to override the default config.xml provided.
<code class="lineno"> 6 </code>It also allows for providing additional xml configuration files that will be copied \
<code class="lineno"> 7 </code>into `/var/jenkins_home`. In the parent chart's values.yaml,
<code class="lineno"> 8 </code>set the `jenkins.Master.CustomConfigMap` value to true...
<code class="lineno"> 9 </code>...
<code class="lineno">10 </code>and provide the file `templates/config.tpl` in your parent chart for your use case. \
<code class="lineno">11 </code>You can start by copying the contents of `config.yaml` from this chart into your par\
<code class="lineno">12 </code>ent charts `templates/config.tpl` as a basis for customization. Finally, you'll need\
<code class="lineno">13 </code> to wrap the contents of `templates/config.tpl`...
<code class="lineno">14 </code>...
</pre></div>

</figure>

<p class="calibre3">To comply with those instructions, I already created the <code class="calibre19">values.yaml</code> file, so let’s take a quick look at it.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>cat helm/jenkins/values.yaml
</pre></div>

</figure>

<p class="calibre3">The output is as follows</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">jenkins</code><code class="calibre19">:</code>
<code class="lineno"> 2 </code>  <code class="calibre19">Master</code><code class="calibre19">:</code>
<code class="lineno"> 3 </code>    <code class="calibre19">ImageTag</code><code class="calibre19">:</code> <code class="s">"2.121.1-alpine"</code>
<code class="lineno"> 4 </code>    <code class="calibre19">Cpu</code><code class="calibre19">:</code> <code class="s">"500m"</code>
<code class="lineno"> 5 </code>    <code class="calibre19">Memory</code><code class="calibre19">:</code> <code class="s">"500Mi"</code>
<code class="lineno"> 6 </code>    <code class="calibre19">ServiceType</code><code class="calibre19">:</code> <code class="calibre19">ClusterIP</code>
<code class="lineno"> 7 </code>    <code class="calibre19">ServiceAnnotations</code><code class="calibre19">:</code>
<code class="lineno"> 8 </code>      <code class="calibre19">service.beta.kubernetes.io/aws-load-balancer-backend-protocol</code><code class="calibre19">:</code> <code class="calibre19">http</code>
<code class="lineno"> 9 </code>    <code class="calibre19">InstallPlugins</code><code class="calibre19">:</code>
<code class="lineno">10 </code>      <code class="calibre19">-</code> <code class="calibre19">blueocean:1.5.0</code>
<code class="lineno">11 </code>      <code class="calibre19">-</code> <code class="calibre19">credentials:2.1.16</code>
<code class="lineno">12 </code>      <code class="calibre19">-</code> <code class="calibre19">ec2:1.39</code>
<code class="lineno">13 </code>      <code class="calibre19">-</code> <code class="calibre19">git:3.9.1</code>
<code class="lineno">14 </code>      <code class="calibre19">-</code> <code class="calibre19">git-client:2.7.2</code>
<code class="lineno">15 </code>      <code class="calibre19">-</code> <code class="calibre19">github:1.29.1</code>
<code class="lineno">16 </code>      <code class="calibre19">-</code> <code class="calibre19">kubernetes:1.7.1</code>
<code class="lineno">17 </code>      <code class="calibre19">-</code> <code class="calibre19">pipeline-utility-steps:2.1.0</code>
<code class="lineno">18 </code>      <code class="calibre19">-</code> <code class="calibre19">script-security:1.44</code>
<code class="lineno">19 </code>      <code class="calibre19">-</code> <code class="calibre19">slack:2.3</code>
<code class="lineno">20 </code>      <code class="calibre19">-</code> <code class="calibre19">thinBackup:1.9</code>
<code class="lineno">21 </code>      <code class="calibre19">-</code> <code class="calibre19">workflow-aggregator:2.5</code>
<code class="lineno">22 </code>      <code class="calibre19">-</code> <code class="calibre19">ssh-slaves:1.26</code>
<code class="lineno">23 </code>      <code class="calibre19">-</code> <code class="calibre19">ssh-agent:1.15</code>
<code class="lineno">24 </code>      <code class="calibre19">-</code> <code class="calibre19">jdk-tool:1.1</code>
<code class="lineno">25 </code>      <code class="calibre19">-</code> <code class="calibre19">command-launcher:1.2</code>
<code class="lineno">26 </code>      <code class="calibre19">-</code> <code class="calibre19">github-oauth:0.29</code>
<code class="lineno">27 </code>      <code class="calibre19">-</code> <code class="calibre19">google-compute-engine:1.0.4</code>
<code class="lineno">28 </code>    <code class="calibre19">Ingress</code><code class="calibre19">:</code>
<code class="lineno">29 </code>      <code class="calibre19">Annotations</code><code class="calibre19">:</code>
<code class="lineno">30 </code>        <code class="calibre19">kubernetes.io/ingress.class</code><code class="calibre19">:</code> <code class="s">"nginx"</code>
<code class="lineno">31 </code>        <code class="calibre19">nginx.ingress.kubernetes.io/ssl-redirect</code><code class="calibre19">:</code> <code class="s">"false"</code>
<code class="lineno">32 </code>        <code class="calibre19">nginx.ingress.kubernetes.io/proxy-body-size</code><code class="calibre19">:</code> <code class="calibre19">50m</code>
<code class="lineno">33 </code>        <code class="calibre19">nginx.ingress.kubernetes.io/proxy-request-buffering</code><code class="calibre19">:</code> <code class="s">"off"</code>
<code class="lineno">34 </code>        <code class="calibre19">ingress.kubernetes.io/ssl-redirect</code><code class="calibre19">:</code> <code class="s">"false"</code>
<code class="lineno">35 </code>        <code class="calibre19">ingress.kubernetes.io/proxy-body-size</code><code class="calibre19">:</code> <code class="calibre19">50m</code>
<code class="lineno">36 </code>        <code class="calibre19">ingress.kubernetes.io/proxy-request-buffering</code><code class="calibre19">:</code> <code class="s">"off"</code>
<code class="lineno">37 </code>    <code class="calibre19">HostName</code><code class="calibre19">:</code> <code class="calibre19">jenkins.acme.com</code>
<code class="lineno">38 </code>    <code class="calibre19">CustomConfigMap</code><code class="calibre19">:</code> <code class="calibre19">true</code>
<code class="lineno">39 </code>    <code class="calibre19">CredentialsXmlSecret</code><code class="calibre19">:</code> <code class="calibre19">jenkins-credentials</code>
<code class="lineno">40 </code>    <code class="calibre19">SecretsFilesSecret</code><code class="calibre19">:</code> <code class="calibre19">jenkins-secrets</code>
<code class="lineno">41 </code>    <code class="c"># DockerAMI:</code>
<code class="lineno">42 </code>    <code class="c"># GProject:</code>
<code class="lineno">43 </code>    <code class="c"># GAuthFile:</code>
<code class="lineno">44 </code>  <code class="calibre19">rbac</code><code class="calibre19">:</code>
<code class="lineno">45 </code>    <code class="calibre19">install</code><code class="calibre19">:</code> <code class="calibre19">true</code>
<code class="lineno">46 </code>    <code class="calibre19">roleBindingKind</code><code class="calibre19">:</code> <code class="calibre19">RoleBinding</code>
</pre></div>

</figure>

<p class="calibre3">If we compare that with <code class="calibre19">helm/jenkins-values.yml</code>, we’ll notice that most entries are almost the same. There is one significant difference though. This time, all the entries are inside <code class="calibre19">jenkins</code>. That way, we’re telling Helm that the values should be applied to the dependency named <code class="calibre19">jenkins</code> and defined in <code class="calibre19">requirements.yaml</code>.</p>

<p class="calibre3">If we ignore the fact that all the entries are now inside <code class="calibre19">jenkins</code>, there is another significant difference in that we set <code class="calibre19">jenkins.Master.CustomConfigMap</code> to <code class="calibre19">true</code>. According to the instructions we saw in the README, this flag will allow us to provide a custom ConfigMap that will replace Jenkins’ <code class="calibre19">config.xml</code> file by parsing <code class="calibre19">templates/config.tmpl</code>. We’ll take a closer look at it soon.</p>

<p class="calibre3">The other new parameter is <code class="calibre19">CredentialsXmlSecret</code>. It holds the name of the Kubernetes secret where we’ll store Jenkins’ <code class="calibre19">credentials.xml</code> file we copied earlier. That parameter is tightly coupled with <code class="calibre19">SecretsFilesSecret</code> which holds the name of yet another Kubernetes secret which, this time, will contain the secrets which we copied to the local directory <code class="calibre19">cluster/jenkins/secrets</code>.</p>

<p class="calibre3">Further on, we have four commented parameters which will enable specific behavior if they are set. We’ll use <code class="calibre19">DockerAMI</code> to set AWS AMI in case we’re hosting our cluster in AWS. The last pair of (new) parameters is <code class="calibre19">GProject</code> and <code class="calibre19">GAuthFile</code>. The former is the GCE project we’ll use if we choose to use Google as our hosting vendor, and the latter is the authentication file which, if you followed that part, we also copied from the prior Jenkins installation. The usage of those parameters will become clearer once we explore <code class="calibre19">config.tpl</code> file.</p>

<p class="calibre3">The <code class="calibre19">helm/jenkins/templates/config.tpl</code> file is the key to our goals, so let’s take a closer look at it.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>cat helm/jenkins/templates/config.tpl
</pre></div>

</figure>

<p class="calibre3">The output is too big to be digested at once, so we’ll break the explanation into various pieces.</p>

<p class="calibre3">How did I create that file? I started by following the instructions in Chart’s README. I copied <code class="calibre19">config.yaml</code> and made the minor changes documented in the README. That was the easy part. Then I inspected the changes we made to <code class="calibre19">config.xml</code> inside our manually configured Jenkins (the one we deleted a few moments ago). They provided enough info about the entries that are missing, and I started modifying <code class="calibre19">config.tpl</code> file. The first change is in the snippet that follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">{{</code><code class="o">-</code> <code class="calibre19">define</code> <code class="s">"override_config_map"</code> <code class="calibre19">}}</code>
<code class="lineno"> 2 </code>
<code class="lineno"> 3 </code><code class="calibre19">apiVersion</code><code class="calibre19">:</code> <code class="calibre19">v1</code>
<code class="lineno"> 4 </code><code class="calibre19">kind</code><code class="calibre19">:</code> <code class="calibre19">ConfigMap</code>
<code class="lineno"> 5 </code><code class="calibre19">metadata</code><code class="calibre19">:</code>
<code class="lineno"> 6 </code>  <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">{{</code> <code class="calibre19">template</code> <code class="s">"jenkins.fullname"</code> <code class="calibre19">.</code> <code class="calibre19">}}</code>
<code class="lineno"> 7 </code><code class="calibre19">data</code><code class="calibre19">:</code>
<code class="lineno"> 8 </code>  <code class="calibre19">config</code><code class="calibre19">.</code><code class="calibre19">xml</code><code class="calibre19">:</code> <code class="err">|</code><code class="o">-</code>
<code class="lineno"> 9 </code>    <code class="o">&lt;</code><code class="err">?</code><code class="calibre19">xml</code> <code class="calibre19">version</code><code class="o">=</code><code class="c">'1.0' encoding='UTF-8'?&gt;</code>
<code class="lineno">10 </code><code class="na">    &lt;hudson&gt;</code>
<code class="lineno">11 </code>      <code class="calibre19">...</code>
<code class="lineno">12 </code>      <code class="o">&lt;</code><code class="calibre19">clouds</code><code class="o">&gt;</code>
<code class="lineno">13 </code>        <code class="o">&lt;</code><code class="calibre19">org</code><code class="calibre19">.</code><code class="calibre19">csanchez</code><code class="calibre19">.</code><code class="calibre19">jenkins</code><code class="calibre19">.</code><code class="calibre19">plugins</code><code class="calibre19">.</code><code class="calibre19">kubernetes</code><code class="calibre19">.</code><code class="calibre19">KubernetesCloud</code> <code class="calibre19">plugin</code><code class="o">=</code><code class="s">"kubernetes@\</code>
<code class="lineno">14 </code><code class="s">{{ template "</code><code class="calibre19">jenkins</code><code class="calibre19">.</code><code class="calibre19">kubernetes</code><code class="o">-</code><code class="calibre19">version</code><code class="s">" . }}"</code><code class="o">&gt;</code>
<code class="lineno">15 </code>          <code class="o">&lt;</code><code class="calibre19">name</code><code class="o">&gt;</code><code class="calibre19">kubernetes</code><code class="o">&lt;/</code><code class="calibre19">name</code><code class="o">&gt;</code>
<code class="lineno">16 </code>          <code class="o">&lt;</code><code class="calibre19">templates</code><code class="o">&gt;</code>
<code class="lineno">17 </code><code class="calibre19">{{</code><code class="o">-</code> <code class="k">if</code> <code class="calibre19">.</code><code class="calibre19">Values</code><code class="calibre19">.</code><code class="calibre19">Agent</code><code class="calibre19">.</code><code class="calibre19">Enabled</code> <code class="calibre19">}}</code>
<code class="lineno">18 </code>            <code class="o">&lt;</code><code class="calibre19">org</code><code class="calibre19">.</code><code class="calibre19">csanchez</code><code class="calibre19">.</code><code class="calibre19">jenkins</code><code class="calibre19">.</code><code class="calibre19">plugins</code><code class="calibre19">.</code><code class="calibre19">kubernetes</code><code class="calibre19">.</code><code class="calibre19">PodTemplate</code><code class="o">&gt;</code>
<code class="lineno">19 </code>              <code class="calibre19">...</code>
<code class="lineno">20 </code>              <code class="o">&lt;</code><code class="calibre19">containers</code><code class="o">&gt;</code>
<code class="lineno">21 </code>                <code class="o">&lt;</code><code class="calibre19">org</code><code class="calibre19">.</code><code class="calibre19">csanchez</code><code class="calibre19">.</code><code class="calibre19">jenkins</code><code class="calibre19">.</code><code class="calibre19">plugins</code><code class="calibre19">.</code><code class="calibre19">kubernetes</code><code class="calibre19">.</code><code class="calibre19">ContainerTemplate</code><code class="o">&gt;</code>
<code class="lineno">22 </code>                  <code class="calibre19">...</code>
<code class="lineno">23 </code>                  <code class="o">&lt;</code><code class="calibre19">envVars</code><code class="o">&gt;</code>
<code class="lineno">24 </code>                    <code class="o">&lt;</code><code class="calibre19">org</code><code class="calibre19">.</code><code class="calibre19">csanchez</code><code class="calibre19">.</code><code class="calibre19">jenkins</code><code class="calibre19">.</code><code class="calibre19">plugins</code><code class="calibre19">.</code><code class="calibre19">kubernetes</code><code class="calibre19">.</code><code class="calibre19">ContainerEnvVar</code><code class="o">&gt;</code>
<code class="lineno">25 </code>                      <code class="o">&lt;</code><code class="calibre19">key</code><code class="o">&gt;</code><code class="calibre19">JENKINS_URL</code><code class="o">&lt;/</code><code class="calibre19">key</code><code class="o">&gt;</code>
<code class="lineno">26 </code>                      <code class="o">&lt;</code><code class="calibre19">value</code><code class="o">&gt;</code><code class="calibre19">http</code><code class="calibre19">:</code><code class="o">//</code><code class="calibre19">{{</code> <code class="calibre19">template</code> <code class="s">"jenkins.fullname"</code> <code class="calibre19">.</code> <code class="calibre19">}}.{{</code> <code class="calibre19">.</code><code class="calibre19">Release</code><code class="calibre19">.</code><code class="o">\</code>
<code class="lineno">27 </code><code class="k">Namespace</code> <code class="calibre19">}}:{{.</code><code class="calibre19">Values</code><code class="calibre19">.</code><code class="calibre19">Master</code><code class="calibre19">.</code><code class="calibre19">ServicePort</code><code class="calibre19">}}{{</code> <code class="k">default</code> <code class="s">""</code> <code class="calibre19">.</code><code class="calibre19">Values</code><code class="calibre19">.</code><code class="calibre19">Master</code><code class="calibre19">.</code><code class="calibre19">JenkinsUriPr</code><code class="o">\</code>
<code class="lineno">28 </code><code class="calibre19">efix</code> <code class="calibre19">}}</code><code class="o">&lt;/</code><code class="calibre19">value</code><code class="o">&gt;</code>
<code class="lineno">29 </code>                    <code class="o">&lt;/</code><code class="calibre19">org</code><code class="calibre19">.</code><code class="calibre19">csanchez</code><code class="calibre19">.</code><code class="calibre19">jenkins</code><code class="calibre19">.</code><code class="calibre19">plugins</code><code class="calibre19">.</code><code class="calibre19">kubernetes</code><code class="calibre19">.</code><code class="calibre19">ContainerEnvVar</code><code class="o">&gt;</code>
<code class="lineno">30 </code>                  <code class="o">&lt;/</code><code class="calibre19">envVars</code><code class="o">&gt;</code>
<code class="lineno">31 </code>                <code class="o">&lt;/</code><code class="calibre19">org</code><code class="calibre19">.</code><code class="calibre19">csanchez</code><code class="calibre19">.</code><code class="calibre19">jenkins</code><code class="calibre19">.</code><code class="calibre19">plugins</code><code class="calibre19">.</code><code class="calibre19">kubernetes</code><code class="calibre19">.</code><code class="calibre19">ContainerTemplate</code><code class="o">&gt;</code>
<code class="lineno">32 </code>              <code class="o">&lt;/</code><code class="calibre19">containers</code><code class="o">&gt;</code>
<code class="lineno">33 </code>              <code class="calibre19">...</code>
<code class="lineno">34 </code>            <code class="o">&lt;/</code><code class="calibre19">org</code><code class="calibre19">.</code><code class="calibre19">csanchez</code><code class="calibre19">.</code><code class="calibre19">jenkins</code><code class="calibre19">.</code><code class="calibre19">plugins</code><code class="calibre19">.</code><code class="calibre19">kubernetes</code><code class="calibre19">.</code><code class="calibre19">PodTemplate</code><code class="o">&gt;</code>
<code class="lineno">35 </code><code class="calibre19">{{</code><code class="o">-</code> <code class="k">end</code> <code class="o">-</code><code class="calibre19">}}</code>
<code class="lineno">36 </code>          <code class="calibre19">...</code>
<code class="lineno">37 </code>          <code class="o">&lt;</code><code class="calibre19">jenkinsUrl</code><code class="o">&gt;</code><code class="calibre19">http</code><code class="calibre19">:</code><code class="o">//</code><code class="calibre19">{{</code> <code class="calibre19">template</code> <code class="s">"jenkins.fullname"</code> <code class="calibre19">.</code> <code class="calibre19">}}.{{</code> <code class="calibre19">.</code><code class="calibre19">Release</code><code class="calibre19">.</code><code class="calibre19">Namespa</code><code class="o">\</code>
<code class="lineno">38 </code><code class="calibre19">ce</code> <code class="calibre19">}}:{{.</code><code class="calibre19">Values</code><code class="calibre19">.</code><code class="calibre19">Master</code><code class="calibre19">.</code><code class="calibre19">ServicePort</code><code class="calibre19">}}{{</code> <code class="k">default</code> <code class="s">""</code> <code class="calibre19">.</code><code class="calibre19">Values</code><code class="calibre19">.</code><code class="calibre19">Master</code><code class="calibre19">.</code><code class="calibre19">JenkinsUriPrefix</code> <code class="calibre19">}}</code><code class="o">\</code>
<code class="lineno">39 </code><code class="na">&lt;/jenkinsUrl&gt;</code>
<code class="lineno">40 </code>          <code class="o">&lt;</code><code class="calibre19">jenkinsTunnel</code><code class="o">&gt;</code><code class="calibre19">{{</code> <code class="calibre19">template</code> <code class="s">"jenkins.fullname"</code> <code class="calibre19">.</code> <code class="calibre19">}}</code><code class="o">-</code><code class="calibre19">agent</code><code class="calibre19">.{{</code> <code class="calibre19">.</code><code class="calibre19">Release</code><code class="calibre19">.</code><code class="calibre19">Names</code><code class="o">\</code>
<code class="lineno">41 </code><code class="calibre19">pace</code> <code class="calibre19">}}:</code><code class="o">50000</code><code class="o">&lt;/</code><code class="calibre19">jenkinsTunnel</code><code class="o">&gt;</code>
<code class="lineno">42 </code>          <code class="calibre19">...</code>
</pre></div>

</figure>

<p class="calibre3">If you pay closer attention, you’ll notice that those are the changes we did previously to the Kubernetes cloud section in Jenkins configuration. We added <code class="calibre19">.{{ .Release.Namespace }}</code> to Jenkins URL and the tunnel so that Pods spun up in a different Namespace can establish communication with the master.</p>

<p class="calibre3">The next difference is the section dedicated to the <code class="calibre19">ec2</code> plugin.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">...</code>
<code class="lineno"> 2 </code><code class="calibre19">{{</code><code class="o">-</code> <code class="k">if</code> <code class="calibre19">.</code><code class="calibre19">Values</code><code class="calibre19">.</code><code class="calibre19">Master</code><code class="calibre19">.</code><code class="calibre19">DockerAMI</code> <code class="calibre19">}}</code>
<code class="lineno"> 3 </code>  <code class="o">&lt;</code><code class="calibre19">hudson</code><code class="calibre19">.</code><code class="calibre19">plugins</code><code class="calibre19">.</code><code class="calibre19">ec2</code><code class="calibre19">.</code><code class="calibre19">EC2Cloud</code> <code class="calibre19">plugin</code><code class="o">=</code><code class="s">"ec2@1.39"</code><code class="o">&gt;</code>
<code class="lineno"> 4 </code>    <code class="o">&lt;</code><code class="calibre19">name</code><code class="o">&gt;</code><code class="calibre19">ec2</code><code class="o">-</code><code class="calibre19">docker</code><code class="o">-</code><code class="calibre19">agents</code><code class="o">&lt;/</code><code class="calibre19">name</code><code class="o">&gt;</code>
<code class="lineno"> 5 </code>    <code class="calibre19">...</code>
<code class="lineno"> 6 </code>    <code class="o">&lt;</code><code class="calibre19">templates</code><code class="o">&gt;</code>
<code class="lineno"> 7 </code>      <code class="o">&lt;</code><code class="calibre19">hudson</code><code class="calibre19">.</code><code class="calibre19">plugins</code><code class="calibre19">.</code><code class="calibre19">ec2</code><code class="calibre19">.</code><code class="calibre19">SlaveTemplate</code><code class="o">&gt;</code>
<code class="lineno"> 8 </code>        <code class="o">&lt;</code><code class="calibre19">ami</code><code class="o">&gt;</code><code class="calibre19">{{.</code><code class="calibre19">Values</code><code class="calibre19">.</code><code class="calibre19">Master</code><code class="calibre19">.</code><code class="calibre19">DockerAMI</code><code class="calibre19">}}</code><code class="o">&lt;/</code><code class="calibre19">ami</code><code class="o">&gt;</code>
<code class="lineno"> 9 </code>        <code class="calibre19">...</code>
<code class="lineno">10 </code><code class="calibre19">{{</code><code class="o">-</code> <code class="calibre19">end</code> <code class="calibre19">}}</code>
<code class="lineno">11 </code><code class="calibre19">...</code>
</pre></div>

</figure>

<p class="calibre3">That section represents the addition that will be created if we use Jenkins’ EC2 plugin. I’ll be honest here and admit that I did not write that XML snippet. I don’t think that anyone could. Instead, I copied it from the previous Jenkins setup, pasted it to <code class="calibre19">config.tpl</code>, wrapped it with <code class="calibre19">{{- if .Values.Master.DockerAMI }}</code> and <code class="calibre19">{{- end }}</code> instructions, and changed <code class="calibre19">&lt;ami&gt;</code> entry to use <code class="calibre19">{{.Values.Master.DockerAMI}}</code> as the value. That way, the section will be rendered only if we provide <code class="calibre19">jenkins.Master.DockerAMI</code> value and the <code class="calibre19">ami</code> entry will be set to whatever our AMI ID is.</p>

<p class="calibre3">Similarly to the section enabled through the existence of the <code class="calibre19">jenkins.Master.DockerAMI</code> value, we can enable Google cloud through the XML wrapped inside <code class="calibre19">{{- if .Values.Master.GProject }}</code> and <code class="calibre19">{{- end }}</code> block. The relevant snippet of the <code class="calibre19">config.tpl</code> file is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">...</code>
<code class="lineno"> 2 </code><code class="calibre19">{{</code><code class="o">-</code> <code class="k">if</code> <code class="calibre19">.</code><code class="calibre19">Values</code><code class="calibre19">.</code><code class="calibre19">Master</code><code class="calibre19">.</code><code class="calibre19">GProject</code> <code class="calibre19">}}</code>
<code class="lineno"> 3 </code>  <code class="o">&lt;</code><code class="calibre19">com</code><code class="calibre19">.</code><code class="calibre19">google</code><code class="calibre19">.</code><code class="calibre19">jenkins</code><code class="calibre19">.</code><code class="calibre19">plugins</code><code class="calibre19">.</code><code class="calibre19">computeengine</code><code class="calibre19">.</code><code class="calibre19">ComputeEngineCloud</code> <code class="calibre19">plugin</code><code class="o">=</code><code class="s">"google-comput\</code>
<code class="lineno"> 4 </code><code class="s">e-engine@1.0.4"</code><code class="o">&gt;</code>
<code class="lineno"> 5 </code>    <code class="o">&lt;</code><code class="calibre19">name</code><code class="o">&gt;</code><code class="calibre19">gce</code><code class="o">-</code><code class="calibre19">docker</code><code class="o">&lt;/</code><code class="calibre19">name</code><code class="o">&gt;</code>
<code class="lineno"> 6 </code>    <code class="o">&lt;</code><code class="calibre19">instanceCap</code><code class="o">&gt;</code><code class="o">2147483647</code><code class="o">&lt;/</code><code class="calibre19">instanceCap</code><code class="o">&gt;</code>
<code class="lineno"> 7 </code>    <code class="o">&lt;</code><code class="calibre19">projectId</code><code class="o">&gt;</code><code class="calibre19">{{.</code><code class="calibre19">Values</code><code class="calibre19">.</code><code class="calibre19">Master</code><code class="calibre19">.</code><code class="calibre19">GProject</code><code class="calibre19">}}</code><code class="o">&lt;/</code><code class="calibre19">projectId</code><code class="o">&gt;</code>
<code class="lineno"> 8 </code>    <code class="o">&lt;</code><code class="calibre19">credentialsId</code><code class="o">&gt;</code><code class="calibre19">{{.</code><code class="calibre19">Values</code><code class="calibre19">.</code><code class="calibre19">Master</code><code class="calibre19">.</code><code class="calibre19">GProject</code><code class="calibre19">}}</code><code class="o">&lt;/</code><code class="calibre19">credentialsId</code><code class="o">&gt;</code>
<code class="lineno"> 9 </code>    <code class="calibre19">...</code>
<code class="lineno">10 </code><code class="calibre19">{{</code><code class="o">-</code> <code class="calibre19">end</code> <code class="calibre19">}}</code>
<code class="lineno">11 </code><code class="calibre19">...</code>
</pre></div>

</figure>

<p class="calibre3">Just as with the EC2, that snippet was copied from the previous Jenkins instance. I enveloped it with the <code class="calibre19">if</code>/<code class="calibre19">end</code> block. All occurrences of the Google project were replaced with <code class="calibre19">{{.Values.Master.GProject}}</code>.</p>

<p class="calibre3">Unfortunately, changing the template that produces Jenkins’ <code class="calibre19">config.xml</code> file is not enough, so I had to modify a few other entries in <code class="calibre19">config.tpl</code>.</p>

<p class="calibre3">If we continue walking through the differences, the next one is the <code class="calibre19">docker-build</code> entry of the ConfigMap. It contains the exact copy of the <code class="calibre19">docker-build</code> node we created when we configured a VM using Vagrant. Since all the credentials are external and the IP is fixed to <code class="calibre19">10.100.198.200</code>, I did not have to modify it in any form or way. A simple copy &amp; paste did the trick. However, we still need to figure out how to move the <code class="calibre19">docker-build</code> ConfigMap entry to <code class="calibre19">nodes/docker-build/config.xml</code> inside Jenkins home. The solution to that problem lies in <code class="calibre19">apply_config.sh</code> entry of the ConfigMap.</p>

<p class="calibre3">We’re almost done with the exploration of the changes, the only thing missing is the mechanism with which we’ll transfer the files generated through the ConfigMap into the adequate folders inside Jenkins home.</p>

<p class="calibre3">The last snippet from <code class="calibre19">config.tpl</code> comes from the <code class="calibre19">apply_config.sh</code> entry in the ConfigMap. The relevant parts are as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">...</code>
<code class="lineno"> 2 </code><code class="calibre19">  apply_config.sh: |-</code>
<code class="lineno"> 3 </code><code class="calibre19">    ...</code>
<code class="lineno"> 4 </code><code class="calibre19">    mkdir -p /var/jenkins_home/nodes/docker-build</code>
<code class="lineno"> 5 </code><code class="calibre19">    cp /var/jenkins_config/docker-build /var/jenkins_home/nodes/docker-build/config.\</code>
<code class="lineno"> 6 </code><code class="calibre19">xml;</code>
<code class="lineno"> 7 </code><code class="cp">{{</code><code class="o">-</code> <code class="k">if</code> <code class="nv">.Values.Master.GAuthFile</code> <code class="cp">}}</code><code class="calibre19"/>
<code class="lineno"> 8 </code><code class="calibre19">    mkdir -p /var/jenkins_home/gauth</code>
<code class="lineno"> 9 </code><code class="calibre19">    cp -n /var/jenkins_secrets/</code><code class="cp">{{</code><code class="nv">.Values.Master.GAuthFile</code><code class="cp">}}</code><code class="calibre19"> /var/jenkins_home/gauth;</code>
<code class="lineno">10 </code><code class="cp">{{</code><code class="o">-</code> <code class="nv">end</code> <code class="cp">}}</code><code class="calibre19"/>
<code class="lineno">11 </code><code class="calibre19">...</code>
<code class="lineno">12 </code><code class="cp">{{</code><code class="o">-</code> <code class="k">if</code> <code class="nv">.Values.Master.CredentialsXmlSecret</code> <code class="cp">}}</code><code class="calibre19"/>
<code class="lineno">13 </code><code class="calibre19">    cp -n /var/jenkins_credentials/credentials.xml /var/jenkins_home;</code>
<code class="lineno">14 </code><code class="cp">{{</code><code class="o">-</code> <code class="nv">end</code> <code class="cp">}}</code><code class="calibre19"/>
<code class="lineno">15 </code><code class="cp">{{</code><code class="o">-</code> <code class="k">if</code> <code class="nv">.Values.Master.SecretsFilesSecret</code> <code class="cp">}}</code><code class="calibre19"/>
<code class="lineno">16 </code><code class="calibre19">    cp -n /var/jenkins_secrets/* /usr/share/jenkins/ref/secrets;</code>
<code class="lineno">17 </code><code class="cp">{{</code><code class="o">-</code> <code class="nv">end</code> <code class="cp">}}</code><code class="calibre19"/>
<code class="lineno">18 </code><code class="calibre19">...</code>
</pre></div>

</figure>

<p class="calibre3">The <code class="calibre19">apply_config.sh</code> script will be executed during Jenkins initialization. The process is already defined in the official Jenkins Chart. I just had to extend it by adding <code class="calibre19">mkdir</code> and <code class="calibre19">cp</code> commands that will copy <code class="calibre19">docker-build</code> config into <code class="calibre19">/var/jenkins_home/nodes/docker-build/config.xml</code>. That should take care of the <code class="calibre19">docker-build</code> agent that uses the Vagrant VM we created earlier. If you choose to skip static VM in favor of AWS EC2 or Google cloud options, the agent will be created nevertheless, but it will be disconnected.</p>

<p class="calibre3">Further down, we can see a similar logic for the <code class="calibre19">gauth</code> directory that will be populated with the file provided as the Kubernetes secret with the name defined as the <code class="calibre19">jenkins.Master.GAuthFile</code> value.</p>

<p class="calibre3">Finally, it is worth mentioning the parts inside <code class="calibre19">{{- if .Values.Master.CredentialsXmlSecret }}</code> and <code class="calibre19">{{- if .Values.Master.SecretsFilesSecret }}</code> blocks. Those already existed in the original <code class="calibre19">config.yaml</code> file used as the base for <code class="calibre19">config.tpl</code>. They are responsible for copying the credentials and the secrets into the appropriate directories inside Jenkins home.</p>

<p class="calibre3">I must admit that all those steps are not easy to figure out. They require knowledge about internal Jenkins workings and are anything but intuitive. I should probably submit a few pull requests to the Jenkins Helm project to simplify the setup. Nevertheless, the current configuration should provide everything we need, even though it might not be easy to understand how we got here.</p>

<p class="calibre3">Now that we got a bit clearer understanding of the changes we did to the <code class="calibre19">config.tpl</code> file and the reasons behind creating a new Chart with <code class="calibre19">stable/jenkins</code> as the requirement, we can move forward and update the dependencies in the Chart located in the <code class="calibre19">helm/jenkins</code> directory.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>helm dependency update helm/jenkins
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>Hang tight while we grab the latest from your chart repositories...
<code class="lineno"> 2 </code>...Unable to get an update from the "local" chart repository (http://127.0.0.1:8879/\
<code class="lineno"> 3 </code>charts):
<code class="lineno"> 4 </code>        Get http://127.0.0.1:8879/charts/index.yaml: dial tcp 127.0.0.1:8879: connec\
<code class="lineno"> 5 </code>t: connection refused
<code class="lineno"> 6 </code>...Unable to get an update from the "chartmuseum" chart repository (http://cm.127.0.\
<code class="lineno"> 7 </code>0.1.nip.io):
<code class="lineno"> 8 </code>        Get http://cm.127.0.0.1.nip.io/index.yaml: dial tcp 127.0.0.1:80: connect: c\
<code class="lineno"> 9 </code>onnection refused
<code class="lineno">10 </code>...Successfully got an update from the "stable" chart repository
<code class="lineno">11 </code>Update Complete. Happy Helming!
<code class="lineno">12 </code>Saving 1 charts
<code class="lineno">13 </code>Downloading jenkins from repo https://kubernetes-charts.storage.googleapis.com
<code class="lineno">14 </code>Deleting outdated charts
</pre></div>

</figure>

<p class="calibre3">We can ignore the failures from the <code class="calibre19">local</code> and <code class="calibre19">chartmuseum</code> repositories. They are still configured in our local Helm even though they’re not currently running.</p>

<p class="calibre3">The important parts of the output are the last entries showing that Helm downloaded <code class="calibre19">jenkins</code> from the official repository. We can confirm that further by listing the files in the <code class="calibre19">helm/jenkins/charts</code> directory.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ls -1 helm/jenkins/charts
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>jenkins-...tgz
</pre></div>

</figure>

<p class="calibre3">We can see that the dependencies specified in the <code class="calibre19">requirements.yaml</code> file are downloaded to the <code class="calibre19">charts</code> directory. Since we specified <code class="calibre19">jenkins</code> as the only one, Helm downloaded a single <code class="calibre19">tgz</code> file.</p>

<p class="calibre3">We’re only one step away from being able to install custom Jenkins Chart with (almost) fully automated setup. The only things missing are the <code class="calibre19">jenkins-credentials</code> and <code class="calibre19">jenkins-secrets</code> Secrets.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kubectl -n jenkins <code class="se">\</code>
<code class="lineno">2 </code>    create secret generic <code class="se">\</code>
<code class="lineno">3 </code>    jenkins-credentials <code class="se">\</code>
<code class="lineno">4 </code>    --from-file cluster/jenkins/credentials.xml
<code class="lineno">5 </code>
<code class="lineno">6 </code>kubectl -n jenkins <code class="se">\</code>
<code class="lineno">7 </code>    create secret generic <code class="se">\</code>
<code class="lineno">8 </code>    jenkins-secrets <code class="se">\</code>
<code class="lineno">9 </code>    --from-file cluster/jenkins/secrets
</pre></div>

</figure>

<p class="calibre3">The <code class="calibre19">jenkins-credentials</code> Secret contains the <code class="calibre19">credentials.xml</code> file we extracted from the previous Jenkins setup. Similarly, the <code class="calibre19">jenkins-secrets</code> Secret contains all the files we stored in <code class="calibre19">cluster/jenkins/secrets</code> directory.</p>

<p class="calibre3">This is it. The moment of truth is upon us. We are about to test whether our attempt to (almost) fully automate Jenkins setup indeed produces the desired effect.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>helm install helm/jenkins <code class="se">\</code>
<code class="lineno">2 </code>    --name jenkins <code class="se">\</code>
<code class="lineno">3 </code>    --namespace jenkins <code class="se">\</code>
<code class="lineno">4 </code>    --set jenkins.Master.HostName<code class="o">=</code><code class="nv">$JENKINS_ADDR</code> <code class="se">\</code>
<code class="lineno">5 </code>    --set jenkins.Master.DockerVM<code class="o">=</code><code class="nv">$DOCKER_VM</code> <code class="se">\</code>
<code class="lineno">6 </code>    --set jenkins.Master.DockerAMI<code class="o">=</code><code class="nv">$AMI_ID</code> <code class="se">\</code>
<code class="lineno">7 </code>    --set jenkins.Master.GProject<code class="o">=</code><code class="nv">$G_PROJECT</code> <code class="se">\</code>
<code class="lineno">8 </code>    --set jenkins.Master.GAuthFile<code class="o">=</code><code class="nv">$G_AUTH_FILE</code>
</pre></div>

</figure>

<p class="calibre3">Please note that, depending on your choices, <code class="calibre19">AMI_ID</code>, <code class="calibre19">G_PROJECT</code>, and <code class="calibre19">G_AUTH_FILE</code> might not be set and, as a result, not all the changes we made to the Chart will be available.</p>

<p class="calibre3">Do you remember the patch I explained before? The one that is a temporary fix for the inability to change ClusterRoleBinding to RoleBinding? We still need to apply it.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kubectl delete clusterrolebinding <code class="se">\</code>
<code class="lineno">2 </code>    jenkins-role-binding
<code class="lineno">3 </code>
<code class="lineno">4 </code>kubectl apply -n jenkins <code class="se">\</code>
<code class="lineno">5 </code>    -f helm/jenkins-patch.yml
</pre></div>

</figure>

<p class="calibre3">Next, we’ll wait until <code class="calibre19">jenkins</code> Deployment rolls out before we open Jenkins and confirm that all the changes we made are indeed applied correctly.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kubectl -n jenkins <code class="se">\</code>
<code class="lineno">2 </code>    rollout status deployment jenkins
</pre></div>

</figure>

<p class="calibre3">The output should show that <code class="calibre19">deployment "jenkins"</code> was <code class="calibre19">successfully rolled out</code>, and we can open Jenkins in our favorite browser.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">"</code>
</pre></div>

</figure>

<p class="calibre3">Just as before, you’ll need the administrative password stored in the <code class="calibre19">jenkins</code> secret.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">JENKINS_PASS</code><code class="o">=</code><code class="k">$(</code>kubectl -n jenkins <code class="se">\</code>
<code class="lineno">2 </code>    get secret jenkins <code class="se">\</code>
<code class="lineno">3 </code>    -o <code class="nv">jsonpath</code><code class="o">=</code><code class="s">"{.data.jenkins-admin-password}"</code> <code class="se">\</code>
<code class="lineno">4 </code>    <code class="calibre19">|</code> base64 --decode<code class="calibre19">;</code> <code class="nb">echo</code><code class="k">)</code>
<code class="lineno">5 </code>
<code class="lineno">6 </code><code class="nb">echo</code> <code class="nv">$JENKINS_PASS</code>
</pre></div>

</figure>

<p class="calibre3">Please copy the output of the latter command, go back to Jenkins’ login screen, and use it as the password of the <code class="calibre19">admin</code> user.</p>

<p class="calibre3">The first thing we’ll check is whether <em class="calibre17">Kubernetes</em> cloud section of the Jenkins’ configuration screen is indeed populated with the correct values.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/configure"</code>
</pre></div>

</figure>

<p class="calibre3">Please confirm that the <em class="calibre17">Kubernetes</em> section fields <em class="calibre17">Jenkins URL</em> and <em class="calibre17">Jenkins tunnel</em> are correctly populated. They should have <em class="calibre17">http://jenkins.jenkins:8080</em> and <em class="calibre17">jenkins-agent.jenkins:50000</em> set as values.</p>

<p class="calibre3">Now that we know that <em class="calibre17">Kubernetes</em> is configured correctly and will be able to communicate with Pods outside the <code class="calibre19">jenkins</code> Namespace, we’ll proceed and confirm that other cloud sections are also configured correctly if we choose to use GKE.</p>

<aside class="warning">
    <h3 id="leanpub-auto-a-note-to-aws-ec2-users" class="calibre22">A note to AWS EC2 users</h3>

  <p class="calibre3">Unlike on-prem and GKE solutions, AWS requires a single manual step to complete the setup.</p>

  <p class="calibre3"><code class="calibre19">cat cluster/devops24.pem</code></p>

  <p class="calibre3">Copy the output, scroll to the <em class="calibre17">EC2 Key Pair’s Private Key</em> field, and paste it. Don’t forget to click the <em class="calibre17">Apply</em> button to persist the change.</p>

</aside>

<p class="calibre3">If you’re using <strong class="calibre18">GKE</strong>, you should observe that the <em class="calibre17">Google Compute Section</em> section fields look OK and that there is the message <em class="calibre17">The credential successfully made an API request to Google Compute Engine</em> below the <em class="calibre17">Service Account Credentials</em> field.</p>

<p class="calibre3">Next, we’ll confirm that the credentials were also created correctly and that Jenkins can decrypt them.</p>

<p class="calibre3">If you chose to use <strong class="calibre18">Vagrant</strong> to create a VM that will be used for building Docker images, please execute the command that follows to open the screen with the credentials.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/credentials/store/system/domain/_/credential/docker-build\</code>
<code class="lineno">2 </code><code class="s">/update"</code>
</pre></div>

</figure>

<p class="calibre3">If, on the other hand, you choose <strong class="calibre18">AWS</strong> to dynamically create nodes for building Docker images, the command that will open the screen with the credentials is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/credentials/store/system/domain/_/credential/aws/update"</code>
</pre></div>

</figure>

<p class="calibre3">Finally, if Google makes you tick and you chose <strong class="calibre18">GKE</strong> to host your cluster, the command that follows will open the screen with GCE credentials.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/credentials/store/system/domain/_/credential/</code><code class="nv">$G_PROJECT</code><code class="s">/u\</code>
<code class="lineno">2 </code><code class="s">pdate"</code>
</pre></div>

</figure>

<p class="calibre3">No matter which method you choose for hosting agents we’ll use to build Docker images, you should confirm that the credentials look OK.</p>

<p class="calibre3">Next, we’ll check whether the agents are indeed registered with Jenkins.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/computer"</code>
</pre></div>

</figure>

<p class="calibre3">If you chose to create a VM with <strong class="calibre18">Vagrant</strong>, you should see that the <em class="calibre17">docker-build</em> agent is created and that it is available. Otherwise, the agent will still be created, but it will NOT be available. Don’t panic if that’s the case. Jenkins will use AWS or GCE to spin them up when needed.</p>

<p class="calibre3">If you chose to use AWS or GCE for spinning agent nodes, you’ll notice the list saying <em class="calibre17">Provision via…</em> That allows us to spin up the VMs in GCE or AWS manually. However, we won’t do that. We’ll let Jenkins Pipeline use that option instead.</p>

<p class="calibre3">Everything seems to be configured correctly, and we can do the last verification. We’ll create a new job and confirm that it works as expected.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/newJob"</code>
</pre></div>

</figure>

<p class="calibre3">Please type <em class="calibre17">my-k8s-job</em> as the job name, select <em class="calibre17">Pipeline</em> as the job type, and click the <em class="calibre17">OK</em> button. Once inside the job configuration screen, click the <em class="calibre17">Pipeline</em> tab to jump to the <em class="calibre17">Script</em> field, and type the code that follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">podTemplate</code><code class="o">(</code>
<code class="lineno"> 2 </code>    <code class="nl">label:</code> <code class="s">"kubernetes"</code><code class="o">,</code>
<code class="lineno"> 3 </code>    <code class="nl">namespace:</code> <code class="s">"go-demo-3-build"</code><code class="o">,</code>
<code class="lineno"> 4 </code>    <code class="nl">serviceAccount:</code> <code class="s">"build"</code><code class="o">,</code>
<code class="lineno"> 5 </code>    <code class="nl">yaml:</code> <code class="s">"""</code>
<code class="lineno"> 6 </code><code class="s">apiVersion: v1</code>
<code class="lineno"> 7 </code><code class="s">kind: Pod</code>
<code class="lineno"> 8 </code><code class="s">spec:</code>
<code class="lineno"> 9 </code><code class="s">  containers:</code>
<code class="lineno">10 </code><code class="s">  - name: kubectl</code>
<code class="lineno">11 </code><code class="s">    image: vfarcic/kubectl</code>
<code class="lineno">12 </code><code class="s">    command: ["sleep"]</code>
<code class="lineno">13 </code><code class="s">    args: ["100000"]</code>
<code class="lineno">14 </code><code class="s">  - name: oc</code>
<code class="lineno">15 </code><code class="s">    image: vfarcic/openshift-client</code>
<code class="lineno">16 </code><code class="s">    command: ["sleep"]</code>
<code class="lineno">17 </code><code class="s">    args: ["100000"]</code>
<code class="lineno">18 </code><code class="s">  - name: golang</code>
<code class="lineno">19 </code><code class="s">    image: golang:1.9</code>
<code class="lineno">20 </code><code class="s">    command: ["sleep"]</code>
<code class="lineno">21 </code><code class="s">    args: ["100000"]</code>
<code class="lineno">22 </code><code class="s">  - name: helm</code>
<code class="lineno">23 </code><code class="s">    image: vfarcic/helm:2.8.2</code>
<code class="lineno">24 </code><code class="s">    command: ["sleep"]</code>
<code class="lineno">25 </code><code class="s">    args: ["100000"]</code>
<code class="lineno">26 </code><code class="s">"""</code>
<code class="lineno">27 </code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">28 </code>    <code class="calibre19">node</code><code class="o">(</code><code class="s">"docker"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">29 </code>        <code class="calibre19">stage</code><code class="o">(</code><code class="s">"docker"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">30 </code>            <code class="calibre19">sh</code> <code class="s">"sudo docker version"</code>
<code class="lineno">31 </code>        <code class="o">}</code>
<code class="lineno">32 </code>    <code class="o">}</code>
<code class="lineno">33 </code>    <code class="calibre19">node</code><code class="o">(</code><code class="s">"kubernetes"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">34 </code>        <code class="calibre19">container</code><code class="o">(</code><code class="s">"kubectl"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">35 </code>            <code class="calibre19">stage</code><code class="o">(</code><code class="s">"kubectl"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">36 </code>                <code class="calibre19">sh</code> <code class="s">"kubectl version"</code>
<code class="lineno">37 </code>            <code class="o">}</code>
<code class="lineno">38 </code>        <code class="o">}</code>
<code class="lineno">39 </code>        <code class="calibre19">container</code><code class="o">(</code><code class="s">"oc"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">40 </code>            <code class="calibre19">stage</code><code class="o">(</code><code class="s">"oc"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">41 </code>                <code class="calibre19">sh</code> <code class="s">"oc version"</code>
<code class="lineno">42 </code>            <code class="o">}</code>
<code class="lineno">43 </code>        <code class="o">}</code>
<code class="lineno">44 </code>        <code class="calibre19">container</code><code class="o">(</code><code class="s">"golang"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">45 </code>            <code class="calibre19">stage</code><code class="o">(</code><code class="s">"golang"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">46 </code>                <code class="calibre19">sh</code> <code class="s">"go version"</code>
<code class="lineno">47 </code>            <code class="o">}</code>
<code class="lineno">48 </code>        <code class="o">}</code>
<code class="lineno">49 </code>        <code class="calibre19">container</code><code class="o">(</code><code class="s">"helm"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">50 </code>            <code class="calibre19">stage</code><code class="o">(</code><code class="s">"helm"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">51 </code>                <code class="calibre19">sh</code> <code class="s">"helm version --tiller-namespace go-demo-3-build"</code>
<code class="lineno">52 </code>            <code class="o">}</code>
<code class="lineno">53 </code>        <code class="o">}</code>
<code class="lineno">54 </code>    <code class="o">}</code>
<code class="lineno">55 </code><code class="o">}</code>
</pre></div>

</figure>

<aside class="information">
    <p class="calibre3">If you prefer to copy and paste, the job is available in the <a href="https://gist.github.com/fbf9fc6611fe400c7950f43cfc89f406">my-k8s-job-docker.groovy Gist</a>.</p>

</aside>

<p class="calibre3">Please note that the job is the same as the one we used to validate the manual setup and, therefore, there’s probably no need to comment on it again. You know what it does, so click the <em class="calibre17">Save</em> button to persist it.</p>

<p class="calibre3">Next, we’ll open the job in BlueOcean and run a build.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/blue/organizations/jenkins/my-k8s-job/activity"</code>
</pre></div>

</figure>

<p class="calibre3">Press the <em class="calibre17">Run</em> button, followed with a click on the row with the new build. Wait until all the stages are finished, and the result is green. Open a bottle of Champagne to celebrate.</p>

<h3 id="leanpub-auto-what-now-5" class="calibre20">What Now?</h3>

<p class="calibre3">If we exclude the case of entering AWS key, our Jenkins setup is fully automated. Kubernetes plugin is pre-configured to support Pods running in other Namespaces, Google and AWS clouds will be set up if we choose to use them, credentials are copied to the correct locations, and they are using the same encryption keys as those used to encrypt the credentials in the first place. All in all, we’re finally ready to work on our continuous deployment pipeline. The next chapter will be the culmination of everything we did thus far.</p>

<p class="calibre3">Please note that the current setup is designed to support “one Jenkins master per team” strategy. Even though you could use the experience you gained so far to run a production-ready Jenkins master that will serve everyone in your company, it is often a better strategy to have one master per team. That approach provides quite a few benefits.</p>

<p class="calibre3">If each team gets a Jenkins master, each team will be able to work independently of others. A team can decide to upgrade their plugins without fear that they will affect others. We can choose to experiment with things that might cause trouble to others by creating a temporary master. Every team can have fine-tuned permissions on the Namespaces that matter to them, and no ability to do anything inside other Namespaces.</p>

<p class="calibre3">The productivity of a team is often directly proportional to the ability to do things without being limited with the actions of other teams and, at the same time freedom not to worry whether their work will negatively affect others. In Kubernetes, we get that freedom through Namespaces. In Jenkins, we get it by having masters dedicated to teams.</p>

<p class="calibre3">The Helm Chart we created is a step towards multi-master strategy. The Jenkins we installed can be considered dedicated to the team in charge of the <em class="calibre17">go-demo-3</em> application. Or it can be devoted to a bigger team. The exact division will differ from one organization to another. What matters is that no matter how many Jenkins masters we need, all we have to do is execute <code class="calibre19">helm install</code> for each. Given enough resources in the cluster, we can have a hundred fully operational Jenkins masters in only a few minutes time. And they will not be Jenkins masters waiting to be configured, but rather masters already loaded with everything a team needs. All they’d need to do is create Pipelines that will execute the steps necessary for their application to move from a commit into production. That’s the subject of the next chapter.</p>

<p class="calibre3">One more chapter is finished and, like all the others, the next one will start from scratch. Please use the commands that follow to clean up the resources we created or, if you’re using a temporary cluster, go ahead and destroy it.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>helm delete <code class="k">$(</code>helm ls -q<code class="k">)</code> --purge
<code class="lineno">2 </code>
<code class="lineno">3 </code>kubectl delete ns <code class="se">\</code>
<code class="lineno">4 </code>    go-demo-3 go-demo-3-build jenkins
</pre></div>

</figure>

<p class="calibre3">If you created a VM using <strong class="calibre18">Vagrant</strong>, I suggest you suspend it instead of destroying it. That way we’ll preserve the same credentials and will be able to reuse those we stored in <code class="calibre19">cluster/jenkins/secrets/</code>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">cd</code> cd/docker-build
<code class="lineno">2 </code>
<code class="lineno">3 </code>vagrant <code class="nb">suspend</code>
<code class="lineno">4 </code>
<code class="lineno">5 </code><code class="nb">cd</code> ../../
</pre></div>

</figure>

<p class="calibre3">Take some time to enjoy what we accomplished so far. The next chapter will be the culmination of our efforts.</p>



</div>
</body></html>