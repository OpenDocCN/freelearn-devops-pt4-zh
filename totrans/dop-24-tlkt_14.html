<html><head></head><body>
<div class="calibre6">
<h2 id="leanpub-auto-continuous-delivery-with-jenkins-and-gitops" class="calibre16">Continuous Delivery With Jenkins And GitOps</h2>

<aside class="tip">
    <p class="calibre3">Continuous delivery is a step down from continuous deployment. Instead of deploying every commit from the master branch to production, we are choosing which build should be promoted. Continuous delivery has that single manual step that forces us (humans) to decide which release should be upgraded in production.</p>

</aside>

<p class="calibre3">Given that we already explored continuous deployment, you might be wondering why are we even talking at this point about continuous delivery. There are a few reasons for that. First of all, I am conscious that many of you will not or can not implement continuous deployment. Your tests might not be as reliable as you’d need them to be. Your processes might not allow full automation. You might have to follow regulations that prevent you from reaching nirvana. There could be many other reasons. The point is that not everyone can apply continuous deployment. Even among those that can get there, there are indeed some that do not want that as the destination. So, we’ll explore continuous delivery as an alternative to continuous deployment.</p>

<p class="calibre3">There are other reasons for writing this chapter. So far, I showed you one possible implementation of the continuous deployment pipeline. We could modify the existing pipeline by adding an <code class="calibre19">input</code> step before making the release and upgrading production. That would add <em class="calibre17">proceed</em> and <em class="calibre17">cancel</em> buttons that we could use to choose whether to upgrade the production release or not. This chapter would be the shortest chapter ever, and that would be boring. Where’s the fun in doing a small variation of the same?</p>

<p class="calibre3">We’ll use this chapter to explore a few alternative approaches to writing Jenkins pipeline. Just as the pipeline from the previous chapter could be easily converted from continuous deployment to continuous delivery process, what we’re going to do next could also go both ways. So, even though our objective is to write a continuous delivery pipeline, the lessons from this chapter could be easily applied to continuous deployment as well.</p>

<p class="calibre3">We’ll use this opportunity to explore declarative pipeline as an alternative to scripted. We’ll switch from using a separate VM for building docker image to using the Docker socket to build it in one of the nodes of the cluster. We’ll explore how we can define our whole productions environment differently. We’ll even introduce GitOps.</p>

<p class="calibre3">The real goal is to give you valid alternatives to the approaches we used so far, thus allowing you to make better decisions when implementing lessons-learned in your organization. I hope that by the end of this chapter you will be able to cherry-pick things that suit you the best and assemble your own process.</p>

<p class="calibre3">That’s it for the prep-talk. You know what continuous delivery is, and you know how to use Kubernetes. Let’s define some pipelines.</p>

<h3 id="leanpub-auto-creating-a-cluster-5" class="calibre20">Creating A Cluster</h3>

<p class="calibre3">Just as before, we’ll start the practical part by making sure that we have the latest version of the <em class="calibre17">k8s-specs</em> repository.</p>

<aside class="information">
    <p class="calibre3">All the commands from this chapter are available in the <a href="https://gist.github.com/cb0ececf6600745daeac8cc3ae400a86">08-jenkins-cd.sh</a> Gist.</p>

</aside>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">cd</code> k8s-specs
<code class="lineno">2 </code>
<code class="lineno">3 </code>git pull
</pre></div>

</figure>

<p class="calibre3">Unlike the previous chapters, you cannot use an existing cluster this time. The reason behind that lies in reduced requirements. This time, the cluster should <strong class="calibre18">NOT have ChartMuseum</strong>. Soon you’ll see why. What we need are the same hardware specs (excluding GKE), with NGINX Ingress and Tiller running inside the cluster, and with the environment variable <code class="calibre19">LB_IP</code> that holds the address of the IP through which we can access the external load balancer, or with the IP of the VM in case of single VM local clusters like minikube, minishift, and Docker For Mac or Windows.</p>

<p class="calibre3">For <strong class="calibre18">GKE</strong> we’ll need to increase memory slightly so we’ll use <strong class="calibre18">n1-highcpu-4</strong> instance types instead of <em class="calibre17">n1-highcpu-2</em> we used so far.</p>

<ul class="calibre21">
  <li class="calibre15">
<a href="https://gist.github.com/d07bcbc7c88e8bd104fedde63aee8374">docker4mac-cd.sh</a>: <strong class="calibre18">Docker for Mac</strong> with 3 CPUs, 4 GB RAM, with <strong class="calibre18">nginx Ingress</strong>, with <strong class="calibre18">tiller</strong>, and with <code class="calibre19">LB_IP</code> variable set to the IP of the cluster.</li>
  <li class="calibre15">
<a href="https://gist.github.com/06bb38787932520906ede2d4c72c2bd8">minikube-cd.sh</a>: <strong class="calibre18">minikube</strong> with 3 CPUs, 4 GB RAM, with <code class="calibre19">ingress</code>, <code class="calibre19">storage-provisioner</code>, and <code class="calibre19">default-storageclass</code> addons enabled, with <strong class="calibre18">tiller</strong>, and with <code class="calibre19">LB_IP</code> variable set to the VM created by minikube.</li>
  <li class="calibre15">
<a href="https://gist.github.com/d96c27204ff4b3ad3f4ae80ca3adb891">kops-cd.sh</a>: <strong class="calibre18">kops in AWS</strong> with 3 t2.small masters and 2 t2.medium nodes spread in three availability zones, with <strong class="calibre18">nginx Ingress</strong>, with <strong class="calibre18">tiller</strong>, and with <code class="calibre19">LB_IP</code> variable set to the IP retrieved by pinging ELB’s hostname. The Gist assumes that the prerequisites are set through <a href="part0018.html#appendix-b">Appendix B</a>.</li>
  <li class="calibre15">
<a href="https://gist.github.com/94cfcb3f9e6df965ec233bbb5bf54110">minishift-cd.sh</a>: <strong class="calibre18">minishift</strong> with 4 CPUs, 4 GB RAM, with version 1.16+, with <strong class="calibre18">tiller</strong>, and with <code class="calibre19">LB_IP</code> variable set to the VM created by minishift.</li>
  <li class="calibre15">
<a href="https://gist.github.com/1b126df156abc91d51286c603b8f8718">gke-cd.sh</a>: <strong class="calibre18">Google Kubernetes Engine (GKE)</strong> with 3 n1-highcpu-4 (4 CPUs, 3.6 GB RAM) nodes (one in each zone), with <strong class="calibre18">nginx Ingress</strong> controller running on top of the “standard” one that comes with GKE, with <strong class="calibre18">tiller</strong>, and with <code class="calibre19">LB_IP</code> variable set to the IP of the external load balancer created when installing nginx Ingress. We’ll use nginx Ingress for compatibility with other platforms. Feel free to modify the YAML files and Helm Charts if you prefer NOT to install nginx Ingress.</li>
  <li class="calibre15">
<a href="https://gist.github.com/2af71594d5da3ca550de9dca0f23a2c5">eks-cd.sh</a>: <strong class="calibre18">Elastic Kubernetes Service (EKS)</strong> with 2 t2.medium nodes, with <strong class="calibre18">nginx Ingress</strong> controller, with a <strong class="calibre18">default StorageClass</strong>, with <strong class="calibre18">tiller</strong>, and with <code class="calibre19">LB_IP</code> variable set tot he IP retrieved by pinging ELB’s hostname.</li>
</ul>

<p class="calibre3">Here we go.</p>

<h3 id="leanpub-auto-defining-the-whole-production-environment" class="calibre20">Defining The Whole Production Environment</h3>

<p class="calibre3">All the chapters until this one followed the same pattern. We’d learn about a new tool and, from there on, we’d streamline its installation through Gists in all subsequent chapters. As an example, we introduced ChartMuseum a few chapters ago, we learned how to install it, and, from there on, there was no point reiterating the same set of steps in the chapters that followed. Instead, we had the installation steps in Gists. Knowing that, you might be wondering why we did not follow the same pattern now. Why was ChartMuseum excluded from the Gists we’re using in this chapter? Why isn’t Jenkins there as well? Are we going to install ChartMuseum and Jenkins with a different configuration? We’re not. Both will have the same configuration, but they will be installed in a slightly different way.</p>

<p class="calibre3">We already saw the benefits provided by Helm. Among other things, it features a templating mechanism that allows us to customize our Kubernetes definitions. We used <code class="calibre19">requirements.yaml</code> file to create our own Jenkins distribution. Helm requirements are a nifty feature initially designed to provide means to define dependencies of our application. As an example, if we’d create an application that uses Redis DB, our application would be defined in templates and Redis would be a dependency defined in <code class="calibre19">requirement.yaml</code>. After all, if the community already has a Chart for Redis, why would we reinvent the wheel by creating our own definitions? Instead, we’d put it as an entry in <code class="calibre19">requirements.yaml</code>. Even though our motivation was slightly different, we did just that with Jenkins. As you might have guessed, dependencies in <code class="calibre19">requirements.yaml</code> are not limited to a single entry. We can define as many dependencies as we need.</p>

<p class="calibre3">We could, for example, create a Chart that would define Namespaces, RoleBindings, and all the other infrastructure-level things that our production environment needs. Such a Chart could treat all production releases as dependencies. If we could do something like that, we could store everything related to production in a single repository. That would simplify the initial installation as well as upgrades of the production applications. Such an approach does not need to be limited to production. There could be another repository for other environments. Testing would be a good example if we still rely on manual tasks in that area.</p>

<p class="calibre3">Since we’d keep those Charts in Git repositories, changes to what constitutes production could be reviewed and, if necessary, approved before they’re merged to the master branch. There are indeed other benefits to having a whole environment in a Git repository. I’ll leave it to your imagination to figure them out.</p>

<p class="calibre3">The beauty of Helm requirements is that they still allow us to keep the definition of an application in the same repository as the code. If we take our <em class="calibre17">go-demo</em> application as an example, the Chart that defines the application can and should continue residing in its repository. However, a different repository could define all the applications running in the production environment as dependencies, including <em class="calibre17">go-demo</em>. That way, we’ll accomplish two things. Everything related to an application, including its Chart would be in the same repository without breaking the everything-in-git rule. So far, our continuous deployment pipeline (the one we defined in the previous chapter) breaks that rule. Jenkins was upgrading production releases without storing that information in Git. We had undocumented deployments. While releases under test are temporary and live only for the duration of those automated tests, production releases last longer and should be documented, even if their life-span is also potentially short (until the next commit).</p>

<p class="calibre3">All in all, our next task is to have the whole production environment in a single repository, without duplicating the information already available in repositories where we keep the code and definitions of our applications.</p>

<p class="calibre3">I already created a repository <a href="https://github.com/vfarcic/k8s-prod">vfarcic/k8s-prod</a> that defines a production environment. Since we’ll have to make some changes to a few files, our first task is to fork it. Otherwise, I’d need to give you my GitHub credentials so that you could push those changes to my repo. As you can probably guess, that is not going to happen.</p>

<p class="calibre3">Please open <a href="https://github.com/vfarcic/k8s-prod">vfarcic/k8s-prod</a> in a browser and fork the repository. I’m sure you already know how to do that. If you don’t, all you have to do is to click on the <em class="calibre17">Fork</em> button located in the top-right corner and follow the wizard instructions.</p>

<p class="calibre3">Next, we’ll clone the forked repository before we explore some of its files.</p>

<p class="calibre3">Please replace <code class="calibre19">[...]</code> with your GitHub username before running the commands that follow.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">GH_USER</code><code class="o">=[</code>...<code class="o">]</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code><code class="nb">cd</code> ..
<code class="lineno">4 </code>
<code class="lineno">5 </code>git clone https://github.com/<code class="nv">$GH_USER</code>/k8s-prod.git
<code class="lineno">6 </code>
<code class="lineno">7 </code><code class="nb">cd</code> k8s-prod
</pre></div>

</figure>

<p class="calibre3">We cloned the forked repository and entered into its root directory.</p>

<p class="calibre3">Let’s see what we have.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>cat helm/Chart.yaml
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="calibre19">apiVersion</code><code class="calibre19">:</code> <code class="calibre19">v1</code>
<code class="lineno">2 </code><code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">prod-env</code>
<code class="lineno">3 </code><code class="calibre19">version</code><code class="calibre19">:</code> <code class="calibre19">0.0.1</code>
<code class="lineno">4 </code><code class="calibre19">description</code><code class="calibre19">:</code> <code class="calibre19">Docker For Mac or Windows Production Environment</code>
<code class="lineno">5 </code><code class="calibre19">maintainers</code><code class="calibre19">:</code>
<code class="lineno">6 </code><code class="calibre19">-</code> <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">Viktor Farcic</code>
<code class="lineno">7 </code>  <code class="calibre19">email</code><code class="calibre19">:</code> <code class="calibre19">viktor@farcic.com</code>
</pre></div>

</figure>

<p class="calibre3">The <code class="calibre19">Chart.yaml</code> file is very uneventful, so we’ll skip explaining it. The only thing that truly matters is the <code class="calibre19">version</code>.</p>

<aside class="warning">
    <p class="calibre3">You might see a different <code class="calibre19">version</code> than the one from the output above. Don’t panic! I probably bumped it in one of my tests.</p>

</aside>

<p class="calibre3">Let’s take a look at the <code class="calibre19">requirements.yaml</code>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>cp helm/requirements-orig.yaml <code class="se">\</code>
<code class="lineno">2 </code>    helm/requirements.yaml
<code class="lineno">3 </code>
<code class="lineno">4 </code>cat helm/requirements.yaml
</pre></div>

</figure>

<p class="calibre3">We copied the original requirements as a precaution since I might have changed <code class="calibre19">requirements.yaml</code> during one of my experiments.</p>

<p class="calibre3">The output of the latter command is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="calibre19">dependencies</code><code class="calibre19">:</code>
<code class="lineno">2 </code><code class="calibre19">-</code> <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">chartmuseum</code>
<code class="lineno">3 </code>  <code class="calibre19">repository</code><code class="calibre19">:</code> <code class="s">"@stable"</code>
<code class="lineno">4 </code>  <code class="calibre19">version</code><code class="calibre19">:</code> <code class="calibre19">1.6.0</code>
<code class="lineno">5 </code><code class="calibre19">-</code> <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">jenkins</code>
<code class="lineno">6 </code>  <code class="calibre19">repository</code><code class="calibre19">:</code> <code class="s">"@stable"</code>
<code class="lineno">7 </code>  <code class="calibre19">version</code><code class="calibre19">:</code> <code class="calibre19">0.16.6</code>
</pre></div>

</figure>

<p class="calibre3">We can see that the requirements for our production environments are <code class="calibre19">chartmuseum</code> and <code class="calibre19">jenkins</code>. Both are located in the <code class="calibre19">stable</code> repository (the official Helm repo).</p>

<p class="calibre3">Offcourse, just stating the requirements is not enough. Our applications almost always require customized versions of both public and private Charts.</p>

<p class="calibre3">We already know from the previous chapters that we can leverage <code class="calibre19">values.yaml</code> file to customize Charts. The repository already has one, so let’s take a quick look.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>cat helm/values-orig.yaml
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">chartmuseum</code><code class="calibre19">:</code>
<code class="lineno"> 2 </code>  <code class="calibre19">env</code><code class="calibre19">:</code>
<code class="lineno"> 3 </code>    <code class="calibre19">open</code><code class="calibre19">:</code>
<code class="lineno"> 4 </code>      <code class="calibre19">DISABLE_API</code><code class="calibre19">:</code> <code class="calibre19">false</code>
<code class="lineno"> 5 </code>      <code class="calibre19">AUTH_ANONYMOUS_GET</code><code class="calibre19">:</code> <code class="calibre19">true</code>
<code class="lineno"> 6 </code>    <code class="calibre19">secret</code><code class="calibre19">:</code>
<code class="lineno"> 7 </code>      <code class="calibre19">BASIC_AUTH_USER</code><code class="calibre19">:</code> <code class="calibre19">admin</code> <code class="c"># Change me!</code>
<code class="lineno"> 8 </code>      <code class="calibre19">BASIC_AUTH_PASS</code><code class="calibre19">:</code> <code class="calibre19">admin</code> <code class="c"># Change me!</code>
<code class="lineno"> 9 </code>  <code class="calibre19">resources</code><code class="calibre19">:</code>
<code class="lineno">10 </code>    <code class="calibre19">limits</code><code class="calibre19">:</code>
<code class="lineno">11 </code>      <code class="calibre19">cpu</code><code class="calibre19">:</code> <code class="calibre19">100m</code>
<code class="lineno">12 </code>      <code class="calibre19">memory</code><code class="calibre19">:</code> <code class="calibre19">128Mi</code>
<code class="lineno">13 </code>    <code class="calibre19">requests</code><code class="calibre19">:</code>
<code class="lineno">14 </code>      <code class="calibre19">cpu</code><code class="calibre19">:</code> <code class="calibre19">80m</code>
<code class="lineno">15 </code>      <code class="calibre19">memory</code><code class="calibre19">:</code> <code class="calibre19">64Mi</code>
<code class="lineno">16 </code>  <code class="calibre19">persistence</code><code class="calibre19">:</code>
<code class="lineno">17 </code>    <code class="calibre19">enabled</code><code class="calibre19">:</code> <code class="calibre19">true</code>
<code class="lineno">18 </code>  <code class="calibre19">ingress</code><code class="calibre19">:</code>
<code class="lineno">19 </code>    <code class="calibre19">enabled</code><code class="calibre19">:</code> <code class="calibre19">true</code>
<code class="lineno">20 </code>    <code class="calibre19">annotations</code><code class="calibre19">:</code>
<code class="lineno">21 </code>      <code class="calibre19">kubernetes.io/ingress.class</code><code class="calibre19">:</code> <code class="s">"nginx"</code>
<code class="lineno">22 </code>      <code class="calibre19">ingress.kubernetes.io/ssl-redirect</code><code class="calibre19">:</code> <code class="s">"false"</code>
<code class="lineno">23 </code>      <code class="calibre19">nginx.ingress.kubernetes.io/ssl-redirect</code><code class="calibre19">:</code> <code class="s">"false"</code>
<code class="lineno">24 </code>    <code class="calibre19">hosts</code><code class="calibre19">:</code>
<code class="lineno">25 </code>      <code class="calibre19">cm.acme-escaped.com</code><code class="calibre19">:</code> <code class="c"># Change me!</code>
<code class="lineno">26 </code>      <code class="calibre19">-</code> <code class="calibre19">/</code>
<code class="lineno">27 </code>
<code class="lineno">28 </code><code class="calibre19">jenkins</code><code class="calibre19">:</code>
<code class="lineno">29 </code>  <code class="calibre19">Master</code><code class="calibre19">:</code>
<code class="lineno">30 </code>    <code class="calibre19">ImageTag</code><code class="calibre19">:</code> <code class="s">"2.129-alpine"</code>
<code class="lineno">31 </code>    <code class="calibre19">Cpu</code><code class="calibre19">:</code> <code class="s">"500m"</code>
<code class="lineno">32 </code>    <code class="calibre19">Memory</code><code class="calibre19">:</code> <code class="s">"500Mi"</code>
<code class="lineno">33 </code>    <code class="calibre19">ServiceType</code><code class="calibre19">:</code> <code class="calibre19">ClusterIP</code>
<code class="lineno">34 </code>    <code class="calibre19">ServiceAnnotations</code><code class="calibre19">:</code>
<code class="lineno">35 </code>      <code class="calibre19">service.beta.kubernetes.io/aws-load-balancer-backend-protocol</code><code class="calibre19">:</code> <code class="calibre19">http</code>
<code class="lineno">36 </code>    <code class="calibre19">GlobalLibraries</code><code class="calibre19">:</code> <code class="calibre19">true</code>
<code class="lineno">37 </code>    <code class="calibre19">InstallPlugins</code><code class="calibre19">:</code>
<code class="lineno">38 </code>    <code class="calibre19">-</code> <code class="calibre19">durable-task:1.22</code>
<code class="lineno">39 </code>    <code class="calibre19">-</code> <code class="calibre19">blueocean:1.7.1</code>
<code class="lineno">40 </code>    <code class="calibre19">-</code> <code class="calibre19">credentials:2.1.18</code>
<code class="lineno">41 </code>    <code class="calibre19">-</code> <code class="calibre19">ec2:1.39</code>
<code class="lineno">42 </code>    <code class="calibre19">-</code> <code class="calibre19">git:3.9.1</code>
<code class="lineno">43 </code>    <code class="calibre19">-</code> <code class="calibre19">git-client:2.7.3</code>
<code class="lineno">44 </code>    <code class="calibre19">-</code> <code class="calibre19">github:1.29.2</code>
<code class="lineno">45 </code>    <code class="calibre19">-</code> <code class="calibre19">kubernetes:1.12.0</code>
<code class="lineno">46 </code>    <code class="calibre19">-</code> <code class="calibre19">pipeline-utility-steps:2.1.0</code>
<code class="lineno">47 </code>    <code class="calibre19">-</code> <code class="calibre19">pipeline-model-definition:1.3.1</code>
<code class="lineno">48 </code>    <code class="calibre19">-</code> <code class="calibre19">script-security:1.44</code>
<code class="lineno">49 </code>    <code class="calibre19">-</code> <code class="calibre19">slack:2.3</code>
<code class="lineno">50 </code>    <code class="calibre19">-</code> <code class="calibre19">thinBackup:1.9</code>
<code class="lineno">51 </code>    <code class="calibre19">-</code> <code class="calibre19">workflow-aggregator:2.5</code>
<code class="lineno">52 </code>    <code class="calibre19">-</code> <code class="calibre19">ssh-slaves:1.26</code>
<code class="lineno">53 </code>    <code class="calibre19">-</code> <code class="calibre19">ssh-agent:1.15</code>
<code class="lineno">54 </code>    <code class="calibre19">-</code> <code class="calibre19">jdk-tool:1.1</code>
<code class="lineno">55 </code>    <code class="calibre19">-</code> <code class="calibre19">command-launcher:1.2</code>
<code class="lineno">56 </code>    <code class="calibre19">-</code> <code class="calibre19">github-oauth:0.29</code>
<code class="lineno">57 </code>    <code class="calibre19">-</code> <code class="calibre19">google-compute-engine:1.0.4</code>
<code class="lineno">58 </code>    <code class="calibre19">-</code> <code class="calibre19">pegdown-formatter:1.3</code>
<code class="lineno">59 </code>    <code class="calibre19">Ingress</code><code class="calibre19">:</code>
<code class="lineno">60 </code>      <code class="calibre19">Annotations</code><code class="calibre19">:</code>
<code class="lineno">61 </code>        <code class="calibre19">kubernetes.io/ingress.class</code><code class="calibre19">:</code> <code class="s">"nginx"</code>
<code class="lineno">62 </code>        <code class="calibre19">nginx.ingress.kubernetes.io/ssl-redirect</code><code class="calibre19">:</code> <code class="s">"false"</code>
<code class="lineno">63 </code>        <code class="calibre19">nginx.ingress.kubernetes.io/proxy-body-size</code><code class="calibre19">:</code> <code class="calibre19">50m</code>
<code class="lineno">64 </code>        <code class="calibre19">nginx.ingress.kubernetes.io/proxy-request-buffering</code><code class="calibre19">:</code> <code class="s">"off"</code>
<code class="lineno">65 </code>        <code class="calibre19">ingress.kubernetes.io/ssl-redirect</code><code class="calibre19">:</code> <code class="s">"false"</code>
<code class="lineno">66 </code>        <code class="calibre19">ingress.kubernetes.io/proxy-body-size</code><code class="calibre19">:</code> <code class="calibre19">50m</code>
<code class="lineno">67 </code>        <code class="calibre19">ingress.kubernetes.io/proxy-request-buffering</code><code class="calibre19">:</code> <code class="s">"off"</code>
<code class="lineno">68 </code>    <code class="calibre19">HostName</code><code class="calibre19">:</code> <code class="calibre19">jenkins.acme.com</code> <code class="c"># Change me!</code>
<code class="lineno">69 </code>    <code class="calibre19">CustomConfigMap</code><code class="calibre19">:</code> <code class="calibre19">true</code>
<code class="lineno">70 </code>    <code class="calibre19">CredentialsXmlSecret</code><code class="calibre19">:</code> <code class="calibre19">jenkins-credentials</code>
<code class="lineno">71 </code>    <code class="calibre19">SecretsFilesSecret</code><code class="calibre19">:</code> <code class="calibre19">jenkins-secrets</code>
<code class="lineno">72 </code>    <code class="calibre19">DockerVM</code><code class="calibre19">:</code> <code class="calibre19">false</code>
<code class="lineno">73 </code>  <code class="calibre19">rbac</code><code class="calibre19">:</code>
<code class="lineno">74 </code>    <code class="calibre19">install</code><code class="calibre19">:</code> <code class="calibre19">true</code>
</pre></div>

</figure>

<p class="calibre3">We can see that the values are split into two groups; <code class="calibre19">chartmuseum</code> and <code class="calibre19">jenkins</code>. Other than that, they are almost the same as the values we used in the previous chapters. The only important difference is that both are now defined in the same file and that they will be used as values for the requirements.</p>

<aside class="information">
    <p class="calibre3">I hope that you noticed that the file is named <code class="calibre19">values-orig.yaml</code> instead of <code class="calibre19">values.yaml</code>. I could not predict in advance what will be the address through which you can access the cluster. We’ll combine that file with a bit of <code class="calibre19">sed</code> magic to generate <code class="calibre19">values.yaml</code> that contains the correct address.</p>

</aside>

<p class="calibre3">Next, we’ll take a look at the templates of this Chart.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ls -1 helm/templates
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>config.tpl
<code class="lineno">2 </code>ns.yaml
</pre></div>

</figure>

<p class="calibre3">The <code class="calibre19">config.tpl</code> file is the same Jenkins configuration template we used before, so there should be no need explaining it. We’ll skip it and jump straight into <code class="calibre19">ns.yaml</code>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>cat helm/templates/ns.yaml
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">apiVersion</code><code class="calibre19">:</code> <code class="calibre19">v1</code>
<code class="lineno"> 2 </code><code class="calibre19">kind</code><code class="calibre19">:</code> <code class="calibre19">ServiceAccount</code>
<code class="lineno"> 3 </code><code class="calibre19">metadata</code><code class="calibre19">:</code>
<code class="lineno"> 4 </code>  <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">build</code>
<code class="lineno"> 5 </code>
<code class="lineno"> 6 </code><code class="nn">---</code>
<code class="lineno"> 7 </code>
<code class="lineno"> 8 </code><code class="calibre19">apiVersion</code><code class="calibre19">:</code> <code class="calibre19">rbac.authorization.k8s.io/v1beta1</code>
<code class="lineno"> 9 </code><code class="calibre19">kind</code><code class="calibre19">:</code> <code class="calibre19">RoleBinding</code>
<code class="lineno">10 </code><code class="calibre19">metadata</code><code class="calibre19">:</code>
<code class="lineno">11 </code>  <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">build</code>
<code class="lineno">12 </code><code class="calibre19">roleRef</code><code class="calibre19">:</code>
<code class="lineno">13 </code>  <code class="calibre19">apiGroup</code><code class="calibre19">:</code> <code class="calibre19">rbac.authorization.k8s.io</code>
<code class="lineno">14 </code>  <code class="calibre19">kind</code><code class="calibre19">:</code> <code class="calibre19">ClusterRole</code>
<code class="lineno">15 </code>  <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">admin</code>
<code class="lineno">16 </code><code class="calibre19">subjects</code><code class="calibre19">:</code>
<code class="lineno">17 </code><code class="calibre19">-</code> <code class="calibre19">kind</code><code class="calibre19">:</code> <code class="calibre19">ServiceAccount</code>
<code class="lineno">18 </code>  <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">build</code>
<code class="lineno">19 </code>
<code class="lineno">20 </code><code class="nn">---</code>
<code class="lineno">21 </code>
<code class="lineno">22 </code><code class="calibre19">apiVersion</code><code class="calibre19">:</code> <code class="calibre19">rbac.authorization.k8s.io/v1beta1</code>
<code class="lineno">23 </code><code class="calibre19">kind</code><code class="calibre19">:</code> <code class="calibre19">RoleBinding</code>
<code class="lineno">24 </code><code class="calibre19">metadata</code><code class="calibre19">:</code>
<code class="lineno">25 </code>  <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">build</code>
<code class="lineno">26 </code>  <code class="calibre19">namespace</code><code class="calibre19">:</code> <code class="calibre19">kube-system</code>
<code class="lineno">27 </code><code class="calibre19">roleRef</code><code class="calibre19">:</code>
<code class="lineno">28 </code>  <code class="calibre19">apiGroup</code><code class="calibre19">:</code> <code class="calibre19">rbac.authorization.k8s.io</code>
<code class="lineno">29 </code>  <code class="calibre19">kind</code><code class="calibre19">:</code> <code class="calibre19">ClusterRole</code>
<code class="lineno">30 </code>  <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">admin</code>
<code class="lineno">31 </code><code class="calibre19">subjects</code><code class="calibre19">:</code>
<code class="lineno">32 </code><code class="calibre19">-</code> <code class="calibre19">kind</code><code class="calibre19">:</code> <code class="calibre19">ServiceAccount</code>
<code class="lineno">33 </code>  <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">build</code>
<code class="lineno">34 </code>  <code class="calibre19">namespace</code><code class="calibre19">:</code> <code class="calibre19">{{</code> <code class="nv">.Release.Namespace</code> <code class="calibre19">}}</code>
</pre></div>

</figure>

<p class="calibre3">That definition holds no mysteries. It is a very similar one to those we used before. The first two entries provide permissions Jenkins builds need for running in the same Namespace, while the third is meant to allow builds to interact with tiller running <code class="calibre19">kube-system</code>. You can see that through the <code class="calibre19">namespace</code> entry that is set to <code class="calibre19">kube-system</code>, and through the reference to the <code class="calibre19">ServiceAccount</code> in the Namespace where we’ll install this Chart.</p>

<p class="calibre3">All in all, this chart is a combination of custom templates meant to provide permissions and a set of requirements that will install the applications our production environment needs. For now, those requirements are only two applications (ChartMuseum and Jenkins), and we are likely going to expand it later with additional ones.</p>

<p class="calibre3">I already mentioned that <code class="calibre19">values-orig.yaml</code> is too generic and that we should update it with the cluster address before we convert it into <code class="calibre19">values.yaml</code>. That’s our next mission.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">ADDR</code><code class="o">=</code><code class="nv">$LB_IP</code>.nip.io
<code class="lineno">2 </code>
<code class="lineno">3 </code><code class="nb">echo</code> <code class="nv">$ADDR</code>
<code class="lineno">4 </code>
<code class="lineno">5 </code><code class="nv">ADDR_ESC</code><code class="o">=</code><code class="k">$(</code><code class="nb">echo</code> <code class="nv">$ADDR</code> <code class="se">\</code>
<code class="lineno">6 </code>    <code class="calibre19">|</code> sed -e <code class="s">"s@\.@\\\.@g"</code><code class="k">)</code>
<code class="lineno">7 </code>
<code class="lineno">8 </code><code class="nb">echo</code> <code class="nv">$ADDR_ESC</code>
</pre></div>

</figure>

<p class="calibre3">We defined the address of the cluster (<code class="calibre19">ADDR</code>) as well as the escaped variant required by ChartMuseum since it uses address as the key, not the value. As you already know from previous chapters, keys cannot contain “special” characters like dots (<code class="calibre19">.</code>).</p>

<p class="calibre3">Now that we have the address of your cluster, we can use <code class="calibre19">sed</code> to modify <code class="calibre19">values-orig.yaml</code> and output the result to <code class="calibre19">values.yaml</code>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>cat helm/values-orig.yaml <code class="se">\</code>
<code class="lineno">2 </code>    <code class="calibre19">|</code> sed -e <code class="s">"s@acme-escaped.com@</code><code class="nv">$ADDR_ESC</code><code class="s">@g"</code> <code class="se">\</code>
<code class="lineno">3 </code>    <code class="calibre19">|</code> sed -e <code class="s">"s@acme.com@</code><code class="nv">$ADDR</code><code class="s">@g"</code> <code class="se">\</code>
<code class="lineno">4 </code>    <code class="calibre19">|</code> tee helm/values.yaml
</pre></div>

</figure>

<p class="calibre3">Later on, we’ll use Jenkins to install (or upgrade) the Chart, so we should push the changes to GitHub.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>git add .
<code class="lineno">2 </code>
<code class="lineno">3 </code>git commit -m <code class="s">"Address"</code>
<code class="lineno">4 </code>
<code class="lineno">5 </code>git push
</pre></div>

</figure>

<p class="calibre3">All Helm dependencies need to be downloaded to the <code class="calibre19">charts</code> directory before they are installed. We’ll do that through the <code class="calibre19">helm dependency update</code> command.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>helm dependency update helm
</pre></div>

</figure>

<p class="calibre3">The relevant part of the output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>...
<code class="lineno">2 </code>Saving 2 charts
<code class="lineno">3 </code>Downloading chartmuseum from repo https://kubernetes-charts.storage.googleapis.com
<code class="lineno">4 </code>Downloading jenkins from repo https://kubernetes-charts.storage.googleapis.com
<code class="lineno">5 </code>Deleting outdated charts
</pre></div>

</figure>

<p class="calibre3">Don’t worry if some of the repositories are not reachable. You might see messages stating that Helm was <code class="calibre19">unable to get an update</code> from <code class="calibre19">local</code> or <code class="calibre19">chartmuseum</code> repositories. Local Helm configuration probably has those (and maybe other) references from previous exercises.</p>

<p class="calibre3">The last lines of the output are essential. We can see that Helm saved two Charts (<code class="calibre19">chartmuseum</code> and <code class="calibre19">jenkins</code>). Those are the Charts we specified as dependencies in <code class="calibre19">requirements.yaml</code>.</p>

<p class="calibre3">We can confirm that the dependencies were indeed downloaded by listing the files in the <code class="calibre19">charts</code> directory.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ls -1 helm/charts
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>chartmuseum-1.6.0.tgz
<code class="lineno">2 </code>jenkins-0.16.6.tgz
</pre></div>

</figure>

<p class="calibre3">Now that the dependencies are downloaded and saved to the <code class="calibre19">charts</code> directory, we can proceed and install our full production environment. It consists of only two applications. We’ll increase that number soon, and I expect that you’ll add other applications you need to your “real” environment if you choose to use this approach.</p>

<aside class="warning">
    <h3 id="leanpub-auto-a-note-to-minishift-users-32" class="calibre22">A note to minishift users</h3>

  <p class="calibre3">Helm will try to install Jenkins dependency Chart with the process in a container running as user <code class="calibre19">0</code>. By default, that is not allowed in OpenShift. We’ll skip discussing the best approach to correct the issue, and I’ll assume you already know how to set the permissions on the per-Pod basis. Instead, we’ll do the most straightforward fix by executing the command that follows that will allow the creation of restricted Pods to run as any user.</p>

  <p class="calibre3"><code class="calibre19">oc patch scc restricted -p '{"runAsUser":{"type": "RunAsAny"}}'</code></p>

</aside>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>helm install helm <code class="se">\</code>
<code class="lineno">2 </code>    -n prod <code class="se">\</code>
<code class="lineno">3 </code>    --namespace prod
</pre></div>

</figure>

<p class="calibre3">The output, limited to the Pods, is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>...
<code class="lineno">2 </code>==&gt; v1/Pod(related)
<code class="lineno">3 </code>NAME                               READY  STATUS   RESTARTS  AGE
<code class="lineno">4 </code>prod-chartmuseum-68bc575fb7-jgs98  0/1    Pending  0         1s
<code class="lineno">5 </code>prod-jenkins-6dbc74554d-gbzp4      0/1    Pending  0         1s
<code class="lineno">6 </code>...
</pre></div>

</figure>

<p class="calibre3">We can see that Helm sent requests to Kube API to create all the resources defined in our Chart. As a result, among other resources, we got the Pods which run containers with Jenkins and ChartMuseum.</p>

<p class="calibre3">However, Jenkins will fail to start without the secrets we were using in previous chapters, so we’ll create them next.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kubectl -n prod <code class="se">\</code>
<code class="lineno">2 </code>    create secret generic <code class="se">\</code>
<code class="lineno">3 </code>    jenkins-credentials <code class="se">\</code>
<code class="lineno">4 </code>    --from-file ../k8s-specs/cluster/jenkins/credentials.xml
<code class="lineno">5 </code>
<code class="lineno">6 </code>kubectl -n prod <code class="se">\</code>
<code class="lineno">7 </code>    create secret generic <code class="se">\</code>
<code class="lineno">8 </code>    jenkins-secrets <code class="se">\</code>
<code class="lineno">9 </code>    --from-file ../k8s-specs/cluster/jenkins/secrets
</pre></div>

</figure>

<p class="calibre3">Let’s list the Charts running inside the cluster and thus confirm that <code class="calibre19">prod</code> was indeed deployed.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>helm ls
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>NAME REVISION UPDATED        STATUS   CHART          NAMESPACE
<code class="lineno">2 </code>prod 1        Tue Aug  7 ... DEPLOYED prod-env-0.0.1 prod
</pre></div>

</figure>

<p class="calibre3">Now that we saw that the Chart was installed, the only thing left is to confirm that the two applications are indeed running correctly. We won’t do real testing of the two applications, but only superficial ones that will give us a piece of mind. We’ll start with ChartMuseum.</p>

<p class="calibre3">First, we’ll wait for ChartMuseum to roll out (if it didn’t already).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kubectl -n prod <code class="se">\</code>
<code class="lineno">2 </code>    rollout status <code class="se">\</code>
<code class="lineno">3 </code>    deploy prod-chartmuseum
</pre></div>

</figure>

<p class="calibre3">The output should state that the <code class="calibre19">deployment "prod-chartmuseum"</code> was <code class="calibre19">successfully rolled out</code>.</p>

<aside class="warning">
    <h3 id="leanpub-auto-a-note-to-minishift-users-33" class="calibre22">A note to minishift users</h3>

  <p class="calibre3">OpenShift ignores Ingress resources so we’ll have to create a Route to accomplish the same effect. Please execute the command that follows.</p>

  <p class="calibre3"><code class="calibre19">oc -n prod create route edge --service prod-chartmuseum --hostname cm.$ADDR --insecure-policy Allow</code></p>

</aside>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>curl <code class="s">"http://cm.</code><code class="nv">$ADDR</code><code class="s">/health"</code>
</pre></div>

</figure>

<p class="calibre3">The output is <code class="calibre19">{"healthy":true}</code>, so ChartMuseum seems to be working correctly.</p>

<p class="calibre3">Next, we’ll turn our attention to Jenkins.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kubectl -n prod <code class="se">\</code>
<code class="lineno">2 </code>    rollout status <code class="se">\</code>
<code class="lineno">3 </code>    deploy prod-jenkins
</pre></div>

</figure>

<p class="calibre3">Once the <code class="calibre19">deployment "prod-jenkins"</code> is <code class="calibre19">successfully rolled out</code>, we can open it in a browser as a very light validation.</p>

<aside class="warning">
    <h3 id="leanpub-auto-a-note-to-minishift-users-34" class="calibre22">A note to minishift users</h3>

  <p class="calibre3">OpenShift requires Routes to make services accessible outside the cluster. To make things more complicated, they are not part of “standard Kubernetes” so we’ll need to create one using <code class="calibre19">oc</code>. Please execute the command that follows.</p>

  <p class="calibre3"><code class="calibre19">oc -n prod create route edge --service prod-jenkins --insecure-policy Allow --hostname jenkins.$ADDR</code></p>

  <p class="calibre3">That command created an <code class="calibre19">edge</code> Router tied to the <code class="calibre19">prod-jenkins</code> Service. Since we do not have SSL certificates for HTTPS communication, we also specified that it is OK to use insecure policy which will allow us to access Jenkins through plain HTTP. The last argument defined the address through which we’d like to access Jenkins UI.</p>

</aside>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">JENKINS_ADDR</code><code class="o">=</code><code class="s">"jenkins.</code><code class="nv">$ADDR</code><code class="s">"</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">"</code>
</pre></div>

</figure>

<p class="calibre3">We’ll need the initial admin password to log in. Just as we did it countless times before, we’ll fetch it from the <code class="calibre19">secret</code> generated through the Chart.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">JENKINS_PASS</code><code class="o">=</code><code class="k">$(</code>kubectl -n prod <code class="se">\</code>
<code class="lineno">2 </code>    get secret prod-jenkins <code class="se">\</code>
<code class="lineno">3 </code>    -o <code class="nv">jsonpath</code><code class="o">=</code><code class="s">"{.data.jenkins-admin-password}"</code> <code class="se">\</code>
<code class="lineno">4 </code>    <code class="calibre19">|</code> base64 --decode<code class="calibre19">;</code> <code class="nb">echo</code><code class="k">)</code>
<code class="lineno">5 </code>
<code class="lineno">6 </code><code class="nb">echo</code> <code class="nv">$JENKINS_PASS</code>
</pre></div>

</figure>

<p class="calibre3">Please go back to Jenkins UI in your favorite browser and log in using <em class="calibre17">admin</em> as the username and the output of <code class="calibre19">JENKINS_PASS</code> as the password. If, later on, your Jenkins session expires and you need to log in again, all you have to do is output <code class="calibre19">JENKINS_PASS</code> variable to find the password.</p>

<p class="calibre3">Now that we have the base production environment, we can turn our attention towards defining a continuous delivery pipeline.</p>

<h3 id="leanpub-auto-what-is-the-continuous-delivery-pipeline" class="calibre20">What Is The Continuous Delivery Pipeline?</h3>

<p class="calibre3">Now that we have a cluster and the third-party applications running in the production environment, we can turn our attention towards defining a continuous delivery pipeline.</p>

<p class="calibre3">Before we proceed, we’ll recap the definitions of continuous deployment and continuous delivery.</p>

<aside class="information">
    <p class="calibre3">Continuous deployment is a fully automated process that executes a set of steps with the goal of converting each commit to the master branch into a fully tested release deployed to production.</p>

</aside>

<aside class="information">
    <p class="calibre3">Continuous delivery is almost a fully automated process that executes a set of steps with the goal of converting each commit to the master branch into a fully tested release that is ready to be deployed to production. We (humans) retain the ability to choose which of the production-ready releases will be deployed to production and when is that deployment going to happen.</p>

</aside>

<p class="calibre3">When compared to continuous deployment, continuous delivery is split into two automated processes with a manual action in between. The first process ensures that a commit is built, tested, and converted into a release. The second is in charge of performing the actual deployment to production and executing a set of tests that validate that deployment.</p>

<p class="calibre3">In other words, the only significant difference between the two processes is that continuous delivery has a manual action that allows us to choose whether we want to proceed with the deployment to production. That choice is not based on technical knowledge since we already validated that a release is production ready. Instead, it is a business or a marketing decision what to deliver to our users and when should that happen.</p>

<p class="calibre3">Since this is not the first time we are discussing continuous deployment and continuous delivery, there’s probably no need to dive deeper into the processes. Instead, we’ll jump straight into one possible implementation of a continuous delivery pipeline.</p>

<p class="calibre3">If we compare the process that follows with the one from the previous chapter, some of the steps will be different. That is not to say that those described here are not well suited in a continuous deployment pipeline. Quite the contrary. The steps are interchangeable. My primary goal is not only to present a possible implementation of a continuous delivery pipeline but also to showcase a different approach that, with small adjustments, can be applied to any type of pipeline.</p>

<h3 id="leanpub-auto-exploring-applications-repository-and-preparing-the-environment" class="calibre20">Exploring Application’s Repository And Preparing The Environment</h3>

<p class="calibre3">Before I wrote this chapter, I forked the <a href="https://github.com/vfarcic/go-demo-3">vfarcic/go-demo-3</a> repository into <a href="https://github.com/vfarcic/go-demo-5">vfarcic/go-demo-5</a>. Even though most the code of the application is still the same, I thought it would be easier to apply and demonstrate the changes in a new repository instead of creating a new branch or doing some other workaround that would allow us to have both processes in the same repository. All in all, <em class="calibre17">go-demo-5</em> is a copy of <em class="calibre17">go-demo-3</em> on top of which I made some changes which we’ll comment soon.</p>

<p class="calibre3">Since we’ll need to change a few configuration files and push them back to the repository, you should fork <a href="https://github.com/vfarcic/go-demo-5">vfarcic/go-demo-5</a>, just as you forked <a href="https://github.com/vfarcic/k8s-prod">vfarcic/k8s-prod</a>.</p>

<p class="calibre3">Next, we’ll clone the repository before we explore the relevant files.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">cd</code> ..
<code class="lineno">2 </code>
<code class="lineno">3 </code>git clone <code class="se">\</code>
<code class="lineno">4 </code>    https://github.com/<code class="nv">$GH_USER</code>/go-demo-5.git
<code class="lineno">5 </code>
<code class="lineno">6 </code><code class="nb">cd</code> go-demo-5
</pre></div>

</figure>

<p class="calibre3">The Chart located in <code class="calibre19">helm</code> directory is the same as the one we used in <em class="calibre17">go-demo-3</em> so we’ll skip commenting it. Instead, we’ll replace my GitHub user (<code class="calibre19">vfarcic</code>) with yours.</p>

<p class="calibre3">Before you execute the commands that follow, make sure you replace <code class="calibre19">[...]</code> with your Docker Hub user.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">DH_USER</code><code class="o">=[</code>...<code class="o">]</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>cat helm/go-demo-5/deployment-orig.yaml <code class="se">\</code>
<code class="lineno">4 </code>    <code class="calibre19">|</code> sed -e <code class="s">"s@vfarcic@</code><code class="nv">$DH_USER</code><code class="s">@g"</code> <code class="se">\</code>
<code class="lineno">5 </code>    <code class="calibre19">|</code> tee helm/go-demo-5/templates/deployment.yaml
</pre></div>

</figure>

<p class="calibre3">In <em class="calibre17">go-demo-3</em>, the resources that define the Namespace, ServiceAccount, RoleBinding, LimitRange, and ResourceQuota were split between <code class="calibre19">ns.yml</code> and <code class="calibre19">build-config.yml</code> files. I got tired of having them separated, so I joined them into a single file <code class="calibre19">build.yml</code>. Other than that, the resources are the same as those we used before so we’ll skip commenting on them as well. The only difference is that the Namespace is now <em class="calibre17">go-demo-5</em>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kubectl apply -f k8s/build.yml --record
</pre></div>

</figure>

<p class="calibre3">Finally, the only thing related to the setup of the environment we’ll use for <em class="calibre17">go-demo-5</em> is to install Tiller, just as we did before.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>helm init --service-account build <code class="se">\</code>
<code class="lineno">2 </code>    --tiller-namespace go-demo-5-build
</pre></div>

</figure>

<p class="calibre3">The two key elements of our pipeline will be <em class="calibre17">Dockerfile</em> and <em class="calibre17">Jenkinsfile</em> files. Let’s explore the former first.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>cat Dockerfile
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>FROM alpine:3.4
<code class="lineno"> 2 </code>MAINTAINER 	Viktor Farcic &lt;viktor@farcic.com&gt;
<code class="lineno"> 3 </code>
<code class="lineno"> 4 </code>RUN mkdir /lib64 &amp;&amp; ln -s /lib/libc.musl-x86_64.so.1 /lib64/ld-linux-x86-64.so.2
<code class="lineno"> 5 </code>
<code class="lineno"> 6 </code>EXPOSE 8080
<code class="lineno"> 7 </code>ENV DB db
<code class="lineno"> 8 </code>CMD ["go-demo"]
<code class="lineno"> 9 </code>
<code class="lineno">10 </code>COPY go-demo /usr/local/bin/go-demo
<code class="lineno">11 </code>RUN chmod +x /usr/local/bin/go-demo
</pre></div>

</figure>

<p class="calibre3">You’ll notice that we are not using multi-stage builds. That makes me sad since I think that is one of the greatest additions to Docker’s build process. The ability to run unit tests and build a binary served us well so far. The process was streamlined through a single <code class="calibre19">docker image build</code> command, it was documented in a single <em class="calibre17">Dockerfile</em> file, and we did not have to sacrifice the size of the final image. So, why did I choose not to use it now?</p>

<p class="calibre3">We’ll switch from building Docker images in a separate VM outside the cluster to using Docker socket to build it in one of the Kubernetes worker nodes. That does reduce security (Docker on that node could be abducted), and it can cause potential problems with Kubernetes (we’re using containers without its knowledge). Yet, using the socket is somewhat easier, cleaner, and faster. Even though we explored this option through Shell commands, we did not use it in our Jenkins pipelines. So, I thought that you should experience both ways of building images in a Jenkins pipeline and choose for yourself which method fits your use-case better. The goal is to find the balance and gain experience that will let you decide what works best for you. There will be quite a few other changes further on. They all aim at giving you better insight into different ways of accomplishing the same goals. You will have to make a choice on how to combine them into the solution that works the best in your organization.</p>

<p class="calibre3">Going back to the reason for NOT using Docker’s multi-stage builds… Given that we’re about to use Docker in one of the worker nodes of the cluster, we depend on Docker version running inside that cluster. At the time of this writing (August 2018), some Kubernetes clusters still use more than a year old Docker. If my memory serves me, multi-stage builds were added in Docker <em class="calibre17">17.05</em>, and some Kubernetes flavors (even when on the latest version), still use Docker <em class="calibre17">17.03</em> or even older. Kops is a good example, even though it is not the only one. Release <em class="calibre17">1.9.x</em> (the latest stable at the time of this writing), uses Docker <em class="calibre17">17.03</em>. Since I’m committed to making all the examples in this book working in many different Kubernetes flavors, I had to remove multi-stage builds.</p>

<p class="calibre3">Check Docker version in your cluster and, if it’s <em class="calibre17">17.05</em> or newer, I’d greatly recommend you continue using multi-stage builds. They are too good of a feature to ignore it, if not necessary.</p>

<p class="calibre3">All in all, the <em class="calibre17">Dockerfile</em> assumes that we already executed our tests and that we built the binary. We’ll see how to do that inside a Jenkins pipeline soon.</p>

<p class="calibre3">We’ll explore the pipeline stored in Jenkinsfile in the repository we cloned. However, before we do that, we’ll go through declarative pipeline syntax since that’s the one we’ll use in this chapter.</p>

<h3 id="leanpub-auto-switching-from-scripted-to-declarative-pipeline" class="calibre20">Switching From Scripted To Declarative Pipeline</h3>

<p class="calibre3"><em class="calibre17">A long time ago in a galaxy far, far away, a group of Jenkins contributors decided to reinvent the way Jenkins jobs are defined and how they operate.</em> (A couple of years in software terms is a lot, and Jenkins contributors are indeed spread throughout the galaxy).</p>

<p class="calibre3">The new type of jobs became known as Jenkins pipeline. It was received well by the community, and the adoption started almost instantly. Everything was excellent, and the benefits of using Pipeline compared to FreeStyle jobs were evident from the start. However, it wasn’t easy for everyone to adopt Pipeline. Those who were used to scripting, and especially those familiar with Groovy, had no difficulties to switch. But, many used Jenkins without being coders. They did not find Pipeline to be as easy as we thought it would be. While I do believe that there is no place in the software industry for those who do not know how to code, it was still evident that something needed to be done to simplify Pipeline syntax even more. So, a new flavor of Pipeline syntax was born. We renamed the existing Pipeline flavor to Scripted Pipeline and created a new one called Declarative Pipeline.</p>

<p class="calibre3">Declarative Pipeline forces more simplified and more opinionated syntax. Its goal is to provide an easier way to define pipelines, to make them more readable, and to lower the entry bar. You can think of the Scripted Pipeline being initially aimed at power users and Declarative Pipeline for everyone else. In the meantime, Declarative Pipeline started getting more and more attention, and today such a separation is not necessarily valid anymore. In some ways, Declarative Pipeline is more advanced, and it is recommended for all users except when one needs something that cannot be (easily) done without switching to Scripted.</p>

<aside class="information">
    <p class="calibre3">The recommendation is to always start with Declarative Pipeline and switch to Scripted only if you need to accomplish something that is not currently supported. Even then, you might be trying to do something you shouldn’t.</p>

</aside>

<p class="calibre3">Right now, you might be asking yourself something along the following lines. “Why did Viktor make us use Scripted Pipeline if Declarative is better?” The previous pipeline required two features that are not yet supported by Declarative. We wanted to use <code class="calibre19">podTemplate</code> for most of the process with an occasional jump into agents based on VMs for building Docker images. That is not yet supported with Declarative Pipeline. However, since we will now switch to using Docker socket to build images inside the nodes of the cluster, that is not an issue anymore. The second reason lies in the inability to define Namespace inside <code class="calibre19">podTemplate</code>. That also is not an issue anymore since we’ll switch to the model of defining a separate Kubernetes cloud for each Namespace where builds should run. You’ll see both changes in action soon when we start exploring the continuous delivery pipeline used for <em class="calibre17">go-demo-5</em>.</p>

<p class="calibre3">Before we jump into defining the pipeline for the <em class="calibre17">go-demo-5</em> application, we’ll briefly explore the general structure of a Declarative pipeline.</p>

<p class="calibre3">The snippet that follows represents a skeleton of a Declarative pipeline.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">pipeline</code> <code class="o">{</code>
<code class="lineno"> 2 </code>  <code class="calibre19">agent</code> <code class="o">{</code>
<code class="lineno"> 3 </code>    <code class="o">...</code>
<code class="lineno"> 4 </code>  <code class="o">}</code>
<code class="lineno"> 5 </code>  <code class="calibre19">environment</code> <code class="o">{</code>
<code class="lineno"> 6 </code>    <code class="o">...</code>
<code class="lineno"> 7 </code>  <code class="o">}</code>
<code class="lineno"> 8 </code>  <code class="calibre19">options</code> <code class="o">{</code>
<code class="lineno"> 9 </code>    <code class="o">...</code>
<code class="lineno">10 </code>  <code class="o">}</code>
<code class="lineno">11 </code>  <code class="calibre19">parameters</code> <code class="o">{</code>
<code class="lineno">12 </code>    <code class="o">...</code>
<code class="lineno">13 </code>  <code class="o">}</code>
<code class="lineno">14 </code>  <code class="calibre19">triggers</code> <code class="o">{</code>
<code class="lineno">15 </code>    <code class="o">...</code>
<code class="lineno">16 </code>  <code class="o">}</code>
<code class="lineno">17 </code>  <code class="calibre19">tools</code> <code class="o">{</code>
<code class="lineno">18 </code>    <code class="o">...</code>
<code class="lineno">19 </code>  <code class="o">}</code>
<code class="lineno">20 </code>  <code class="calibre19">stages</code> <code class="o">{</code>
<code class="lineno">21 </code>    <code class="o">...</code>
<code class="lineno">22 </code>  <code class="o">}</code>
<code class="lineno">23 </code>  <code class="calibre19">post</code> <code class="o">{</code>
<code class="lineno">24 </code>    <code class="o">...</code>
<code class="lineno">25 </code>  <code class="o">}</code>
<code class="lineno">26 </code><code class="o">}</code>
</pre></div>

</figure>

<p class="calibre3">A Declarative Pipeline is always enclosed in a <code class="calibre19">pipeline</code> block. That allows Jenkins to distinguish the Declarative from the Scripted flavor. Inside it are different sections, each with a specific purpose.</p>

<p class="calibre3">The <code class="calibre19">agent</code> section specifies where the entire Pipeline, or a specific stage, will execute in the Jenkins environment depending on where the agent section is placed. The section must be defined at the top-level inside the pipeline block, but stage-level usage is optional. We can define different types of agents inside this block. In our case, we’ll use <code class="calibre19">kubernetes</code> type which translates to <code class="calibre19">podTemplate</code> we used before. The <code class="calibre19">agent</code> section is mandatory.</p>

<p class="calibre3">The <code class="calibre19">post</code> section defines one or more additional steps that are run upon the completion of a Pipeline’s or stage’s run (depending on the location of the <code class="calibre19">post</code> section within the Pipeline). It supports any of the following post-condition blocks: <code class="calibre19">always</code>, <code class="calibre19">changed</code>, <code class="calibre19">fixed</code>, <code class="calibre19">regression</code>, <code class="calibre19">aborted</code>, <code class="calibre19">failure</code>, <code class="calibre19">success</code>, <code class="calibre19">unstable</code>, and <code class="calibre19">cleanup</code>. These condition blocks allow the execution of steps inside each condition depending on the completion status of the Pipeline or stage.</p>

<p class="calibre3">The <code class="calibre19">stages</code> block is where most of the action is happening. It contains a sequence of one or more <code class="calibre19">stage</code> directives inside of which are the <code class="calibre19">steps</code> which constitute the bulk of our pipeline. The <code class="calibre19">stages</code> section is mandatory.</p>

<p class="calibre3">The <code class="calibre19">environment</code> directive specifies a sequence of key-value pairs which will be defined as environment variables for the all steps, or stage-specific steps, depending on where the environment directive is located within the Pipeline. This directive supports a special helper method <code class="calibre19">credentials()</code> which can be used to access pre-defined Credentials by their identifier in the Jenkins environment.</p>

<p class="calibre3">The <code class="calibre19">options</code> directive allows configuring Pipeline-specific options from within the Pipeline itself. Pipeline provides a number of these options, such as <code class="calibre19">buildDiscarder</code>, but they may also be provided by plugins, such as <code class="calibre19">timestamps</code>.</p>

<p class="calibre3">The <code class="calibre19">parameters</code> directive provides a list of parameters which a user should provide when triggering the Pipeline. The values for these user-specified parameters are made available to Pipeline steps via the params object.</p>

<p class="calibre3">The <code class="calibre19">triggers</code> directive defines automated ways in which the Pipeline should be re-triggered. In most cases, we should trigger a build through a Webhook. In such situations, <code class="calibre19">triggers</code> block does not provide any value.</p>

<p class="calibre3">Finally, the last section is <code class="calibre19">tools</code>. It allows us to define tools to auto-install and put on the <code class="calibre19">PATH</code>. Since we’re using containers, <code class="calibre19">tools</code> are pointless. The tools we need are already defined as container images and accessible through containers of the build Pod. Even if we’d use a VM for parts of our pipeline, like in the previous chapter, we should still bake the tools we need inside VM images instead of wasting our time installing them at runtime.</p>

<p class="calibre3">You can find much more info about the declarative pipeline in <a href="https://jenkins.io/doc/book/pipeline/syntax/">Pipeline Syntax</a> page. As a matter of fact, parts of the descriptions you just read are from that page.</p>

<p class="calibre3">You probably got bored to death with the previous explanations. If you didn’t, the chances are that they were insufficient. We’ll fix that by going through an example that will much better illustrate how Declarative Pipeline works. We’ll use most of those blocks in the example that follows. The exceptions are <code class="calibre19">parameters</code> (we don’t have a good use case for them), <code class="calibre19">triggers</code> (useless when we’re using Webhooks), and <code class="calibre19">tools</code> (a silly feature in the era of containers and tools for building VM images). Once we’re finished exploring the pipeline of the <em class="calibre17">go-demo-5</em> project, you’ll have enough experience to get you started with your own Declarative Pipelines, if you choose to use them.</p>

<h3 id="leanpub-auto-demystifying-declarative-pipeline-through-a-practical-example" class="calibre20">Demystifying Declarative Pipeline Through A Practical Example</h3>

<p class="calibre3">Let’s take a look at a <em class="calibre17">Jenkinsfile.orig</em> which we’ll use as a base to generate <em class="calibre17">Jenkinsfile</em> that will contain the correct address of the cluster and the GitHub user.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>cat Jenkinsfile.orig
</pre></div>

</figure>

<p class="calibre3">The output is too big for us to explore it in one go, so we’ll comment on each section separately. The first in line is the <code class="calibre19">options</code> block.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="o">...</code>
<code class="lineno">2 </code><code class="calibre19">options</code> <code class="o">{</code>
<code class="lineno">3 </code>  <code class="calibre19">buildDiscarder</code> <code class="nf">logRotator</code><code class="o">(</code><code class="nl">numToKeepStr:</code> <code class="s">'5'</code><code class="o">)</code>
<code class="lineno">4 </code>  <code class="calibre19">disableConcurrentBuilds</code><code class="o">()</code>
<code class="lineno">5 </code><code class="o">}</code>
<code class="lineno">6 </code><code class="o">...</code>
</pre></div>

</figure>

<p class="calibre3">The first option will result in only the last five builds being preserved in history. Most of the time there is no reason for us to keep all the builds we ever made. The last successful build of a branch is often the only one that matters. We set them to five just to prove that I’m not cheap. By discarding the old builds, we’re ensuring that Jenkins will perform faster. Please note that the last successful build is kept even if, in this case, more than five last builds failed.</p>

<p class="calibre3">The second option disables concurrent builds. Each branch will have a separate job (just as in the previous chapter). If commits to different branches happen close to each other, Jenkins will process them in parallel by running builds for corresponding jobs. However, there is often no need for us to run multiple builds of the same job (branch) at the same time. In some cases, that can even produce adverse effects. With <code class="calibre19">disableConcurrentBuilds</code>, if we ever make multiple commits rapidly, they will be queued and executed sequentially.</p>

<p class="calibre3">It’s up to you to decide whether those options are useful. If they are, use them. If they aren’t, discard them. My mission was to show you a few of the many <code class="calibre19">options</code> we can use.</p>

<p class="calibre3">The next block is <code class="calibre19">agent</code>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="o">...</code>
<code class="lineno"> 2 </code><code class="calibre19">agent</code> <code class="o">{</code>
<code class="lineno"> 3 </code>  <code class="calibre19">kubernetes</code> <code class="o">{</code>
<code class="lineno"> 4 </code>    <code class="calibre19">cloud</code> <code class="s">"go-demo-5-build"</code>
<code class="lineno"> 5 </code>    <code class="calibre19">label</code> <code class="s">"go-demo-5-build"</code>
<code class="lineno"> 6 </code>    <code class="calibre19">serviceAccount</code> <code class="s">"build"</code>
<code class="lineno"> 7 </code>    <code class="calibre19">yamlFile</code> <code class="s">"KubernetesPod.yaml"</code>
<code class="lineno"> 8 </code>  <code class="o">}</code>      
<code class="lineno"> 9 </code><code class="o">}</code>
<code class="lineno">10 </code><code class="o">...</code>
</pre></div>

</figure>

<p class="calibre3">In our case, the <code class="calibre19">agent</code> block contains a <code class="calibre19">kubernetes</code> block. That is an indication that the pipeline should create a Pod based on Kubernetes Cloud configuration. That is further refined with the <code class="calibre19">cloud</code> entry which specifies that it must be the cloud config named <code class="calibre19">go-demo-5-build</code>. We’ll create that cloud later. For now, we’ll have to assume that it’ll exist.</p>

<p class="calibre3">The benefit of that approach is that we can define only part of the agent information outside Pipeline and help other teams worry less about the things they need to put into their Jenkinsfile. As an example, you will not see a mention of a Namespace where the build should create a Pod that acts as a Jenkins agent. That will be defined elsewhere, and every build that uses <code class="calibre19">go-demo-5-build</code> will be run in that same Namespace.</p>

<p class="calibre3">There is another, less apparent reason for using a <code class="calibre19">cloud</code> dedicated to the builds in <code class="calibre19">go-demo-5-build</code> Namespace. Declarative syntax does not allow us to specify Namespace. So, we’ll have to have as many <code class="calibre19">cloud</code> configurations as there are Namespaces, or more.</p>

<p class="calibre3">The <code class="calibre19">label</code> defines the prefix that will be used to name the Pods that will be spin by the builds based on this pipeline.</p>

<p class="calibre3">Next, we’re defining <code class="calibre19">serviceAccount</code> as <code class="calibre19">build</code>. We already created that ServiceAccount inside the <em class="calibre17">go-demo-5-build</em> Namespace when we applied the configuration from <em class="calibre17">build.yml</em>. Now we’re telling Jenkins that it should use it when creating Pod.</p>

<p class="calibre3">Finally, we changed the way we define a Pod that will act as Jenkins agent. Instead of embedding Pod definition inside <em class="calibre17">Jenkinsfile</em>, we’re using an external file defined as <em class="calibre17">yamlFile</em>. My opinion on that feature is still divided. Having a Pod definition in Jenkinsfile (as we did in the previous chapter) allows us to inspect everything related to the job from a single location. On the other hand, moving the Pod definition to <code class="calibre19">yamlFile</code> enable us to focus on the flow of the pipeline, and leave lengthy Pod definition outside. It’s up to you to choose which approach you like more. We’ll explore the content of the <code class="calibre19">KubernetesPod.yaml</code> a bit later.</p>

<p class="calibre3">The next section in Jenkinsfile.orig is <code class="calibre19">environment</code>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="o">...</code>
<code class="lineno">2 </code><code class="calibre19">environment</code> <code class="o">{</code>
<code class="lineno">3 </code>  <code class="calibre19">image</code> <code class="o">=</code> <code class="s">"vfarcic/go-demo-5"</code>
<code class="lineno">4 </code>  <code class="calibre19">project</code> <code class="o">=</code> <code class="s">"go-demo-5"</code>
<code class="lineno">5 </code>  <code class="calibre19">domain</code> <code class="o">=</code> <code class="s">"acme.com"</code>
<code class="lineno">6 </code>  <code class="calibre19">cmAddr</code> <code class="o">=</code> <code class="s">"cm.acme.com"</code>
<code class="lineno">7 </code><code class="o">}</code>
<code class="lineno">8 </code><code class="o">...</code>
</pre></div>

</figure>

<p class="calibre3">The <code class="calibre19">environment</code> block defines a few variables that we’ll use in our steps. They are similar to those we used before, and they should be self-explanatory. Later on, we’ll have to change <code class="calibre19">vfarcic</code> to your Docker Hub user and <code class="calibre19">acme.com</code> to the address of your cluster.</p>

<p class="calibre3">You should note that Declarative Pipeline allows us to use the variables defined in <code class="calibre19">environment</code> block both as “normal” (e.g., <code class="calibre19">${VARIABLE_NAME}</code>) and as environment variables <code class="calibre19">${env.VARIABLE_NAME}</code>.</p>

<p class="calibre3">Now we reached the “meat” of the pipeline. The <code class="calibre19">stages</code> block contains three <code class="calibre19">stage</code> sub-blocks, with <code class="calibre19">steps</code> inside each.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="o">...</code>
<code class="lineno"> 2 </code><code class="calibre19">stages</code> <code class="o">{</code>
<code class="lineno"> 3 </code>  <code class="calibre19">stage</code><code class="o">(</code><code class="s">"build"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno"> 4 </code>    <code class="calibre19">steps</code> <code class="o">{</code>
<code class="lineno"> 5 </code>      <code class="o">...</code>
<code class="lineno"> 6 </code>    <code class="o">}</code>
<code class="lineno"> 7 </code>  <code class="o">}</code>
<code class="lineno"> 8 </code>  <code class="calibre19">stage</code><code class="o">(</code><code class="s">"func-test"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno"> 9 </code>    <code class="calibre19">steps</code> <code class="o">{</code>
<code class="lineno">10 </code>      <code class="o">...</code>
<code class="lineno">11 </code>    <code class="o">}</code>
<code class="lineno">12 </code>  <code class="o">}</code>
<code class="lineno">13 </code>  <code class="calibre19">stage</code><code class="o">(</code><code class="s">"release"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">14 </code>    <code class="calibre19">steps</code> <code class="o">{</code>
<code class="lineno">15 </code>      <code class="o">...</code>
<code class="lineno">16 </code>    <code class="o">}</code>
<code class="lineno">17 </code>  <code class="o">}</code>
<code class="lineno">18 </code><code class="o">}</code>
<code class="lineno">19 </code><code class="o">...</code>
</pre></div>

</figure>

<p class="calibre3">Just as in the continuous deployment pipeline, we’re having <code class="calibre19">build</code>, <code class="calibre19">func-test</code>, and <code class="calibre19">release</code> stages. However, the <code class="calibre19">deploy</code> stage is missing. This time, we are NOT going to deploy a new release to production automatically. We’ll need a manual intervention to do that. One possible way to accomplish that would be to add the <code class="calibre19">deploy</code> block to the pipeline and an additional <code class="calibre19">input</code> step in front of it. It would pause the execution of the pipeline until we choose to click the button to proceed with deployment to production. However, we will not take that approach. Instead, we’ll opt for GitOps principle which we’ll discuss later. For now, just remember that our pipeline’s goal is to make a release, not to deploy it to production.</p>

<p class="calibre3">Let us briefly go through each of the stages of the pipeline. The first one is the <code class="calibre19">build</code> stage.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="o">...</code>
<code class="lineno"> 2 </code><code class="calibre19">stage</code><code class="o">(</code><code class="s">"build"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno"> 3 </code>  <code class="calibre19">steps</code> <code class="o">{</code>
<code class="lineno"> 4 </code>    <code class="calibre19">container</code><code class="o">(</code><code class="s">"golang"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno"> 5 </code>      <code class="calibre19">script</code> <code class="o">{</code>
<code class="lineno"> 6 </code>        <code class="calibre19">currentBuild</code><code class="o">.</code><code class="na">displayName</code> <code class="o">=</code> <code class="k">new</code> <code class="calibre19">SimpleDateFormat</code><code class="o">(</code><code class="s">"yy.MM.dd"</code><code class="o">).</code><code class="na">format</code><code class="o">(</code><code class="k">new</code> <code class="calibre19">Date</code><code class="o">(</code><code class="err">\</code>
<code class="lineno"> 7 </code><code class="o">))</code> <code class="o">+</code> <code class="s">"-${env.BUILD_NUMBER}"</code>
<code class="lineno"> 8 </code>      <code class="o">}</code>
<code class="lineno"> 9 </code>      <code class="calibre19">k8sBuildGolang</code><code class="o">(</code><code class="s">"go-demo"</code><code class="o">)</code>
<code class="lineno">10 </code>    <code class="o">}</code>
<code class="lineno">11 </code>    <code class="calibre19">container</code><code class="o">(</code><code class="s">"docker"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">12 </code>      <code class="calibre19">k8sBuildImageBeta</code><code class="o">(</code><code class="calibre19">image</code><code class="o">,</code> <code class="k">false</code><code class="o">)</code>
<code class="lineno">13 </code>    <code class="o">}</code>
<code class="lineno">14 </code>  <code class="o">}</code>
<code class="lineno">15 </code><code class="o">}</code>
<code class="lineno">16 </code><code class="o">...</code>
</pre></div>

</figure>

<p class="calibre3">The first set of steps of the <code class="calibre19">build</code> stage starts in the <code class="calibre19">golang</code> container. The first action is to customize the name of the build by changing the value of the <code class="calibre19">displayName</code>. However, that is not allowed in Declarative Pipeline. Luckily, there is a way to bypass that limitation by defining the <code class="calibre19">script</code> block. Inside it can be any set of pipeline instructions we’d typically define in a Scripted Pipeline. A <code class="calibre19">script</code> block is a nifty way to temporarily switch from Declarative to Scripted Pipeline which allows much more freedom and is not bound by Declarative’s strict format rules.</p>

<p class="calibre3">There was no particular reason for using <code class="calibre19">golang</code> container to set the <code class="calibre19">displayName</code>. We could have done it in any of the other containers available in our agent defined through <code class="calibre19">yamlFile</code>. The only reason why we chose <code class="calibre19">golang</code> over any other lies in the next step.</p>

<p class="calibre3">Since, this time, our Dockerfile does not use multi-stage builds and, therefore, does not run unit tests nor it builds the binary needed for the final image, we have to run those steps separately. Given that the application is written in Go, we need its compiler available in the <code class="calibre19">golang</code> container. The actual steps are defined as <a href="https://github.com/vfarcic/jenkins-shared-libraries/blob/master/vars/k8sBuildGolang.groovy">k8sBuildGolang.groovy</a> inside the same repository we used in the previous chapter. Feel free to explore it, and you’ll see that it contains the same commands we used before inside the first stage of our multi-stage build defined in <em class="calibre17">go-demo-3 Dockerfile</em>.</p>

<p class="calibre3">Once the unit tests are executed, and the binary is built, we’re switching to the <code class="calibre19">docker</code> container to build the image. This one is based on the same shared libraries we used before, just as the most of the other steps in this pipeline. Since you’re already familiar with them, I’ll comment only if there is a substantial change in the way we utilize those libraries, or if we add a new one that we haven’t used before. If you already forgot how those libraries work, please consult their code (<code class="calibre19">*.groovy</code>) or their corresponding helper files (<code class="calibre19">*.txt</code>) located in the <em class="calibre17">vars</em> dir of the <em class="calibre17">jenkins-shared-libraries</em> repository you already forked.</p>

<p class="calibre3">Let’s move into the next stage.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="o">...</code>
<code class="lineno"> 2 </code><code class="calibre19">stage</code><code class="o">(</code><code class="s">"func-test"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno"> 3 </code>  <code class="calibre19">steps</code> <code class="o">{</code>
<code class="lineno"> 4 </code>    <code class="calibre19">container</code><code class="o">(</code><code class="s">"helm"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno"> 5 </code>      <code class="calibre19">k8sUpgradeBeta</code><code class="o">(</code><code class="calibre19">project</code><code class="o">,</code> <code class="calibre19">domain</code><code class="o">,</code> <code class="s">"--set replicaCount=2 --set dbReplicaCount=1"</code><code class="o">)</code>
<code class="lineno"> 6 </code>    <code class="o">}</code>
<code class="lineno"> 7 </code>    <code class="calibre19">container</code><code class="o">(</code><code class="s">"kubectl"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno"> 8 </code>      <code class="calibre19">k8sRolloutBeta</code><code class="o">(</code><code class="calibre19">project</code><code class="o">)</code>
<code class="lineno"> 9 </code>    <code class="o">}</code>
<code class="lineno">10 </code>    <code class="calibre19">container</code><code class="o">(</code><code class="s">"golang"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">11 </code>      <code class="calibre19">k8sFuncTestGolang</code><code class="o">(</code><code class="calibre19">project</code><code class="o">,</code> <code class="calibre19">domain</code><code class="o">)</code>
<code class="lineno">12 </code>    <code class="o">}</code>
<code class="lineno">13 </code>  <code class="o">}</code>
<code class="lineno">14 </code>  <code class="calibre19">post</code> <code class="o">{</code>
<code class="lineno">15 </code>    <code class="calibre19">always</code> <code class="o">{</code>
<code class="lineno">16 </code>      <code class="calibre19">container</code><code class="o">(</code><code class="s">"helm"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">17 </code>        <code class="calibre19">k8sDeleteBeta</code><code class="o">(</code><code class="calibre19">project</code><code class="o">)</code>
<code class="lineno">18 </code>      <code class="o">}</code>
<code class="lineno">19 </code>    <code class="o">}</code>
<code class="lineno">20 </code>  <code class="o">}</code>
<code class="lineno">21 </code><code class="o">}</code>
<code class="lineno">22 </code><code class="o">...</code>
</pre></div>

</figure>

<p class="calibre3">The steps of the <code class="calibre19">func-test</code> stage are the same as those we used in the continuous deployment pipeline. The only difference is in the format of the blocks that surround them. We’re jumping from one container to another and executing the same shared libraries as before.</p>

<p class="calibre3">The real difference is in the <code class="calibre19">post</code> section of the stage. It contains an <code class="calibre19">always</code> block that guarantees that the steps inside it will execute no matter the outcome of the steps in this stage. In our case, the <code class="calibre19">post</code> section has only one step that invokes that <code class="calibre19">k8sDeleteBeta</code> library which deletes the installation of the release under test.</p>

<p class="calibre3">As you can see, the <code class="calibre19">func-test</code> stage we just explored is functionally the same as the one we used in the previous chapter when we defined the continuous deployment pipeline. However, I’d argue that the <code class="calibre19">post</code> section available in Declarative Pipeline is much more elegant and easier to understand than <code class="calibre19">try/catch/finally</code> blocks we used inside the Scripted Pipeline. That would be even more evident if we’d use a more complex type of <code class="calibre19">post</code> criteria, but we don’t have a good use-case for them.</p>

<p class="calibre3">It’s time to move into the next stage.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="o">...</code>
<code class="lineno"> 2 </code><code class="calibre19">stage</code><code class="o">(</code><code class="s">"release"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno"> 3 </code>  <code class="calibre19">when</code> <code class="o">{</code>
<code class="lineno"> 4 </code>      <code class="calibre19">branch</code> <code class="s">"master"</code>
<code class="lineno"> 5 </code>  <code class="o">}</code>
<code class="lineno"> 6 </code>  <code class="calibre19">steps</code> <code class="o">{</code>
<code class="lineno"> 7 </code>    <code class="calibre19">container</code><code class="o">(</code><code class="s">"docker"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno"> 8 </code>      <code class="calibre19">k8sPushImage</code><code class="o">(</code><code class="calibre19">image</code><code class="o">,</code> <code class="k">false</code><code class="o">)</code>
<code class="lineno"> 9 </code>    <code class="o">}</code>
<code class="lineno">10 </code>    <code class="calibre19">container</code><code class="o">(</code><code class="s">"helm"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">11 </code>      <code class="calibre19">k8sPushHelm</code><code class="o">(</code><code class="calibre19">project</code><code class="o">,</code> <code class="s">""</code><code class="o">,</code> <code class="calibre19">cmAddr</code><code class="o">,</code> <code class="k">true</code><code class="o">,</code> <code class="k">true</code><code class="o">)</code>
<code class="lineno">12 </code>    <code class="o">}</code>
<code class="lineno">13 </code>  <code class="o">}</code>
<code class="lineno">14 </code><code class="o">}</code>
<code class="lineno">15 </code><code class="o">...</code>
</pre></div>

</figure>

<p class="calibre3">The <code class="calibre19">release</code> stage, just as its counterpart from the previous chapter, features the same step that tags and pushes the production release to Docker Hub (<code class="calibre19">k8sPushImage</code>) as well as the one that packages and pushes the Helm Chart to ChartMuseum (<code class="calibre19">k8sPushHelm</code>). The only difference is that the latter library invocation now uses two additional arguments. The third one, when set to <code class="calibre19">true</code>, replaces the <code class="calibre19">image.tag</code> value to the tag of the image built in the previous step. The fourth argument, also when set to <code class="calibre19">true</code>, fails the build if the version of the Chart is unchanged or, in other words, if it already exists in ChartMuseum. When combining those two, we are guaranteeing that the <code class="calibre19">image.tag</code> value in the Chart is the same as the image we built, and that the version of the Chart is unique. The latter forces us to update the version manually. If we’d work on continuous deployment, manual update (or any other manual action), would be unacceptable. But, continuous delivery does involve a human decision when and what to deploy to production. We’re just ensuring that the human action of changing the version of the Chart was indeed performed. Please open the source code of <a href="https://github.com/vfarcic/jenkins-shared-libraries/blob/master/vars/k8sPushHelm.groovy">k8sPushHelm.groovy</a> to check the code behind that library and compare it with the statements you just read.</p>

<p class="calibre3">You’ll notice that there is a <code class="calibre19">when</code> statement above the steps. Generally speaking, it is used to limit the executions within a stage only to those cases that match the condition. In our case, that condition states that the stage should be executed only if the build is using a commit from the <code class="calibre19">master</code> branch. It is equivalent to the <code class="calibre19">if ("${BRANCH_NAME}" == "master")</code> block we used in the continuous deployment pipeline in the previous chapter. There are other conditions we could have used but, for our use-case, that one is enough.</p>

<aside class="information">
    <p class="calibre3">You might want to explore other types of <code class="calibre19">when</code> conditions 
by going through the <a href="https://jenkins.io/doc/book/pipeline/syntax/#when">when statement documentation</a>.</p>

</aside>

<p class="calibre3">You’ll notice that we did not define <code class="calibre19">git</code> or <code class="calibre19">checkout scm</code> step anywhere in our pipeline script. There’s no need for that with Declarative Pipeline. It is intelligent enough to know that we want to clone the code of the commit that initiated a build (through a Webhook, if we’d have it). When a build starts, cloning the code will be one of its first actions.</p>

<p class="calibre3">Now that we went through the content of the <em class="calibre17">Jenkinsfile.orig</em> file, we should go back to the referenced <code class="calibre19">KubernetesPod.yaml</code> file that defines the Pod that will be used as a Jenkins agent.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>cat KubernetesPod.yaml
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">apiVersion</code><code class="calibre19">:</code> <code class="calibre19">v1</code>
<code class="lineno"> 2 </code><code class="calibre19">kind</code><code class="calibre19">:</code> <code class="calibre19">Pod</code>
<code class="lineno"> 3 </code><code class="calibre19">spec</code><code class="calibre19">:</code>
<code class="lineno"> 4 </code>  <code class="calibre19">containers</code><code class="calibre19">:</code>
<code class="lineno"> 5 </code>  <code class="calibre19">-</code> <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">docker</code>
<code class="lineno"> 6 </code>    <code class="calibre19">image</code><code class="calibre19">:</code> <code class="calibre19">docker:18.06</code>
<code class="lineno"> 7 </code>    <code class="calibre19">command</code><code class="calibre19">:</code> <code class="calibre19">[</code><code class="s">"cat"</code><code class="calibre19">]</code>
<code class="lineno"> 8 </code>    <code class="calibre19">tty</code><code class="calibre19">:</code> <code class="calibre19">true</code>
<code class="lineno"> 9 </code>    <code class="calibre19">volumeMounts</code><code class="calibre19">:</code>
<code class="lineno">10 </code>    <code class="calibre19">-</code> <code class="calibre19">mountPath</code><code class="calibre19">:</code> <code class="calibre19">/var/run/docker.sock</code>
<code class="lineno">11 </code>      <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">docker-socket</code>
<code class="lineno">12 </code>  <code class="calibre19">-</code> <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">helm</code>
<code class="lineno">13 </code>    <code class="calibre19">image</code><code class="calibre19">:</code> <code class="calibre19">vfarcic/helm:2.9.1</code>
<code class="lineno">14 </code>    <code class="calibre19">command</code><code class="calibre19">:</code> <code class="calibre19">[</code><code class="s">"cat"</code><code class="calibre19">]</code>
<code class="lineno">15 </code>    <code class="calibre19">tty</code><code class="calibre19">:</code> <code class="calibre19">true</code>
<code class="lineno">16 </code>  <code class="calibre19">-</code> <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">kubectl</code>
<code class="lineno">17 </code>    <code class="calibre19">image</code><code class="calibre19">:</code> <code class="calibre19">vfarcic/kubectl</code>
<code class="lineno">18 </code>    <code class="calibre19">command</code><code class="calibre19">:</code> <code class="calibre19">[</code><code class="s">"cat"</code><code class="calibre19">]</code>
<code class="lineno">19 </code>    <code class="calibre19">tty</code><code class="calibre19">:</code> <code class="calibre19">true</code>
<code class="lineno">20 </code>  <code class="calibre19">-</code> <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">golang</code>
<code class="lineno">21 </code>    <code class="calibre19">image</code><code class="calibre19">:</code> <code class="calibre19">golang:1.9</code>
<code class="lineno">22 </code>    <code class="calibre19">command</code><code class="calibre19">:</code> <code class="calibre19">[</code><code class="s">"cat"</code><code class="calibre19">]</code>
<code class="lineno">23 </code>    <code class="calibre19">tty</code><code class="calibre19">:</code> <code class="calibre19">true</code>
<code class="lineno">24 </code>  <code class="calibre19">volumes</code><code class="calibre19">:</code>
<code class="lineno">25 </code>  <code class="calibre19">-</code> <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">docker-socket</code>
<code class="lineno">26 </code>    <code class="calibre19">hostPath</code><code class="calibre19">:</code>
<code class="lineno">27 </code>      <code class="calibre19">path</code><code class="calibre19">:</code> <code class="calibre19">/var/run/docker.sock</code>
<code class="lineno">28 </code>      <code class="calibre19">type</code><code class="calibre19">:</code> <code class="calibre19">Socket</code>
</pre></div>

</figure>

<p class="calibre3">That Pod definition is almost the same as the one we used inside <em class="calibre17">Jenkinsfile</em> in the <em class="calibre17">go-demo-3</em> repository. Apart from residing in a separate file, the only difference is in an additional container named <code class="calibre19">docker</code>. In this scenario, we are not using external VMs to build Docker images. Instead, we have an additional container through which we can execute Docker-related steps. Since we want to execute Docker commands on the node, and avoid running Docker-in-Docker, we mounted <code class="calibre19">/var/run/docker.sock</code> as a Volume.</p>

<aside class="warning">
    <h3 id="leanpub-auto-a-note-to-minishift-users-35" class="calibre22">A note to minishift users</h3>

  <p class="calibre3">We need to relax security so that Pods are allowed to use <code class="calibre19">hostPath</code> volume plug-in. Please execute the command that follows.</p>

  <p class="calibre3"><code class="calibre19">oc adm policy add-scc-to-user hostmount-anyuid -z build -n go-demo-5-build</code></p>

</aside>

<h3 id="leanpub-auto-creating-and-running-a-continuous-delivery-job" class="calibre20">Creating And Running A Continuous Delivery Job</h3>

<p class="calibre3">That’s it. We explored (soon to be) <em class="calibre17">Jenkinsfile</em> that contains our continuous delivery pipeline and <em class="calibre17">KubernetesPod.yaml</em> that contains the Pod definition that will be used to create Jenkins agents. There are a few other things we need to do but, before we discuss them, we’ll change the address and the Docker Hub user in <em class="calibre17">Jenkinsfile.orig</em>, store the output as <em class="calibre17">Jenkinsfile</em> and push the changes to the forked GitHub repository.</p>

<aside class="warning">
    <h3 id="leanpub-auto-a-note-to-minishift-users-36" class="calibre22">A note to minishift users</h3>

  <p class="calibre3">We’ll use a slightly modified version of Jenkins file. Just as in the previous chapter, we’ll add the <code class="calibre19">ocCreateEdgeRouteBuild</code> step that will accomplish the same results as if we’d have NGINX Ingress controller.
Please use <code class="calibre19">Jenkinsfile.oc</code> instead of <code class="calibre19">Jenkinsfile.orig</code> in the command that follows.</p>

</aside>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>cat Jenkinsfile.orig <code class="se">\</code>
<code class="lineno"> 2 </code>    <code class="calibre19">|</code> sed -e <code class="s">"s@acme.com@</code><code class="nv">$ADDR</code><code class="s">@g"</code> <code class="se">\</code>
<code class="lineno"> 3 </code>    <code class="calibre19">|</code> sed -e <code class="s">"s@vfarcic@</code><code class="nv">$DH_USER</code><code class="s">@g"</code> <code class="se">\</code>
<code class="lineno"> 4 </code>    <code class="calibre19">|</code> tee Jenkinsfile
<code class="lineno"> 5 </code>
<code class="lineno"> 6 </code>git add .
<code class="lineno"> 7 </code>
<code class="lineno"> 8 </code>git commit -m <code class="s">"Jenkinsfile"</code>
<code class="lineno"> 9 </code>
<code class="lineno">10 </code>git push
</pre></div>

</figure>

<p class="calibre3">Since we are into running Git commands, we might just as well merge your <em class="calibre17">jenkins-shared-libraries</em> fork with the <code class="calibre19">upstream/master</code>. That will ensure that you have the latest version that includes potential changes I might have made since the time you forked the repository.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="nb">cd</code> ..
<code class="lineno"> 2 </code>
<code class="lineno"> 3 </code>git clone https://github.com/<code class="nv">$GH_USER</code>/jenkins-shared-libraries.git
<code class="lineno"> 4 </code>
<code class="lineno"> 5 </code><code class="nb">cd</code> jenkins-shared-libraries
<code class="lineno"> 6 </code>
<code class="lineno"> 7 </code>git remote add upstream <code class="se">\</code>
<code class="lineno"> 8 </code>    https://github.com/vfarcic/jenkins-shared-libraries.git
<code class="lineno"> 9 </code>
<code class="lineno">10 </code>git fetch upstream
<code class="lineno">11 </code>
<code class="lineno">12 </code>git checkout master
<code class="lineno">13 </code>
<code class="lineno">14 </code>git merge upstream/master
<code class="lineno">15 </code>
<code class="lineno">16 </code><code class="nb">cd</code> ../go-demo-5
</pre></div>

</figure>

<p class="calibre3">We’re almost ready to create a Jenkins pipeline for <em class="calibre17">go-demo-5</em>. The only thing missing is to create a new Kubernetes Cloud configuration.</p>

<p class="calibre3">For now, we have only one Kubernetes Cloud configured in Jenkins. Its name is <em class="calibre17">kubernetes</em>. However, the pipeline we just explored uses a cloud named <code class="calibre19">go-demo-5-build</code>. So, we should create a new one before we create jobs tied to the <em class="calibre17">go-demo-5</em> repository.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/configure"</code>
</pre></div>

</figure>

<p class="calibre3">Please scroll to the bottom of the page, expand the <em class="calibre17">Add a new cloud</em> list, and select <em class="calibre17">Kubernetes</em>. A new set of fields will appear.</p>

<p class="calibre3">Type <em class="calibre17">go-demo-5-build</em> as the name. It matches the <code class="calibre19">cloud</code> entry inside <code class="calibre19">kubernetes</code> block of our pipeline.</p>

<p class="calibre3">Next, type <em class="calibre17">go-demo-5-build</em> as the <em class="calibre17">Kubernetes Namespace</em>.</p>

<p class="calibre3">Just as with the other Kubernetes Cloud that was already defined in our Jenkins instance, the value of the <em class="calibre17">Jenkins URL</em> should be <em class="calibre17">http://prod-jenkins.prod:8080</em>, and the <em class="calibre17">Jenkins tunnel</em> should be set to <em class="calibre17">prod-jenkins-agent.prod:50000</em>.</p>

<p class="calibre3">Don’t forget to click the <em class="calibre17">Save</em> button to persist the changes.</p>

<p class="calibre3">Right now, we have two Kubernetes Clouds configured in our Jenkins instance. One is called <em class="calibre17">kubernetes</em>, and it uses the <em class="calibre17">prod</em> Namespace, while the other (the new one) is called <em class="calibre17">go-demo-5-build</em> and it can be used for all the builds that should be performed in the <em class="calibre17">go-demo-5-build</em> Namespace.</p>

<p class="calibre3">Even though we have two Kubernetes Clouds, their configurations are almost the same. Besides having different names, the only substantial difference is in the Namespace they use. I wanted to keep it simple and demonstrate that multiple clouds are possible, and often useful. In the “real world” situations, you’ll probably use more fields and differentiate them even further. As an example, we could have defined the default set of containers that will be used with those clouds.</p>


<figure class="image1">
  <img src="../images/00043.jpeg" alt="Figure 8-1: Jenkins Kubernetes Cloud settings for go-demo-5-build" class="calibre25"/>
  <figcaption class="calibre26">Figure 8-1: Jenkins Kubernetes Cloud settings for go-demo-5-build</figcaption>
</figure>


<p class="calibre3">Now we’re ready to create a job that will be tied to the <em class="calibre17">go-demo-5</em> repository and validate whether the pipeline defined in the <em class="calibre17">Jenkinsfile</em> works as expected.</p>

<p class="calibre3">We’ll create our job from the BlueOcean home screen.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/blue/organizations/jenkins/"</code>
</pre></div>

</figure>

<p class="calibre3">Please click the <em class="calibre17">Create a New Pipeline</em> button and select <em class="calibre17">GitHub</em> as the repository type. Type <em class="calibre17">Your GitHub access token</em> and click the <em class="calibre17">Connect</em> button. A moment later, you’ll see the list of organizations that token belongs to. Select the one where you forked the applications. The list of repositories will appear. Select <em class="calibre17">go-demo-5</em> and click the <em class="calibre17">Create Pipeline</em> button.</p>

<p class="calibre3">Jenkins will create jobs for each branch of the <em class="calibre17">go-demo-5</em> repository. There is only one (<em class="calibre17">master</em>), so there will be one job in total. We already explored in the previous chapter how Jenkins handles multiple repositories by creating a job for each, so I thought that there is no need to demonstrate the same feature again. Right now, <em class="calibre17">master</em> job/branch should be more than enough.</p>

<p class="calibre3">Please wait until the build is finished.</p>


<figure class="image1">
  <img src="../images/00044.jpeg" alt="Figure 8-2: Continuous delivery Jenkins pipeline" class="calibre25"/>
  <figcaption class="calibre26">Figure 8-2: Continuous delivery Jenkins pipeline</figcaption>
</figure>


<p class="calibre3">Since the build was executed against the <em class="calibre17">master</em> branch, the <code class="calibre19">when</code> condition inside the <code class="calibre19">release</code> stage evaluated to <code class="calibre19">true</code> so the production-ready image was pushed to Docker Hub and the Helm Chart with the updated tag was pushed to ChartMuseum. We’ll check the latter by retrieving the list of all the Charts.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>curl <code class="s">"http://cm.</code><code class="nv">$ADDR</code><code class="s">/index.yaml"</code>
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">apiVersion</code><code class="calibre19">:</code> <code class="calibre19">v1</code>
<code class="lineno"> 2 </code><code class="calibre19">entries</code><code class="calibre19">:</code>
<code class="lineno"> 3 </code>  <code class="calibre19">go-demo-5</code><code class="calibre19">:</code>
<code class="lineno"> 4 </code>  <code class="calibre19">-</code> <code class="calibre19">apiVersion</code><code class="calibre19">:</code> <code class="calibre19">v1</code>
<code class="lineno"> 5 </code>    <code class="calibre19">created</code><code class="calibre19">:</code> <code class="s">"2018-08-08T20:47:34.322943263Z"</code>
<code class="lineno"> 6 </code>    <code class="calibre19">description</code><code class="calibre19">:</code> <code class="calibre19">A silly demo based on API written in Go and MongoDB</code>
<code class="lineno"> 7 </code>    <code class="calibre19">digest</code><code class="calibre19">:</code> <code class="calibre19">a30aa7921b890b1f919286113e4a8193a2d4d3137e8865b958acd1a2bfd97c7e</code>
<code class="lineno"> 8 </code>    <code class="calibre19">home</code><code class="calibre19">:</code> <code class="calibre19">http://www.devopstoolkitseries.com/</code>
<code class="lineno"> 9 </code>    <code class="calibre19">keywords</code><code class="calibre19">:</code>
<code class="lineno">10 </code>    <code class="calibre19">-</code> <code class="calibre19">api</code>
<code class="lineno">11 </code>    <code class="calibre19">-</code> <code class="calibre19">backend</code>
<code class="lineno">12 </code>    <code class="calibre19">-</code> <code class="calibre19">go</code>
<code class="lineno">13 </code>    <code class="calibre19">-</code> <code class="calibre19">database</code>
<code class="lineno">14 </code>    <code class="calibre19">-</code> <code class="calibre19">mongodb</code>
<code class="lineno">15 </code>    <code class="calibre19">maintainers</code><code class="calibre19">:</code>
<code class="lineno">16 </code>    <code class="calibre19">-</code> <code class="calibre19">email</code><code class="calibre19">:</code> <code class="calibre19">viktor@farcic.com</code>
<code class="lineno">17 </code>      <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">Viktor Farcic</code>
<code class="lineno">18 </code>    <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">go-demo-5</code>
<code class="lineno">19 </code>    <code class="calibre19">sources</code><code class="calibre19">:</code>
<code class="lineno">20 </code>    <code class="calibre19">-</code> <code class="calibre19">https://github.com/vfarcic/go-demo-5</code>
<code class="lineno">21 </code>    <code class="calibre19">urls</code><code class="calibre19">:</code>
<code class="lineno">22 </code>    <code class="calibre19">-</code> <code class="calibre19">charts/go-demo-5-0.0.1.tgz</code>
<code class="lineno">23 </code>    <code class="calibre19">version</code><code class="calibre19">:</code> <code class="calibre19">0.0.1</code>
<code class="lineno">24 </code><code class="calibre19">generated</code><code class="calibre19">:</code> <code class="s">"2018-08-08T21:03:01Z"</code>
</pre></div>

</figure>

<p class="calibre3">We can see that we have only one Chart (so far). It is the <em class="calibre17">go-demo-5</em> Chart. The important thing to note is the version of the Chart. In that output, it’s <code class="calibre19">0.0.1</code>. However, I might have bumped it later since I wrote this text, so your version might be different. We’ll need that version soon, so let’s put it into an environment variable.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">VERSION</code><code class="o">=[</code>...<code class="o">]</code>
</pre></div>

</figure>

<p class="calibre3">Please make sure to change <code class="calibre19">[...]</code> with the version you obtained earlier from <code class="calibre19">index.yaml</code>.</p>

<p class="calibre3">Among other things, the build modified the Chart before pushing it to ChartMuseum. It changed the image tag to the new release. We’ll add ChartMuseum as a repository in our local Helm client so that we can inspect the Chart and confirm that <code class="calibre19">image.tag</code> value is indeed correct.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>helm repo add chartmuseum <code class="se">\</code>
<code class="lineno">2 </code>    http://cm.<code class="nv">$ADDR</code>
<code class="lineno">3 </code>
<code class="lineno">4 </code>helm repo list
</pre></div>

</figure>

<p class="calibre3">The output of the latter command is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>NAME            URL
<code class="lineno">2 </code>stable          https://kubernetes-charts.storage.googleapis.com
<code class="lineno">3 </code>local           http://127.0.0.1:8879/charts
<code class="lineno">4 </code>chartmuseum     http://cm.18.219.191.38.nip.io
</pre></div>

</figure>

<p class="calibre3">You might have additional repositories configured in your local Helm client. That’s not of importance. What matters right now is that the output showed <code class="calibre19">chartmuseum</code> as one of the repositories.</p>

<p class="calibre3">Now that we added the new repository, we should update local cache.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>helm repo update
</pre></div>

</figure>

<p class="calibre3">Finally, we can inspect the Chart Jenkins pushed to ChartMuseum.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>helm inspect chartmuseum/go-demo-5 <code class="se">\</code>
<code class="lineno">2 </code>    --version <code class="nv">$VERSION</code>
</pre></div>

</figure>

<p class="calibre3">The output, limited to relevant parts, is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nn">...</code>
<code class="lineno">2 </code><code class="calibre19">version</code><code class="calibre19">:</code> <code class="calibre19">0.0.1</code>
<code class="lineno">3 </code><code class="nn">...</code>
<code class="lineno">4 </code><code class="calibre19">image</code><code class="calibre19">:</code>
<code class="lineno">5 </code>  <code class="calibre19">tag</code><code class="calibre19">:</code> <code class="calibre19">18.08.08-3</code>
<code class="lineno">6 </code><code class="nn">...</code>
</pre></div>

</figure>

<p class="calibre3">We can see that the build modified the <code class="calibre19">image.tag</code> before it packaged the Chart and pushed it to ChartMuseum.</p>

<p class="calibre3">The first build of our continuous delivery pipeline was successful. However, the whole process is still not finished. We are yet to design the process that will allow us to choose which release to deploy to production. Even though our goal is to let Jenkins handle deployments to production, we’ll leave it aside for a while and first explore how we could do it manually from a terminal.</p>

<p class="calibre3">Did I mention that we’ll introduce GitOps to the process?</p>

<h3 id="leanpub-auto-what-is-gitops-and-do-we-want-it" class="calibre20">What Is GitOps And Do We Want It?</h3>

<p class="calibre3"><strong class="calibre18">Git is the only source of truth.</strong> If you understand that sentence, you know GitOps. Every time we want to apply a change, we need to push a commit to Git. Want to change the configuration of your servers? Commit a change to Git, and let an automated process propagate it to servers. Want to upgrade ChartMuseum? Change <em class="calibre17">requirements.yaml</em>, push the change to the <em class="calibre17">k8s-prod</em> repository, and let an automated process do the rest. Want to review a change before applying it? Make a pull request. Want to rollback a release? You probably get the point, and I can save you from listing hundreds of other “want to” questions.</p>

<p class="calibre3">Did we do GitOps in the previous chapter? Was our continuous deployment process following GitOps? The answer to both questions is <em class="calibre17">no</em>. We did keep the code, configurations, and Kubernetes definitions in Git. Most of it, at least. However, we were updating production releases with new image tags without committing those changes to Git. Our complete source of truth was the cluster, not Git. It contained most of the truth, but not all of it.</p>

<p class="calibre3">Does this mean that we should fully embrace GitOps? I don’t think so. Some things would be impractical to do by committing them to Git.</p>

<p class="calibre3">Take installations of applications under test as an example. We need to install a test release inside a test environment (a Namespace), we need to run some automated tests, and we need to remove the applications once we’re finished. If we’d fully embrace GitOps, we’d need to push a definition of the application under test to a Git repository and probably to initiate a different pipeline that would install it. After that, we’d run some tests and remove the information we just pushed to Git so that yet another process can remove it from the cluster. Using GitOps with temporary installations would only increase the complexity of the process and slow it down without any tangible benefit. Why would we store something in Git only to remove it a few minutes later?</p>

<p class="calibre3">There are other use cases where I think GitOps is not a right fit. Take auto-scaling as an example. We might want to use <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscaler (HPA)</a>. Or, maybe we want Prometheus to fire alerts which will result in increasing and decreasing the number of replicas of our applications depending on, let’s say, response times. If those changes are infrequent (e.g., once a month or even once a week), storing them in Git and firing Webhooks that will do the scaling makes sense. But, if scaling is more frequent, the information in Git would vary so much that it would only result in more confusion.</p>

<p class="calibre3">The same can be said to auto-scaling of infrastructure. Should we ignore the fact that GKE (and most other Kubernetes clusters) can automatically increase and decrease the number of nodes of the cluster depending on resource usage and how many pending Pods we have? We probably shouldn’t.</p>

<p class="calibre3">Those examples should not discourage you from applying GitOps logic. Instead, they should demonstrate that we should not see the world as black-and-white. The fact that I think that we should not embrace GitOps hundred percent does not mean that we should not embrace it at all. We should always try to strike a balance between different practices and create a combination that best fits our scenarios.</p>

<p class="calibre3">In our case, we’ll use GitOps to an extent, even though we might not follow the mantra to the fullest.</p>

<p class="calibre3">Now, let’s try to upgrade our production environment by adding the <em class="calibre17">go-demo-5</em> release to it.</p>

<h3 id="leanpub-auto-upgrading-the-production-environment-using-gitops-practices" class="calibre20">Upgrading The Production Environment Using GitOps Practices</h3>

<p class="calibre3">Right now, our production environment contains Jenkins and ChartMuseum. On the other hand, we created a new production-ready release of <em class="calibre17">go-demo-5</em>. Now we should let our business, marketing, or some other department make a decision on whether they’d like to deploy the new release to production and when should that happen. We’ll imagine that they gave us the green light to install the <em class="calibre17">go-demo-5</em> release and that it should be done now. Our users are ready for it.</p>

<p class="calibre3">We’ll deploy the new release manually first. That way we’ll confirm that our deployment process works as expected. Later on, we’ll try to automate the process through Jenkins.</p>

<p class="calibre3">Our whole production environment is stored in the <em class="calibre17">k8s-prod</em> repository. The applications that constitute it are defined in <em class="calibre17">requirements.yaml</em> file. Let’s take another look at it.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">cd</code> ../k8s-prod
<code class="lineno">2 </code>
<code class="lineno">3 </code>cat helm/requirements.yaml
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="calibre19">dependencies</code><code class="calibre19">:</code>
<code class="lineno">2 </code><code class="calibre19">-</code> <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">chartmuseum</code>
<code class="lineno">3 </code>  <code class="calibre19">repository</code><code class="calibre19">:</code> <code class="s">"@stable"</code>
<code class="lineno">4 </code>  <code class="calibre19">version</code><code class="calibre19">:</code> <code class="calibre19">1.6.0</code>
<code class="lineno">5 </code><code class="calibre19">-</code> <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">jenkins</code>
<code class="lineno">6 </code>  <code class="calibre19">repository</code><code class="calibre19">:</code> <code class="s">"@stable"</code>
<code class="lineno">7 </code>  <code class="calibre19">version</code><code class="calibre19">:</code> <code class="calibre19">0.16.6</code>
</pre></div>

</figure>

<p class="calibre3">We already discussed those dependencies and used them to install Jenkins and ChartMuseum from the <code class="calibre19">@stable</code> repository. Since we do not want to bump versions of the two, we’ll leave them intact, and we’ll add <em class="calibre17">go-demo-5</em> to the mix.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">echo</code> <code class="s">"- name: go-demo-5</code>
<code class="lineno">2 </code><code class="s">  repository: \"@chartmuseum\"</code>
<code class="lineno">3 </code><code class="s">  version: </code><code class="nv">$VERSION</code><code class="s">"</code> <code class="se">\</code>
<code class="lineno">4 </code>  <code class="calibre19">|</code> tee -a helm/requirements.yaml
<code class="lineno">5 </code>
<code class="lineno">6 </code>cat helm/requirements.yaml
</pre></div>

</figure>

<p class="calibre3">The output of the latter command is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">dependencies</code><code class="calibre19">:</code>
<code class="lineno"> 2 </code><code class="calibre19">-</code> <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">chartmuseum</code>
<code class="lineno"> 3 </code>  <code class="calibre19">repository</code><code class="calibre19">:</code> <code class="s">"@stable"</code>
<code class="lineno"> 4 </code>  <code class="calibre19">version</code><code class="calibre19">:</code> <code class="calibre19">1.6.0</code>
<code class="lineno"> 5 </code><code class="calibre19">-</code> <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">jenkins</code>
<code class="lineno"> 6 </code>  <code class="calibre19">repository</code><code class="calibre19">:</code> <code class="s">"@stable"</code>
<code class="lineno"> 7 </code>  <code class="calibre19">version</code><code class="calibre19">:</code> <code class="calibre19">0.16.6</code>
<code class="lineno"> 8 </code><code class="calibre19">-</code> <code class="calibre19">name</code><code class="calibre19">:</code> <code class="calibre19">go-demo-5</code>
<code class="lineno"> 9 </code>  <code class="calibre19">repository</code><code class="calibre19">:</code> <code class="s">"@chartmuseum"</code>
<code class="lineno">10 </code>  <code class="calibre19">version</code><code class="calibre19">:</code> <code class="calibre19">0.0.1</code>
</pre></div>

</figure>

<p class="calibre3">Our dependencies increased from two to three dependencies.</p>

<p class="calibre3">Usually, that would be all, and we would upgrade the Chart. However, we still need to change the <code class="calibre19">host</code> value. In the “real world” situation, you’d have it pre-defined since hosts rarely change. But, in our case, I could not know your host in advance so we’ll need to overwrite the <code class="calibre19">ingress.host</code> value of <code class="calibre19">go-demo-5</code>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">echo</code> <code class="s">"go-demo-5:</code>
<code class="lineno">2 </code><code class="s">  ingress:</code>
<code class="lineno">3 </code><code class="s">    host: go-demo-5.</code><code class="nv">$ADDR</code><code class="s">"</code> <code class="se">\</code>
<code class="lineno">4 </code>    <code class="calibre19">|</code> tee -a helm/values.yaml
<code class="lineno">5 </code>
<code class="lineno">6 </code>cat helm/values.yaml
</pre></div>

</figure>

<p class="calibre3">The latter command outputs the final version of the values. The section related to <code class="calibre19">go-demo-5</code> should be similar to the one that follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nn">...</code>
<code class="lineno">2 </code><code class="calibre19">go-demo-5</code><code class="calibre19">:</code>
<code class="lineno">3 </code>  <code class="calibre19">ingress</code><code class="calibre19">:</code>
<code class="lineno">4 </code>    <code class="calibre19">host</code><code class="calibre19">:</code> <code class="calibre19">go-demo-5.18.219.191.38.nip.io</code>
</pre></div>

</figure>

<p class="calibre3">We already discussed that we’ll document production environment in Git and, therefore, adhere to GitOps principles.</p>

<p class="calibre3">Typically, we’d push the changes we made to a different branch or to a forked repo, and we’d make a pull request. Someone would review it and accept the changes or provide notes with potential improvements. I’m sure you already know how pull requests work, the value behind code reviews, and all the other good things we’re doing with code in Git. So, we’ll skip all that and push directly to the <em class="calibre17">master</em> branch. Just remember that we’re not going to skip pull request and the rest because we should, but because I’m trying to skip the things you already know and jump straight to the point.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>git add .
<code class="lineno">2 </code>
<code class="lineno">3 </code>git commit -m <code class="s">"Added go-demo-5"</code>
<code class="lineno">4 </code>
<code class="lineno">5 </code>git push
</pre></div>

</figure>

<p class="calibre3">As you already know, we need to update local Helm cache with the new dependencies.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>helm dependency update helm
</pre></div>

</figure>

<p class="calibre3">The last lines of the output are as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>...
<code class="lineno">2 </code>Saving 3 charts
<code class="lineno">3 </code>Downloading chartmuseum from repo https://kubernetes-charts.storage.googleapis.com
<code class="lineno">4 </code>Downloading jenkins from repo https://kubernetes-charts.storage.googleapis.com
<code class="lineno">5 </code>Downloading go-demo-5 from repo http://cm.18.219.191.38.nip.io
<code class="lineno">6 </code>Deleting outdated charts
</pre></div>

</figure>

<p class="calibre3">We can see that this time Helm downloaded three Charts, including <code class="calibre19">go-demo-5</code> we just added as a dependency in <code class="calibre19">requirements.yaml</code>. We can confirm that by listing the files in the <code class="calibre19">helm/charts</code> directory.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ls -1 helm/charts
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>chartmuseum-1.6.0.tgz
<code class="lineno">2 </code>go-demo-5-0.0.1.tgz
<code class="lineno">3 </code>jenkins-0.16.6.tgz
</pre></div>

</figure>

<p class="calibre3">The <em class="calibre17">go-demo-5</em> package is there, and we are ready to <code class="calibre19">update</code> our production environment.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>helm upgrade prod helm <code class="se">\</code>
<code class="lineno">2 </code>    --namespace prod
</pre></div>

</figure>

<p class="calibre3">Let’s take a look at the Pods running inside the <code class="calibre19">prod</code> Namespace.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kubectl -n prod get pods
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>NAME                                READY     STATUS              RESTARTS   AGE
<code class="lineno">2 </code>prod-chartmuseum-68bc575fb7-dn6h5   1/1       Running             0          4h
<code class="lineno">3 </code>prod-go-demo-5-66c9d649bd-kq45m     1/1       Running             2          51s
<code class="lineno">4 </code>prod-go-demo-5-66c9d649bd-lgjb7     1/1       Running             2          51s
<code class="lineno">5 </code>prod-go-demo-5-66c9d649bd-pwnjg     1/1       Running             2          51s
<code class="lineno">6 </code>prod-go-demo-5-db-0                 2/2       Running             0          51s
<code class="lineno">7 </code>prod-go-demo-5-db-1                 0/2       ContainerCreating   0          15s
<code class="lineno">8 </code>prod-jenkins-676cc64756-bj45v       1/1       Running             0          4h
</pre></div>

</figure>

<p class="calibre3">Judging by the <code class="calibre19">AGE</code>, we can see that ChartMuseum and Jenkins were left intact. That makes sense since we did not change any of their properties. The new Pods are those related to <em class="calibre17">go-demo-5</em>. The output will differ depending on when we executed <code class="calibre19">get pods</code>. In my case, we can see that three replicas of the <em class="calibre17">go-demo-5</em> API are running and that we are in the process of deploying the second database Pod. Soon all three DB replicas will be running, and our mission will be accomplished.</p>

<aside class="warning">
    <h3 id="leanpub-auto-a-note-to-minishift-users-37" class="calibre22">A note to minishift users</h3>

  <p class="calibre3">OpenShift ignores Ingress resources so we’ll have to create a Route to accomplish the same effect. Please execute the command that follows.</p>

  <p class="calibre3"><code class="calibre19">oc -n prod create route edge --service prod-go-demo-5 --hostname go-demo-5.$ADDR --insecure-policy Allow</code></p>

</aside>

<p class="calibre3">To be on the safe side, we’ll confirm that the newly deployed <em class="calibre17">go-demo-5</em> application is indeed accessible.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kubectl -n prod rollout status <code class="se">\</code>
<code class="lineno">2 </code>    deployment prod-go-demo-5
<code class="lineno">3 </code>
<code class="lineno">4 </code>curl -i <code class="s">"http://go-demo-5.</code><code class="nv">$ADDR</code><code class="s">/demo/hello"</code>
</pre></div>

</figure>

<p class="calibre3">We waited until <code class="calibre19">rollout status</code> confirms that the application is deployed and we sent a request to it. The output of the latter command should show the status code <code class="calibre19">200 OK</code> and the familiar message <code class="calibre19">hello, world!</code>.</p>

<p class="calibre3">As the last validation, we’ll describe the application and confirm that the image is indeed correct (and not <code class="calibre19">latest</code>).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kubectl -n prod <code class="se">\</code>
<code class="lineno">2 </code>    describe deploy prod-go-demo-5
</pre></div>

</figure>

<p class="calibre3">The output, limited to the relevant parts, is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nn">...</code>
<code class="lineno">2 </code><code class="calibre19">Pod Template</code><code class="calibre19">:</code>
<code class="lineno">3 </code>  <code class="calibre19">...</code>
<code class="lineno">4 </code>  <code class="calibre19">Containers</code><code class="calibre19">:</code>
<code class="lineno">5 </code>   <code class="calibre19">api</code><code class="calibre19">:</code>
<code class="lineno">6 </code>    <code class="calibre19">Image</code><code class="calibre19">:</code> <code class="calibre19">vfarcic/go-demo-5:18.08.08-3</code>
<code class="lineno">7 </code>    <code class="calibre19">...</code>
</pre></div>

</figure>

<p class="calibre3">Now that we explored how to perform the upgrade manually, we’ll try to replicate the same process from a Jenkins job.</p>

<h3 id="leanpub-auto-creating-a-jenkins-job-that-upgrades-the-whole-production-environment" class="calibre20">Creating A Jenkins Job That Upgrades The Whole Production Environment</h3>

<p class="calibre3">Before we upgrade the production environment, we’ll create one more release of <em class="calibre17">go-demo-5</em> so that we have something new to deploy.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/blue/organizations/jenkins/go-demo-5/branches"</code>
</pre></div>

</figure>

<p class="calibre3">We opened the <em class="calibre17">branches</em> screen of the <em class="calibre17">go-demo-5</em> job.</p>

<p class="calibre3">Please click the play button from the right side of the <em class="calibre17">master</em> row and wait until the new build is finished.</p>

<p class="calibre3">Lo and behold! Our build failed! If you explore the job in detail, you will know why it’s broken. You’ll see <em class="calibre17">“Did you forget to increment the Chart version?”</em> message.</p>


<figure class="image1">
  <img src="../images/00045.jpeg" alt="Figure 8-3: go-demo-5 failed build" class="calibre25"/>
  <figcaption class="calibre26">Figure 8-3: go-demo-5 failed build</figcaption>
</figure>


<p class="calibre3">Our job does not allow us to push a commit to the <em class="calibre17">master</em> branch without bumping the version of the <em class="calibre17">go-demo-5</em> Chart. That way, we guarantee that every production-ready release is versioned correctly. Let’s fix that.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">cd</code> ../go-demo-5
</pre></div>

</figure>

<p class="calibre3">Please open <em class="calibre17">helm/go-demo-5/Chart.yaml</em> in your favorite editor and increment the <code class="calibre19">version</code>. If, for example, the current version is <code class="calibre19">0.0.1</code>, change it to <code class="calibre19">0.02</code>, if it’s <code class="calibre19">0.0.2</code>, change it to <code class="calibre19">0.0.3</code>, and so on. You get the point. Just increase it.</p>

<p class="calibre3">Next, we’ll push the change to the <em class="calibre17">master</em> branch.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>git add .
<code class="lineno">2 </code>
<code class="lineno">3 </code>git commit -m <code class="s">"Version bump"</code>
<code class="lineno">4 </code>
<code class="lineno">5 </code>git push
</pre></div>

</figure>

<p class="calibre3">Usually, you’d push the change to a branch, make a pull request, and let someone review it. Such a pull request would execute a Jenkins build that would give the reviewer the information about the quality of the changes. If the build was successful and the review did not reveal any deficiencies, we would merge the pull request.</p>

<p class="calibre3">We skipped all that, and we pushed the changes directly to the <em class="calibre17">master</em> branch only to speed things up.</p>

<p class="calibre3">Now let’s go back to Jenkins and run another build.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/blue/organizations/jenkins/go-demo-5/branches"</code>
</pre></div>

</figure>

<p class="calibre3">Please click the play button from the right side of the <em class="calibre17">master</em> row and wait until the new build is finished. This time, it should be successful, and we’ll have a new <em class="calibre17">go-demo-5</em> release waiting to be deployed to the production environment.</p>

<h3 id="leanpub-auto-automating-upgrade-of-the-production-environment" class="calibre20">Automating Upgrade Of The Production Environment</h3>

<p class="calibre3">Now that we have a new release waiting, we would go through the same process as before. Someone would make a decision whether the release should be deployed to production or to let it rot until the new one comes along. If the decision is made that our users should benefit from the features available in that release, we’d need to update a few files in our <em class="calibre17">k8s-prod</em> repository.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">cd</code> ../k8s-prod
</pre></div>

</figure>

<p class="calibre3">The first file we’ll update is <em class="calibre17">helm/requirements.yaml</em>. Please open it in your favorite editor and change the <em class="calibre17">go-demo-5</em> version to match the version of the Chart we pushed a few moments ago.</p>

<p class="calibre3">We should also increase the version of the <em class="calibre17">prod-env</em> Chart as a whole. Open <em class="calibre17">helm/Chart.yaml</em> and bump the version.</p>

<p class="calibre3">Let’s take a look at <em class="calibre17">Jenkinsfile.orig</em> from the repository.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>cat Jenkinsfile.orig
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="k">import</code> <code class="nn">java.text.SimpleDateFormat</code>
<code class="lineno"> 2 </code>
<code class="lineno"> 3 </code><code class="calibre19">pipeline</code> <code class="o">{</code>
<code class="lineno"> 4 </code>  <code class="calibre19">options</code> <code class="o">{</code>
<code class="lineno"> 5 </code>    <code class="calibre19">buildDiscarder</code> <code class="nf">logRotator</code><code class="o">(</code><code class="nl">numToKeepStr:</code> <code class="s">'5'</code><code class="o">)</code>
<code class="lineno"> 6 </code>    <code class="calibre19">disableConcurrentBuilds</code><code class="o">()</code>
<code class="lineno"> 7 </code>  <code class="o">}</code>
<code class="lineno"> 8 </code>  <code class="calibre19">agent</code> <code class="o">{</code>
<code class="lineno"> 9 </code>    <code class="calibre19">kubernetes</code> <code class="o">{</code>
<code class="lineno">10 </code>      <code class="calibre19">cloud</code> <code class="s">"kubernetes"</code>
<code class="lineno">11 </code>      <code class="calibre19">label</code> <code class="s">"prod"</code>
<code class="lineno">12 </code>      <code class="calibre19">serviceAccount</code> <code class="s">"build"</code>
<code class="lineno">13 </code>      <code class="calibre19">yamlFile</code> <code class="s">"KubernetesPod.yaml"</code>
<code class="lineno">14 </code>    <code class="o">}</code>      
<code class="lineno">15 </code>  <code class="o">}</code>
<code class="lineno">16 </code>  <code class="calibre19">environment</code> <code class="o">{</code>
<code class="lineno">17 </code>    <code class="calibre19">cmAddr</code> <code class="o">=</code> <code class="s">"cm.acme.com"</code>
<code class="lineno">18 </code>  <code class="o">}</code>
<code class="lineno">19 </code>  <code class="calibre19">stages</code> <code class="o">{</code>
<code class="lineno">20 </code>    <code class="calibre19">stage</code><code class="o">(</code><code class="s">"deploy"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">21 </code>      <code class="calibre19">when</code> <code class="o">{</code>
<code class="lineno">22 </code>        <code class="calibre19">branch</code> <code class="s">"master"</code>
<code class="lineno">23 </code>      <code class="o">}</code>
<code class="lineno">24 </code>      <code class="calibre19">steps</code> <code class="o">{</code>
<code class="lineno">25 </code>        <code class="calibre19">container</code><code class="o">(</code><code class="s">"helm"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">26 </code>          <code class="calibre19">sh</code> <code class="s">"helm repo add chartmuseum http://${cmAddr}"</code>
<code class="lineno">27 </code>          <code class="calibre19">sh</code> <code class="s">"helm repo update"</code>
<code class="lineno">28 </code>          <code class="calibre19">sh</code> <code class="s">"helm dependency update helm"</code>
<code class="lineno">29 </code>          <code class="calibre19">sh</code> <code class="s">"helm upgrade -i prod helm --namespace prod --force"</code>
<code class="lineno">30 </code>        <code class="o">}</code>
<code class="lineno">31 </code>      <code class="o">}</code>
<code class="lineno">32 </code>    <code class="o">}</code>
<code class="lineno">33 </code>    <code class="calibre19">stage</code><code class="o">(</code><code class="s">"test"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">34 </code>      <code class="calibre19">when</code> <code class="o">{</code>
<code class="lineno">35 </code>        <code class="calibre19">branch</code> <code class="s">"master"</code>
<code class="lineno">36 </code>      <code class="o">}</code>
<code class="lineno">37 </code>      <code class="calibre19">steps</code> <code class="o">{</code>
<code class="lineno">38 </code>        <code class="calibre19">echo</code> <code class="s">"Testing..."</code>
<code class="lineno">39 </code>      <code class="o">}</code>
<code class="lineno">40 </code>      <code class="calibre19">post</code> <code class="o">{</code>
<code class="lineno">41 </code>        <code class="calibre19">failure</code> <code class="o">{</code>
<code class="lineno">42 </code>          <code class="calibre19">container</code><code class="o">(</code><code class="s">"helm"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">43 </code>            <code class="calibre19">sh</code> <code class="s">"helm rollback prod 0"</code>
<code class="lineno">44 </code>          <code class="o">}</code>
<code class="lineno">45 </code>        <code class="o">}</code>
<code class="lineno">46 </code>      <code class="o">}</code>
<code class="lineno">47 </code>    <code class="o">}</code>
<code class="lineno">48 </code>  <code class="o">}</code>
<code class="lineno">49 </code><code class="o">}</code>
</pre></div>

</figure>

<p class="calibre3">This time we’re using the <code class="calibre19">kubernetes</code> Cloud configured to spin up Pods in the <code class="calibre19">prod</code> Namespace. The <code class="calibre19">build</code> ServiceAccount already has the permissions to access Tiller in <code class="calibre19">kube-system</code> thus allowing us to install applications anywhere inside the cluster. We won’t need to go that far. Full permissions inside the <code class="calibre19">prod</code> Namespace are more than enough.</p>

<p class="calibre3">Just as with the <em class="calibre17">Jenkinsfile</em> inside the <em class="calibre17">go-demo-5</em> repository, the definition of the agent Pod is inside the <code class="calibre19">KubernetesPod.yaml</code> file. I’ll let you explore it yourself.</p>

<p class="calibre3">The <code class="calibre19">environment</code> block contains <code class="calibre19">cmAddr</code> set to <code class="calibre19">cm.acme.com</code>. That’s why we’re exploring <em class="calibre17">Jenkinsfile.orig</em>. We’ll need to create our own <em class="calibre17">Jenkinsfile</em> that will contain the correct address.</p>

<p class="calibre3">We have only two stages; <code class="calibre19">deploy</code> and <code class="calibre19">test</code>. Both of them have the <code class="calibre19">when</code> block that limits the execution of the steps only to builds initiated through a commit to the <code class="calibre19">branch "master"</code>.</p>

<p class="calibre3">The <code class="calibre19">deploy</code> stage runs in the <code class="calibre19">helm</code> container. The steps inside it are performing the same actions we did manually a while ago. They add <code class="calibre19">chartmuseum</code> to the list of repositories, they update the repos, the update the dependencies, and, finally, they <code class="calibre19">upgrade</code> the Chart. Since we already executed all those steps from a terminal, it should be pretty clear what they do.</p>

<p class="calibre3">The <code class="calibre19">test</code> stage has a simple <code class="calibre19">echo</code> step. I’ll be honest with you. I did not write tests we’d need, and the <code class="calibre19">echo</code> is only a placeholder. You should know how to write your own tests for the applications you’re developing, and there’s probably no need for you to see yet another set of tests written in Go.</p>

<p class="calibre3">The critical part of the stage is the <code class="calibre19">post</code> section that’ll rollback the <code class="calibre19">upgrade</code> if one of the tests fail. This is the part where we’re ignoring GitOps principles. The chances that those tests will fail are meager. The new release was already tested, and containers guarantee that our applications will behave the same in any environment. The tests we’re running in this pipeline are more like sanity checks, than some kind of in-depth validation.</p>

<p class="calibre3">If we’d adhere fully to GitOps, if tests do fail, we’d need to change the version of the Chart to the previous value, and we’d need to push it back to the repository. That would trigger yet another build that would perform another upgrade, only this time to the previous release. Instead, we’re rolling back directly inside the pipeline assuming that someone will fix the issue soon after and initiate another upgrade that will contain the correction.</p>

<aside class="warning">
    <h3 id="leanpub-auto-a-note-to-minishift-users-38" class="calibre22">A note to minishift users</h3>

  <p class="calibre3">We already created a Route as a substitute for Ingress. Since, from now on, we’re only updating the <em class="calibre17">go-demo-5</em> application while preserving the related Service, there’s no need to add the <code class="calibre19">oc create route</code> command to the pipeline.</p>

</aside>

<p class="calibre3">As you can see, we are applying GitOps principles only partially. In my opinion, they make sense in some cases and do not in others. It’s up to you to decide whether you’ll go towards full GitOps, or, like me, adopt it only partially.</p>

<p class="calibre3">Now, let’s create <em class="calibre17">Jenkinsfile</em> with the correct address.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>cat Jenkinsfile.orig <code class="se">\</code>
<code class="lineno">2 </code>    <code class="calibre19">|</code> sed -e <code class="s">"s@acme.com@</code><code class="nv">$ADDR</code><code class="s">@g"</code> <code class="se">\</code>
<code class="lineno">3 </code>    <code class="calibre19">|</code> tee Jenkinsfile
</pre></div>

</figure>

<p class="calibre3">With all the files updated, we can proceed and push the changes to GitHub. Just as before, we’re taking a shortcut by skipping the processes of making a pull request, reviewing it, approving it, and executing any other steps that we would normally do.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>git add .
<code class="lineno">2 </code>
<code class="lineno">3 </code>git commit -m <code class="s">"Jenkinsfile"</code>
<code class="lineno">4 </code>
<code class="lineno">5 </code>git push
</pre></div>

</figure>

<p class="calibre3">The only thing left is to create a new Jenkins job and hope that everything works correctly.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$JENKINS_ADDR</code><code class="s">/blue/pipelines"</code>
</pre></div>

</figure>

<p class="calibre3">Please click the <em class="calibre17">New Pipeline</em> button, and select <em class="calibre17">GitHub</em> and the organization. Next, we’ll choose <em class="calibre17">k8s-prod</em> repository and click the <em class="calibre17">Create Pipeline</em> button.</p>

<p class="calibre3">The new job was created, and all we have to do is wait for a few moments until it’s finished.</p>


<figure class="image1">
  <img src="../images/00046.jpeg" alt="Figure 8-4: k8s-prod build screen" class="calibre25"/>
  <figcaption class="calibre26">Figure 8-4: k8s-prod build screen</figcaption>
</figure>


<p class="calibre3">Let’s see the <code class="calibre19">history</code> of the Chart.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>helm <code class="nb">history</code> prod
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>REVISION UPDATED      STATUS     CHART          DESCRIPTION
<code class="lineno">2 </code>1        Wed Aug  ... SUPERSEDED prod-env-0.0.1 Install complete
<code class="lineno">3 </code>2        Wed Aug  ... SUPERSEDED prod-env-0.0.1 Upgrade complete
<code class="lineno">4 </code>3        Wed Aug  ... DEPLOYED   prod-env-0.0.2 Upgrade complete
</pre></div>

</figure>

<p class="calibre3">We can see from the <code class="calibre19">CHART</code> column that the currently deployed release is <code class="calibre19">0.0.2</code> (or whichever version you defined last).</p>

<p class="calibre3">Our system is working! We have a fully operational continuous delivery pipeline.</p>

<h3 id="leanpub-auto-high-level-overview-of-the-continuous-delivery-pipeline" class="calibre20">High-Level Overview Of The Continuous Delivery Pipeline</h3>

<p class="calibre3">Let’s step back and paint a high-level picture of the continuous delivery pipeline we created. To be more precise, we’ll draw a diagram instead of painting anything. But, before we dive into a continuous delivery diagram, we’ll refresh our memory with the one we used before for describing continuous deployment.</p>


<figure class="image1">
  <img src="../images/00047.jpeg" alt="Figure 8-5: Continuous deployment process" class="calibre25"/>
  <figcaption class="calibre26">Figure 8-5: Continuous deployment process</figcaption>
</figure>


<p class="calibre3">The continuous deployment pipeline contains all the steps from pushing a commit to deploying and testing a release in production.</p>

<p class="calibre3">Continuous delivery removes one of the stages from the continuous deployment pipeline. We do NOT want to deploy a new release automatically. Instead, we want humans to decide whether a release should be upgraded in production. If it should, we need to decide when will that happen. Those (human) decisions are, in our case, happening as Git operations. We’ll comment on them soon. For now, the important note is that the <em class="calibre17">deploy</em> stage is now removed from pipelines residing in application repositories.</p>


<figure class="image1">
  <img src="../images/00048.jpeg" alt="Figure 8-6: Continuous deployment process" class="calibre25"/>
  <figcaption class="calibre26">Figure 8-6: Continuous deployment process</figcaption>
</figure>


<p class="calibre3">The fact that our application pipeline (e.g., <em class="calibre17">go-demo-5</em>) does not perform deployment does not mean that it is not automated. The decisions which versions to use and when to initiate the upgrade process is manual, but everything else proceeding those actions is automated.</p>

<p class="calibre3">In our case, there is a separate repository (<em class="calibre17">k8s-prod</em>) that contains a full definition of what constitutes production environment. Whenever we make a decision to install a new application or to upgrade an existing one, we need to update files in <em class="calibre17">k8s-prod</em> and push them to the repository. Whether that push is performed directly to the <em class="calibre17">master</em> branch or to a separate branch, is of no importance to the process that relies solely on the <em class="calibre17">master</em> branch. If you choose to use separate branches (as you should), you can do pull requests, code reviews, and all the other things we usually do with code. But, as I already mentioned, those actions are irrelevant from the automation perspective. The <em class="calibre17">master</em> branch is the one that matters. Once a commit reaches it, it initiates a Webhook request that notifies Jenkins that there is a change and, from there on, we run a build that upgrades the production environment and executes light-weight tests with sanity checks. Except, that we did not set up GitHub Webhooks. I expect that you will have them once you create a “real” cluster with a “real” domain.</p>


<figure class="image">
  <img src="../images/00049.jpeg" alt="Figure 8-7: Continuous deployment process" class="calibre25"/>
  <figcaption class="calibre26">Figure 8-7: Continuous deployment process</figcaption>
</figure>


<p class="calibre3">How does continuous delivery of applications combine with unified deployment to the production environment?</p>

<p class="calibre3">Let’s imagine that we have four applications in total. We’ll call them <em class="calibre17">app 1</em>, <em class="calibre17">app 2</em>, <em class="calibre17">app 3</em>, and <em class="calibre17">app 4</em>. Those applications are developed independently of each other. Whenever we push a commit to the <em class="calibre17">master</em> branch of one of those applications, corresponding continuous delivery pipeline is initiated and, if all the steps are successful, results in a new production-ready release. Pipelines are launched when code is pushed to other branches as well, but in those cases, production-ready releases are NOT created. So, we’ll ignore them in this story.</p>

<p class="calibre3">We are accumulating production-ready releases in those applications and, at some point, someone makes a decision to upgrade the production environment. Those upgrades might involve an update of a single application, or it might entail update of a few. It all depends on the architecture of our applications (sometimes they are not independent), business decisions, and quite a few other criteria. No matter how we made the decision which applications to update and which releases to use, we need to make appropriate changes in the repository that serves as the source of truth of the production environment.</p>

<p class="calibre3">Let’s say that we decided to upgrade app 1 to the release 2 and app 4 to release 4, to install the release 3 of app 2 for the first time, and to leave app 3 intact. In such a situation, we’d bump versions of app 1 and 4 in <code class="calibre19">requirements.yaml</code>. We’d add a new entry for app 2 since that’s the first time we’re installing that application. Finally, we’d leave app 3 in <code class="calibre19">requirements.yaml</code> as-is since we are not planning to upgrade it.</p>

<p class="calibre3">Once we’re finished with modifications to <code class="calibre19">requirements.yaml</code>, all that’s left is to bump the version in <code class="calibre19">Chart.yaml</code> and push the changes directly to master or to make a pull request and merge it after a review. No matter the route, once the change reaches the <em class="calibre17">master</em> branch, it fires a Webhook which, in turn, initiates a new build of the Jenkins job related to the repository. If all of the steps are successful, the Chart representing the production environment is upgraded and, with it, all the applications specified in <code class="calibre19">requirements.yaml</code> are upgraded as well. To be more precise, not all the dependencies are upgraded, but only those we modified. All in all, the production environment will converge to the desired stage after which we’ll execute the last round of tests. If something fails, we roll back. Otherwise, another iteration of production deployments is finished, until we repeat the same process.</p>


<figure class="image1">
  <img src="../images/00050.jpeg" alt="Figure 8-8: Continuous deployment process" class="calibre25"/>
  <figcaption class="calibre26">Figure 8-8: Continuous deployment process</figcaption>
</figure>


<h3 id="leanpub-auto-to-continuously-deploy-or-to-continuously-deliver" class="calibre20">To Continuously Deploy Or To Continuously Deliver?</h3>

<p class="calibre3">Should we use the continuous deployment (CDP) or the continuous delivery (CD) process? That’s a hard question to answer which mainly depends on your internal processes. There are a few questions that might guide us.</p>

<ol class="calibre14">
  <li value="1" class="calibre15">Are your applications truly independent and can be deployed without changing anything else in your cluster?</li>
  <li value="2" class="calibre15">Do you have such a high level of trust in your automated tests that you are confident that there’s no need for manual actions?</li>
  <li value="3" class="calibre15">Are the teams working on applications authorized to make decisions on what to deploy to production and when?</li>
  <li value="4" class="calibre15">Are those teams self-sufficient and do not depend on other teams?</li>
  <li value="5" class="calibre15">Do you really want to upgrade production with every commit to the <em class="calibre17">master</em> branch?</li>
</ol>

<p class="calibre3">If you answered with <em class="calibre17">no</em> to at least one of those questions, you cannot do continuous deployment. You should aim for continuous delivery, or not even that. Continuous delivery is almost as hard to practice as continuous deployment. The chances are that you cannot get there any time soon. If you can’t, that’s still not the end of the world. The lessons from this chapter can be easily modified to serve other processes.</p>

<p class="calibre3">If you answered with <em class="calibre17">no</em> to the second question (the one about tests), you cannot do either of those two processes. It’s not that one requires less confidence in tests than the other. The level of trust is the same. We do not use continuous delivery because we trust our tests less, but because we choose not to deploy every commit to production. Our business might not be ready to deploy every production-ready release. Or, maybe, we need to wait for a marketing campaign to start (I’m ignoring the fact that we’d solve that with feature toggles). There might be many reasons to use continuous delivery instead of deployment, but none of them is technical. Both processes produce production-ready releases, and only one of them deploys it to production automatically.</p>

<p class="calibre3">Now, if you do NOT trust your tests, you need to fall back to continuous integration. Fortunately, the pipeline can be very similar. The major difference is that you should create one more repository (call it <em class="calibre17">k8s-test</em>) and have a similar Jenkinsfile inside it. When you think you’re ready, you’ll bump the versions in that repo and let Jenkins upgrade the test environment. From there on, you can let the army of manual testers do their work. They will surely find more problems than you’re willing to fix but, once they stop finding those that impede you from upgrading the production, you can bump those versions in the repository that describes your production environment (<em class="calibre17">k8s-prod</em>). Apart from different Namespaces and, maybe, reduced number of replicas and Ingress hosts, the two repositories should contain the same Chart with similar dependencies, and changes to their <em class="calibre17">master</em> branch would result in very similar automated processes. You can even skip having the <em class="calibre17">k8s-test</em> repository and create a <em class="calibre17">test-env</em> branch in <em class="calibre17">k8s-prod</em>. That way, you can make changes to <em class="calibre17">test-env</em>, deploy them to the cluster, run manual testing, and, once you’re confident that the release is production-ready, merge the branch to <em class="calibre17">master</em>.</p>

<h3 id="leanpub-auto-what-now-7" class="calibre20">What Now?</h3>

<p class="calibre3">We are finished with the exploration of continuous delivery processes. Destroy the cluster if you created it only for the purpose of this chapter. Have a break. You deserve it.</p>



</div>
</body></html>