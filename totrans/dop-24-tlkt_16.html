<html><head></head><body>
<div class="calibre6">
<h2 id="appendix-b" class="calibre16">Appendix B: Using Kubernetes Operations (kops)</h2>

<p class="calibre3">The text that follows provides the essential information you’ll need to create a Kubernetes cluster in AWS using kops. This appendix contains a few sub-chapters from <a href="https://amzn.to/2GvzDjy">The DevOps 2.3 Toolkit: Kubernetes</a>. Please refer to it for a more detailed information.</p>

<h3 id="leanpub-auto-preparing-for-the-cluster-setup" class="calibre20">Preparing For The Cluster Setup</h3>

<p class="calibre3">We’ll continue using the specifications from the <code class="calibre19">vfarcic/k8s-specs</code> repository, so the first thing we’ll do is to go inside the directory where we cloned it, and pull the latest version.</p>

<aside class="information">
    <p class="calibre3">All the commands from this appendix are available in the <a href="https://gist.github.com/49bccadae317379bef6f81b4e5985f84">99-appendix-b.sh</a> Gist.</p>

</aside>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">cd</code> k8s-specs
<code class="lineno">2 </code>
<code class="lineno">3 </code>git pull
</pre></div>

</figure>

<p class="calibre3">I will assume that you already have an AWS account. If that’s not the case, please head over to <a href="https://aws.amazon.com/">Amazon Web Services</a> and sign-up.</p>

<aside class="information">
    <p class="calibre3">If you are already proficient with AWS, you might want to skim through the text that follows and only execute the commands.</p>

</aside>

<p class="calibre3">The first thing we should do is get the AWS credentials.</p>

<p class="calibre3">Please open <a href="https://console.aws.amazon.com/ec2/">Amazon EC2 Console</a>, click on your name from the top-right menu and select <em class="calibre17">My Security Credentials</em>. You will see the screen with different types of credentials. Expand the <em class="calibre17">Access Keys (Access Key ID and Secret Access Key)</em> section and click the <em class="calibre17">Create New Access Key</em> button. Expand the <em class="calibre17">Show Access Key</em> section to see the keys.</p>

<p class="calibre3">You will not be able to view the keys later on, so this is the only chance you’ll have to <em class="calibre17">Download Key File</em>.</p>

<p class="calibre3">We’ll put the keys as environment variables that will be used by the <a href="https://aws.amazon.com/cli/">AWS Command Line Interface (AWS CLI)</a>.</p>

<p class="calibre3">Please replace <code class="calibre19">[...]</code> with your keys before executing the commands that follow.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">export</code> <code class="nv">AWS_ACCESS_KEY_ID</code><code class="o">=[</code>...<code class="o">]</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code><code class="nb">export</code> <code class="nv">AWS_SECRET_ACCESS_KEY</code><code class="o">=[</code>...<code class="o">]</code>
</pre></div>

</figure>

<p class="calibre3">We’ll need to install <a href="https://aws.amazon.com/cli/">AWS Command Line Interface (CLI)</a> and gather info about your account.</p>

<p class="calibre3">If you haven’t already, please open the <a href="http://docs.aws.amazon.com/cli/latest/userguide/installing.html">Installing the AWS Command Line Interface</a> page, and follow the installation method best suited for your OS.</p>

<aside class="tip">
    <h3 id="leanpub-auto-a-note-to-windows-users-7" class="calibre22">A note to Windows users</h3>

  <p class="calibre3">I found the most convenient way to get AWS CLI installed on Windows is to use <a href="https://chocolatey.org/">Chocolatey</a>.  Download and install Chocolatey, then run <code class="calibre19">choco install awscli</code> from an Administrator Command Prompt. Later on in the chapter, Chocolatey will be used to install jq.</p>

</aside>

<p class="calibre3">Once you’re done, we’ll confirm that the installation was successful by outputting the version.</p>

<aside class="warning">
    <h3 id="leanpub-auto-a-note-to-windows-users-8" class="calibre22">A note to Windows users</h3>

  <p class="calibre3">You might need to reopen your <em class="calibre17">GitBash</em> terminal for the changes to the environment variable <code class="calibre19">PATH</code> to take effect.</p>

</aside>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>aws --version
</pre></div>

</figure>

<p class="calibre3">The output (from my laptop) is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>aws-cli/1.11.15 Python/2.7.10 Darwin/16.0.0 botocore/1.4.72
</pre></div>

</figure>

<p class="calibre3">Amazon EC2 is hosted in multiple locations worldwide. These locations are composed of regions and availability zones. Each region is a separate geographic area composed of multiple isolated locations known as availability zones. Amazon EC2 provides you the ability to place resources, such as instances, and data in multiple locations.</p>

<p class="calibre3">Next, we’ll define the environment variable <code class="calibre19">AWS_DEFAULT_REGION</code> that will tell AWS CLI which region we’d like to use by default.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">export</code> <code class="nv">AWS_DEFAULT_REGION</code><code class="o">=</code>us-east-2
</pre></div>

</figure>

<p class="calibre3">For now, please note that you can change the value of the variable to any other region, as long as it has at least three availability zones. We’ll discuss the reasons for using <code class="calibre19">us-east-2</code> region and the need for multiple availability zones soon.</p>

<p class="calibre3">Next, we’ll create a few Identity and Access Management (IAM) resources. Even though we could create a cluster with the user you used to register to AWS, it is a good practice to create a separate account that contains only the privileges we’ll need for the exercises that follow.</p>

<p class="calibre3">First, we’ll create an IAM group called <code class="calibre19">kops</code>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>aws iam create-group <code class="se">\</code>
<code class="lineno">2 </code>    --group-name kops
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="calibre19">{</code>
<code class="lineno">2 </code>    <code class="k">"Group"</code><code class="calibre19">:</code> <code class="calibre19">{</code>
<code class="lineno">3 </code>        <code class="k">"Path"</code><code class="calibre19">:</code> <code class="s">"/"</code><code class="calibre19">,</code>
<code class="lineno">4 </code>        <code class="k">"CreateDate"</code><code class="calibre19">:</code> <code class="s">"2018-02-21T12:58:47.853Z"</code><code class="calibre19">,</code>
<code class="lineno">5 </code>        <code class="k">"GroupId"</code><code class="calibre19">:</code> <code class="s">"AGPAIF2Y6HJF7YFYQBQK2"</code><code class="calibre19">,</code>
<code class="lineno">6 </code>        <code class="k">"Arn"</code><code class="calibre19">:</code> <code class="s">"arn:aws:iam::036548781187:group/kops"</code><code class="calibre19">,</code>
<code class="lineno">7 </code>        <code class="k">"GroupName"</code><code class="calibre19">:</code> <code class="s">"kops"</code>
<code class="lineno">8 </code>    <code class="calibre19">}</code>
<code class="lineno">9 </code><code class="calibre19">}</code>
</pre></div>

</figure>

<p class="calibre3">We don’t care much for any of the information from the output except that it does not contain an error message thus confirming that the group was created successfully.</p>

<p class="calibre3">Next, we’ll assign a few policies to the group thus providing the future users of the group with sufficient permissions to create the objects we’ll need.</p>

<p class="calibre3">Since our cluster will consist of <a href="https://aws.amazon.com/ec2">EC2</a> instances, the group will need to have the permissions to create and manage them. We’ll need a place to store the state of the cluster so we’ll need access to <a href="https://aws.amazon.com/s3">S3</a>. Furthermore, we need to add <a href="https://aws.amazon.com/vpc/">VPCs</a> to the mix so that our cluster is isolated from prying eyes. Finally, we’ll need to be able to create additional IAMs.</p>

<p class="calibre3">In AWS, user permissions are granted by creating policies. We’ll need <code class="calibre19">AmazonEC2FullAccess</code>, <code class="calibre19">AmazonS3FullAccess</code>, <code class="calibre19">AmazonVPCFullAccess</code>, and <code class="calibre19">IAMFullAccess</code>.</p>

<p class="calibre3">The commands that attach the required policies to the <code class="calibre19">kops</code> group are as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>aws iam attach-group-policy <code class="se">\</code>
<code class="lineno"> 2 </code>    --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess <code class="se">\</code>
<code class="lineno"> 3 </code>    --group-name kops
<code class="lineno"> 4 </code>
<code class="lineno"> 5 </code>aws iam attach-group-policy <code class="se">\</code>
<code class="lineno"> 6 </code>    --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess <code class="se">\</code>
<code class="lineno"> 7 </code>    --group-name kops
<code class="lineno"> 8 </code>
<code class="lineno"> 9 </code>aws iam attach-group-policy <code class="se">\</code>
<code class="lineno">10 </code>    --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess <code class="se">\</code>
<code class="lineno">11 </code>    --group-name kops
<code class="lineno">12 </code>
<code class="lineno">13 </code>aws iam attach-group-policy <code class="se">\</code>
<code class="lineno">14 </code>    --policy-arn arn:aws:iam::aws:policy/IAMFullAccess <code class="se">\</code>
<code class="lineno">15 </code>    --group-name kops
</pre></div>

</figure>

<p class="calibre3">Now that we have a group with the sufficient permissions, we should create a user as well.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>aws iam create-user <code class="se">\</code>
<code class="lineno">2 </code>    --user-name kops
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="calibre19">{</code>
<code class="lineno">2 </code>    <code class="k">"User"</code><code class="calibre19">:</code> <code class="calibre19">{</code>
<code class="lineno">3 </code>        <code class="k">"UserName"</code><code class="calibre19">:</code> <code class="s">"kops"</code><code class="calibre19">,</code>
<code class="lineno">4 </code>        <code class="k">"Path"</code><code class="calibre19">:</code> <code class="s">"/"</code><code class="calibre19">,</code>
<code class="lineno">5 </code>        <code class="k">"CreateDate"</code><code class="calibre19">:</code> <code class="s">"2018-02-21T12:59:28.836Z"</code><code class="calibre19">,</code>
<code class="lineno">6 </code>        <code class="k">"UserId"</code><code class="calibre19">:</code> <code class="s">"AIDAJ22UOS7JVYQIAVMWA"</code><code class="calibre19">,</code>
<code class="lineno">7 </code>        <code class="k">"Arn"</code><code class="calibre19">:</code> <code class="s">"arn:aws:iam::036548781187:user/kops"</code>
<code class="lineno">8 </code>    <code class="calibre19">}</code>
<code class="lineno">9 </code><code class="calibre19">}</code>
</pre></div>

</figure>

<p class="calibre3">Just as when we created the group, the contents of the output are not important, except as a confirmation that the command was executed successfully.</p>

<p class="calibre3">The user we created does not yet belong to the <code class="calibre19">kops</code> group. We’ll fix that next.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>aws iam add-user-to-group <code class="se">\</code>
<code class="lineno">2 </code>    --user-name kops <code class="se">\</code>
<code class="lineno">3 </code>    --group-name kops
</pre></div>

</figure>

<p class="calibre3">Finally, we’ll need access keys for the newly created user. Without them, we would not be able to act on its behalf.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>aws iam create-access-key <code class="se">\</code>
<code class="lineno">2 </code>    --user-name kops &gt;kops-creds
</pre></div>

</figure>

<p class="calibre3">We created access keys and stored the output in the <code class="calibre19">kops-creds</code> file. Let’s take a quick look at its content.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>cat kops-creds
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="calibre19">{</code>
<code class="lineno">2 </code>    <code class="k">"AccessKey"</code><code class="calibre19">:</code> <code class="calibre19">{</code>
<code class="lineno">3 </code>        <code class="k">"UserName"</code><code class="calibre19">:</code> <code class="s">"kops"</code><code class="calibre19">,</code>
<code class="lineno">4 </code>        <code class="k">"Status"</code><code class="calibre19">:</code> <code class="s">"Active"</code><code class="calibre19">,</code>
<code class="lineno">5 </code>        <code class="k">"CreateDate"</code><code class="calibre19">:</code> <code class="s">"2018-02-21T13:00:24.733Z"</code><code class="calibre19">,</code>
<code class="lineno">6 </code>        <code class="k">"SecretAccessKey"</code><code class="calibre19">:</code> <code class="s">"..."</code><code class="calibre19">,</code>
<code class="lineno">7 </code>        <code class="k">"AccessKeyId"</code><code class="calibre19">:</code> <code class="s">"..."</code>
<code class="lineno">8 </code>    <code class="calibre19">}</code>
<code class="lineno">9 </code><code class="calibre19">}</code>
</pre></div>

</figure>

<p class="calibre3">Please note that I removed the values of the keys. I do not yet trust you enough with the keys of my AWS account.</p>

<p class="calibre3">We need the <code class="calibre19">SecretAccessKey</code> and <code class="calibre19">AccessKeyId</code> entries. So, the next step is to parse the content of the <code class="calibre19">kops-creds</code> file and store those two values as the environment variables <code class="calibre19">AWS_ACCESS_KEY_ID</code> and <code class="calibre19">AWS_SECRET_ACCESS_KEY</code>.</p>

<p class="calibre3">In the spirit of full automation, we’ll use <a href="https://stedolan.github.io/jq/">jq</a> to parse the contents of the <code class="calibre19">kops-creds</code> file. Please download and install the distribution suited for your OS.</p>

<aside class="tip">
    <h3 id="leanpub-auto-a-note-to-windows-users-9" class="calibre22">A note to Windows users</h3>

  <p class="calibre3">Using Chocolatey, install <code class="calibre19">jq</code> from an Administrator Command Prompt via <code class="calibre19">choco install jq</code>.</p>

</aside>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">export</code> <code class="nv">AWS_ACCESS_KEY_ID</code><code class="o">=</code><code class="k">$(</code><code class="se">\</code>
<code class="lineno">2 </code>    cat kops-creds <code class="calibre19">|</code> jq -r <code class="se">\</code>
<code class="lineno">3 </code>    <code class="s">'.AccessKey.AccessKeyId'</code><code class="k">)</code>
<code class="lineno">4 </code>
<code class="lineno">5 </code><code class="nb">export</code> <code class="nv">AWS_SECRET_ACCESS_KEY</code><code class="o">=</code><code class="k">$(</code>
<code class="lineno">6 </code>    cat kops-creds <code class="calibre19">|</code> jq -r <code class="se">\</code>
<code class="lineno">7 </code>    <code class="s">'.AccessKey.SecretAccessKey'</code><code class="k">)</code>
</pre></div>

</figure>

<p class="calibre3">We used <code class="calibre19">cat</code> to output contents of the file and combined it with <code class="calibre19">jq</code> to filter the input so that only the field we need is retrieved.</p>

<p class="calibre3">From now on, all the AWS CLI commands will not be executed by the administrative user you used to register to AWS, but as <code class="calibre19">kops</code>.</p>

<aside class="warning">
    <p class="calibre3">It is imperative that the <code class="calibre19">kops-creds</code> file is secured and not accessible to anyone but people you trust. The best method to secure it depends from one organization to another. No matter what you do, do not write it on a post-it and stick it to your monitor. Storing it in one of your GitHub repositories is even worse.</p>

</aside>

<p class="calibre3">Next, we should decide which availability zones we’ll use. So, let’s take a look at what’s available in the <code class="calibre19">us-east-2</code> region.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>aws ec2 describe-availability-zones <code class="se">\</code>
<code class="lineno">2 </code>    --region <code class="nv">$AWS_DEFAULT_REGION</code>
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">{</code>
<code class="lineno"> 2 </code>    <code class="k">"AvailabilityZones"</code><code class="calibre19">:</code> <code class="calibre19">[</code>
<code class="lineno"> 3 </code>        <code class="calibre19">{</code>
<code class="lineno"> 4 </code>            <code class="k">"State"</code><code class="calibre19">:</code> <code class="s">"available"</code><code class="calibre19">,</code> 
<code class="lineno"> 5 </code>            <code class="k">"RegionName"</code><code class="calibre19">:</code> <code class="s">"us-east-2"</code><code class="calibre19">,</code> 
<code class="lineno"> 6 </code>            <code class="k">"Messages"</code><code class="calibre19">:</code> <code class="calibre19">[],</code> 
<code class="lineno"> 7 </code>            <code class="k">"ZoneName"</code><code class="calibre19">:</code> <code class="s">"us-east-2a"</code>
<code class="lineno"> 8 </code>        <code class="calibre19">},</code> 
<code class="lineno"> 9 </code>        <code class="calibre19">{</code>
<code class="lineno">10 </code>            <code class="k">"State"</code><code class="calibre19">:</code> <code class="s">"available"</code><code class="calibre19">,</code> 
<code class="lineno">11 </code>            <code class="k">"RegionName"</code><code class="calibre19">:</code> <code class="s">"us-east-2"</code><code class="calibre19">,</code> 
<code class="lineno">12 </code>            <code class="k">"Messages"</code><code class="calibre19">:</code> <code class="calibre19">[],</code> 
<code class="lineno">13 </code>            <code class="k">"ZoneName"</code><code class="calibre19">:</code> <code class="s">"us-east-2b"</code>
<code class="lineno">14 </code>        <code class="calibre19">},</code> 
<code class="lineno">15 </code>        <code class="calibre19">{</code>
<code class="lineno">16 </code>            <code class="k">"State"</code><code class="calibre19">:</code> <code class="s">"available"</code><code class="calibre19">,</code> 
<code class="lineno">17 </code>            <code class="k">"RegionName"</code><code class="calibre19">:</code> <code class="s">"us-east-2"</code><code class="calibre19">,</code> 
<code class="lineno">18 </code>            <code class="k">"Messages"</code><code class="calibre19">:</code> <code class="calibre19">[],</code> 
<code class="lineno">19 </code>            <code class="k">"ZoneName"</code><code class="calibre19">:</code> <code class="s">"us-east-2c"</code>
<code class="lineno">20 </code>        <code class="calibre19">}</code>
<code class="lineno">21 </code>    <code class="calibre19">]</code>
<code class="lineno">22 </code><code class="calibre19">}</code>
</pre></div>

</figure>

<p class="calibre3">As we can see, the region has three availability zones. We’ll store them in an environment variable.</p>

<aside class="warning">
    <h3 id="leanpub-auto-a-note-to-windows-users-10" class="calibre22">A note to Windows users</h3>

  <p class="calibre3">Please use <code class="calibre19">tr '\r\n' ', '</code> instead of <code class="calibre19">tr '\n' ','</code> in the command that follows.</p>

</aside>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="nb">export</code> <code class="nv">ZONES</code><code class="o">=</code><code class="k">$(</code>aws ec2 <code class="se">\</code>
<code class="lineno"> 2 </code>    describe-availability-zones <code class="se">\</code>
<code class="lineno"> 3 </code>    --region <code class="nv">$AWS_DEFAULT_REGION</code> <code class="se">\</code>
<code class="lineno"> 4 </code>    <code class="calibre19">|</code> jq -r <code class="se">\</code>
<code class="lineno"> 5 </code>    <code class="s">'.AvailabilityZones[].ZoneName'</code> <code class="se">\</code>
<code class="lineno"> 6 </code>    <code class="calibre19">|</code> tr <code class="s">'\n'</code> <code class="s">','</code> <code class="calibre19">|</code> tr -d <code class="s">' '</code><code class="k">)</code>
<code class="lineno"> 7 </code>
<code class="lineno"> 8 </code><code class="nv">ZONES</code><code class="o">=</code><code class="si">${</code><code class="nv">ZONES</code><code class="calibre19">%?</code><code class="si">}</code>
<code class="lineno"> 9 </code>
<code class="lineno">10 </code><code class="nb">echo</code> <code class="nv">$ZONES</code>
</pre></div>

</figure>

<p class="calibre3">Just as with the access keys, we used <code class="calibre19">jq</code> to limit the results only to the zone names, and we combined that with <code class="calibre19">tr</code> that replaced new lines with commas. The second command removes the trailing comma.</p>

<p class="calibre3">The output of the last command that echoed the values of the environment variable is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>us-east-2a,us-east-2b,us-east-2c
</pre></div>

</figure>

<p class="calibre3">We’ll discuss the reasons behind the usage of three availability zones later on. For now, just remember that they are stored in the environment variable <code class="calibre19">ZONES</code>.</p>

<p class="calibre3">The last preparation step is to create SSH keys required for the setup. Since we might create some other artifacts during the process, we’ll create a directory dedicated to the creation of the cluster.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>mkdir -p cluster
<code class="lineno">2 </code>
<code class="lineno">3 </code><code class="nb">cd</code> cluster
</pre></div>

</figure>

<p class="calibre3">SSH keys can be created through the <code class="calibre19">aws ec2</code> command <code class="calibre19">create-key-pair</code>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>aws ec2 create-key-pair <code class="se">\</code>
<code class="lineno">2 </code>    --key-name devops23 <code class="se">\</code>
<code class="lineno">3 </code>    <code class="calibre19">|</code> jq -r <code class="s">'.KeyMaterial'</code> <code class="se">\</code>
<code class="lineno">4 </code>    &gt;devops23.pem
</pre></div>

</figure>

<p class="calibre3">We created a new key pair, filtered the output so that only the <code class="calibre19">KeyMaterial</code> is returned, and stored it in the <code class="calibre19">devops.pem</code> file.</p>

<p class="calibre3">For security reasons, we should change the permissions of the <code class="calibre19">devops23.pem</code> file so that only the current user can read it.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>chmod <code class="o">400</code> devops23.pem
</pre></div>

</figure>

<p class="calibre3">Finally, we’ll need only the public segment of the newly generated SSH key, so we’ll use <code class="calibre19">ssh-keygen</code> to extract it.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ssh-keygen -y -f devops23.pem <code class="se">\</code>
<code class="lineno">2 </code>    &gt;devops23.pub
</pre></div>

</figure>

<p class="calibre3">All those steps might look a bit daunting if this is your first contact with AWS. Nevertheless, they are pretty standard. No matter what you do in AWS, you’d need to perform, more or less, the same actions. Not all of them are mandatory, but they are good practice. Having a dedicated (non-admin) user and a group with only required policies is always a good idea. Access keys are necessary for any <code class="calibre19">aws</code> command. Without SSH keys, no one can enter into a server.</p>

<p class="calibre3">The good news is that we’re finished with the prerequisites, and we can turn our attention towards creating a Kubernetes cluster.</p>

<h3 id="leanpub-auto-creating-a-kubernetes-cluster-in-aws" class="calibre20">Creating A Kubernetes Cluster In AWS</h3>

<p class="calibre3">We’ll start by deciding the name of our soon to be created cluster. We’ll choose to call it <code class="calibre19">devops23.k8s.local</code>. The latter part of the name (<code class="calibre19">.k8s.local</code>) is mandatory if we do not have a DNS at hand. It’s a naming convention kops uses to decide whether to create a gossip-based cluster or to rely on a publicly available domain. If this would be a “real” production cluster, you would probably have a DNS for it. However, since I cannot be sure whether you do have one for the exercises in this book, we’ll play it safe, and proceed with the gossip mode.</p>

<p class="calibre3">We’ll store the name into an environment variable so that it is easily accessible.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">export</code> <code class="nv">NAME</code><code class="o">=</code>devops23.k8s.local
</pre></div>

</figure>

<p class="calibre3">When we create the cluster, kops will store its state in a location we’re about to configure. If you used Terraform, you’ll notice that kops uses a very similar approach. It uses the state it generates when creating the cluster for all subsequent operations. If we want to change any aspect of a cluster, we’ll have to change the desired state first, and then apply those changes to the cluster.</p>

<p class="calibre3">At the moment, when creating a cluster in AWS, the only option for storing the state are <a href="https://aws.amazon.com/s3">Amazon S3</a> buckets. We can expect availability of additional stores soon. For now, S3 is our only option.</p>

<p class="calibre3">The command that creates an S3 bucket in our region is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">export</code> <code class="nv">BUCKET_NAME</code><code class="o">=</code>devops23-<code class="k">$(</code>date +%s<code class="k">)</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>aws s3api create-bucket <code class="se">\</code>
<code class="lineno">4 </code>    --bucket <code class="nv">$BUCKET_NAME</code> <code class="se">\</code>
<code class="lineno">5 </code>    --create-bucket-configuration <code class="se">\</code>
<code class="lineno">6 </code>    <code class="nv">LocationConstraint</code><code class="o">=</code><code class="nv">$AWS_DEFAULT_REGION</code>
</pre></div>

</figure>

<p class="calibre3">We created a bucket with a unique name and the output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="calibre19">{</code>
<code class="lineno">2 </code>    <code class="k">"Location"</code><code class="calibre19">:</code> <code class="s">"http://devops23-1519993212.s3.amazonaws.com/"</code>
<code class="lineno">3 </code><code class="calibre19">}</code>
</pre></div>

</figure>

<p class="calibre3">For simplicity, we’ll define the environment variable <code class="calibre19">KOPS_STATE_STORE</code>. Kops will use it to know where we store the state. Otherwise, we’d need to use <code class="calibre19">--store</code> argument with every <code class="calibre19">kops</code> command.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">export</code> <code class="nv">KOPS_STATE_STORE</code><code class="o">=</code>s3://<code class="nv">$BUCKET_NAME</code>
</pre></div>

</figure>

<p class="calibre3">There’s only one thing missing before we create the cluster. We need to install kops.</p>

<p class="calibre3">If you are a <strong class="calibre18">MacOS user</strong>, the easiest way to install <code class="calibre19">kops</code> is through <a href="https://brew.sh/">Homebrew</a>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>brew update <code class="o">&amp;&amp;</code> brew install kops
</pre></div>

</figure>

<p class="calibre3">As an alternative, we can download a release from GitHub.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>curl -Lo kops https://github.com/kubernetes/kops/releases/download/<code class="k">$(</code>curl -s https:/<code class="se">\</code>
<code class="lineno">2 </code>/api.github.com/repos/kubernetes/kops/releases/latest <code class="calibre19">|</code> grep tag_name <code class="calibre19">|</code> cut -d <code class="s">'"'</code> -<code class="se">\</code>
<code class="lineno">3 </code>f <code class="o">4</code><code class="k">)</code>/kops-darwin-amd64
<code class="lineno">4 </code>
<code class="lineno">5 </code>chmod +x ./kops
<code class="lineno">6 </code>
<code class="lineno">7 </code>sudo mv ./kops /usr/local/bin/
</pre></div>

</figure>

<p class="calibre3">If, on the other hand, you’re a <strong class="calibre18">Linux user</strong>, the commands that will install <code class="calibre19">kops</code> are as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>wget -O kops https://github.com/kubernetes/kops/releases/download/<code class="k">$(</code>curl -s https://<code class="se">\</code>
<code class="lineno">2 </code>api.github.com/repos/kubernetes/kops/releases/latest <code class="calibre19">|</code> grep tag_name <code class="calibre19">|</code> cut -d <code class="s">'"'</code> -f<code class="se">\</code>
<code class="lineno">3 </code> <code class="o">4</code><code class="k">)</code>/kops-linux-amd64
<code class="lineno">4 </code>
<code class="lineno">5 </code>chmod +x ./kops
<code class="lineno">6 </code>
<code class="lineno">7 </code>sudo mv ./kops /usr/local/bin/
</pre></div>

</figure>

<p class="calibre3">Finally, if you are a <strong class="calibre18">Windows user</strong>, you cannot install <code class="calibre19">kops</code>. At the time of this writing, its releases do not include Windows binaries. Don’t worry. I am not giving up on you, dear <em class="calibre17">Windows user</em>. We’ll manage to overcome the problem soon by exploiting Docker’s ability to run any Linux application. The only requirement is that you have <a href="https://www.docker.com/docker-windows">Docker For Windows</a> installed.</p>

<p class="calibre3">I already created a Docker image that contains <code class="calibre19">kops</code> and its dependencies. So, we’ll create an alias <code class="calibre19">kops</code> that will create a container instead running a binary. The result will be the same.</p>

<p class="calibre3">The command that creates the <code class="calibre19">kops</code> alias is as follows. Execute it only if you are a <strong class="calibre18">Windows user</strong>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>mkdir config
<code class="lineno"> 2 </code>
<code class="lineno"> 3 </code><code class="nb">alias</code> <code class="nv">kops</code><code class="o">=</code><code class="s">"docker run -it --rm \</code>
<code class="lineno"> 4 </code><code class="s">    -v </code><code class="nv">$PWD</code><code class="s">/devops23.pub:/devops23.pub \</code>
<code class="lineno"> 5 </code><code class="s">    -v </code><code class="nv">$PWD</code><code class="s">/config:/config \</code>
<code class="lineno"> 6 </code><code class="s">    -e KUBECONFIG=/config/kubecfg.yaml \</code>
<code class="lineno"> 7 </code><code class="s">    -e NAME=</code><code class="nv">$NAME</code><code class="s"> -e ZONES=</code><code class="nv">$ZONES</code><code class="s"> \</code>
<code class="lineno"> 8 </code><code class="s">    -e AWS_ACCESS_KEY_ID=</code><code class="nv">$AWS_ACCESS_KEY_ID</code><code class="s"> \</code>
<code class="lineno"> 9 </code><code class="s">    -e AWS_SECRET_ACCESS_KEY=</code><code class="nv">$AWS_SECRET_ACCESS_KEY</code><code class="s"> \</code>
<code class="lineno">10 </code><code class="s">    -e KOPS_STATE_STORE=</code><code class="nv">$KOPS_STATE_STORE</code><code class="s"> \</code>
<code class="lineno">11 </code><code class="s">    vfarcic/kops"</code>
</pre></div>

</figure>

<p class="calibre3">We won’t go into details of all the arguments the <code class="calibre19">docker run</code> command uses. Their usage will become clear when we start using <code class="calibre19">kops</code>. Just remember that we are passing all the environment variables we might use as well as mounting the SSH key and the directory where <code class="calibre19">kops</code> will store <code class="calibre19">kubectl</code> configuration.</p>

<p class="calibre3">We are, finally, ready to create a cluster. But, before we do that, we’ll spend a bit of time discussing the requirements we might have. After all, not all clusters are created equal, and the choices we are about to make might severely impact our ability to accomplish the goals we might have.</p>

<p class="calibre3">The first question we might ask ourselves is whether we want to have high-availability. It would be strange if anyone would answer no. Who doesn’t want to have a cluster that is (almost) always available? Instead, we’ll ask ourselves what the things that might bring our cluster down are.</p>

<p class="calibre3">When a node is destroyed, Kubernetes will reschedule all the applications that were running inside it into the healthy nodes. All we have to do is to make sure that, later on, a new server is created and joined the cluster, so that its capacity is back to the desired values. We’ll discuss later how are new nodes created as a reaction to failures of a server. For now, we’ll assume that will happen somehow.</p>

<p class="calibre3">Still, there is a catch. Given that new nodes need to join the cluster, if the failed server was the only master, there is no cluster to join. All is lost. The part is where master servers are. They host the critical components without which Kubernetes cannot operate.</p>

<p class="calibre3">So, we need more than one master node. How about two? If one fails, we still have the other one. Still, that would not work.</p>

<p class="calibre3">Every piece of information that enters one of the master nodes is propagated to the others, and only after the majority agrees, that information is committed. If we lose majority (50%+1), masters cannot establish a quorum and cease to operate. If one out of two masters is down, we can get only half of the votes, and we would lose the ability to establish the quorum. Therefore, we need three masters or more. Odd numbers greater than one are “magic” numbers. Given that we won’t create a big cluster, three should do.</p>

<p class="calibre3">With three masters, we are safe from a failure of any single one of them. Given that failed servers will be replaced with new ones, as long as only one master fails at the time, we should be fault tolerant and have high availability.</p>

<aside class="tip">
    <p class="calibre3">Always set an odd number greater than one for master nodes.</p>

</aside>

<p class="calibre3">The whole idea of having multiple masters does not mean much if an entire data center goes down.</p>

<p class="calibre3">Attempts to prevent a data center from failing are commendable. Still, no matter how well a data center is designed, there is always a scenario that might cause its disruption. So, we need more than one data center. Following the logic behind master nodes, we need at least three. But, as with almost anything else, we cannot have any three (or more) data centers. If they are too far apart, the latency between them might be too high. Since every piece of information is propagated to all the masters in a cluster, slow communication between data centers would severely impact the cluster as a whole.</p>

<p class="calibre3">All in all, we need three data centers that are close enough to provide low latency, and yet physically separated, so that failure of one does not impact the others. Since we are about to create the cluster in AWS, we’ll use availability zones (AZs) which are physically separated data centers with low latency.</p>

<aside class="tip">
    <p class="calibre3">Always spread your cluster between at least three data centers which are close enough to warrant low latency.</p>

</aside>

<p class="calibre3">There’s more to high-availability to running multiple masters and spreading a cluster across multiple availability zones. We’ll get back to this subject later. For now, we’ll continue exploring the other decisions we have to make.</p>

<p class="calibre3">Which networking shall we use? We can choose between <em class="calibre17">kubenet</em>, <em class="calibre17">CNI</em>, <em class="calibre17">classic</em>, and <em class="calibre17">external</em> networking.</p>

<p class="calibre3">The classic Kubernetes native networking is deprecated in favor of kubenet, so we can discard it right away.</p>

<p class="calibre3">The external networking is used in some custom implementations and for particular use cases, so we’ll discard that one as well.</p>

<p class="calibre3">That leaves us with kubenet and CNI.</p>

<p class="calibre3">Container Network Interface (CNI) allows us to plug in a third-party networking driver. Kops supports <a href="http://docs.projectcalico.org/v2.0/getting-started/kubernetes/installation/hosted/">Calico</a>, <a href="https://github.com/coreos/flannel">flannel</a>, <a href="https://github.com/projectcalico/canal">Canal (Flannel + Calico)</a>, <a href="https://github.com/kopeio/networking">kopeio-vxlan</a>, <a href="https://github.com/kubernetes/kops/blob/master/docs/networking.md#kube-router-example-for-cni-ipvs-based-service-proxy-and-network-policy-enforcer">kube-router</a>, <a href="https://github.com/romana/romana">romana</a>, <a href="https://github.com/weaveworks/weave-kube">weave</a>, and <a href="https://github.com/kubernetes/kops/blob/master/docs/networking.md#amazon-vpc-backend">amazon-vpc-routed-eni</a> networks. Each of those networks comes with pros and cons and differs in its implementation and primary objectives. Choosing between them would require a detailed analysis of each. We’ll leave a comparison of all those for some other time and place. Instead, we’ll focus on <code class="calibre19">kubenet</code>.</p>

<p class="calibre3">Kubenet is kops’ default networking solution. It is Kubernetes native networking, and it is considered battle tested and very reliable. However, it comes with a limitation. On AWS, routes for each node are configured in AWS VPC routing tables. Since those tables cannot have more than fifty entries, kubenet can be used in clusters with up to fifty nodes. If you’re planning to have a cluster bigger than that, you’ll have to switch to one of the previously mentioned CNIs.</p>

<aside class="tip">
    <p class="calibre3">Use kubenet networking if your cluster is smaller than fifty nodes.</p>

</aside>

<p class="calibre3">The good news is that using any of the networking solutions is easy. All we have to do is specify the <code class="calibre19">--networking</code> argument followed with the name of the network.</p>

<p class="calibre3">Given that we won’t have the time and space to evaluate all the CNIs, we’ll use kubenet as the networking solution for the cluster we’re about to create. I encourage you to explore the other options on your own (or wait until I write a post or a new book).</p>

<p class="calibre3">Finally, we are left with only one more choice we need to make. What will be the size of our nodes? Since we won’t run many applications, <em class="calibre17">t2.small</em> should be more than enough and will keep AWS costs to a minimum. <em class="calibre17">t2.micro</em> is too small, so we elected the second smallest among those AWS offers.</p>

<aside class="information">
    <p class="calibre3">You might have noticed that we did not mention persistent volumes. We’ll explore them in the next chapter.</p>

</aside>

<p class="calibre3">The command that creates a cluster using the specifications we discussed is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>kops create cluster <code class="se">\</code>
<code class="lineno"> 2 </code>    --name <code class="nv">$NAME</code> <code class="se">\</code>
<code class="lineno"> 3 </code>    --master-count <code class="o">3</code> <code class="se">\</code>
<code class="lineno"> 4 </code>    --node-count <code class="o">1</code> <code class="se">\</code>
<code class="lineno"> 5 </code>    --node-size t2.small <code class="se">\</code>
<code class="lineno"> 6 </code>    --master-size t2.small <code class="se">\</code>
<code class="lineno"> 7 </code>    --zones <code class="nv">$ZONES</code> <code class="se">\</code>
<code class="lineno"> 8 </code>    --master-zones <code class="nv">$ZONES</code> <code class="se">\</code>
<code class="lineno"> 9 </code>    --ssh-public-key devops23.pub <code class="se">\</code>
<code class="lineno">10 </code>    --networking kubenet <code class="se">\</code>
<code class="lineno">11 </code>    --authorization RBAC <code class="se">\</code>
<code class="lineno">12 </code>    --yes
</pre></div>

</figure>

<p class="calibre3">We specified that the cluster should have three masters and one worker node. Remember, we can always increase the number of workers, so there’s no need to start with more than what we need at the moment.</p>

<p class="calibre3">The sizes of both worker nodes and masters are set to <code class="calibre19">t2.small</code>. Both types of nodes will be spread across the three availability zones we specified through the environment variable <code class="calibre19">ZONES</code>. Further on, we defined the public key and the type of networking.</p>

<p class="calibre3">We used <code class="calibre19">--kubernetes-version</code> to specify that we prefer to run version <code class="calibre19">v1.8.4</code>. Otherwise, we’d get a cluster with the latest version considered stable by kops. Even though running latest stable version is probably a good idea, we’ll need to be a few versions behind to demonstrate some of the features kops has to offer.</p>

<p class="calibre3">By default, kops sets <code class="calibre19">authorization</code> to <code class="calibre19">AlwaysAllow</code>. Since this is a simulation of a production-ready cluster, we changed it to <code class="calibre19">RBAC</code>, which we already explored in one of the previous chapters.</p>

<p class="calibre3">The <code class="calibre19">--yes</code> argument specifies that the cluster should be created right away. Without it, <code class="calibre19">kops</code> would only update the state in the S3 bucket, and we’d need to execute <code class="calibre19">kops apply</code> to create the cluster. Such two-step approach is preferable, but I got impatient and would like to see the cluster in all its glory as soon as possible.</p>

<p class="calibre3">The output of the command is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>...
<code class="lineno"> 2 </code>kops has set your kubectl context to devops23.k8s.local
<code class="lineno"> 3 </code>
<code class="lineno"> 4 </code>Cluster is starting.  It should be ready in a few minutes.
<code class="lineno"> 5 </code>
<code class="lineno"> 6 </code>Suggestions:
<code class="lineno"> 7 </code> * validate cluster: kops validate cluster
<code class="lineno"> 8 </code> * list nodes: kubectl get nodes --show-labels
<code class="lineno"> 9 </code> * ssh to the master: ssh -i ~/.ssh/id_rsa admin@api.devops23.k8s.local
<code class="lineno">10 </code>The admin user is specific to Debian. If not using Debian please use the appropriate\
<code class="lineno">11 </code> user based on your OS.
<code class="lineno">12 </code> * read about installing addons: https://github.com/kubernetes/kops/blob/master/docs\
<code class="lineno">13 </code>/addons.md
</pre></div>

</figure>

<p class="calibre3">We can see that the <code class="calibre19">kubectl</code> context was changed to point to the new cluster which is starting, and will be ready soon. Further down are a few suggestions of the next actions. We’ll skip them, for now.</p>

<aside class="warning">
    <h3 id="leanpub-auto-a-note-to-windows-users-11" class="calibre22">A note to Windows users</h3>

  <p class="calibre3">Kops was executed inside a container. It changed the context inside the container that is now gone. As a result, your local <code class="calibre19">kubectl</code> context was left intact. We’ll fix that by executing <code class="calibre19">kops export kubecfg --name ${NAME}</code> and <code class="calibre19">export KUBECONFIG=$PWD/config/kubecfg.yaml</code>. The first command exported the config to <code class="calibre19">/config/kubecfg.yaml</code>. That path was specified through the environment variable <code class="calibre19">KUBECONFIG</code> and is mounted as <code class="calibre19">config/kubecfg.yaml</code> on local hard disk. The latter command exports <code class="calibre19">KUBECONFIG</code> locally. Through that variable, <code class="calibre19">kubectl</code> is now instructed to use the configuration in <code class="calibre19">config/kubecfg.yaml</code> instead of the default one. Before you run those commands, please give AWS a few minutes to create all the EC2 instances and for them to join the cluster. After waiting and executing those commands, you’ll be all set.</p>

</aside>

<p class="calibre3">We’ll use kops to retrieve the information about the newly created cluster.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kops get cluster
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>NAME               CLOUD ZONES
<code class="lineno">2 </code>devops23.k8s.local aws   us-east-2a,us-east-2b,us-east-2c
</pre></div>

</figure>

<p class="calibre3">This information does not tell us anything new. We already knew the name of the cluster and the zones it runs in.</p>

<p class="calibre3">How about <code class="calibre19">kubectl cluster-info</code>?</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kubectl cluster-info
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>Kubernetes master is running at https://api-devops23-k8s-local-ivnbim-609446190.us-e\
<code class="lineno">2 </code>ast-2.elb.amazonaws.com
<code class="lineno">3 </code>KubeDNS is running at https://api-devops23-k8s-local-ivnbim-609446190.us-east-2.elb.\
<code class="lineno">4 </code>amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
<code class="lineno">5 </code>
<code class="lineno">6 </code>To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
</pre></div>

</figure>

<p class="calibre3">We can see that the master is running as well as KubeDNS. The cluster is probably ready. If in your case KubeDNS did not appear in the output, you might need to wait for a few more minutes.</p>

<p class="calibre3">We can get more reliable information about the readiness of our new cluster through the <code class="calibre19">kops validate</code> command.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kops validate cluster
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>Using cluster from kubectl context: devops23.k8s.local
<code class="lineno"> 2 </code>
<code class="lineno"> 3 </code>Validating cluster devops23.k8s.local
<code class="lineno"> 4 </code>
<code class="lineno"> 5 </code>INSTANCE GROUPS
<code class="lineno"> 6 </code>NAME              ROLE   MACHINETYPE MIN MAX SUBNETS
<code class="lineno"> 7 </code>master-us-east-2a Master t2.small    1   1   us-east-2a
<code class="lineno"> 8 </code>master-us-east-2b Master t2.small    1   1   us-east-2b
<code class="lineno"> 9 </code>master-us-east-2c Master t2.small    1   1   us-east-2c
<code class="lineno">10 </code>nodes             Node   t2.small    1   1   us-east-2a,us-east-2b,us-east-2c
<code class="lineno">11 </code>
<code class="lineno">12 </code>NODE STATUS
<code class="lineno">13 </code>NAME                 ROLE   READY
<code class="lineno">14 </code>ip-172-20-120-133... master True
<code class="lineno">15 </code>ip-172-20-34-249...  master True
<code class="lineno">16 </code>ip-172-20-65-28...   master True
<code class="lineno">17 </code>ip-172-20-95-101...  node   True
<code class="lineno">18 </code>
<code class="lineno">19 </code>Your cluster devops23.k8s.local is ready
</pre></div>

</figure>

<p class="calibre3">That is useful. We can see that the cluster uses four instance groups or, to use AWS terms, four auto-scaling groups (ASGs). There’s one for each master, and there’s one for all the (worker) nodes.</p>

<p class="calibre3">The reason each master has a separate ASG lies in need to ensure that each is running in its own availability zone (AZ). That way we can guarantee that failure of the whole AZ will affect only one master. Nodes (workers), on the other hand, are not restricted to any specific AZ. AWS is free to schedule nodes in any AZ that is available.</p>

<p class="calibre3">We’ll discuss ASGs in more detail later on.</p>

<p class="calibre3">Further down the output, we can see that there are four servers, three with masters, and one with worker node. All are ready.</p>

<p class="calibre3">Finally, we got the confirmation that our <code class="calibre19">cluster devops23.k8s.local is ready</code>.</p>

<h3 id="leanpub-auto-installing-ingress-and-tiller-server-side-helm" class="calibre20">Installing Ingress And Tiller (Server Side Helm)</h3>

<p class="calibre3">To install Ingres, please execute the commands that follow.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kubectl create <code class="se">\</code>
<code class="lineno">2 </code>    -f https://raw.githubusercontent.com/kubernetes/kops/master/addons/ingress-nginx<code class="se">\</code>
<code class="lineno">3 </code>/v1.6.0.yaml
<code class="lineno">4 </code>
<code class="lineno">5 </code>kubectl -n kube-ingress <code class="se">\</code>
<code class="lineno">6 </code>    rollout status <code class="se">\</code>
<code class="lineno">7 </code>    deployment ingress-nginx
</pre></div>

</figure>

<h3 id="leanpub-auto-destroying-the-cluster" class="calibre20">Destroying The Cluster</h3>

<p class="calibre3">The appendix is almost finished, and we do not need the cluster anymore. We want to destroy it as soon as possible. There’s no good reason to keep it running when we’re not using it. But, before we proceed with the destructive actions, we’ll create a file that will hold all the environment variables we used in this chapter. That will help us the next time we want to recreate the cluster.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">echo</code> <code class="s">"export AWS_ACCESS_KEY_ID=</code><code class="nv">$AWS_ACCESS_KEY_ID</code><code class="s"/>
<code class="lineno">2 </code><code class="s">export AWS_SECRET_ACCESS_KEY=</code><code class="nv">$AWS_SECRET_ACCESS_KEY</code><code class="s"/>
<code class="lineno">3 </code><code class="s">export AWS_DEFAULT_REGION=</code><code class="nv">$AWS_DEFAULT_REGION</code><code class="s"/>
<code class="lineno">4 </code><code class="s">export ZONES=</code><code class="nv">$ZONES</code><code class="s"/>
<code class="lineno">5 </code><code class="s">export NAME=</code><code class="nv">$NAME</code><code class="s">"</code> <code class="se">\</code>
<code class="lineno">6 </code>    &gt;kops
</pre></div>

</figure>

<p class="calibre3">We echoed the variables with the values into the <code class="calibre19">kops</code> file, and now we can delete the cluster.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>kops delete cluster <code class="se">\</code>
<code class="lineno">2 </code>    --name <code class="nv">$NAME</code> <code class="se">\</code>
<code class="lineno">3 </code>    --yes
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>...
<code class="lineno">2 </code>Deleted kubectl config for devops23.k8s.local
<code class="lineno">3 </code>
<code class="lineno">4 </code>Deleted cluster: "devops23.k8s.local"
</pre></div>

</figure>

<p class="calibre3">Kops removed references of the cluster from our <code class="calibre19">kubectl</code> configuration and proceeded to delete all the AWS resources it created. Our cluster is no more. We can proceed and delete the S3 bucket as well.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>aws s3api delete-bucket <code class="se">\</code>
<code class="lineno">2 </code>    --bucket <code class="nv">$BUCKET_NAME</code>
</pre></div>

</figure>

</div>
</body></html>