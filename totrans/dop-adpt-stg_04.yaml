- en: '*Chapter 3*: Measuring the Success of DevOps'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have to be able to point to metrics and measurements that show the success
    of DevOps within your organization. Selecting the right metrics is critical to
    showcasing your progress, ensuring teams stay aligned with the vision and empowerment
    of people. This chapter looks at the various metrics used in DevOps and how to
    measure success.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Common metrics used to measure success
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing metrics for your team
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating rollups at an organizational level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common metrics used to measure success
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Firstly, it's important to know why to measure your performance. I speak to
    many leaders of various businesses, and a frightening trend is that they all think
    measuring success is a tool that can be used to help with performance management.
  prefs: []
  type: TYPE_NORMAL
- en: The reality is that tracking of performance is a tool for improvement. **Continuous
    improvement** (**CI**) is a key pillar of DevOps, so if you have no idea how you
    are performing, how can you improve? Improvement should be the main goal of the
    metrics used in DevOps, ones that can drive tangible results and highlight growth
    areas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we look at the metrics you can use, I like to put them into three buckets.
    Then, as you will see later in the chapter, depending on the type of team you
    are running, you can pick appropriate metrics from each bucket to look at your
    performance and generate useful methods of feedback. The number of metrics you
    pick from each bucket depends on your goals and your style of team. Have a look
    at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Venn diagram showing the relationship of velocity, quality,
    and stability'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17192_03_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 – Venn diagram showing the relationship of velocity, quality, and
    stability
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind the preceding diagram is to illustrate that in an ideal world,
    you have a balance of metrics from each of the three categories, but you can have
    scenarios where you have more from one category. In all scenarios, you will notice
    that stability is present throughout.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following possibilities exist in this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Velocity** + **Stability**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quality** + **Stability**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Velocity** + **Quality** + **Stability**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stability is core because whatever we are doing within our organization, no
    matter what changes we are going through, stability should be central to what
    we do, and in no circumstances should we impact this.
  prefs: []
  type: TYPE_NORMAL
- en: First, let's look at the metrics you would associate with velocity. In DevOps,
    when we talk about velocity, we mean working with both speed and direction.
  prefs: []
  type: TYPE_NORMAL
- en: Common velocity metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Velocity is vitally important in DevOps, as we are going on a journey that
    tries to break down silos in your organization and improve collaboration and communication.
    Having metrics that look at velocity can be very useful when it comes to highlighting
    areas for improvement. With that in mind, let''s look at some of the common velocity
    metrics, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Deployment duration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment frequency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change volume
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test automation coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lead time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cycle time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment failure rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environment provisioning time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From looking at these metrics at a high level, let's now look at them in more
    detail to understand what each one means.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment duration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deployment duration is the amount of time it takes to execute your **continuous
    deployment** (**CD**) pipeline. If you are producing a build and running a deployment
    at the same time rather than just picking up the latest build artifact, then record
    both the CI and CD pipelines, but make sure you have a way of knowing how long
    each pipeline is taking to execute. Most tooling provides you with the ability
    to look at the start and end time of each pipeline and the steps executed within.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment frequency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Measuring deployment frequency enables you to look at how many times you deploy.
    In mature organizations, the target is to deploy numerous times a day. Whether
    you do or not is dependent on several factors.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Charting progress over time with an increasing number of deployments can show
    real progress in your DevOps transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Change volume
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In DevOps, there is often a common misconception that you don't follow the normal
    change management procedures. In reality, it's quite the opposite—transparency
    is important, and there is no better tool for transparency in service management
    than change management. You can measure the number of changes sprint to sprint,
    or even just monthly, to get an idea of the number of releases you are shipping.
  prefs: []
  type: TYPE_NORMAL
- en: Test automation coverage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Test automation is also a key part of automation in DevOps. When talking about
    measuring coverage in test automation, we mean the amount of the application or
    code base that is covered by automated tests.
  prefs: []
  type: TYPE_NORMAL
- en: Lead time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In DevOps, if you are looking to ship features quickly, then lead time is an
    important metric for you to measure. Lead time is the amount of elapsed time between
    adding an item to the backlog and that item shipping to release. This lets you
    measure how long it takes on average to take an item from backlog to production.
  prefs: []
  type: TYPE_NORMAL
- en: Cycle time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Very similar to lead time is cycle time. The slight difference in this metric
    is that rather than measuring from when an item is added to the backlog to when
    that item is shipped, cycle time looks at the time from when work on that item
    is started to when it is completed, or shipped.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment failure rate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Identifying the rate of failure in deployment helps teams determine the quality
    of code and testing, moving from other stages to production. It is a leading indicator
    for code and pipeline maturity. Failure of deployments is obviously something
    you need to know about and monitoring can help with this, but recording your deployment
    failure rate as a percentage is also important. This lets you understand how often
    your deployments fail. Mature organizations look for a value below 5% for large
    volumes of deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Environment provisioning time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When using **infrastructure as code** (**IaC**) to deploy your environments,
    much like when measuring deployment duration, environment provisioning time allows
    you to understand how long it takes to deploy your environment. In environments
    with a high number of microservices this is a great metric, as you will be able
    to see as you deploy more microservices how your provisioning time hopefully decreases.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: As your organization progresses through maturity, it's useful to see where you
    have come from on your journey. Track this metric from the beginning so that you
    can see the progress you are making.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at some of the metrics associated with quality.
  prefs: []
  type: TYPE_NORMAL
- en: Common quality metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we discussed earlier, measuring stability is important; second on that list
    is quality. You can have a high velocity, meaning you are working at a fast rate,
    but the quality may suffer because of that. This isn''t a scenario you want, as
    low quality starts to erode trust in what you are doing and how you are doing
    it. Here are some common quality metrics you can use in your organization:'
  prefs: []
  type: TYPE_NORMAL
- en: Defect density
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defect aging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unit test coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code vulnerabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standards violations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defect reintroduction rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we understand the quality metrics we can use, let's look at them again
    in more detail to understand what they mean.
  prefs: []
  type: TYPE_NORMAL
- en: Defect density
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can measure defect density in several different ways. The most common way
    is to calculate the number of defects per 1,000 lines of code. Using this metric
    is useful to help with sprint planning. You can, over time, use this metric to
    estimate the number of defects you may be presented with from sprint to sprint.
  prefs: []
  type: TYPE_NORMAL
- en: With the adoption of **integrated development environments** (**IDEs**) and
    automation tools it can be hard to identify lines of code, but it is still an
    important metric, and most development tools will be able to get past this limitation.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The calculation for defect density is defect count/**lines of code** (**LOC**)
    of the release. Note that this is on the specific release, not the overall code
    base.
  prefs: []
  type: TYPE_NORMAL
- en: Defect aging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a valuable metric to measure, and is simply the measure of time between
    a defect getting reported to the backlog and the current date, provided that defect
    is still open. Tracking this metric is important when it comes to technical debt.
    It allows you to understand how long on average you keep defects open for before
    they are resolved.
  prefs: []
  type: TYPE_NORMAL
- en: Code quality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we talk about code quality, it's easy to think we are talking about the
    number of standards violations. We're going to talk about that metric as another
    quality metric you can use. In this context, when we talk about code quality,
    we mean in the overall context of an application. This can be represented as a
    percentage of the overall application. The degrading part of this metric is the
    number of violations against code quality, defined by many of the rulesets available
    for whichever language you are programming in.
  prefs: []
  type: TYPE_NORMAL
- en: Unit test coverage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Coverage of unit testing is measured as a percentage. It covers the percentage
    of the application that is covered by unit tests written by developers. In **test-driven
    deployment** (**TDD**) environments where the tests are written before the functional
    code, organizations look for 80% coverage as an absolute minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Code vulnerabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Scanning your code for known vulnerabilities is a fundamental aspect of good
    security practice. For this reason, understanding the number of vulnerabilities
    by release is a key metric. You can introduce vulnerabilities in other areas of
    your application when you write new features or fix others. Tracking this metric
    then becomes important for ensuring you are following good security practices.
  prefs: []
  type: TYPE_NORMAL
- en: Standards violations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Static analysis tools can look at your source code in detail and highlight areas
    of code that do not conform to standards. These are generally community-driven
    or professionally set standards. However, some tools allow organizations to set
    their own rules for standards. This metric provides you with information and insights
    on how your developers are developing to standards baselines.
  prefs: []
  type: TYPE_NORMAL
- en: Defect reintroduction rate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite what you might think, this metric tracks the effectiveness of your developers'
    local testing. We are measuring with this metric the number of defects that are
    reported as breaking other functionality and causing other defects to be raised.
    You will sometimes see this metric being called *defect leakage*.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let's look at the common metrics for stability. You will recognize
    some of these if you have a service management background.
  prefs: []
  type: TYPE_NORMAL
- en: Common stability metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Stability is critical—just as poor quality can erode trust internally and within
    your customer base, so can poor stability. Nobody wants to use a product or platform
    that is not stable. Instrumentation is designed to help you understand what is
    happening and how it affects stability. The following metrics exist to help you
    measure stability:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean Time to Recovery** (**MTTR**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment downtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change failure rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incidents per deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unapproved changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of hotfixes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Platform availability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's now look at these common stability metrics in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: MTTR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I find this metric powerful and more useful than measuring availability, especially
    in the cloud world, where availability of the platform is less within your control
    than within a traditional data center environment. Measuring MTTR looks at the
    time from when the system or product fails to when it is available again. Over
    time, this calculates an average that you want to see decreasing over time.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment downtime
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This interesting metric looks over time at the average time your application
    or product is unavailable during a deployment. You can measure this as a percentage
    of overall availability over the month or sprint, or measure the specific blocks
    of time.
  prefs: []
  type: TYPE_NORMAL
- en: Change failure rate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we discussed earlier, it's important to make use of change management, own
    your failures, and measure the change failure rate as a percentage of changes
    implemented. This may be something the change management team already measures,
    but it is recommended to make specific measurement for your DevOps teams.
  prefs: []
  type: TYPE_NORMAL
- en: Incidents per deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is no better metric to understand the impact releases make on your user
    community than by tracking the number of incidents raised per deployment. Systems
    such as ServiceNow have the ability to link in releases with incidents, so it's
    easy to see which release the incident is attributed to. This can go back in the
    backlog as a bug.
  prefs: []
  type: TYPE_NORMAL
- en: Unapproved changes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Any good change management function will track the number of unauthorized or
    unapproved changes on a platform. Some of them may be emergency releases and waiting
    for paperwork to catch up, but some of these may be genuine and represent learning
    opportunities.
  prefs: []
  type: TYPE_NORMAL
- en: Number of hotfixes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is all well and good measuring the number of deployments you make and how
    quickly they happen, but what about the number of bug fixes or hotfixes you release?
    Looking to put measures in place to reduce this number is also a key differentiator
    between immature and mature DevOps organizations.
  prefs: []
  type: TYPE_NORMAL
- en: Platform availability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a typical metric that looks at measuring the time of availability of
    a platform, but representing this as a percentage. In its most basic form, the
    higher the percentage, the more available the platform was. Some organizations
    have credit schemes to compensate clients who do not get over a contractually
    agreed availability threshold.
  prefs: []
  type: TYPE_NORMAL
- en: That wraps up our look at the common metrics you can use to measure success
    in DevOps. But how do we apply these in meaningful scenarios, and what sort of
    baseline targets should you be looking at?
  prefs: []
  type: TYPE_NORMAL
- en: Designing metrics for your team
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we understand the key metrics that are involved in DevOps, it is next
    important to understand where those metrics can be used and in which scenarios.
    You can have too many metrics that you track in an organization, and these can
    then be counterproductive.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing which metrics to use depends on many different parameters. However,
    we will now look at some example scenarios, describe what the goal of their DevOps
    transformation is, and look at the metrics that will help them identify their
    success.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scenario 1: Small organization with a dedicated DevOps team'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For small organizations, one thing that is common between them is their ability
    to become more agile and break down silos that exist between teams. Smaller teams
    allow for faster feedback loops and cycle time. In fact, most small organizations
    have fewer silos overall, and some may have no silos.
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, let's imagine we have a dedicated DevOps team at our organization,
    comprising six people. This organization runs one single product, which is sold
    to customers on a **software-as-a-service** (**SaaS**) basis.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the interaction is very simple. The team works well together
    due to the size of the organization, and roles and responsibilities are well defined.
    As with most organizations of this size, with the growth they have seen comes
    teething issues, such as a drop in quality due to pressure to execute.
  prefs: []
  type: TYPE_NORMAL
- en: 'For them, it''s important to focus on stability as well as quality, to ensure
    that high quality leads to better stability. Let''s now have a look at four metrics
    they could use and why, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**MTTR**—Understanding how long it takes to recover the application platform
    is critical. The organization needs to look at how the platform needs to evolve
    in the future. This is important as the platform grows and scales, and information
    discovered here can lead to architectural improvements that reduce the average
    recovery time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Platform availability** (> 99%)—Providing a contractual incentive to keep
    the platform available may help improve stability, but be warned: it could also
    cause unwanted pressure on the team and make the problem worse. Simple measurement
    and a discussion on what causes the downtime and how to fix it longer-term is
    much more productive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unit test coverage** (> 80%)—Ensuring good coverage of testing is very important.
    As this organization suffers from high levels of defects, ensuring good unit test
    coverage will ensure better testing is performed and that code is performing as
    expected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Defect density** (< 1/1,000 lines)—Releases at this organization have presented
    problems before. Understanding the density of defects will help them plan better,
    as well as understand where the problems are when they are developing and which
    ones transpire into defects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's now look at a different scenario for a medium-sized organization with
    an advocacy team.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scenario 2: Medium organization with advocacy team'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this scenario, our organization has separate operations and development
    teams, and they're trying to work better together with the help of an advocacy
    team. Their aim is to facilitate the right level of collaboration and communication
    between them using different techniques, while still continuing with their day-to-day
    work.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in the previous chapter, advocacy teams are not given specific
    deliverable tasks in the sprint team, but are there to drive forward the best
    practices of DevOps and help the team achieve the goals set out for them.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a team which is of a medium size, stability as well as quality is important
    to them on their journey, but understanding velocity is also important. The team
    needs a broad view of their performance over time so that adjustments can be made
    as they become more mature. Let''s look at the metrics this team can use to track
    their performance, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lead time**—Tracking lead time allows them to understand where time is used,
    from the allocation of a backlog item to when it is delivered. This helps the
    team plan better in the future, give appropriate estimates, and help identify
    areas where processes can be streamlined.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cycle time**—Similarly, understanding the average time taken from work starting
    to shipping also gives the team metrics that help them improve their estimation
    and planning meetings, delivering over time to improve customer satisfaction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unit test coverage**—As a new team in DevOps, having high-quality code is
    important, but understanding where you are now is even more important. This helps
    highlight the amount of technical debt inherited by the lack of quality unit test
    coverage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code quality**—In a similar way to unit test coverage, this metric will help
    the team understand where skills gaps with their developers may exist and can
    be improved by targeting trouble areas.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MTTR**—Remember: stability is important, as is understanding how long it
    takes to recover service. This information for the team feeds back into their
    improvement cycles to again help them improve.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment downtime**—Finally, any new team at DevOps needs to understand
    the impact of their work during releases. Measuring the downtime of your releases
    helps you improve the automation process in the future, or even move away from
    manual deployments to automated ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's now look at a large organization scenario where they have numerous DevOps
    teams.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scenario 3: Large organization with numerous DevOps teams'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you have a large organization with numerous DevOps teams of various sizes,
    it's important to make sure each team focuses on their own priorities in terms
    of what their goals are. The overall goal of the business must remain in sight,
    though, and metrics can help ensure that the goal is tracked.
  prefs: []
  type: TYPE_NORMAL
- en: For this scenario, our large organization is looking to increase the pace of
    development and release across the board. Of course, as we discussed earlier in
    the chapter, this cannot be at the expense of stability.
  prefs: []
  type: TYPE_NORMAL
- en: Their challenge from a DevOps perspective is changing ways of working that have
    been carried out in a legacy fashion for a number of years, and some red tape
    exists that makes the process changes difficult and slow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now look at the metrics they can use to ensure the wider outcome of
    increased pace is achieved, while keeping an eye on stability, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lead time**—Understanding how quickly things are dealt with from the backlog
    is important, especially in environments where teams are looking to pivot quickly
    and improve results. This can help you understand what you need to do in terms
    of making sure your processes are lean.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment frequency**—Where the goal is to improve the release cadence,
    this metric is a must. You can understand how often you deploy, and do this in
    conjunction with other metrics here. Make sure that is not just a number but a
    number of quality releases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Change failure rate**—Mistakes happen, especially in fast-moving environments.
    We can use this metric to help all teams understand if the releases they are doing
    are of high quality, not in terms of functionality but through adherence to the
    existing change management policies in place as they change the way they deploy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of hotfixes**—It''s OK to release hotfixes; they''re a staple of the
    development life cycle. Tracking the number of hotfixes can help teams understand
    stability, but can also evaluate quality in parallel. It''s a really useful metric
    to use in fast environments looking for quick change, but as discussed previously,
    mistakes can happen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In these types of organizations, it can easily be the case that teams go their
    own way. Keeping them stitched together in terms of the overall goal is tricky,
    but finding common metrics can help explain that. Teams may have the same metrics,
    but leading and lagging indicators may be different based on products or acumen.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's now look at another small organization scenario, this time with an outsourced
    DevOps team.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scenario 4: Small organization with outsourced DevOps team'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For some smaller organizations who are looking to reap the benefits that the
    adoption of DevOps can bring them, outsourcing can be used to enable a specialist
    third-party team to work with the organization to achieve a number of goals.
  prefs: []
  type: TYPE_NORMAL
- en: This could be assistance with delivery, execution of agile methodologies, or
    support of environments and providing automation as part of the whole solution.
    Third parties can be used in numerous ways, and depending on the size of the organization
    and their requirements, this will change the scope of the third-party involvement.
  prefs: []
  type: TYPE_NORMAL
- en: For our small organization, a big focus for them is around the need to provide
    higher levels of automation, especially around testing. This will really help
    them drive forward where they are with DevOps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now look at the metrics we can use for this team, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Test automation coverage**—Due to the size of the team, they have outsourced
    test automation. Use this metric to look at the coverage of automation provided,
    and build up this number over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment failure rate**—Deployment failure rates have many focuses, but
    this team has decided to look at failed testing gates. Using this metric will
    help the team understand what is failing, how often, and—through discovery—why
    it is happening.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment downtime**—In a similar way to the preceding metric, tracking
    the amount of downtime in deployments can help with your third-party interactions.
    This can help you both work on and improve the CI and CD pipelines within your
    organization as you do more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Platform availability**—Understanding how the third party is working within
    your environment is critical. Understanding platform availability is essential,
    and holding them to account when they make mistakes that cause outages is something
    you would need to consider. This needs to be handled properly, with no aggressive
    tones and an attitude of working together to improve things rather than penalizing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In all four scenarios, you could use various different metrics to measure yourself;
    however, that doesn't mean that some metrics are worse than others. It comes down
    to what you are trying to measure, and you measure what you are trying to improve
    overall.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have looked at the various metrics you can use in different scenarios,
    what happens when you have multiple teams practicing, as in *Scenario 3*? How
    do you ensure that you report at an appropriate level? Let's look at the answers
    in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Creating rollups at an organizational level
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regardless of if you are practicing DevOps in your organization or not, clear
    communication is one of the keys to success. This is also true when it comes to
    communication of your **key performance indicators** (**KPIs**).
  prefs: []
  type: TYPE_NORMAL
- en: You must ensure that the data you present back to leaders within your organization
    is clear, concise, and tells the appropriate picture about the performance within
    your organization.
  prefs: []
  type: TYPE_NORMAL
- en: In DevOps, especially when you are communicating organization-wide progress,
    you will first have to go on a journey of explaining what the metrics mean to
    the wider business. It's not immediately obvious what the metrics mean and show.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Try to display clear wording to executive leaders, even if this means changing
    the explanation of the metric. It's easier to relate it to something they understand
    than having to face questions on how it's measured, why you measure it, and more
    in executive meetings.
  prefs: []
  type: TYPE_NORMAL
- en: Another critical factor in DevOps, especially when measuring velocity, is to
    understand that not all teams are equal. Even from the inside, when it appears
    teams are delivering very similar things, the way they work and the way they operate
    as a team means the velocity of both teams is unlikely to be a comparable metric.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, I would *never* recommend comparing teams by using plain metrics
    such as velocity measured in **story points**. Teams can use this metric internally
    to see how effective they are at planning the work assigned to them and, throughout
    sprints, use the output from the previous sprint to see how they perform and where
    they can be better at planning.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: If you are using story points to measure velocity of completed user stories,
    never make this metric public on executive dashboards.
  prefs: []
  type: TYPE_NORMAL
- en: Reporting when multiple teams work on one product
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If your organization has multiple teams working on one product and each team
    is responsible for a different part of the product, then creating a rollup is
    quite simple. As with any project, you would report the overall progress against
    any plans. The same can be said in this scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Each individual team may be working on individual features and requirements
    from different business analysts, but they will be working for—and aligned to—one
    common goal. For that reason, you need to understand what the end goal looks like,
    and from there you can create metrics that measure that goal.
  prefs: []
  type: TYPE_NORMAL
- en: This style is what might be known as an executive scorecard, or sometimes a
    business scorecard. It lists out the KPIs that show if you are on a path to success,
    or if blockers are in your way.
  prefs: []
  type: TYPE_NORMAL
- en: Reporting when multiple teams work on multiple products
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you have multiple teams working on multiple products, you can employ similar
    tactics to those outlined previously. Think of each product team as one, and create
    reports that reflect the work done by that team on that product.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember the previous discussion: no two teams are equal, and the same is said
    regardless of whether they are in the same product group or different product
    groups. Be careful not to compare teams across different products, even if they
    are working on the same deliverable, just in different products.'
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your organization, your multiple products may be completely unrelated,
    in which case it does not make any sense to create reporting that rolls up performance
    to a higher level.
  prefs: []
  type: TYPE_NORMAL
- en: If, for example, you are an organization that has products related to one another
    by a higher piece of marketing (maybe your organization has an overarching product
    that is actually made up of numerous products), then try where you can to align
    your reporting to that top level.
  prefs: []
  type: TYPE_NORMAL
- en: It is the top level that is understood across the business, so when it comes
    to reporting the velocity, quality, or stability metrics we discussed earlier
    in the chapter, make sure they relate to the highest level you can go that makes
    practical sense.
  prefs: []
  type: TYPE_NORMAL
- en: Creating goals that are S.M.A.R.T
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Creating goals for your product or taking goals from an executive level and
    then disseminating them down to your team to be more actionable chunks of work
    can be a difficult task.
  prefs: []
  type: TYPE_NORMAL
- en: Within your department, you may need to break up a higher-level goal into more
    manageable goals between different teams. This is when the collaboration and communication
    with DevOps comes into its own. When one larger goal is split into numerous goals
    for smaller teams, working together and speaking to each other is critical in
    ensuring you achieve the fundamental task.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common tool in the business world for setting measurable and achievable goals
    is using the **S.M.A.R.T** method. If this isn''t something you have heard of
    before, this is what it stands for:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Specific**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Measurable**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Achievable**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Realistic**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Timely**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are different versions of S.M.A.R.T. goals, but these are the definitions
    I prefer. It really means that to set a proper goal, it has to be something that
    answers the following five questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What exactly do you want to do?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do you know when you have reached it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the goal within your power to achieve?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is it realistic that you can achieve this goal?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When do you want to accomplish the goal?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I have used this method many different times before, you can find a lot more
    details about this method from *Mind Tools* ([https://www.mindtools.com/pages/article/smart-goals.htm](https://www.mindtools.com/pages/article/smart-goals.htm)).
  prefs: []
  type: TYPE_NORMAL
- en: 'One easy example to follow is that you want to become trained in a specific
    tool—for example: *I want to understand how to create pipelines in Azure DevOps*.
    How would we now go about making this goal S.M.A.R.T.? Here''s how:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Specific**—I want to learn how to create pipelines using **YAML Ain''t Markup
    Language** (**YAML**) in Azure DevOps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Measurable**—Ability to create working pipelines to deploy *Application X*
    without assistance from our **subject-matter experts** (**SMEs**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Achievable**—I need to learn how to build basic pipelines, then understand
    our own process so that I can learn appropriate items to add into the pipeline
    to complete the build.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Realistic**—By using online videos, working with our experts, and taking
    online courses I am able to achieve this goal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Timely**—I will have achieved this in 6 months.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you use the model shown here, you provide clarity toward the goals you
    are trying to achieve, how you plan to achieve them, what you need to achieve
    them, and—finally—when you will achieve them.
  prefs: []
  type: TYPE_NORMAL
- en: You may have multiple lines in a sheet describing your various goals, and you
    may use steps to describe the ways in which you will get there. The key is getting
    it down on paper.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have looked at some of the most common metrics you can use
    to measure success in DevOps and looked at ensuring the importance of defining
    what success looks like. We have looked through some scenarios of different teams,
    highlighting the metrics that can be used to track their success. Finally, we
    looked at how to ensure you track at an organizational level rather than focusing
    too much on individual teams.
  prefs: []
  type: TYPE_NORMAL
- en: One of the biggest challenges in DevOps is measuring success. Using the skills
    you have learned in this chapter, you can implement meaningful goals and metrics
    to measure success in your organization.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we explore how you build a culture within DevOps and how
    to break down silos in your organization for maximum efficiency.
  prefs: []
  type: TYPE_NORMAL
