<html><head></head><body>
		<div>&#13;
			<div id="_idContainer128" class="Content">&#13;
			</div>&#13;
		</div>&#13;
		<div id="_idContainer129" class="Content">&#13;
			<h1 id="_idParaDest-103">6. <a id="_idTextAnchor125"/>Open Technical Practices – Beginnings, Starting Right</h1>&#13;
		</div>&#13;
		<div id="_idContainer161" class="Content">&#13;
			<p>So far, we have talked about the foundational culture and collaboration practices that support our discovery, options, and delivery Mobius loop. The next two chapters establish the technical practices that teams should implement to make the foundation even stronger.  </p>&#13;
			<p>Think of the Mobius loop as an<a id="_idIndexMarker602"/><a id="_idIndexMarker603"/> engine turning from discovery through options generation and into delivery. This cycle continues by doing more delivery until we need to revisit the outcomes we've targeted. Delivery is where we take the concept and make it real. As we deliver, we will learn a lot and garner feedback from our stakeholders and our team. At some point in time, we will need to revisit the discovery side of the loop, either to adjust what we know or to realign what we deliver next.</p>&#13;
			<div>&#13;
				<div id="_idContainer130" class="IMG---Figure">&#13;
					<img src="../Images/B16297_06_01.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 6.1: The Mobius loop</p>&#13;
			<p>Imagine we've done one iteration and<a id="_idIndexMarker604"/><a id="_idIndexMarker605"/> built some new features for our product, such as a leaderboard for our PetBattle app – it's very likely we'd be OK to demo and release that functionality without investing time or effort in building out a solid technical foundation. But as our iterations continue and the application's complexity grows, we may find ourselves beginning to drown in manual testing or technical debt. As we hit iteration five or six, things that were working will most likely start to break and our ability to predict how much we can do will start to fall apart. This could demotivate the team and have people lose confidence in our product or ability to execute. Breaking trust at this stage is a difficult thing to come back from.</p>&#13;
			<p>To successfully build software<a id="_idIndexMarker606"/><a id="_idIndexMarker607"/> incrementally, we need to ensure we can operate smoothly and sustainably. Constant rewrites and fixes will hinder this.</p>&#13;
			<p>For these reasons, it's important we support our delivery capability with a set of foundational technical <a id="_idIndexMarker608"/><a id="_idIndexMarker609"/>practices, much like we did with the collaboration and culture. Practices such as <strong class="bold">configuration-as-code</strong>, <strong class="bold">infrastructure-as-code</strong>, and even <strong class="bold">everything-as-code</strong> can help ensure a team's work is <a id="_idIndexMarker610"/><a id="_idIndexMarker611"/>repeatable. Identifying how a team will do testing and <a id="_idIndexMarker612"/>automating that testing can lead to higher-quality output and simplify defect management. Picking the right development workflow and tools will accelerate the team's ability to deliver the software and not spend all their time managing their tools instead.</p>&#13;
			<p>Adding practices to the foundation is not a one-time activity. As the applications grow in number and complexity, it's important to bolster the foundation with new and more comprehensive use of practices.</p>&#13;
			<p>In the next two chapters, we will share the technical practices implemented on the foundational level that have enabled us to achieve the best success. They are part of the foundation because they are not time-boxed practices; rather, they are continually carried out as part of our daily work. Later in the book, we'll explore how bolstering these practices with great use of the platform can enable sustainability and scalability of continuous delivery.</p>&#13;
			<p>In this chapter, we want to start off right by covering the following:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>Green from go!</li>&#13;
				<li>Pairing and mobbing</li>&#13;
				<li>The container-native approach</li>&#13;
				<li>Pipelines: CI-CD²</li>&#13;
				<li>Everything as code</li>&#13;
				<li>Developer workflows</li>&#13;
			</ul>&#13;
			<h2 id="_idParaDest-104"><a id="_idTextAnchor126"/>Green from Go!</h2>&#13;
			<p><strong class="bold">Green from go!</strong> really just means setting yourself up the correct way when kicking off a new piece of work. For us, that means having all our foundational pieces of software up and running before writing any application <a id="_idIndexMarker613"/><a id="_idIndexMarker614"/>software. It is another mantra of ours. Much like <strong class="bold">show, not tell</strong>, this <a id="_idIndexMarker615"/><a id="_idIndexMarker616"/>one is all about getting things working with a light touch into a usable state.</p>&#13;
			<p>For example, take choosing the tool we want to use to automate the building of our code, such as Jenkins or Tekton, or choosing how we manage secrets or access to private information. The ambition with <strong class="bold">green from go!</strong> should be obvious – clear the pathway to <a id="_idIndexMarker617"/><a id="_idIndexMarker618"/>empower developers to get on and do what they do best – writing amazing, high-quality software that delights end users.</p>&#13;
			<p>When we engage in any new piece of software delivery, we always ensure the tools we need to do our jobs are in place. We will have picked a few that will help us automate taking our code, compiling it, and delivering it to an environment where it can be tested. This means all the tools need to be re-creatable from scripts or another mechanism so we can easily kick-start any engagement to the same level with consistency and the learning from each run can be brought to the next one.</p>&#13;
			<p>Green from go! will also include any<a id="_idIndexMarker619"/><a id="_idIndexMarker620"/> reference apps or pipelines using the tools. This could be a simple <a id="_idIndexMarker621"/><a id="_idIndexMarker622"/>AngularJS application scaffold with an end-to-end build process running against it, proving that we can take code and deliver it to users. The level of complexity of this process should be low enough to allow the teams to evolve it to their specific needs. For example, a team might want to do more non-functional testing in their pipeline, or perhaps another team wants to try out a new cool testing framework. The priority here is to have enough of a foundation to not slow the team down in doing these repeatable tasks when kicking off new work but to get them focused on writing new functionality.</p>&#13;
			<p>It's also important to not have too much in the kit bag – it is our experience that when bringing new teams on board using an accelerator, the technical burden it can have can cause some team members to not engage with it. In order for teams to take ownership and maintain something that accelerates their delivery, they often need to feel like they helped to build it. If something is too complex to engage with, it becomes "that thing that Brian<a id="_idIndexMarker623"/><a id="_idIndexMarker624"/> knows all about, so I don't need to know it." This kind of behavior is dangerous in a team as it can lead to silos and gaps in the team's collective ownership and responsibility. Often, when the build system then fails or breaks in some way, that person becomes the single point of failure when trying to recover it.</p>&#13;
			<p>In the next section, let's look at two practices that help build collective ownership and understanding.</p>&#13;
			<h2 id="_idParaDest-105"><a id="_idTextAnchor127"/>Pair Programming and Mob Programming</h2>&#13;
			<p>Pair programming and mob <a id="_idIndexMarker625"/><a id="_idIndexMarker626"/>programming help<a id="_idIndexMarker627"/><a id="_idIndexMarker628"/> us deal with a phenomenon that people term <strong class="bold">Unicorn Developers</strong>. It has various names across<a id="_idIndexMarker629"/><a id="_idIndexMarker630"/> different regions<a id="_idIndexMarker631"/><a id="_idIndexMarker632"/> and companies, such <a id="_idIndexMarker633"/><a id="_idIndexMarker634"/>as <strong class="bold">the Hero Developer</strong> or <strong class="bold">the Rockstar Developer</strong>. But we <a id="_idIndexMarker635"/><a id="_idIndexMarker636"/>all can identify who they are when we see them.</p>&#13;
			<p>For those who don't know; the <a id="_idIndexMarker637"/><a id="_idIndexMarker638"/>Unicorn Developer is the one who has all the knowledge and keeps it to themselves. They're the person who writes the most magnificent code, and the code that is usually the least understood. They have all the keys and secrets in their head, including all the ideas and knowledge. They are often the one producing so much new work that they don't have time to document it, meaning no one else can continue on the work in their absence. At this point, you can probably identify if your team has a Unicorn; it may even be you!</p>&#13;
			<div>&#13;
				<div id="_idContainer131" class="IMG---Figure">&#13;
					<img src="../Images/B16297_06_02.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 6.2: The Unicorn</p>&#13;
			<p>So why do we have a problem with <a id="_idIndexMarker639"/><a id="_idIndexMarker640"/>the Unicorn?</p>&#13;
			<p>The Unicorn is a bottleneck and an icon of failed projects. They are the single point of failure in the system. When the Unicorn goes on holiday, projects grind to a halt. When things go wrong, the Unicorn has to step in to fix things, meaning new work cannot be completed while they are preoccupied.</p>&#13;
			<p>Organizations want to create high-performing teams around their products – they want entire teams of <strong class="bold">Rockstars</strong>. A great philosophy in achieving this is to "mob to learn, pair to build":</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>Pair programming involves two<a id="_idIndexMarker641"/><a id="_idIndexMarker642"/> engineers working together with one computer on one problem at a time.</li>&#13;
				<li>Mob programming<span id="footnote-023-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-023">1</a></span> involves<a id="_idIndexMarker643"/><a id="_idIndexMarker644"/> an entire team <a id="_idIndexMarker645"/><a id="_idIndexMarker646"/>working together with one machine to solve one problem.</li>&#13;
			</ul>&#13;
			<p>Pairs and mobs of engineers allow for more knowledge transfer and create a shared understanding of the problem and the journey to get to the solution.</p>&#13;
			<h3 id="_idParaDest-106"><a id="_idTextAnchor128"/>Mob to Learn, Pair to Build</h3>&#13;
			<p>To understand <a id="_idIndexMarker647"/>why pairing is different, let's use an analogy. Imagine you're a carpenter and you look at a magnificent rocking chair. What do you learn from seeing the completed piece? Probably not a lot; you might see how one or two pieces connect, but not all of them. Now imagine you worked with the carpenter assembling and crafting the pieces. You'd get to experience the hidden parts, the joinery that was used, how it was created, and how it's all connected. You'd identify the problems faced while fitting the parts together and understand why they're assembled in a given order. You could take a step back and review the furniture as it's being built, giving you a better perspective on the creation process. The same applies when writing and engineering software. Pairing makes better programmers.</p>&#13;
			<p>I can hear the skeptics out there thinking to themselves, hmmm sounds like two developers doing one person's job. Doesn't sound very cost effective to me…. Well, there are a number of interesting advantages to <a id="_idIndexMarker648"/><a id="_idIndexMarker649"/>pair programming and <a id="_idIndexMarker650"/><a id="_idIndexMarker651"/>mobbing:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li><strong class="bold">Mentoring of team members</strong>: New team members can be brought up to speed quickly when they work alongside others, tackling the same problem as someone who knows the code better. Sharing tips and tricks or shortcuts can widen both pairs' skill depth. This sharing can also bring junior associates up to speed very rapidly.</li>&#13;
				<li><strong class="bold">Half the code</strong>: When you ask an organization to spend two developers on one problem, the usual math kicks in of "won't that mean half the code gets written? In truth, hopefully even less code gets written! Two minds working on the same problem makes for more efficiently written code, so less spaghetti.</li>&#13;
				<li><strong class="bold">No pull requests</strong>: Pairing means you have to share your thought process with your pair. This synchronization means code is being reviewed as it's written. Often, people reviewing pull requests are too busy writing their own code and they can only give very superficial reviews. When you pair, you review as you go and therefore write<a id="_idIndexMarker652"/><a id="_idIndexMarker653"/> leaner, better-understood code. You won't cut corners when pairing as someone is watching.</li>&#13;
				<li><strong class="bold">Team bonding</strong>: Humans are social creatures; we share and interact all the time. Pairing and mobbing<a id="_idIndexMarker654"/><a id="_idIndexMarker655"/> facilitates this interaction. Instead of sitting in a room with headphones in, ignoring the world around you, developers in pairs look happier. A room with mobbing and pairing going on is louder. Happy coders lead to better code.</li>&#13;
				<li><strong class="bold">The knowledge stays in the team</strong>: With more than one mind solving the same problem, the understanding and logic stays with them. As pairs naturally shuffle from task to task, the depth of knowledge stays with the team and not with an individual. This means when holidays or even flu season take over, the team can still continue to work at pace knowing<a id="_idIndexMarker656"/><a id="_idIndexMarker657"/> the Unicorn is not leaving with key information.</li>&#13;
			</ul>&#13;
			<div id="footnote-023" class="_idFootnote" epub:type="footnote"><p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-023-backlink">1</a>	A relatively new practice championed by Woody Zuill – <a href="https://woodyzuill.com/">https://woodyzuill.com/</a></p>&#13;
</div>&#13;
			<div>&#13;
				<div id="_idContainer132" class="IMG---Figure">&#13;
					<img src="../Images/B16297_06_03.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 6.3: Mob programming in action</p>&#13;
			<p>When tackling new problems (whether that's a new framework, a new language, or even a particularly hard problem), we will all group together. Huddled around one computer with a very large screen, we can explore the problem we're trying to solve. We mob around the problem until the cross-functional team is satisfied that they have enough knowledge or a rough scaffold of how to complete their tasks. The team then breaks away into groups of two to pull items from the backlog and begin implementation.</p>&#13;
			<div>&#13;
				<div id="_idContainer133" class="IMG---Figure">&#13;
					<img src="../Images/B16297_06_04.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 6.4: Pair programming in action</p>&#13;
			<p>Mobbing and pairing <a id="_idIndexMarker658"/><a id="_idIndexMarker659"/>allows teams to <a id="_idIndexMarker660"/><a id="_idIndexMarker661"/>cross-skill. Sharing experience and expertise leads to better teams. Well-oiled teams working like this can continue to build their product sustainably and at pace, driving toward outcomes, not outputs.</p>&#13;
			<p>You can learn more and collaborate about these practices by going to the Open Practice Library site at <a href="https://openpracticelibrary.com/practice/pair-programming/">https://openpracticelibrary.com/practice/pair-programming/</a> and <a href="https://openpracticelibrary.com/practice/mob-programming/">https://openpracticelibrary.com/practice/mob-programming/</a>.</p>&#13;
			<h2 id="_idParaDest-107"><a id="_idTextAnchor129"/>Containers and Being Container-Native</h2>&#13;
			<p>Before we can define exactly what containers are (hint: they<a id="_idIndexMarker662"/><a id="_idIndexMarker663"/> are Linux processes!) and what container-native means, we need to look back in time to see what led to containers.</p>&#13;
			<h3 id="_idParaDest-108"><a id="_idTextAnchor130"/>Container History</h3>&#13;
			<p>If you are over a<a id="_idIndexMarker664"/><a id="_idIndexMarker665"/> certain age (over 30!), it is very likely your first computer program involved compiling source code and statically linking it with libraries from the operating system. Computer scientists then invented dynamic linking – which is great: you could patch one library and all of the programs you had written would pick up that change once restarted. This of course created a different problem – managing all of the dependencies. Packaging technologies such as RPM and YUM were created to help solve the dependency problem when distributing and managing Linux operating systems. Operating system distributions are one mechanism for collaboratively sharing and managing lots of different software packages at scale, and ultimately it is the software communities that evolve around these different software packages that solve real-world problems.</p>&#13;
			<p>Of course, running your application on one physical machine is fine, but running lots of applications across lots of machines becomes a standard requirement as you scale. Virtualization solved how to run many operating systems on one machine in an isolated fashion. Indeed, the prominent form of cloud computing has been running virtual machines on someone else's hardware.</p>&#13;
			<p>Virtualized infrastructure<a id="_idIndexMarker666"/><a id="_idIndexMarker667"/> solved many problems of running applications at scale. However, configuring all of the pieces required to <a id="_idIndexMarker668"/><a id="_idIndexMarker669"/>manage a fleet of <strong class="bold">virtual machines</strong> (<strong class="bold">VMs</strong>) leading to an explosion of tooling and technology related to configuration management. There was also the problem of "VM sprawl" – lots of VMs everywhere using too many resources that were difficult to patch and manage as a whole. Every application was managed independently, possibly by different teams. It was seen as desirable to reduce the interdependency between each application and so each application was also deployed in its own VM. To help control this spread of VMs, they were managed centrally by an infrastructure and operations team. Silos between teams were built! Many tools were created to help configure VMs. Each VM has overhead for running system processes and daemons, and so a lot of effort has been spent building tools that help avoid over-allocating machine resources to help save money.</p>&#13;
			<p>For developers, the user interface in a VM deployed within an organization was not particularly self-service. Requesting a VM to be provisioned takes time. Workflow, ticketing, and provisioning systems were automated to try and help speed up this service request process. This was made radically better by public cloud services with an API-driven infrastructure, where provisioning a VM takes minutes and there is real self-service for developers. The control and VM sprawl issues still exist, though.</p>&#13;
			<p>The application stack that developers used is still dependent on the operating system and libraries packaged into the VM that came with the kernel (for example, <span class="P---Screen-Text">libc</span>, <span class="P---Screen-Text">libssl</span>). And developers were usually not allowed to change the VM configuration, either because of perceived security or stability concerns. This was an infrastructure or operations team responsibility. Often, VMs were not easy to update, patch, and manage. It was not clear to the infrastructure or operations team what the effect of updating and rebooting a machine would have on the applications they supported.</p>&#13;
			<h3 id="_idParaDest-109"><a id="_idTextAnchor131"/>How Containers Work</h3>&#13;
			<p>It is often said that the journey of <a id="_idIndexMarker670"/><a id="_idIndexMarker671"/>containers is one of <strong class="bold">process isolation</strong>. The containers concept likely started with chroot in 1979, then <a id="_idIndexMarker672"/><a id="_idIndexMarker673"/>graduated to BSD Linux jails in the early 2000s where Solaris Containers picked them up in 2004.<span id="footnote-022-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-022">2</a></span> Solaris zones were a form of technology that isolated and combined system resource controls and boundary separation. From the outside, they looked like VMs, but they were not.</p>&#13;
			<div id="footnote-022" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-022-backlink">2</a>	<a href="https://www.section.io/engineering-education/history-of-container-technology/">https://www.section.io/engineering-education/history-of-container-technology/</a></p>&#13;
			</div>&#13;
			<p>Technology companies that run a large number of workloads are always looking at ways to save resources and ultimately be more efficient. Roll forward to 2006 and a number of technology enhancements were made within the core Linux kernel that was related to the isolation of Linux processes. Google introduced the technology initially called process containers that was later renamed cgroups. It was designed for limiting, accounting, and isolating resource usage (CPU, memory, disk I/O, and network) of a collection of processes.</p>&#13;
			<p>A novel solution to the <a id="_idIndexMarker674"/><a id="_idIndexMarker675"/>dependency problem for containers was introduced by Docker in 2013. Packaging applications and their dependencies into container images lead to an explosion in popularity for containers. Container images were made freely available and distributed online via <a id="_idIndexMarker676"/><a id="_idIndexMarker677"/>container registries such as <a href="http://dockerhub.io">dockerhub.io</a> and <a href="http://quay.io">quay.io</a>.</p>&#13;
			<p>A running container is really just Linux processes with extra protections and data structures supporting the process in the kernel. Running containers on a single machine was easy; running thousands of containers across a compute farm was a much harder problem to solve. Enter into the scene container orchestration engines of which the Kubernetes project is by far the most widely used today. The OpenShift Container Platform is a product that brings together Linux, Kubernetes, and container technologies to allow enterprises to run containers safely and securely at scale in the enterprise.</p>&#13;
			<p>Of course, to get to real business value, it is not enough to package your applications as containers and deploy a Kubernetes platform such as OpenShift. Just because you build it does not mean that all users will immediately flock to the platform! Modern application delivery using trusted supply chains forces new tools and ways of working onto your teams. New behaviors are required.</p>&#13;
			<p>With containers, the developer's user experience has been radically changed. Developers can now self-service their applications without having to go through the provisioning of a VM. Of course, someone still had to provision the platform! Provisioning and starting of containers took seconds and minutes, and today with serverless-focused technology stacks, milliseconds.</p>&#13;
			<p>Developers can control the packaging, running, and upgrading of their applications easily using container images. The application is no longer tied to the version of libraries packaged in the kernel. It is possible to pull out all of an application's code and dependencies into a container image. You can run multiple versions of the same application together without being dependent on the same version of libraries in the kernel.</p>&#13;
			<p>The immutable nature of a container image also improved the overall service quality of applications. Teams could ensure that exactly the same container image would be run in different environments, such as development and production. To be able to run this immutable container image in different environments, developers started to learn that by <a id="_idIndexMarker678"/><a id="_idIndexMarker679"/>externalizing their application configuration they could easily run the same container anywhere. The application configuration management was now built in as part of the container deployment process and the platform. This led to clearer boundaries between what the <strong class="bold">developers</strong> controlled (their applications and configuration) and what <strong class="bold">ITOps</strong> controlled (the platform itself).</p>&#13;
			<div>&#13;
				<div id="_idContainer134" class="IMG---Figure">&#13;
					<img src="../Images/B16297_06_05.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 6.5: Containerization providing clear boundaries</p>&#13;
			<p>In a multi-tenant environment, different groups of users can isolate via projects so as to increase utilization of the underlying infrastructure. In OpenShift there are built-in mechanisms for controlling network ingress and egress, role-based access control, and security, as well as out-of-the-box metrics, monitoring, and alerting capabilities. The platform supports the idea of mounting persistent data storage into your containers. The platform supports these stateful applications so that when a container is stopped/restarted or moved to another compute node, so too is the persistent volume.</p>&#13;
			<p>The demarcation of team roles within a container ecosystem is different compared to virtualized infrastructure. <strong class="bold">InfraOps</strong> teams can manage<a id="_idIndexMarker680"/><a id="_idIndexMarker681"/> the OpenShift platform and supporting infrastructure, while development teams can self-service <a id="_idIndexMarker682"/><a id="_idIndexMarker683"/>provision and run application services on the platform. It is a "set up and get out of the way" mentality. Of course, there are still complexities that need to be discussed and agreed upon before you can reach this goal. When to run cluster-wide services and operators, how to perform rolling platform upgrades while managing business application service levels under change, security, storage, high availability, and load <a id="_idIndexMarker684"/><a id="_idIndexMarker685"/>balancing/networking concerns usually require everyone's involvement. It is the coming together of these teams and the DevOps conversations <a id="_idIndexMarker686"/><a id="_idIndexMarker687"/>between them that form the backbone of modern DevOps practices today.</p>&#13;
			<p>You can learn more and collaborate about the containers practice by going to the Open Practice Library page at <a href="http://openpracticelibrary.com/practice/containers">openpracticelibrary.com/practice/containers</a>.</p>&#13;
			<h2 id="_idParaDest-110"><a id="_idTextAnchor132"/>Pipelines — CI or CD or CD²?</h2>&#13;
			<p><em class="italics">"The job of a pipeline is to prove your code is not releasable."</em> – Jez Humble</p>&#13;
			<p>OK – let's set the scene and get some basics out of the way. How do <a id="_idIndexMarker688"/><a id="_idIndexMarker689"/>we take our code from individual<a id="_idIndexMarker690"/><a id="_idIndexMarker691"/> lines of text on a laptop to being an application running in a container in production? Well, there are lots of ways with lots of kooky-sounding names! Teams call the journey our software goes through a pipeline, but there are numerous ways to implement one.</p>&#13;
			<p>Let's pause for a minute and think about what a<a id="_idIndexMarker692"/><a id="_idIndexMarker693"/> software pipeline really is with the help of our friend Derek, the DevOps Dinosaur!</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-111" class="Author-Heading"><a id="_idTextAnchor133"/>Derek the DevOps Dinosaur</h2>&#13;
			<div>&#13;
				<div id="_idContainer135" class="IMG---Figure" style="float: right; margin: 6px; hight:6cm; width:6cm;">&#13;
					<img src="../Images/Donal.jpg" alt="" width="250" height="250"/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Before I joined the Red Hat Open Innovation Labs team, I was a developer working for a large system integrator. While there, someone asked me to explain what a pipeline is – referring to some build automation I had created. The person asking me was an executive partner and had very limited technical knowledge. He wanted to know what a pipeline is in simple language that he could understand and relate to when talking to his customers. His questions were fundamental, such as what does one look like and what should it do? </p>&#13;
			<p>While thinking of ways to describe a pipeline in a simplified, relatable way, I kept thinking about whether I could explain it in a way that a three-year-old would understand – I could probably explain it to him. And so, Derek the DevOps Dinosaur was born.</p>&#13;
			</div>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h3><a id="_idTextAnchor134"/>Let's Forget about Software for a Minute…</h3>&#13;
			<p>Imagine for a moment that we're not building software. We're not configuring Jenkins, dealing with shells, Ansible, or any <a id="_idIndexMarker694"/><a id="_idIndexMarker695"/>other automation tool. Let's imagine we're building dinosaurs! Big, scary, tough, monstrous, and ferocious ones with lots of teeth! Close your eyes and imagine the scary dinosaur for yourself. Maybe you're imagining some kind of hybrid Jurassic Park dinosaur. Think about the parts of the dinosaur you'd want to build – how many teeth does it have? How many arms and legs? When I think of my scary dinosaur, I think of Derek.️</p>&#13;
			<div>&#13;
				<div id="_idContainer136" class="IMG---Figure">&#13;
					<img src="../Images/B16297_06_06.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Figure 6.6: Introducing Derek</p>&#13;
			<p>So, how do we know Derek is as big and scary as I need him to be? Let's start with his parts. First, we might analyze each of the individual parts of our dinosaur. Give them a quick check-over and ensure they meet the standard we set. For example, do I have two arms and two legs for my dinosaur? Has he got enough teeth? If it all looks good, we can then pop the parts in the ️Dino-Constructor 5000™. </p>&#13;
			<p>With the Dino-Constructor 5000™ complete, we should hopefully produce our dinosaur, Derek.</p>&#13;
			<div>&#13;
				<div id="_idContainer137" class="IMG---Figure">&#13;
					<img src="../Images/B16297_06_07.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Figure 6.7: Introducing the Dino-Constructor 5000™</p>&#13;
			<h3><a id="_idTextAnchor135"/>How Do I Know My Dinosaur Is Fierce Enough?</h3>&#13;
			<p>So, we've got a Dinosaur. But remember, we're here to build ferocious scary dinosaurs that are tough and fit. How do we know Derek is tough enough? Well, we could put him through a series of obstacles. Let's build an obstacle course for Derek.</p>&#13;
			<div>&#13;
				<div id="_idContainer138" class="IMG---Figure">&#13;
					<img src="../Images/B16297_06_08.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Figure 6.8: The dinosaur obstacle course</p>&#13;
			<p>We'll start him on a climbing wall, much like the one you'd see recruits on in an army movie. Then if he's lucky enough to get over that hurdle, he's on to the next obstacle where he must jump over some spikes, Indiana Jones style! Next, we check how fit our dinosaur is; if he's able to run fast on the treadmill, he gets to pass on to the next step. Here he must try swimming past some fish that are trying to nibble on him. Once through that, perhaps he has to jump through a ring of fire. If Derek is capable and makes it through the obstacles, he can then run toward his pen – however, if Derek is not careful, he may be stopped by the swinging blade that was menacingly looming over him the whole time, like something from a Mario level. At any time, the blade could drop and stop Derek dead in his tracks. Let's for a moment assume Derek was careful and has made it into the pen where the other dinosaurs are.</p>&#13;
			<div>&#13;
				<div id="_idContainer139" class="IMG---Figure">&#13;
					<img src="../Images/B16297_06_09.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Figure 6.9: The dinosaur pen</p>&#13;
			<p>Derek can now live out his days with the other dinosaurs in the Dino Petting Zoo, Danny and Debbie. However, unfortunately for Derek, Debbie the dinosaur is quite mean to him. She keeps stealing all of the precious grass and water that Derek likes to eat (Derek is vegetarian in this metaphor!). So, in order to give Derek the isolation and correct amount of things he needs to be strong and healthy, the zookeeper comes along and moves him to a new pen.</p>&#13;
			<p>Derek, as it turns out, is quite a popular dinosaur at the zoo, so the zookeeper decides to make clones of him and puts them all in a pen with Derek. He is happy here and has enough of all the things he needs to survive.</p>&#13;
			<div>&#13;
				<div id="_idContainer140" class="IMG---Figure">&#13;
					<img src="../Images/B16297_06_10.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Figure 6.10: The zookeeper moves Derek to a new pen</p>&#13;
			<h3><a id="_idTextAnchor136"/>But Wait – We're Building Software, Not Dinosaurs!</h3>&#13;
			<p>Sorry to shatter the illusion, but yes, we're (sadly) not in the business of making dinosaurs. We are here to build software applications. What we have just done to our dinosaur is the same thing we do to our code base on every commit. We build our code, run it through a series of obstacles, and then deploy it for our users to consume it. This is a pipeline; it's quite simple really!</p>&#13;
			<p>Let's look at our dinosaur pipeline in more detail. In the first step, we assess the individual pieces that make up our dinosaur – its arms, legs, teeth, and so on. We ask questions such as are there enough parts? Does each hand have three fingers? I often think of this step as the static code analysis part of a pipeline. In the<a id="_idIndexMarker696"/><a id="_idIndexMarker697"/> JavaScript world, this could be as <a id="_idIndexMarker698"/><a id="_idIndexMarker699"/>simple as linting the code base or perhaps even running something more complex such as SonarQube to inspect the <a id="_idIndexMarker700"/><a id="_idIndexMarker701"/>code quality. The Dino-Constructor 5000™ represents the compile step of any language.</p>&#13;
			<p>The obstacle course we built for Derek represents the steps we should carry out to further <a id="_idIndexMarker702"/><a id="_idIndexMarker703"/>assess our code quality. The initial hurdle Derek must get over could represent some unit testing. It is important that these obstacles are tough enough of a challenge while also not being so easy that they provide no value. For example, if Derek can make it over the climbing wall with ease, then it's probably not testing all the parts of him. Imagine for a moment that we decided to add another arm to Derek. We now have a terrifying three-armed dinosaur! If we were to ask him to climb the wall again, he would find it much simpler than before. In this regard it is important to increase the difficulty of the climb, perhaps widening the gaps or making it steeper so it presents more of a challenge. Thinking back to code, the logic still holds. When we introduce new features to our applications, we need to improve the testing coverage to include this. Writing tests is not a one-time thing; it must continue to evolve alongside our application development.</p>&#13;
			<p>The other obstacles represent additional testing types. The small piranha pool Derek must swim through in order to get to safety could represent some early integration tests. The treadmill he must run on may be a kind of performance testing. The final obstacle Derek must pass unscathed is the giant blade hanging above him. Constantly looming, this testing type is, in my eyes, often the one that gets forgotten about. Derek may think he is free and run toward the pen only for the blade to drop on him and mean he can go no further – this is an example of security testing. Often forgotten about until the last minute, it can be a showstopper for final deployment in a lot of cases.</p>&#13;
			<div>&#13;
				<div id="_idContainer141" class="IMG---Figure">&#13;
					<img src="../Images/B16297_06_11.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Figure 6.11: Failing to make the cut when moving to a new pen</p>&#13;
			<p>Once Derek arrives at the dinosaur pen, he has to share the space with some other dinosaurs. Perhaps, at this point, the code has been deployed to a public cloud or a VM with competition for shared resources. Hopefully, by now, the operations team has noticed the application is running out of memory or there is a lack of compute. To combat this problem, the team might automate the containerization of the application. Once the code is in a container, it becomes shippable. We can move the container between cloud providers or even just between environments. At this stage, the code is packaged up with all of the dependencies it requires to run. This ability to move code without the need to rebuild and test can be safely achieved by building immutable container images. Versioning the application configuration separately from the built software means we can also horizontally scale our software easily by running more instances based on user demand.</p>&#13;
			<h3 id="_idParaDest-112" class="Author-Heading"><a id="_idTextAnchor137"/>A Final Thought on Building Dinosaurs</h3>&#13;
			<p>All of these testing types can, and should, be automated as part of a software pipeline. On each automated process that should execute building, testing, and deploying, the code should check if each proceeding step is successful. Through this process, teams can deliver new features faster. Teams can introduce new code without fear of regression. Container platforms such as Red Hat OpenShift and Kubernetes can ensure an application always exists in the desired state. These platforms can also be used to run our software pipelines, using build tools such as Jenkins to run the stages. Dynamic provisioning of test tools such as Zalenium to execute our browser tests as well as using Jenkins to build makes creating pipelines repeatable and reusable.</p>&#13;
			<p>By automating all steps in a pipeline like this, we can ultimately get the dev and ops teams' awesome output into the hands of users quicker.</p>&#13;
			</div>&#13;
			<p>Thanks to Derek, we now know that a pipeline is a series of steps we use to build, package, test, and deploy our software. Now, let's look at some of the terminology people use to describe a software delivery pipeline.</p>&#13;
			<h3 id="_idParaDest-113"><a id="_idTextAnchor138"/>Continuous Integration</h3>&#13;
			<p><strong class="bold">Continuous Integration</strong> (<strong class="bold">CI</strong>) is a software <a id="_idIndexMarker704"/><a id="_idIndexMarker705"/>development practice that was popularized by the authors of Extreme Programming. There have been countless books written about it but the shortest definitions are sometimes the simplest! The three-word definition of CI is to "integrate code continuously." That is to say, developers and teams should regularly commit and push their code into the repository and have some automated process to compile, package, and test that code. This process should happen frequently – many times throughout the day for maximum effect.</p>&#13;
			<div>&#13;
				<div id="_idContainer142" class="IMG---Figure">&#13;
					<img src="../Images/B16297_06_12.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 6.12: Continuous Integration</p>&#13;
			<p>More teams fall down on this CI hurdle than you may think. Often, teams think they are practicing CI when in fact they are not.</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-114" class="Author-Heading"><a id="_idTextAnchor139"/>Integrate Continuously</h2>&#13;
			<div>&#13;
				<div id="_idContainer143" class="IMG---Figure" style="float: right; margin: 6px; hight:6cm; width:6cm;">&#13;
					<img src="../Images/Noel.jpg" alt="" width="250" height="250"/>&#13;
				</div>&#13;
			</div>&#13;
			<p>I worked on a Labs residency for a security company a few years ago. The team was fairly junior with several team members who'd just graduated. The team decided to create feature branches when writing their code so as to not break the workflows of others. Unfortunately, this led us to having these branches that lived for the duration of the sprint. We had lots of automation that was triggered when code was merged but we weren't merging frequently enough.</p>&#13;
			<p>For two sprints, we had the same mad dash to merge all our features at the end of the sprint just before the weekly demo – and it was chaotic, to say the least! This resulted in bi-weekly "mini-integrations." We had lots of automation set up to validate our code but we were not using it frequently enough. As you can imagine, there is nothing continuous about this process – we were not integrating continuously!</p>&#13;
			<p>To remedy this, we talked about it over our retrospective. If the tool you're using, in our case Jenkins, can give you data about the frequency of builds or its usage stats, these can be great things to print out or bring to a sprint retrospective. A brilliant Scrum Master I once worked with always did this and it helped the team focus during the retro on actionable things that we could do to make things faster. In our case on this residency, we were operating in one-week iterations. This meant only four days of actual development time! Through the retrospective, we identified a few actions from looking at the data being supplied to the team:</p>&#13;
			<ol>&#13;
				<li value="1"><strong class="bold">Integrate continuously</strong> – This was a big change for us, to try as often as possible to merge features together and get that validation we needed to avoid the merge hell we were encountering during demos.</li>&#13;
				<li><strong class="bold">Smaller features</strong> – The team realized that work was being broken down into too-large chunks. Each chunk was taking most of the sprint to complete. A smaller task size for each feature meant we could validate faster in smaller chunks whether things would work or not.</li>&#13;
			</ol>&#13;
			</div>&#13;
			<p>You can learn more and collaborate about the <a id="_idIndexMarker706"/><a id="_idIndexMarker707"/>CI practice by going to the Open Practice Library page at <a href="http://openpracticelibrary.com/practice/continuous-integration">openpracticelibrary.com/practice/continuous-integration</a>.</p>&#13;
			<h3 id="_idParaDest-115"><a id="_idTextAnchor140"/>Continuous Delivery</h3>&#13;
			<p><strong class="bold">Continuous Delivery</strong> (<strong class="bold">CD</strong>) is a development process <a id="_idIndexMarker708"/><a id="_idIndexMarker709"/>where on every code change, teams build, test, and package their code such that it can go all the way to production. It is delivered to the doorway of production in an automated way but not let in. Lots of teams get to this state, and it is a great place to get to, but are held back from releasing all the way to production usually due to organizational release cadences or additional approvals being required. The important thing here is that they could release to production if needs be.</p>&#13;
			<div>&#13;
				<div id="_idContainer144" class="IMG---Figure">&#13;
					<img src="../Images/B16297_06_13.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 6.13: Continuous Delivery</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-116" class="Author-Heading"><a id="_idTextAnchor141"/>Building Confidence in the Quality of the Software Delivery Pipeline</h2>&#13;
			<div>&#13;
				<div id="_idContainer145" class="IMG---Figure" style="float: right; margin: 6px; hight:6cm; width:6cm;">&#13;
					<img src="../Images/Donal.jpg" alt="" width="250" height="250"/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Early in my career when the concepts of automated testing and CD were new to me but still at the bleeding edge for some industries, I was working for a large retailer in the UK. They operated a very traditional approach to software deployments with a maximum of one release per quarter.</p>&#13;
			<p>Deployments were a scary thing to them – they would involve a team of specialists who would come in during the dark hours of a Sunday morning to begin their manual task. They would take down the website, put up a holding page, and begin working through the script they were asked to run. Mostly this was a success, but on some occasions when things went wrong, they may have been left with outages for days!</p>&#13;
			<p>This engagement was to build a mobile channel for the retailer to reach their customers. My role was to write some of the integration services between the mobile app and the commerce platform as well as to write a suite of automated integration tests. The retailer I was working for was very traditional and so they had in their project plan a three-week block prior to going live in which all the testing would occur. The retailer thought we were wasting our time writing automated tests and radiating the scores on a wall for all to see – they were confident the three-week window would be enough!</p>&#13;
			<p>Our team was not willing to wait until the end to find out all the issues; they wanted feedback as they proceeded. We created a series of automated jobs in Jenkins to build the apps and APIs and <a id="_idIndexMarker710"/><a id="_idIndexMarker711"/>deploy them to the user acceptance test environment. This meant that for months before the testing team was even engaged, we were delivering application revisions for them to test. Our automated tests emulated user behavior from the mobile app and tested for the happy path and all known error or sad paths through calling APIs with different input parameters. We also got hold of the user acceptance testing team's regression test scripts that would be manually executed and codified them as a set of tests for doing the same API calls. This excited the business as they began to see the app evolve. Features were getting added and issues were being fixed as they showed it off internally. It was a new experience for them, as they were only used to seeing the whole thing at the end.</p>&#13;
			<p>Fast forward to the end of the project and the business had started to see the value of the tests we'd written. On every change, we had automated the building of the mobile app, deploying it to the app store, and we ran a huge suite of integration tests. They continued to do their manual testing phase at the end, which did throw up a few bugs (which we then wrote automated tests for and fixed). However, when they compared the number of issues found during this phase against other similar projects, there were far fewer.</p>&#13;
			<p>On the day of go live, the team was all set to push the app to the app stores and do the final <a id="_idIndexMarker712"/><a id="_idIndexMarker713"/>deployment of the APIs. The retailer had marketing campaigns and other events aligned with this go live date, so the pressure was on! The teams were making minor app fixes right up to this point. Every change required the business to sign off the release, which meant involving the manual test team. Due to the pressure of the release window, the business decided to only do a quick smoke test of the app to see if the issue being fixed was resolved on a specific release candidate. This smoke test passed, so they were ready to roll – however, our automated tests threw up two failures in a service delivering product reviews within the application. There had been a minor change to the data format in the system of record further down the architecture that meant some data transformation functions were not working. This was not caught by the manual test team as they were not smoke testing this functionality. We flagged it up that our tests had spotted a regression, and the release was paused while this issue was resolved.</p>&#13;
			<p>It may seem like a trivial example, but this marked a big turning point for the retailer. They'd witnessed first-hand the speed, reliability, and effectiveness of our automated test suite as well as the speed at which we could build, validate, and deliver a production-ready application. The act of writing and running automated tests built huge trust within the wider organization, prompting them to change their ways radically in favor of more automation and more test automation.</p>&#13;
			</div>&#13;
			<p>You can learn more and <a id="_idIndexMarker714"/><a id="_idIndexMarker715"/>collaborate about the CD practice by going to the Open Practice Library page at <a href="http://openpracticelibrary.com/practice/continuous-delivery">openpracticelibrary.com/practice/continuous-delivery</a>.</p>&#13;
			<h3 id="_idParaDest-117"><a id="_idTextAnchor142"/>Continuous Deployment (CD²)</h3>&#13;
			<p><strong class="bold">Continuous Deployment</strong> (<strong class="bold">CD²</strong>) takes the process of CD but goes one step further and delivers applications into <a id="_idIndexMarker716"/><a id="_idIndexMarker717"/>production and therefore into the hands of our end users. I think of CD as a big train – one that operates on a very reliable timetable. It bundles up all the changes, taking everything in our repositories and compiling, packaging, testing, and promoting the application through all environments, verifying it at each stage.</p>&#13;
			<div>&#13;
				<div id="_idContainer146" class="IMG---Figure">&#13;
					<img src="../Images/B16297_06_14.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 6.14: CI, CD, and CD²</p>&#13;
			<p>By continuously delivering to production, you speed up the delivery of features and fixes to end users compared to holding back for big bang releases. Delivering faster leads to business agility – the ability to react to changing customer and market demands and generate feedback from features sooner. Developers will not have to wait weeks or months from when their code is written for an end user to try it. A quick feedback loop is vital and time and money should be spent considering the best tooling to enable this speedy delivery.</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-118" class="Author-Heading"><a id="_idTextAnchor143"/>When the Work Is Done, Ship It!</h2>&#13;
			<div>&#13;
				<div id="_idContainer148" class="IMG---Figure" style="float: right; margin: 6px; hight:6cm; width:6cm;">&#13;
					<img src="../Images/Tim.jpg" alt="" width="250" height="250"/>&#13;
				</div>&#13;
			</div>&#13;
			<div>&#13;
				<div id="_idContainer147" class="IMG---Figure" style="float: right; margin: 6px; hight:6cm; width:6cm;">&#13;
					<img src="../Images/Noel.jpg" alt="" width="250" height="250"/>&#13;
				</div>&#13;
			</div>&#13;
			&#13;
			<p>Thinking about the ability to deliver at speed, where every change could be deployed to production, it's important to set the technology up to allow changes to flow freely with confidence. This requires strong buy-in from the people around the team, such as leadership and product owners, who can often block such efforts because too much change is considered harmful to quality or end user experience. These conceptions are often formed from previous bad experiences around failed deliveries. So, it is a two-way street – trust is built in that the team can execute with excellence.</p>&#13;
			</div>&#13;
			&#13;
			&#13;
			<p>One of the best product owners we've worked with was at a European car manufacturer. They were replacing a knowledge base application used by dealers and mechanics to diagnose problems and order parts. Historically, changes to this application were farmed out to suppliers with each one patching on their changes. They would hire a systems integrator to add some new functionality and in doing so would often introduce new bugs or issues. This outsourcing of development meant that architectural design decisions were made outside of the customers' product team, which led to technical debt and an unsustainable solution in the long run. The team decided to wipe the slate clean and rebuild the application by bringing the development in-house. We were engaged to help kick start this team the right way, using a residency, and help them build a product team connected to their end users.</p>&#13;
			<p>A number of sprints into the engagement but still early in the development, the team was creating the authentication flow for users. I was pair programming with one of the engineers and we'd written the logout functionality. We had test-written and demonstrated the feature to the product owner running in our test environment. The Definition of Done the team agreed meant we had to show the feature to someone from the product team so they could accept it. So, as far as the engineering effort was concerned, we were done. The product owner did a quick test and it looked good in the test environment, so at the end of the sprint when we promoted all our changes up to production, our feature was released.</p>&#13;
			<p>The UX folks were doing<a id="_idIndexMarker718"/><a id="_idIndexMarker719"/> some usability testing with the latest increment of the application when they noticed some buggy behavior with logout not working from one of the screens. This was reported to the engineer and me, who worked on it initially, and we could spot the issue immediately. This was a small fix, so we wrote another test and made the change.</p>&#13;
			<p>We demonstrated the process to the product owner – writing a test that failed, writing code that would make the test pass, iterating, and then delivering that fixed logout code all the way to production. The ability to deliver small incremental improvements into the hands of our end users when they were ready to be shipped paved the way to deliver continuously.</p>&#13;
			<p>The lesson here is that Scrum may start to impede a team's ability to continuously deliver small incremental changes safely to production because Scrum delivers at the end of a sprint. "When the work is done, just ship it to production."</p>&#13;
			</div>&#13;
			<p>We have learned about the<a id="_idIndexMarker720"/><a id="_idIndexMarker721"/> role of software pipelines, which codify the steps required to build, package, test, and deploy our application code into various environments up to but not necessarily including production – the practice of CD. We then looked at an approach to continuously deploying small incremental changes all the way to production.</p>&#13;
			<p>You can learn more about<a id="_idIndexMarker722"/><a id="_idIndexMarker723"/> CD² by going to the Open Practice Library page at <a href="http://openpracticelibrary.com/practice/continuous-deployment">openpracticelibrary.com/practice/continuous-deployment</a>.</p>&#13;
			<h2 id="_idParaDest-119"><a id="_idTextAnchor144"/>Everything-as-Code</h2>&#13;
			<p>You may have heard about this one before: [insert software term here]-as-code. </p>&#13;
			<p>Examples include infrastructure-as-code, config-as-code, tests-as-code, and now everything-as-code. This practice has been around for a long time but some organizations have been slow to adopt it.</p>&#13;
			<p>Here's the problem – historically, organizations <a id="_idIndexMarker724"/><a id="_idIndexMarker725"/>have had to get expensive specialists to deploy complex environments. They would spend hours going through pages of instructions, line by line, eventually getting the deployment to work. A number of weeks would pass and the organization would like to create another environment, exactly like this one, for further testing. What do they do now? Call the specialist and ask them to come back at a great cost! This is fine, if you like hiring expensive specialists a lot.</p>&#13;
			<p>So, what's the solution? The everything-as-code practice is simple: you treat every part of a system as you would any other line of code. You write it down and store it in a version control system, such as Git. Do we really mean to automate every part of the system? Yes.</p>&#13;
			<p>We start by automating the infrastructure layer, the lowest level, from the bare metal servers to the operating systems, networks, application configuration, and on up through to application deployments.</p>&#13;
			<p>This automation effort sounds like an awful lot of <a id="_idIndexMarker726"/><a id="_idIndexMarker727"/>work, and could be expensive in terms of people's time – why should you invest in doing it? Here's why:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li><strong class="bold">Traceability</strong>: Having your <a id="_idIndexMarker728"/><a id="_idIndexMarker729"/>environment descriptions and structure stored in a version control system allows us to audit changes made to the system, tracked to the individual who made them.</li>&#13;
				<li><strong class="bold">Repeatable</strong>: Moving from one cloud provider to another should be a simple task. Picking a deployment target should be like shopping around for the best price that week. By storing all things as code, systems can be re-created in moments in various providers.</li>&#13;
				<li><strong class="bold">GitOps</strong>: A single source of the truth means no more tribal knowledge or experts needed to set up cables or attach hard drives.</li>&#13;
				<li><strong class="bold">Phoenix server</strong>: No more fears of configuration drift. If a server needs to be patched or randomly dies, that's OK. Just create it again from scratch using the stored configuration.</li>&#13;
				<li><strong class="bold">Cross-functional teams</strong>: Writing all things as code improves collaboration between silos in an organization. The development team is able to<a id="_idIndexMarker730"/><a id="_idIndexMarker731"/> contribute to the environment creation or can recreate their own like-for-like environments in a sandbox.</li>&#13;
				<li><strong class="bold">De-risking</strong>: Changes can be applied to environments or<a id="_idIndexMarker732"/><a id="_idIndexMarker733"/> application deployments and reverted to previous states quickly, thus de-risking big upgrades of any kind.</li>&#13;
			</ul>&#13;
			<p>There are plenty of approaches to implementing everything-as-code:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li><strong class="bold">Networks and infrastructure</strong>: Ansible <a id="_idIndexMarker734"/><a id="_idIndexMarker735"/>can be used to<a id="_idIndexMarker736"/><a id="_idIndexMarker737"/> declaratively define the system you're implementing, and Istio can help with managing network traffic between apps and services.</li>&#13;
				<li><strong class="bold">Application environments</strong>: Containerization provides a proven, repeatable way to package applications and their dependencies in a way that both developers and operators love.</li>&#13;
				<li><strong class="bold">Developer workflows/build automation</strong>: Use Jenkins' Pipeline as Code or Tekton to describe how your application is taken from source, compiled, tested, and turned into something runnable.</li>&#13;
				<li><strong class="bold">Configuration drift</strong>: ArgoCD is a tool that implements the GitOps pattern for your application and support tooling.</li>&#13;
				<li><strong class="bold">Testing as code</strong>: Selenium tests written as acceptance criteria in the behavior-driven development form can bring business analysts and <a id="_idIndexMarker738"/><a id="_idIndexMarker739"/>developers one step closer together.</li>&#13;
				<li><strong class="bold">Security and compliance</strong>: Open Policy Agent and Advanced Cluster Manager are tools that enforce policies across the whole stack.</li>&#13;
			</ul>&#13;
			<p>Teams who treat the whole system as code are stronger, faster, and better for it. We should no longer think about just infrastructure-as-code but automating the whole system – everything from application properties to networks and security policies. Then we codify it!</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-120" class="Author-Heading"><a id="_idTextAnchor145"/>Can You Build a Second One of Those for Me, Please?</h2>&#13;
			<div>&#13;
				<div id="_idContainer149" class="IMG---Figure" style="float: right; margin: 6px; hight:6cm; width:6cm;">&#13;
					<img src="../Images/Author_4.jpg" alt="" width="250" height="250"/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Spending time on automating the creation of test environments? "Sounds costly and a waste of my time" – I can hear some people reading this saying to themselves.</p>&#13;
			<p>While working for a customer in the UK, I was building mobile apps and a bunch of JavaScript services to supply data to the apps in a consumable way optimized for the mobile. The services layer of adapters was deployed on IBM's MobileFirst (then Worklight), a big Java app that required a specialist to configure and install. We had several environments, from dev to system integration test environments to user acceptance test environments and production. All the common environments you'd imagine in a very traditional ecosystem.</p>&#13;
			<p>The specialist spent two weeks configuring and installing the user acceptance test servers. Two of them were made available to allow us to have more than one thing under test at any given time. You wanted a third? Well, that meant bringing back that expensive specialist to build the third one and another week of their time. In production we had eight servers, each manually configured and deployed!</p>&#13;
			<p>When I look back on this engagement and think about the pressure we faced to get the servers configured and deployed along with the time taken for each one, it seems like madness. The consultant would rock up, spend the day messing around on the terminal making manual changes here and there and manually testing the results. None of the config files back then were stored in Git or even turned into scripts that she could execute to make spinning up the next one faster. Every piece of information was tribal and in her head. We wanted a third server? We had to hire her to come back and do it all again!</p>&#13;
			<p>Some years later on another engagement for a public sector client, I saw similar behavior. I thought maybe creating servers in this way was a localized instance but on the government contract, there were teams spinning up servers for the developers to use that were not using any scripting or automation. If you wanted a server, you raised a ticket and waited a week. If you wanted an exact copy of that one, you raised another ticket and sometimes received one that was identical. In this case, the team was manually executing shell commands inside each VM and more often than not forgot to run a command or two!</p>&#13;
			<p>These examples may feel a bit old now – but the reality is that I still see organizations with a traditional approach to infrastructure, automation, and repeatability. Not being able to test changes on representative hardware can be a challenge for teams trying to go fast. Teams need to have the power to spin up and spin down application stacks on demand. Modern approaches to how we package applications, such as containers, can really help to bring down this wall. No longer does a developer need to stub out test cases with database calls, because they can just spin up a real database in a container and test against it.</p>&#13;
</div>&#13;
			<p>You can learn more and <a id="_idIndexMarker740"/><a id="_idIndexMarker741"/>collaborate about the everything-as-code practice by going to the Open Practice Library page at <a href="http://openpracticelibrary.com/practice/everything-as-code">openpracticelibrary.com/practice/everything-as-code</a>.</p>&#13;
			<p>So, what approach did the PetBattle team take while practicing everything-as-code?</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-121" class="Author-Heading"><a id="_idTextAnchor146"/>Establishing the Technical Foundation for PetBattle</h2>&#13;
			<p>This section will cover the beginning of our journey of PetBattle as the development team tries to set <a id="_idIndexMarker742"/><a id="_idIndexMarker743"/>up a technical foundation with tools we will cover in later chapters. Any section in a box such as this one is going to lean in a bit more on the technical side.</p>&#13;
			<p>PetBattle began life as a hobby for some engineers – a pet project, if you will. This project provides the team with a real-world application where they can try out new frameworks and technology. In order to wrap some modern software practices around PetBattle, they enhance the application with some build and test automation. As the demand for PetBattle increases, we will look at autoscaling and how we can apply practices from the Open Practice Library to identify how we should build things.</p>&#13;
			<p>For PetBattle, we embrace modern software development paradigms – we monitor and respond to configuration drift so the team can implement GitOps to monitor this drift. Our environments should be like a phoenix, able to rise from the ashes! In other words, we can destroy them with confidence as we can recreate them from code.</p>&#13;
			<p>Let's look at PetBattle's first piece of software they want to deploy, Jenkins. This section will explore how to deploy and manage Jenkins on OpenShift using Jenkins.</p>&#13;
			<p>The PetBattle team is using OpenShift to deploy their applications. They have chosen to use Jenkins to get started with automating some of their tasks for building and deploying their software automatically. Jenkins is an open source automation server that can run many tasks and is supported on OpenShift. Jenkins also has a strong helpful community surrounding it and there is a large plugin ecosystem too, making automating almost any task you can think of a cinch!</p>&#13;
			</div>&#13;
			<p>Now that we have established PetBattle's technical foundation, let's explore Jenkins a little more and the role it can play in strengthening foundations.</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-122" class="Author-Heading"><a id="_idTextAnchor147"/>Jenkins – Our Best Friend!</h2>&#13;
			<div>&#13;
				<div id="_idContainer151" class="IMG---Figure" style="float: right; margin: 6px; hight:6cm; width:6cm;">&#13;
					<img src="../Images/Donal.jpg" alt="" width="250" height="250"/>&#13;
				</div>&#13;
			</div>&#13;
			<div>&#13;
				<div id="_idContainer150" class="IMG---Figure" style="float: right; margin: 6px; hight:6cm; width:6cm;">&#13;
					<img src="../Images/Noel.jpg" alt="" width="250" height="250"/>&#13;
				</div>&#13;
				</div>&#13;
				&#13;
				<p>We like to think of Jenkins as our friend. We remember the days when teams would have someone build the app on their local machine and send it to the ops team via email. To do deployments, it would be a specialized team that came in, usually overnight, and did the deployment so as to minimize interruptions.</p>&#13;
			</div>&#13;
&#13;
						&#13;
						&#13;
			&#13;
			&#13;
			<p>Deployments were seen as a risky, scary thing. One time, a team we worked with went out the night before a big deployment. When they stumbled into work in the wee hours of the morning they were not quite in the sharpest of mindsets. As you'd imagine when running an upgrade, they skipped a step and broke things. The reason we think of Jenkins as our friend is that he doesn't do things like that. He does not go out the night before and arrive at work tired (unless, of course, you forget to feed him lots of RAM and CPU). Jenkins also won't forget a line to execute in a script; he's pretty good in that way. But he's also pretty dumb in other ways; Jenkins is only as clever as the instructions you feed him. Jenkins in his vanilla form is fairly basic, so we give him additional superpowers to be able to run builds for specific technology using agents and to report test scores in a machine-readable way using plugins. But once he's got it once, he will do it over and over again without failing – especially if you configure him as code.</p>&#13;
			&#13;
<div>&#13;
			<h3 id="_idParaDest-123"><a id="_idTextAnchor148"/>Helm Overview</h3>&#13;
			<p>This next section is going to get a bit <a id="_idIndexMarker744"/><a id="_idIndexMarker745"/>more detailed on the technical side of things. Prepare for some code snippets and whatnot! If this is not your thing, feel free to skip over it to the next section all about Git and developer workflows. We'll mark any section that's going to have code snippets and be a bit lower level with this handy sign!</p>&#13;
			<div style="display:block; overflow-x:auto; padding:.5em; margin: 0.5px;">&#13;
			<div>&#13;
				<div id="_idContainer152" class="IMG---Figure" style="float: left; margin: 2px; hight:7cm; width:7cm;">&#13;
					<img src="../Images/Techie.jpg" alt="" width="220" height="220"/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Jenkins comes with OpenShift, and there are several ways for the team to install and configure it. Any member of the cross-functional team could go to the OpenShift console and install it from the catalog. It's as simple as clicking a few buttons in the UI and choosing to add a persistent hard disk or not. This is a great way for the team to get moving fast but also would not honor our technical foundation practice of everything-as-code! </p>&#13;
			<p>PetBattle now has two choices for how they could create an instance of Jenkins while honoring our everything-as-code practice. They could use OpenShift or Helm templates containing <a id="_idIndexMarker746"/><a id="_idIndexMarker747"/>all of the Kubernetes and OpenShift objects that would be required to deploy a working Jenkins. For the purposes of this book, we will focus exclusively on Helm as our Kubernetes package manager.</p>&#13;
			</div>&#13;
			<p>Helm is an application package manager for Kubernetes that allows both developers and operators to easily package the resources and configuration that make up an application into a release. Helm is used for application life cycle management for installing, upgrading, and rolling back application deployments, thus simplifying the installation of an application on an OpenShift cluster. In Helm, applications are packaged up and distributed as Helm charts. A Helm chart <a id="_idIndexMarker748"/><a id="_idIndexMarker749"/>is made up of several YAML files and templates. These Helm templates should output Kubernetes YAML once processed. Let's take a look at an example Helm chart.</p>&#13;
			<p>From our experience using Jenkins on OpenShift with customers, we have written a chart to deploy the Red Hat instance of Jenkins and give it a few superpowers. We'll look at those afterward. Let's first explore the anatomy of a chart:</p>&#13;
			<p class="snippet">jenkins</p>&#13;
			<p class="snippet">├── Chart.yaml</p>&#13;
			<p class="snippet">├── README.md</p>&#13;
			<p class="snippet">├── templates</p>&#13;
			<p class="snippet">│   ├── PersistentVolumeClaim.yaml</p>&#13;
			<p class="snippet">│   ├── buildconfigs.yaml</p>&#13;
			<p class="snippet">│   ├── deploymentconfig.yaml</p>&#13;
			<p class="snippet">│   ├── imagestreams.yaml</p>&#13;
			<p class="snippet">│   ├── rolebinding.yaml</p>&#13;
			<p class="snippet">│   ├── route.yaml</p>&#13;
			<p class="snippet">│   ├── secret.yaml</p>&#13;
			<p class="snippet">│   ├── serviceaccount.yaml</p>&#13;
			<p class="snippet">│   └── services.yaml</p>&#13;
			<p class="snippet">└── values.yaml</p>&#13;
			<p>The Jenkins chart, like all Helm charts, is made up of a number YAML files:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li><strong class="inline">Chart.yaml</strong>: This is the manifest of our Jenkins <a id="_idIndexMarker750"/><a id="_idIndexMarker751"/>chart. It <a id="_idIndexMarker752"/><a id="_idIndexMarker753"/>contains metadata such as the name, description, and maintainer information. The manifest also contains the application version and the version of the chart. If the chart has any dependencies on another chart or charts, they would also be listed here. </li>&#13;
				<li><strong class="inline">README.md</strong>: Instructions for the chart, how to install it, and how to customize it.  </li>&#13;
				<li><strong class="inline">templates/*</strong>: This folder contains all the resources that need to be deployed to install and configure a running Jenkins instance, such as deployments, services, routes, and pvc.</li>&#13;
				<li><strong class="inline">values.yaml</strong>: These are the sensible (default) values that the chart can be run with so a user can just install the chart and get up and running quickly. Customizations to these values can be supplied on the command line or by supplying your own values.yaml file when installing a chart.</li>&#13;
			</ul>&#13;
			<p>Red Hat <strong class="bold">Communities of Practice</strong> (<strong class="bold">CoP</strong>) is an organization that creates reusable software based on experiences and learnings from <a id="_idIndexMarker754"/><a id="_idIndexMarker755"/>working with customers. This software is then open sourced and shared. We can add the CoP Helm Charts repository, which contains a Jenkins Helm chart for us to use.</p>&#13;
			<p>To start, we need the <a id="_idIndexMarker756"/><a id="_idIndexMarker757"/><span class="P---Screen-Text">helm</span> command-line tool. From your laptop, follow the instructions on the <strong class="inline">helm.sh</strong> (<a href="https://helm.sh/docs/intro/install/">https://helm.sh/docs/intro/install/</a>) website to install the <span class="P---Screen-Text">helm</span> tool. Then add the Red Hat CoP <span class="P---Screen-Text">helm</span> repository as follows:</p>&#13;
			<p class="snippet">helm repo add redhat-cop \</p>&#13;
			<p class="snippet">     https://redhat-cop.github.io/helm-charts</p>&#13;
			<p>We can search this helm repository for Jenkins chart versions we can use:</p>&#13;
			<p> </p>&#13;
			<div>&#13;
				<div id="_idContainer153" class="IMG---Figure">&#13;
					<img src="../Images/B16297_06_15.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 6.15: Searching the helm repository for the Jenkins chart</p>&#13;
			<h3 id="_idParaDest-124"><a id="_idTextAnchor149"/>Installing Jenkins Using Helm</h3>&#13;
			<p>The quickest way to get access to your <a id="_idIndexMarker758"/><a id="_idIndexMarker759"/>very own OpenShift cluster is to install CodeReady Containers <a id="_idIndexMarker760"/><a id="_idIndexMarker761"/>on your laptop. Linux, Windows, and macOS are supported. You will need to log in and follow the instructions<a id="_idIndexMarker762"/><a id="_idIndexMarker763"/> located here: <a href="https://developers.redhat.com/products/codeready-containers/overview">https://developers.redhat.com/products/codeready-containers/overview</a>, and you should<a id="_idIndexMarker764"/><a id="_idIndexMarker765"/> see a two-step process similar to <em class="italics">Figure 6.16</em>:</p>&#13;
			<div>&#13;
				<div id="_idContainer154" class="IMG---Figure">&#13;
					<img src="../Images/B16297_06_16.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 6.16: Installing CodeReady Containers</p>&#13;
			<p>Other OpenShift clusters you have access to may also work, as long as you have sufficient resources and privileges. The CodeReady Containers install gives you cluster administrator privilege (the highest level of privilege) and is limited by how much RAM, CPU, and disk space your laptop has. We recommend 8 G RAM, 4 vCPUs, and 31 GB of disk space as a minimum, which would correspond to starting CRC on linux with:</p>&#13;
			<p class="snippet">crc start -c 4 -m 12288</p>&#13;
			<p>There are more detailed OpenShift sizing instructions in the Appendix. </p>&#13;
			<p>To install the Jenkins chart, we will log in to OpenShift, create a new project, and install the Helm chart. If you're <a id="_idIndexMarker766"/><a id="_idIndexMarker767"/>missing any of the tools needed to run these commands, have no fear, as they can be downloaded and installed to match your OpenShift cluster version directly from the OpenShift console. Click on the <span class="P---Screen-Text">?</span> icon and then <span class="P---Screen-Text">Command Line Tools</span> to find the latest instructions.</p>&#13;
			<div>&#13;
				<div id="_idContainer155" class="IMG---Figure">&#13;
					<img src="../Images/B16297_06_17.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 6.17: Downloading Command Line Tools from OpenShift</p>&#13;
			<p>The string after<a id="_idIndexMarker768"/><a id="_idIndexMarker769"/> installation, <span class="P---Screen-Text">my-jenkins</span>, is the release name<a id="_idIndexMarker770"/><a id="_idIndexMarker771"/> that is used by the Helm template engine:</p>&#13;
			<p class="snippet">oc login &lt;cluster_api&gt; -u &lt;name&gt; -p &lt;password&gt;</p>&#13;
			<p class="snippet">oc new-project example</p>&#13;
			<p class="snippet">helm install my-jenkins redhat-cop/jenkins  </p>&#13;
			<p>It allows us to create multiple releases in the one namespace, which is useful for testing purposes:</p>&#13;
			<div>&#13;
				<div id="_idContainer156" class="IMG---Figure">&#13;
					<img src="../Images/B16297_06_18.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 6.18: Creating multiple releases in a single namespace</p>&#13;
			<p>Helm charts can be<a id="_idIndexMarker772"/><a id="_idIndexMarker773"/> installed in a number of ways. You can also run helm template against a local copy of the chart. If you are interested in doing this, fetch the chart and run this:</p>&#13;
			<p class="snippet">helm fetch redhat-cop/jenkins --version 0.0.23</p>&#13;
			<p class="snippet">helm template test jenkins-0.0.23.tgz</p>&#13;
			<p>This can be <a id="_idIndexMarker774"/><a id="_idIndexMarker775"/>useful if you want to play around and see what the output is before applying it to an OpenShift cluster or if you want to validate things while debugging or testing a chart's configuration. You can <a id="_idIndexMarker776"/><a id="_idIndexMarker777"/>also supply <strong class="inline">--dry-run</strong> to the helm install command to verify the chart before installing it.</p>&#13;
			<p>Let me just pause a minute and say that this is not a book about Helm! There are great books out there written specifically for it, such as Learn Helm (<a href="https://www.packtpub.com/product/learn-helm/9781839214295">https://www.packtpub.com/product/learn-helm/9781839214295</a>) by <em class="italics">Andy Block</em> and <em class="italics">Austin Dewey</em>. Our aim is just to scratch the surface to show how easy it is to get going in a reusable and repeatable way with Helm and OpenShift.</p>&#13;
			<p>Using <span class="P---Screen-Text">helm install</span> as demonstrated previously is great as it will create a life cycle managed by the Helm CLI to run upgrades <a id="_idIndexMarker778"/><a id="_idIndexMarker779"/>and roll back releases if needed. These revisions are integrated into OpenShift and can be viewed in the UI or on the command line. Every time a new revision is deployed to the cluster, a new secret will be created, making rollback very simple:</p>&#13;
			<p class="snippet">oc get secrets -n example | grep helm</p>&#13;
			<p>To see all the pods being spun up by the Jenkins chart, you can run this:</p>&#13;
			<p class="snippet">oc get pods --watch -o wide -n example</p>&#13;
			<p>You should see a large volume of pods being created – this is because this Helm chart contains lots of additional configuration-as-code for Jenkins. Write once and deploy many times:</p>&#13;
			<div>&#13;
				<div id="_idContainer157" class="IMG---Figure">&#13;
					<img src="../Images/B16297_06_19.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 6.19: Pods being created</p>&#13;
			<p>You may notice a bunch of agent build pods in the output. Jenkins by itself is a bit useless. One of Jenkins' superpowers is his ability to be extended using what are called plugins – small bits of code that provide new functions and features. To install these plugins, we could wait until Jenkins is deployed and configure the plugins manually through the UI – but this is the everything-as-code world, so we don't want to do that!</p>&#13;
			<p>The Jenkins Helm <a id="_idIndexMarker780"/><a id="_idIndexMarker781"/>chart is configured to pre-install a bunch of useful Jenkins agent plugins. These agents know how to build container images using various language-specific stacks.  The configuration for the agent plugins is defined in the Helm chart's <strong class="inline">values.yaml</strong> file, which you can see by using this:</p>&#13;
			<p class="snippet">helm show values redhat-cop/jenkins</p>&#13;
			<p class="snippet">buildconfigs:</p>&#13;
			<p class="snippet"># Jenkins agents for running builds etc</p>&#13;
			<p class="snippet">  - name: "jenkins-agent-ansible"</p>&#13;
			<p class="snippet">    source_context_dir: "jenkins-agents/jenkins-agent-ansible"</p>&#13;
			<p class="snippet">    source_repo: *jarepo</p>&#13;
			<p class="snippet">    source_repo_ref: "master"</p>&#13;
			<p class="snippet">...</p>&#13;
			<p>The Helm chart is defining a list of build configurations to build each agent image. The Jenkins agent images use an <a id="_idIndexMarker782"/><a id="_idIndexMarker783"/>OpenShift project called <strong class="bold">Source-to-Image</strong> (<strong class="bold">S2I</strong>) to do their language-specific build of your applications. S2I is a toolkit and workflow for building reproducible container images from source code; you can read about it here: <a href="https://github.com/openshift/source-to-image">https://github.com/openshift/source-to-image</a>. You <a id="_idIndexMarker784"/><a id="_idIndexMarker785"/>basically feed S2I your source code via a Git repository URL and it takes care of the rest.</p>&#13;
			<p>Using language-specific agents makes Jenkins easier to extend. We do not have to install tools into the base Jenkins image; rather, we define an agent and add it to the Helm chart agent plugins values list. OpenShift makes it very easy to create agents for Jenkins. We can extend the base image with any binary we want to use in our pipelines and apply the label <strong class="inline">role=jenkins-slave</strong> to make it discoverable in Jenkins. This gives us a near "serverless" ability for Jenkins to dynamically provision an agent when it's required. In this case, a pod gets launched and Jenkins will connect to it, execute its tasks, and destroy it when it's done. This means no agents lying idle waiting to be executed and a clean slate every time we run a build.</p>&#13;
			<p>There are a bunch of Jenkins agents available in the CoP; you can use them or create your own: <a href="https://github.com/redhat-cop/containers-quickstarts/tree/master/jenkins-agents">https://github.com/redhat-cop/containers-quickstarts/tree/master/jenkins-agents</a>.</p>&#13;
			<p>Apart from the agent plugins, the Jenkins image is extensible from the base image in a number of different ways. You can specify a list of plugins to install when you build the Jenkins image. We use S2I to build our Jenkins image and add our list of <strong class="inline">plugins.txt</strong> from this Git repository: <a href="https://github.com/rht-labs/s2i-config-jenkins">https://github.com/rht-labs/s2i-config-jenkins</a>.</p>&#13;
			<p>Once the Jenkins build has completed, a Jenkins deployment and running container instance will be available.</p>&#13;
			<div>&#13;
				<div id="_idContainer158" class="IMG---Figure">&#13;
					<img src="../Images/B16297_06_20.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 6.20: Available Jenkins deployment and a running container instance</p>&#13;
			<p>All of the S2I plugins and agents<a id="_idIndexMarker786"/><a id="_idIndexMarker787"/> are configured. You can log in to Jenkins using its route, which is available in the OpenShift web <a id="_idIndexMarker788"/><a id="_idIndexMarker789"/>console, or by running this:</p>&#13;
			<p class="snippet">oc get route jenkins </p>&#13;
			<p>By running this single <strong class="inline">helm install</strong> command, we get a sensible starting point to be able to do lots of things with our build server, Jenkins. By codifying the Jenkins configuration, we can repeatedly deploy Jenkins into many environments without ever having to touch the Jenkins UI.</p>&#13;
			<p>Now that we have our build server, before starting development we should familiarize ourselves with the types of code workflows developers use. If you are an experienced developer, you will already be pretty familiar with the next section's content.</p>&#13;
			<h2 id="_idParaDest-125"><a id="_idTextAnchor150"/>Developer Workflows</h2>&#13;
			<p>Git is a <strong class="bold">version control system</strong> (<strong class="bold">VCS</strong>) created by Linus Torvalds (author of the Linux kernel) to track changes in source code and easily manage these changes across many file types and <a id="_idIndexMarker790"/><a id="_idIndexMarker791"/>developers. Git differs from other VCS in that it is decentralized. This means that unlike, for example, <strong class="bold">Subversion</strong> (<strong class="bold">svn</strong>), each <a id="_idIndexMarker792"/><a id="_idIndexMarker793"/>developer retains a complete copy of the source code locally when they check it out. Locally, each developer has a copy of all the history and <a id="_idIndexMarker794"/><a id="_idIndexMarker795"/>can rewind or fast forward to different versions as they need to. An engineer makes their changes and applies those changes as a delta on top of another's work. This is known as a commit. Git can be conceptualized as a tree, with a trunk of these changes or commits on top of each other. Branches can spring out from the trunk as independent pieces of functionality, or work that is not ready can be merged back to the trunk. Once something is committed to Git, it is forever in the history and can always be found – so be careful not to add something secret, such as a password, by mistake!</p>&#13;
			<p>Git is the underlying technology behind<a id="_idIndexMarker796"/><a id="_idIndexMarker797"/> some big companies <a id="_idIndexMarker798"/><a id="_idIndexMarker799"/>such as GitHub and GitLab. They have taken the Git product and added some social features and issue-tracking capabilities to help manage a code base.</p>&#13;
			<p>There are many workflows for Git that development teams can use when writing code, and choosing the correct one <a id="_idIndexMarker800"/><a id="_idIndexMarker801"/>can seem like a daunting task. Some are designed to give teams a sense of safety and security, especially in large complex projects, while others promote speed and trust within the teams. The most popular source code management workflows for Git are Trunk, GitFlow, and GitHub Flow. Let's explore each in detail and see how we could use them to help us promote CD.</p>&#13;
			<h3 id="_idParaDest-126"><a id="_idTextAnchor151"/>GitFlow</h3>&#13;
			<p>GitFlow was first published <a id="_idIndexMarker802"/><a id="_idIndexMarker803"/>about 10 <a id="_idIndexMarker804"/><a id="_idIndexMarker805"/>years ago by Vincent Driessen. The workflow was built from his experience using Git, a relatively new tool at the time. As teams moved to Git from a non-branching-based code repository, some new concepts and core practices had to be defined. GitFlow tried to answer this by adding a well-thought-out structure to branch names and their conventions.</p>&#13;
			<p>A well-defined branching strategy is at <a id="_idIndexMarker806"/><a id="_idIndexMarker807"/>the heart of GitFlow. Changes are committed to different named branches depending on the type of change. New features are developed on branches that are called <strong class="inline">feature-*</strong> branches. <strong class="inline">hotfixes-*</strong> branches are created for patching changes to bugs in production and a release branch. GitFlow describes two reserved and long-living branches:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li><strong class="bold">Master</strong>: This is the branch that <a id="_idIndexMarker808"/><a id="_idIndexMarker809"/>contains our releases or our production-ready code. Sometimes this branch is referred to as the main branch.</li>&#13;
				<li><strong class="bold">Develop</strong>: This branch is our integration branch. It is usually the most turbulent and very likely to <a id="_idIndexMarker810"/><a id="_idIndexMarker811"/>contain bugs or other issues as it is the place where teams first bring their code together.</li>&#13;
			</ul>&#13;
			<p>The naming and usage conventions defined in GitFlow make it easier for a new developer to discover what each branch is doing. The developer can bring additional changes made by other team members<a id="_idIndexMarker812"/><a id="_idIndexMarker813"/> into their feature branch, when they choose to, by merging in any new changes. Branching in this way avoids breaking things for other engineers by ensuring that the feature functionality is complete before asking to merge their code from the feature into the develop branch. When a set of features is ready to be promoted to the mainline master branch, the developers merge their code to the master via the release branch.</p>&#13;
			<p>You may be reading this and thinking, this sounds complex! And in some ways, it is. But in a large project with a single code base, this can be exactly what is required to ensure developers are free to work on their code without having to manage their code.</p>&#13;
			<h3 id="_idParaDest-127"><a id="_idTextAnchor152"/>GitHub Flow</h3>&#13;
			<p>GitHub Flow is similar to GitFlow in<a id="_idIndexMarker814"/><a id="_idIndexMarker815"/> that it shares some of the same words in its name. Branching is a <a id="_idIndexMarker816"/><a id="_idIndexMarker817"/>core pillar of Git, and GitHub Flow uses this by keeping one long-lived branch, that is, the main or master branch. Developers then work in branches off main, where they can commit changes and experiment without affecting the main branch.</p>&#13;
			<p>These could be feature branches like in GitFlow, but there is no naming convention to be followed. It is important to name the branch sensibly using a descriptive name, such as sign-up-form or refactor-auth-service. No branches called another-new-feature-branch, please!</p>&#13;
			<p>At any point in time, the developer can raise a pull request, where other engineers can discuss the code and its approach, and design by providing feedback for the changes that are still in progress. The original author can then incorporate this discussion into the software. When the team is happy and the code is reviewed, the changes can be approved and merged to the main branch.</p>&#13;
			<p>GitHub Flow is great at promoting the peer review of work and promoting transparency in how a decision was made. Git by its nature is searchable, and the discussion on a merge request provides valuable insight and traceability into how architectural and coding decisions were made.</p>&#13;
			<h3 id="_idParaDest-128"><a id="_idTextAnchor153"/>Trunk-Based Development</h3>&#13;
			<p>Both GitHub Flow and GitFlow use <a id="_idIndexMarker818"/><a id="_idIndexMarker819"/>branching strategies <a id="_idIndexMarker820"/><a id="_idIndexMarker821"/>and merging patterns to bring independent development activities together. Branching in Git is made extremely easy. However, when <a id="_idIndexMarker822"/><a id="_idIndexMarker823"/>merging all of the branches together, conflicts can still occur that require human intervention.</p>&#13;
			<p>Depending on the frequency of this branching, some teams end up in merge hell, where everyone tries to bring their changes in at once, leading to the complex and often frustrating event of trying to unpick all the changes that were made while maintaining a working code base!</p>&#13;
			<p>Trunk-based <a id="_idIndexMarker824"/><a id="_idIndexMarker825"/>development (<a href="https://trunkbaseddevelopment.com/">https://trunkbaseddevelopment.com/</a>) takes a somewhat different approach to this particular problem by saying <strong class="bold">no</strong> to branches!</p>&#13;
			<div>&#13;
				<div id="_idContainer159" class="IMG---Figure">&#13;
					<img src="../Images/B16297_06_21.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 6.21: Merge hell</p>&#13;
			<p>In trunk-based development, developers collaborate on one single main branch referred to as the trunk. Devs work on their changes and apply them directly to the trunk. In a perfect world, the commits are small in nature and frequent throughout the development process. The golden rules here are never break the build and always be release-ready. In this regard, a developer must always ensure this is the case. Development could be automated using some CI process, but the key is that the trust within the team must be there.</p>&#13;
			<p>In a large-scale enterprise, this constant merging to master sounds like it could create headaches. How, for example, do you do a peer review of the code? For large-scale application development with many engineers and many teams, it is suggested that very short-lived feature <a id="_idIndexMarker826"/><a id="_idIndexMarker827"/>branches can be a great help. They provide decision logs during the review process, but the key here is short. Short-lived feature branches should only be alive for a day or two at most (definitely no longer than a sprint) and are deleted once the code is merged to prevent them from becoming feature release branches.</p>&#13;
			<h3 id="_idParaDest-129"><a id="_idTextAnchor154"/>Too Many Choices — Tell Me What to Do</h3>&#13;
			<p>Each of these Git workflows has been tried and tested with teams for years. Some teams choose one as a standard, whereas others adopt one or more depending on their own context.</p>&#13;
			<p>The original author of GitFlow recently<a id="_idIndexMarker828"/><a id="_idIndexMarker829"/> revised his views to suggest GitFlow does not work well for "applications that are delivered continuously" such as web apps. Branches can create distance between production code and work in progress and GitFlow sees code moving between several branches before it's released. If we think of a developer working on their feature, they get it merged into the develop branch and then they move on to a new feature branch.</p>&#13;
			<p>The new feature sits waiting (potentially along with some other features that have been completed) until the end of the <a id="_idIndexMarker830"/><a id="_idIndexMarker831"/>development cycle. At this point, it's bundled up and moved across to master via the release branch, possibly two weeks after the work was completed. All of these extra steps mean a developer is not getting the feedback they need from users in the field for a long time after the development is complete. In terms of a feedback loop, if the rework is required on the item or a bug arises, it could take weeks before it is rectified. Add to that the context switch for the developer, who has to go back over what they previously did, which could end up having an impact on the team's velocity.</p>&#13;
			<p>In CI, does having feature <a id="_idIndexMarker832"/><a id="_idIndexMarker833"/>branches slow us down? It's very easy for a developer to effectively hide out on their branch while development is happening. We have worked with teams in the past who have claimed to be doing CI, but their build system remains idle until the day of review. At this point, all the developers rush to integrate their features in a last-minute mini-integration session that often reveals misunderstanding in the designs or broken software. Long-lived feature branches do not easily marry up with CI.</p>&#13;
			<p>Short-lived feature branches are a great <a id="_idIndexMarker834"/><a id="_idIndexMarker835"/>way to help with some of these concerns. Developers can work away on small chunks in an isolated way and still merge frequently. Short feedback loops are the king of improving <a id="_idIndexMarker836"/><a id="_idIndexMarker837"/>software delivery metrics. If branches add time to this loop, how <a id="_idIndexMarker838"/><a id="_idIndexMarker839"/>can we tighten it further? Peer reviews can often be a burden to teams by creating a dependency on one individual or breaking the focus of another engineer in order to complete a feature. By pairing engineers, you gain implicit peer review. Pushing changes as a pair straight to the trunk is a great way to achieve speed. In a container ecosystem, you only want to build once and verify your application is working before deploying it to many places. Trunk-based development underpins this by encouraging frequent small changes pushed straight to the head, where CI and CD can then take over.</p>&#13;
			<p>From our experience in kickstarting product teams with varying skill sets, choosing the right one should be seen as more of a pathway, a sliding scale from immature to mature teams. Teams that are new to Git may find the use of feature branches a comforting way to not step on the toes of other developers. The book <em class="italics">Accelerate</em><span id="footnote-021-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-021">3</a></span> measured the software delivery performance of many teams and concluded that high-performing teams use trunk-based development.</p>&#13;
			&#13;
			<p>No matter what you choose as your approach to managing code, the key here is the frequency of delivery. How long will it take you to get software into the hands of your end users? Does having feature branches slow you down? Or do those branches provide you with a safe place for your team to start? As the team matures and becomes more familiar with each other and the tools, your software output can increase.</p>&#13;
			<p>The big call to action here is to let the teams choose the way that works best for them and build the automation around the workflow and tools. This allows the developers to focus on the hard stuff – writing code not managing the code. Initially, this will just be a guess. Teams should use retrospectives to assess whether things are working or not and evolve accordingly. It's important to not set out one dogma to fit all development activities across all teams because every team is going to be different. One shoe size is not going to fit everyone!</p>&#13;
			<div id="footnote-021" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-021-backlink">3</a>	<a href="https://itrevolution.com/book/accelerate/">https://itrevolution.com/book/accelerate/</a></p>&#13;
			</div>&#13;
			<h2 id="_idParaDest-130"><a id="_idTextAnchor155"/>Conclusion</h2>&#13;
			<p>In this chapter, we learned that we could get off to a great start by being green from go! By automating the deployment of our <a id="_idIndexMarker840"/><a id="_idIndexMarker841"/>application build and packaging tools, Jenkins and Helm can establish a technical foundation that will allow our teams to integrate continuously (CI) and continuously deploy (CD) our code to production.</p>&#13;
			<p>We learned that we can align our developer code workflow across our team and begin to iterate on our CI/CD pipelines to help us deliver applications faster. We can increase code quality and understanding by pairing developers together to help shorten the code review feedback loop.</p>&#13;
			<p>As a team, we learned <a id="_idIndexMarker842"/><a id="_idIndexMarker843"/>all of these new skills and techniques together by trying mob programming and, in the process, said goodbye to our love for Unicorn developers.</p>&#13;
			<div>&#13;
				<div id="_idContainer160" class="IMG---Figure">&#13;
					<img src="../Images/B16297_06_22.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 6.22: Adding technical practices to the foundation</p>&#13;
			<p>In the second half of <em class="italics">Open Technology Practices</em>, we <a id="_idIndexMarker844"/><a id="_idIndexMarker845"/>will learn about the bigger picture, discover what GitOps is all about, vastly improve our code quality through testing, and finish off with some lessons about our emerging architecture.</p>&#13;
		</div>&#13;
</body></html>