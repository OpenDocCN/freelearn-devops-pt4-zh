<html><head></head><body>
		<div>&#13;
			<div id="_idContainer418" class="Content">&#13;
			</div>&#13;
		</div>&#13;
		<div id="_idContainer419" class="Content">&#13;
			<h1 id="_idParaDest-298">13. <a id="_idTextAnchor384"/>Measure and Learn</h1>&#13;
		</div>&#13;
		<div id="_idContainer443" class="Content">&#13;
			<p><em class="italics">Startup success can be engineered by following the process, which means it can be learned, which means it can be taught.</em> – Eric Ries</p>&#13;
			<p>In his book, <em class="italics">The Lean Startup</em>, Eric Ries describes <a id="_idIndexMarker1887"/><a id="_idIndexMarker1888"/>a startup company as a <em class="italics">human institution designed to </em><em class="italics"><a id="_idIndexMarker1889"/><a id="_idIndexMarker1890"/></em><em class="italics">create a new product or service under conditions of extreme uncertainty</em>. He outlines a process to help deal with this uncertainty where a tight feedback loop is created around the creation of a <strong class="bold">minimum viable product</strong> (<strong class="bold">MVP</strong>). He argues that being able to react, fail fast, and use a data-driven approach to measurement assists in decision-making that is based on reason rather than emotion. This ability to learn from small experiments can be seen as a form of business agility – the ability to pivot quickly in the face of ever-changing circumstances. In lean terms, this feedback loop can be summarized as <strong class="bold">Build, Measure, Learn</strong>.</p>&#13;
			<p>The cultural and human aspects of this process cannot be overlooked. Uncertainty and humanity are common bedfellows. Ron Westrum, an American sociologist, posits that organizations with better "information flow" function more effectively. He argues that a good culture requires trust and cooperation between people across the organization, and therefore it reflects the level of collaboration and trust inside the organization.</p>&#13;
			<p>Second, better organizational culture can indicate higher-quality decision-making. In a team with this type of culture, not only is better information available for making decisions, but those decisions are more easily reversed if they turn out to be wrong because the team is more likely to be open and transparent rather than closed and rigid.</p>&#13;
			<p>So, how can we take these ideas and make them actionable in our delivery processes? When timelines for delivery are tight and deadlines are fast approaching, a team's ability to deliver and operate software systems is critical to the business performance.</p>&#13;
			<p>With software, there are often two competing forces at work. Innovation, which inherently is accompanied by system change, and running software, which is serving end customers and implies that the system is stable. We can identify two important areas to focus on here:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>To help measure the effectiveness of a team's development and delivery practices</li>&#13;
				<li>To start measuring and monitoring activities that allow the rapid diagnosis of issues</li>&#13;
			</ul>&#13;
			<p>In this chapter, we are going to explore different mechanisms and techniques to take measurements out of our Delivery Loops and use those measurements to drive decisions and next steps. We call this approach <strong class="bold">metrics-driven transformation</strong>. We will also learn what to measure and how we can take these measurements and make them visible, ultimately to help answer the question of whether or not we have moved the needle at all during our transformation journey.</p>&#13;
			<h2 id="_idParaDest-299"><a id="_idTextAnchor385"/><a id="_idTextAnchor386"/>Metrics-Driven Transformation</h2>&#13;
			<p>Metrics-driven transformation <a id="_idIndexMarker1891"/><a id="_idIndexMarker1892"/>focuses on using value-based business metrics to understand how technology-related investments impact organizational performance and provide specific tools and guidance to help improve those metrics.</p>&#13;
			<p>In the previous chapter, we looked at different approaches to doing delivery, whether that be Waterfall, or using an Agile framework such as Scrum or Kanban. As we complete loops of delivery, we want to take value-based metrics to validate hypotheses, confirm the results of experiments, clarify the impact of our feature deliveries, determine whether we have moved toward the Target Outcomes we set out, and make decisions around what to do next.</p>&#13;
			<p>There are many different levels of measurements we can take in our delivery ecosystem and a growing number of sources we can collect them from. In this chapter, we will explore metrics we can collect automatically from our software and our platform as well as practices we can use to collect metrics from our users, customers, employees, and the wider organization. Let's start by re-visiting some of the practices we've already used and see how we can use them to collect measurements and learning. </p>&#13;
			<h2 id="_idParaDest-300"><a id="_idTextAnchor387"/><a id="_idTextAnchor388"/>Where to Measure and Learn</h2>&#13;
			<p>In <em class="italics">Chapter 10</em>, <em class="italics">Setting Outcomes</em>, we introduced the practice of setting Target Outcomes based on all of the learning that came out of the practices on the Discovery Loop. We showed how to make these measurable and how to visualize them as an information radiator so everyone can inspect them. Using metrics, we can inspect where we are now in quantifiable terms, where we've been, and where we want to get to for each measurable outcome.</p>&#13;
			<p>We explained the<a id="_idIndexMarker1893"/><a id="_idIndexMarker1894"/> difference between primary (business-focused) outcomes and supporting enabling outcomes, which are non-functional-based outcomes. Since then, we have organized all of our <a id="_idIndexMarker1895"/><a id="_idIndexMarker1896"/>work around those Target Outcomes and have radiated them in other practice artifacts, including the Value Slice on the Options Pivot and Scrum boards in the Delivery Loop. Those <a id="_idIndexMarker1897"/><a id="_idIndexMarker1898"/>outcomes should be our starting point for measurement, and re-visualizing them will allow us to measure the progress we have made (or not) toward the target value:</p>&#13;
			<div>&#13;
				<div id="_idContainer420" class="IMG---Figure">&#13;
					<img src="../Images/B16297_13_01.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 13.1: Measuring targets</p>&#13;
			<p>When measuring the outcomes, we need to be aware of any prior measurement that may form an historic baseline, and potentially what any estimate of value may be against the actual value. Thinking about these different forms of measurement leads us to ask the question, <em class="italics">Where and when should we take measures and learn, and where should we inspect them?</em></p>&#13;
			<h3 id="_idParaDest-301">T<a id="_idTextAnchor389"/><a id="_idTextAnchor390"/>he Showcase</h3>&#13;
			<p>Ideally, we take<a id="_idIndexMarker1899"/><a id="_idIndexMarker1900"/> measurements as soon as a delivery item is complete. That might be at the end of a sprint in Scrum or after delivering a completed feature in Kanban. Perhaps we can build in metrics data collection to the application code and run a report just before or during a Showcase. For example, if we have a Target Outcome around increasing the user base to 10,000 people, every Showcase could provide an update on what the current user base is and whether it is moving in the right direction toward the target.</p>&#13;
			<p>There may be a lag in the availability of data that means it cannot be presented at Showcase events. In this case, there are two plans of attack. First, we can use the Showcase itself to collect some measurements and learning from stakeholders. Alternatively, we could collect measurements and learning from stakeholders and users on the latest Product Increment. But how?</p>&#13;
			<p>Perhaps we have an outcome around stakeholder confidence and/or user happiness in the application we are incrementally building. If so, then what better opportunity than to ask the stakeholders after seeing the Showcase of the latest increment how confident or happy they are with what they have seen? This can be quantitative, in that we can ask stakeholders to rate the product on a score of 1-10. It can also be qualitative in the conversation that happens around this scoring and supplementary feedback from collecting feedback data.</p>&#13;
			<p>Perhaps we have an outcome around employee engagement and team skills. Again, what better opportunity than at the end of a Delivery Loop iteration to survey the team on their happiness level and ask them to rate themselves on different skills? This visualization will not only allow us to see trends, but also identify the positive and negative effects as a result of the team education and cultural activities that we are undertaking.</p>&#13;
			<p>Showcase events allow the opportunity to show many different kinds of metrics: Software Delivery metrics, Platform metrics, and Team Velocity metrics. We will explore the details of these shortly. If these are things that will be of interest to the audience and help them understand the impact of everything the team is doing, by all means, include them. However, the event where a more in-depth conversation can happen is the event that typically follows the Showcase, the Retrospective. </p>&#13;
			<h3 id="_idParaDest-302">T<a id="_idTextAnchor391"/><a id="_idTextAnchor392"/>he Retrospective</h3>&#13;
			<p>We introduced the<a id="_idIndexMarker1901"/><a id="_idIndexMarker1902"/> Retrospective practice in the previous chapter and looked at many different formats of running them. Let's consider metrics a little further and also an engineer's perspective of Retros.</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-303" class="Author-Heading"><a id="_idTextAnchor393"/>The Retrospective – an Engineering Perspective</h2>&#13;
			<div>&#13;
				<div id="_idContainer421" class="IMG---Figure" style="float: right; margin: 6px; hight:6cm; width:6cm;">&#13;
					<img src="../Images/Author_4.jpg" alt="" width="250" height="250"/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Back in the late 1980s, I vividly remember being introduced to<a id="_idIndexMarker1903"/><a id="_idIndexMarker1904"/> <strong class="bold">feedback loops</strong> and <strong class="bold">control theory</strong> in chemical engineering classes, way before software engineering totally enveloped my career! If you wanted to control the level in a tank, the flow rate in a pipe, or nearly any dynamically changing system, you needed to learn how to keep that system stable. The software engineering practice of Retrospectives always makes me think of those second-order feedback loops, the physical connection back into our software design and development process that allows us to learn and adapt so that the system can become more stable.</p>&#13;
			<p>Retrospectives all have a common goal of allowing the team to inspect what has just happened, while allowing them to adapt and refine what goes on in the future. This is a critical function. If there is no feedback or feedback is not actioned, the team may start to lose faith in the whole delivery process. In the engineering world, if feedback fails, the tank overflows!</p>&#13;
			<p>It should come as no surprise that metrics should be included as hot topics in any Retro. Teams that discuss their <a id="_idIndexMarker1905"/><a id="_idIndexMarker1906"/>poorly performing SDO or CI/CD metrics can gain a lot of insight into what is going wrong with the software delivery process. So, celebrate when the build starts to take 20 minutes to complete. This means that the metrics measurement and trending are in place and the team can now take action to find out why it has gotten so slow and seek to improve it. </p>&#13;
			<p>It's possible to save a lot of wasted time and resources by listening to timely feedback. If feedback is ignored, you can incur an opportunity cost by spending the next year carrying out remediation projects instead of rolling out new, revenue-generating features. The moral of the story is to Retro, Retro, Retro! You can really never have enough feedback.</p>&#13;
			</div>&#13;
			<p>With all the tools and technology put in place when we created the technical foundation (in <em class="italics">Chapter 6, Open Technical Practices – Beginnings, Starting Right</em>, and <em class="italics">Chapter 7, Open Technical Practices – The Midpoint</em>), there are huge amounts of data, metrics, and analysis we can collect and conduct. If you run pub Retros (as introduced in the previous chapter), there is nothing better than taking a few print-outs of reports from these tools and taking them down the pub to analyze together over a pint of Guinness!</p>&#13;
			<div>&#13;
				<div id="_idContainer422" class="IMG---Figure">&#13;
					<img src="../Images/B16297_13_02.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 13.2: Pub Retrospective – discussing metrics</p>&#13;
			<p>Export your build statistics, latest test results, static code analysis reports, Burndown charts, and whatever else you can find, lay them all out on the table, and ask yourselves: what does this data tell us about us as a team that we don't know already? What can we learn about ourselves? What patterns exist? What can we do in the next sprint to make these measurements better?</p>&#13;
			<p>Retrospective <a id="_idIndexMarker1907"/><a id="_idIndexMarker1908"/>actions that are<a id="_idIndexMarker1909"/><a id="_idIndexMarker1910"/> often taken include an increased focus on CI/CD infrastructure, increasing thresholds<a id="_idIndexMarker1911"/><a id="_idIndexMarker1912"/> around code coverage for testing, and adding more or faster feedback loops for the team to learn from.</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-304" class="Author-Heading"><a id="_idTextAnchor394"/>Inspecting the Build Stats at Retrospectives</h2>&#13;
			<p>This is one of my favorite examples for highlighting the power and impact of the Retrospective. I collected this chart back in 2008 when delivering an access control management solution for a UK telecoms company. My team was building a Java-based application using Scrum and continuous delivery. They used Hudson for continuous integration and had a huge number of automated tests that were incrementally built into the app as part of the team's Definition of Done.</p>&#13;
			<p>Several build graphs were exported every 2 weeks and taken to the team's Retrospective. The following chart shows the duration of the build as well as the success of the build (red means the build failed, yellow means the build succeeded but some automated tests failed, and blue means a successful build with all automated tests passing):</p>&#13;
			<div>&#13;
				<div id="_idContainer423" class="IMG---Figure">&#13;
					<img src="../Images/B16297_13_03.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Figure 13.3: Inspecting the build time statistics at Retrospectives (the graph has been annotated with a few labels to reflect the sprint that the data was a part of)</p>&#13;
			<p>As we can see, the build was very unstable in Sprint 1. A much earlier version of this chart was taken to the pub Retro at the end of Sprint 1. The team inspected it and agreed that, during Sprint 2, they would invest some time investigating the build stability issues.</p>&#13;
			<p>Two weeks later, Sprint 2 was completed and the ScrumMaster brought another print-out of this chart from Hudson. The good news was that the Sprint 1 Retro action had paid off as the build was much more stable. However, it was noticed that the build was sometimes taking more than 15 minutes to complete. This was a much longer feedback loop than needed, so a further retrospective action was taken in Sprint 3 to address this. We can see that from Sprint 3 onward, the build was mostly stable and relatively quick. </p>&#13;
			<p>Imagine if we had not used the Retrospective to look at this data. Imagine if we had let the slow build just fester over time. Imagine how much time would have been lost. This is why we do metrics-driven Retrospectives.</p>&#13;
			</div>&#13;
			<p>As we come out of the sprint, we <a id="_idIndexMarker1913"/><a id="_idIndexMarker1914"/>now have the opportunity to learn the results of experiments we've designed.</p>&#13;
			<h3 id="_idParaDest-305"><a id="_idTextAnchor395"/><a id="_idTextAnchor396"/>Experiments – the Results!</h3>&#13;
			<p>In <em class="italics">Chapter 11</em>, <em class="italics">The Options Pivot</em>, when we<a id="_idIndexMarker1915"/><a id="_idIndexMarker1916"/> introduced the Options Pivot, we introduced some of the advanced deployment strategies we could design in our experiments. If we decided to use one or more of these strategies when designing our experiments, now is the time to measure and learn from what actually happened.</p>&#13;
			<p>If we designed an A/B test, we look at the data collected about the traffic, interaction, time spent, and other relevant metrics so we can judge the effectiveness of the two different versions based on the change in users' behavior. </p>&#13;
			<p>If we designed a Canary Release, have we learned from the behavior of the users that were a part of the Canary Release candidate to validate whether the feature should be rolled out to the wider population? Similarly, with dark launches and feature toggles, we collect metrics from usage to assess whether the feature release should be extended to a larger user group, or should be switched off and dark launches rolled back.</p>&#13;
			<p>All of the learning captured from these experiments and the resulting analytics is quantitative. This means that you can study the data, observe trends, decide to extend the experiment, or create whole new experiments. As you run more experiments and gather more data, your decision-making capability becomes stronger and is based on metrics gathered from it.</p>&#13;
			<p>Of course, you can't validate everything through the use of numbers. Some commentary with further learning and understanding is needed by talking to end users, known as quantitative feedback. Let's explore a couple of practices to help us do this, starting with user testing.</p>&#13;
			<h3 id="_idParaDest-306"><a id="_idTextAnchor397"/><a id="_idTextAnchor398"/>User Testing</h3>&#13;
			<p>User-based testing is a<a id="_idIndexMarker1917"/><a id="_idIndexMarker1918"/> technique that focuses on user interactions with a product. These types of evaluation directly involve end users and focus on the person. Let's dive a little more into this by looking at two user testing practices: usability and guerilla testing.</p>&#13;
			<h3 id="_idParaDest-307"><a id="_idTextAnchor399"/><a id="_idTextAnchor400"/>Usability Testing</h3>&#13;
			<p>In a usability testing session, the <a id="_idIndexMarker1919"/><a id="_idIndexMarker1920"/>team observes real users interacting with the product. Typically, a facilitator <a id="_idIndexMarker1921"/><a id="_idIndexMarker1922"/>sits with a user and asks them to complete tasks and explain their thinking as they go. The team sits in a separate room and observes the testing by video link.</p>&#13;
			<p>A usability test is not a focus group; it's focused on what the user thinks and does in the real world. An Empathy Map, as introduced in <em class="italics">Chapter 8</em>, <em class="italics">Discovering the Why and Who</em>, can be a very useful supporting practice. </p>&#13;
			<p>A usability test can be run on an existing product, a prototype, or even a competitor's product. The prototype could be working code, or it could be something as simple as a few clickable images. Test early and often to create products that delight users and solve real needs.</p>&#13;
			<p>Usability testing often highlights something that's obvious to someone who has been working on a product, but might be confusing to a user. What we think users need might not be what they actually need. Indeed, what users think they need may not be what they actually need! Usability testing can help answer questions such as, <em class="italics">Are we on the right track?</em> <em class="italics">What problems do we still need to solve?</em>, or <em class="italics">Which features should we build next?</em> With early feedback from real users, teams can avoid sinking time into a feature that's confusing or not useful.</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-308" class="Author-Heading"><a id="_idTextAnchor401"/>"We Are Not Our Users"</h2>&#13;
			<div>&#13;
				<div id="_idContainer424" class="IMG---Figure" style="float: right; margin: 6px; hight:6cm; width:6cm;">&#13;
					<img src="../Images/Donal.jpg" alt="" width="250" height="250"/>&#13;
				</div>&#13;
			</div>&#13;
			<p>One of the best designers I've ever worked with once told me that <em class="italics">We are not our users</em>. Since I spent a fair amount of time in front of a code editor among my development team, I thought I got what he meant. I didn't fully appreciate what he meant until the following incident occurred sometime later. </p>&#13;
			<p>We were building a lightweight prototype for a mortgage application. The frontend was using an existing bank API for calculating mortgages and the application we were building on top asked a few initial questions before prompting the person to enter an estimate of their income. The text box initially filled in by the user had a placeholder saying, <em class="italics">e.g. 100,000.00</em>. </p>&#13;
			<p>While doing some development and testing with the product owner and user experience designer, we must<a id="_idIndexMarker1923"/><a id="_idIndexMarker1924"/> have filled out the form hundreds of times! Every time we used the form, we filled it out the exact same way, just by putting 100000 into that box. Not once had any of us ever put a decimal place in the box!</p>&#13;
			<p>Fast-forward a few weeks and we were doing some real-world testing. Of course, the very first person we have to use the application goes through the form, putting "120000.00" into the box and hitting submit. As you can probably imagine, we expected everything to work swimmingly and the user feedback session to continue; but instead, the application crashed. </p>&#13;
			&#13;
			<div>&#13;
				<div id="_idContainer425" class="IMG---Figure">&#13;
					<img src="../Images/B16297_13_04.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Figure 13.4: Mortgage application</p>&#13;
			<p>The business analyst who was running the usability test immediately phoned in to say that the app was broken. We replayed the exact thing the user did, only to discover that the bank's API was unable to accept a decimal point. But for us, the real surprise was that no one on our team had ever noticed this issue before. As a small team, we were quick to fix the issue by updating the placeholder text and sending only an integer to the API.</p>&#13;
			<p>This example always reminds me that we are not our users. Users do weird and wonderful things in an application. Even if you try to test all the scenarios, there is almost always going to be some edge case you have not thought of, and the chances are the first person to use your app will do the one weird thing!</p>&#13;
			</div>&#13;
			<p>Read more about the usability testing practice, share your own experiences, ask questions, and improve the practice further in the Open Practice Library at <a href="http://openpracticelibrary.com/practice/usability-testing">openpracticelibrary.com/practice/usability-testing</a>.</p>&#13;
			<p>Some of the challenges you may encounter when trying to organize usability testing include getting actual customers to access legacy systems or not having the time, money, or experts to conduct this level of testing. Guerrilla testing offers a low-cost alternative or supplementary form of qualitative user testing. </p>&#13;
			<h3 id="_idParaDest-309"><a id="_idTextAnchor402"/><a id="_idTextAnchor403"/>Guerrilla Testing</h3>&#13;
			<p>Guerrilla testing is a low-cost, lean, and <a id="_idIndexMarker1925"/><a id="_idIndexMarker1926"/>Agile method of collecting data for testing and validating a hypothesis in a short session focused on specific tasks. Participants are not recruited in advance, but instead are approached in a number of environments by the team, where similar demographics are targeted; for example, customers in coffee shops, or administrators in an office environment.</p>&#13;
			<p>This testing provides a simple method for collecting enough data to make well-informed strategic design decisions. It can also assist senior stakeholders and product teams in understanding the importance of usability testing and customer feedback. Everyone on the team can facilitate without any research experts. It is a flexible approach that can be implemented at any stage of product development.</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-310" class="Author-Heading"><a id="_idTextAnchor404"/>Guerrilla testing with a box of donuts in a busy Dublin bank!</h2>&#13;
			<div>&#13;
				<div id="_idContainer426" class="IMG---Figure">&#13;
					<img src="../Images/Donal.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Doing any usability testing or generating feedback from a control group doesn't have to be a large costly exercise involving huge numbers of focus groups; it can be a simple activity! In the previous example, we undertook guerrilla testing after every sprint. It was an amazing way to generate real-world feedback and cost us next to nothing. </p>&#13;
			<p>We would go into a branch of a bank, choosing a different one each week in different parts of the city to get a range of viewpoints. All we brought with us was the application and a massive box of donuts! The plan was simple. All we would do is lure people over to our stand and ask them to take part in a simple feedback session in exchange for free donuts! Users would road test the application and give us some valuable insight into things we had overlooked or failed to account for. The cost of the box of donuts was cheap but the feedback it gave us was invaluable.</p>&#13;
			</div>&#13;
			<p>Read more about the guerilla <a id="_idIndexMarker1927"/><a id="_idIndexMarker1928"/>testing practice, share your own experiences, ask questions, and improve the practice further in the Open Practice Library at <a href="http://openpracticelibrary.com/practice/guerilla-testing/">openpracticelibrary.com/practice/guerilla-testing/</a>.</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-311" class="Author-Heading"><a id="_idTextAnchor405"/>PetBattle Usability Testing</h2>&#13;
			<p>By looking at both the smooth delivery of changes and the overall stability of the production system, the PetBattle team hopes to avoid the types of failures seen when the hobbyist version of the application went viral and, at the same time, rapidly introduce new fixes and features to the software product.</p>&#13;
			<p>The first area the team would like to gather data from and measure is how long it takes to get new features into their testing environment. By automating and reducing the amount of time it takes to get fixes and features into an environment where testing can occur, this will shorten the feedback loop so the team can get fast feedback on their changes rather than having to wait for a release to happen. The PetBattle team is not sure exactly what the right metrics to measure here are, but thought that starting with this one would help them discover more software delivery metrics that could be useful.</p>&#13;
			<p>The second area of measurement the team would like to focus on is some simple performance testing. In the test environment, the plan is to concurrently load up the PetBattle application to see how it behaves and to try to identify any bugs or bottlenecks in the system. Some pre-emptive checking here will hopefully reveal any issues the team saw in the hobbyist version of the app. Again, the team is not quite sure exactly what parts of the system to focus on with their performance testing, or what metrics they should target yet, but deploying the PetBattle suite into an environment where this testing activity can happen is the right first step.</p>&#13;
			</div>&#13;
			<p>Taking a Measure and Learn <a id="_idIndexMarker1929"/><a id="_idIndexMarker1930"/>approach to our Showcases, Retrospectives, experiments and user testing moves us into a continuous cycle of building something small, finding a way to measure from it, and gathering the data and learning from it, which then generates further ideas.</p>&#13;
			<p>The <em class="italics">Build, Measure, Learn</em> feedback loop made famous in <em class="italics">The Lean Startup</em> is <a id="_idIndexMarker1931"/><a id="_idIndexMarker1932"/>one of the most powerful tools and mindsets that is enabled by the platform, technology, and cultural tools we've been equipped with throughout this book:</p>&#13;
			<div>&#13;
				<div id="_idContainer427" class="IMG---Figure">&#13;
					<img src="../Images/B16297_13_05.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 13.5: Lean – Build, Measure, Learn</p>&#13;
			<p>Let's go into a little more detail about what to measure. </p>&#13;
			<h2 id="_idParaDest-312">W<a id="_idTextAnchor406"/><a id="_idTextAnchor407"/>hat to Measure?</h2>&#13;
			<p><em class="italics">What you measure is what you get.</em> – H. Thomas Johnson</p>&#13;
			<p>In assessing what to measure as part of the <em class="italics">Build, Measure, Learn,</em> feedback loop, we are going to be standing on the <a id="_idIndexMarker1933"/><a id="_idIndexMarker1934"/>shoulders of giants. There is a whole literature section on DevOps metrics available to us and we are going to call out our current favorites here. Top of that list is the DevOps DORA report<span id="footnote-078-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-078">1</a></span> and the book <em class="italics">Accelerate</em>,<span id="footnote-077-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-077">2</a></span> which are works where scientists described how they have taken a data-driven approach to measuring DevOps culture and practice. In the <strong class="bold">DORA</strong> report, effective IT delivery organizations take about an hour to get code from <a id="_idIndexMarker1935"/><a id="_idIndexMarker1936"/>committed into trunk (in Git) to "running in production". This sounds great! So let's look at some of the detailed metrics that allow us to hone in on such a goal.</p>&#13;
			&#13;
			<h3 id="_idParaDest-313">M<a id="_idTextAnchor408"/><a id="_idTextAnchor409"/>easuring Service Delivery and Operational Performance (SDO)</h3>&#13;
			<p>A key question for any team is, <em class="italics">What does good look like?</em></p>&#13;
			<p>From the research in <a id="_idIndexMarker1937"/><a id="_idIndexMarker1938"/>Accelerate, it appears <a id="_idIndexMarker1939"/><a id="_idIndexMarker1940"/>that leading organizations update their software many times a day instead of once every few months, increasing their ability to use software to explore the market, respond to events, and release features faster than their competition. This huge increase in responsiveness does not come at a cost in stability or quality though, since failures are found and fixed quickly.</p>&#13;
			<p>Measuring software delivery performance is a difficult task. It would be easy to think that output-based measurements would suffice. Some often state that measurements might look similar to the following:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>Lines of code written and committed by developers per day</li>&#13;
				<li>Team utilization as a measure of team productivity</li>&#13;
				<li>Team velocity (or the number of stories/features delivered per sprint)</li>&#13;
			</ul>&#13;
			<p>Unfortunately, if we dig into these a little, we can quickly find problems with all of these measurements.</p>&#13;
			<div id="footnote-078" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-078-backlink">1</a>	<a href="https://www.devops-research.com/research.html#reports">https://www.devops-research.com/research.html#reports</a></p>&#13;
				<div id="footnote-077" class="_idFootnote" epub:type="footnote">&#13;
					<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-077-backlink">2</a>	<a href="https://itrevolution.com/book/accelerate">https://itrevolution.com/book/accelerate</a></p>&#13;
				</div>&#13;
			</div>&#13;
			<p>Is a solution that can be written with a 100-line piece of code better than a 200-line piece of code? On the surface it would seem so, but which solution is easier to maintain over time? Which solution is clearer and easier for new developers to discover and figure out? It may be that the 200-line piece of code is much easier to maintain and learn for the team rather than the expertly crafted bit of black magic code that only the expert understands. If a team is 100% utilized on feature development, when do they get time to learn, do unplanned work, or manage technical debt? Not having time for these activities ultimately slows delivery and innovation to a grinding halt. If a team delivers 10 <a id="_idIndexMarker1941"/><a id="_idIndexMarker1942"/>stories per sprint, is this better than a team that delivers 5 stories per sprint? Can we really compare different teams' output when they work on unrelated work items? Probably not. Are the user stories the same size and value for our business and end customers? It is not easy to judge across teams with different products and ownership.</p>&#13;
			<p>These measurements are what we call output-based. We can instead shift the focus from what a team produces more toward the goals or Target Outcomes. By focusing on the global outcomes, the team members aren't pitted against each other by measuring the wrong thing. A classic example is rewarding Development Team members for throughput on new features and Operations Team members for service stability. Such measurements incentivize developers to throw poor-quality code into production as quickly as possible and operations to place painful change management processes in the way to slow down change. </p>&#13;
			<p>By not focusing on measurement or the right things to measure and focusing on outputs rather than outcomes is a quick way to get into trouble. Luckily, the DORA report starts to lay out some of the key metrics that a team can use for SDO:</p>&#13;
			<div>&#13;
				<div id="_idContainer428" class="IMG---Figure">&#13;
					<img src="../Images/B16297_13_06.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 13.6: DORA – Performance metrics</p>&#13;
			<p><strong class="bold">Software Development Metrics</strong> are metrics that capture the effectiveness of the development and delivery process by<a id="_idIndexMarker1943"/><a id="_idIndexMarker1944"/> measuring the following:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li><strong class="bold">Lead Time</strong>: The time code is checked in to when it is released into production</li>&#13;
				<li><strong class="bold">Deployment Frequency</strong>: How often <a id="_idIndexMarker1945"/><a id="_idIndexMarker1946"/>the team can release code into production</li>&#13;
			</ul>&#13;
			<p>Lead time is a key element of Lean Theory: by shortening the amount of time it takes to get product features to end users, the team can shorten the feedback loop for learning what their end users like or do not like. If the team is building the wrong thing, they can correct their course (or pivot) quickly when they have short lead times.</p>&#13;
			<p>Another element to this is the size of the work delivered, or <strong class="bold">Batch Size</strong>. By delivering small incremental value to end users quickly, lead times are kept lower. With a much faster delivery cadence, how do we make sure system stability does not suffer?</p>&#13;
			<p><strong class="bold">Software Deployment Metrics</strong> are the metrics that <a id="_idIndexMarker1947"/><a id="_idIndexMarker1948"/>capture system stability and release quality. They are measured by the following:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li><strong class="bold">Time to Restore</strong>: The time taken from detecting a user-impacting incident to having it fixed or resolved</li>&#13;
				<li><strong class="bold">Change Failure Rate</strong>: The number of released changes that fail or result in a user-impacting incident</li>&#13;
			</ul>&#13;
			<p>What percentage of the changes made to production fail? How long does it take to restore service to end users in the event of failure? Development teams will have happier customers if outages are fixed quickly when changes occur and ideally have changes that do not fail at all!</p>&#13;
			<p><strong class="bold">Service Operation Metrics</strong> captures <a id="_idIndexMarker1949"/><a id="_idIndexMarker1950"/>operational performance via metrics called <strong class="bold">Service Availability</strong>. Assume your product sales website has crashed. The company accountant might<a id="_idIndexMarker1951"/><a id="_idIndexMarker1952"/> ask the question, <em class="italics">Is our application generating revenue right now?</em> Measuring service availability is a good way to link a technical goal with the desired business outcome.</p>&#13;
			<p>The metrics defined so far go a long way in helping a team understand key software, delivery, and operations metrics that are desirable for better organizational outcomes. To help capture and radiate these metrics, Red Hat has been investing in an open source project that has resulted in a dashboarding tool called Pelorus<span id="footnote-076-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-076">3</a></span>.</p>&#13;
			<div id="footnote-076" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-076-backlink">3</a>	<a href="https://github.com/konveyor/pelorus/">https://github.com/konveyor/pelorus/</a></p>&#13;
			</div>&#13;
			<h3 id="_idParaDest-314">Pe<a id="_idTextAnchor410"/><a id="_idTextAnchor411"/>lorus</h3>&#13;
			<p>Pelorus is an executive dashboard that helps visualize<a id="_idIndexMarker1953"/><a id="_idIndexMarker1954"/> the progress we make on the SDO success metrics. It makes use of open source tools such as Prometheus and Grafana to track progress both locally (within a team) and globally (across the organization):</p>&#13;
			<div>&#13;
				<div id="_idContainer429" class="IMG---Figure">&#13;
					<img src="../Images/B16297_13_07.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 13.7: Pelorus – SDO dashboard and metrics</p>&#13;
			<p>Pelorus consists of a set of <a id="_idIndexMarker1955"/><a id="_idIndexMarker1956"/>exporters to customize data points to capture metrics from different providers. The sources from which exporters automate the collection of metrics are growing as more people contribute. This currently includes OpenShift (as Deploy time exporter), Git providers GitHub, GitLab, and <a id="_idIndexMarker1957"/><a id="_idIndexMarker1958"/>Bitbucket (as Commit time exporters), and JIRA and ServiceNow (as established Issue trackers). </p>&#13;
			<p>Using the data points that are collected from the providers, metric indicators are calculated to represent a measure. Each outcome is made measurable by a set of representative measures: <strong class="bold">Lead Time for Change</strong>, <strong class="bold">Deployment Frequency</strong>, <strong class="bold">Mean Time to Restore</strong>, and<strong class="bold"> Change Failure Rate</strong>.</p>&#13;
			<p>Pelorus offers a great opportunity to have real-time information radiators next to teams giving out these important metrics. Teams can also regularly inspect and discuss these at Showcase and Retrospective events and ask themselves what improvement actions or experiments they can run to try and improve these metrics further. What else should we measure?</p>&#13;
			<h3 id="_idParaDest-315">Mea<a id="_idTextAnchor412"/><a id="_idTextAnchor413"/>suring Lean Metrics</h3>&#13;
			<p>The Lean movement also proposed a<a id="_idIndexMarker1959"/><a id="_idIndexMarker1960"/> number of metrics to help <a id="_idIndexMarker1961"/><a id="_idIndexMarker1962"/>measure software delivery performance. In their simplest form, you only need two pieces of information per work item or story: the start and finish date. This sounds easy to measure! But, of course, you need some policy around what those definitions are. With these two measurements we can start to measure a lot of different things, in particular:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li><strong class="bold">Time in Process</strong>: The units of time per unit of work.</li>&#13;
				<li><strong class="bold">Lead Time</strong>: The amount of time between when a feature is requested to when it is delivered into production.</li>&#13;
				<li><strong class="bold">Flow Efficiency (or touch time/lead time)</strong>: The time that the product is actually being worked on by the team, and value is being added.</li>&#13;
				<li><strong class="bold">Due Date Performance</strong>: How often the feature gets delivered on time.</li>&#13;
			</ul>&#13;
			<p>Most of these metrics are<a id="_idIndexMarker1963"/><a id="_idIndexMarker1964"/> part of what is known as <strong class="bold">lagging</strong> indicators for performance. In other words, they measure what has already happened. It is also possible to formulate <strong class="bold">leading</strong> indicators that can help <a id="_idIndexMarker1965"/><a id="_idIndexMarker1966"/>predict future performance from this data. One such measurement example is based on the flow of work items or stories into and out of a team. This net flow of work items allows us to predict the confidence of due date delivery. As teams accept more and more work, this slows down their ability to deliver items on time. So, the net flow of stories in and out of a team becomes a lead indicator for measuring work item delivery performance.</p>&#13;
			<p>It is surprising what start and end dates can tell us. The distribution of items with time can also help categorize the type of work undertaken by a team. For example, normal work items may look different to priority work items that come to the team from production failures. Another example may be work that requires long wait times for approval from, say, security or compliance teams. These would have large lead times compared to normal work items that don't require such approval.</p>&#13;
			<h3 id="_idParaDest-316">Mea<a id="_idTextAnchor414"/>suring SLOs, SLAs, and SLIs</h3>&#13;
			<p>The goal of these <a id="_idIndexMarker1967"/><a id="_idIndexMarker1968"/><strong class="bold">service level</strong> (<strong class="bold">SL</strong>) metrics is to get customers, vendors, and users all on the same page regarding system behavior and <a id="_idIndexMarker1969"/><a id="_idIndexMarker1970"/>performance. In particular, everyone needs to know and agree upon <a id="_idIndexMarker1971"/><a id="_idIndexMarker1972"/>common SL questions, such as the following:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>How long for and how often will the system be available?</li>&#13;
				<li>If there is an outage, how quickly will the response be to restore the service?</li>&#13;
				<li>How fast will the system respond if we make a single request?</li>&#13;
				<li>What about if we make many concurrent requests?</li>&#13;
				<li>Users of a service want to know the answer to these questions so they can plan and design how they consume any given service. For example, if you are building a solution that must be available all the time for your end users and it depends on a third-party service that is only available during business hours, you may need a different design, to implement a caching solution, or use another service with higher availability.</li>&#13;
			</ul>&#13;
			<p>The SL acronyms <a id="_idIndexMarker1973"/><a id="_idIndexMarker1974"/>can be broadly defined as follows:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li><strong class="bold">SLA</strong>: An SL agreement that is normally <a id="_idIndexMarker1975"/><a id="_idIndexMarker1976"/>a formal contract made between your organization and your clients, vendors, and users.</li>&#13;
				<li><strong class="bold">SLO</strong>: SL objectives are the <a id="_idIndexMarker1977"/><a id="_idIndexMarker1978"/>outcomes your team must deliver to meet the agreement.<a id="_idIndexMarker1979"/><a id="_idIndexMarker1980"/></li>&#13;
				<li><strong class="bold">SLI</strong>: SL indicators are the actual <a id="_idIndexMarker1981"/><a id="_idIndexMarker1982"/>metric numbers that are<a id="_idIndexMarker1983"/><a id="_idIndexMarker1984"/> used to measure a team's performance.</li>&#13;
			</ul>&#13;
			<p>It can be very hard to measure SLAs properly. A service may be available but with degraded service performance, for example. It can also become more complicated when only some portion of your users experience a partial outage. Capturing this type of SLA complexity and measuring it accurately is hard to do.</p>&#13;
			<p>One benefit of SLAs <a id="_idIndexMarker1985"/><a id="_idIndexMarker1986"/>is that they allow IT managers to quantitatively measure business outcomes. So rather than having to deal with a generic qualitative complaint, such as, "My application won't load and it is very slow", they can measure application availability (uptime) and percentile page load speed instead.</p>&#13;
			<p>An SLO is an agreement about a metric within a given SLA. A simple example may be that we agree to return search results quickly to our end users, with an average search latency of 200 milliseconds. In contrast, the SLI typically measures the SLO. So, we might define a target upper bound for our search, for example, by specifying that search latency for 99% of all performed searches must be less than 300 milliseconds.</p>&#13;
			<p>By quantitatively specifying and publishing SLAs, SLIs, and SLOs for our services, end users can set their expectations on how well the service will perform. This prevents qualitative complaints about the service being slow or over-reliance on a service where users expect it to be more available than it actually is.</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-317" class="Author-Heading"><a id="_idTextAnchor415"/>PetBattle Service Levels</h2>&#13;
			<p>The current PetBattle hobbyist application<a id="_idIndexMarker1987"/><a id="_idIndexMarker1988"/> has no SLAs – zero, zilch, nada. PetBattle V2 is going to be<a id="_idIndexMarker1989"/><a id="_idIndexMarker1990"/> built for high availability. Given the app's popularity and the fact it is the sole source of potential income for our fledgling company, it has been designated as "mission-critical". Redundancy is going to be built in at every level: at the cloud infrastructure layer (network, storage, and compute), and the application layer (scalable redundant highly available services). PetBattle V2 will be re-designed to have a shared-nothing architecture to allow maximum horizontal scalability without bottlenecks. PetBattle V2 will be required to meet a 99.99% availability SLA. That's slightly more than 4 minutes of downtime allowed per month!</p>&#13;
			<p>Any external system that has a lower availability SLA will negatively impact the PetBattle V2 SLA when it fails. The PetBattle team writes the code and runs the service, and ideally there are as few external service providers in the Development and Delivery Loop as possible. Any new service that is incorporated into the application must be designed to meet these SLAs.</p>&#13;
			</div>&#13;
			<p>By quantitatively specifying and publishing SLAs, SLIs, and SLOs for our services, end users can set their expectations on how well the service will perform. This prevents metrics-driven qualitative complaints about the service being slow or over-reliance on a service where users expect it to be more available than it actually is.</p>&#13;
			<h3 id="_idParaDest-318"><a id="_idTextAnchor416"/><a id="_idTextAnchor417"/><a id="_idTextAnchor418"/>Measuring Security</h3>&#13;
			<p>Malicious users are out there. How <a id="_idIndexMarker1991"/><a id="_idIndexMarker1992"/>can we ensure that the user data that is kept in our applications is not misused? Or that our application services are not put to unintended usage, causing organizational or reputational harm? It is commonplace to see data leaks and security breaches related to software applications and services in the media today. Answering these questions is most often the primary concern of an <strong class="bold">information security</strong> (<strong class="bold">InfoSec</strong>) analyst <a id="_idIndexMarker1993"/><a id="_idIndexMarker1994"/>or team <a id="_idIndexMarker1995"/><a id="_idIndexMarker1996"/>of people. A modern approach to help tackle these security concerns is called <strong class="bold">shifting security left</strong>. The term is associated with teams that build InfoSec into the software delivery process instead of making it a separate phase that happens downstream of the development process.</p>&#13;
			<p>Building security into software development not only improves delivery performance, but also improves security quality. By designing and implementing security and compliance metrics into the system, it becomes possible to measure continual compliance against security standards:</p>&#13;
			<div>&#13;
				<div id="_idContainer430" class="IMG---Figure">&#13;
					<img src="../Images/B16297_13_08.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 13.8: Standards-based system compliance</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-319" class="Author-Heading"><a id="_idTextAnchor419"/>PetBattle Security</h2>&#13;
			<p>The PetBattle team has asked the founders to hire an InfoSec professional who can be part of the development process. They are worried about potential data breaches of user details in PetBattle V2, especially if we need to start collecting payment details when we start monetizing the site. The PetBattle team has educated their developers of the common security risks, such as the OWASP Top 10 and how to prevent them. </p>&#13;
			<p>The team plans to implement security scanning and testing measurement into their build pipeline. By using the OpenShift platform with a trusted software supply chain, the team can spend significantly less time remediating security issues that may arise.</p>&#13;
			<p>The team has identified that everyone needs to become very familiar with the detailed network design of the overall solution. This should help avert attacks by malicious actors. The team also wants to make sure all of the platforms and applications are easy to patch—so they can easily keep frameworks up to date when <a id="_idIndexMarker1997"/><a id="_idIndexMarker1998"/><strong class="bold">Common Vulnerabilities and Exposures</strong> (<strong class="bold">CVEs</strong>)<span id="footnote-075-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-075">4</a></span> arise.</p>&#13;
			</div>&#13;
			<div id="footnote-075" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-075-backlink">4</a>	<a href="https://cve.mitre.org">https://cve.mitre.org</a></p>&#13;
			</div>&#13;
			<p>Having a secure system involves making sure all of the layers in the stack are themselves secure –  having a secure hardware environment, securing the operating system, securing the containers image layers being used, securing the dependencies that your application uses, securing your application code, securing the network that exposes your application services, and ultimately ensuring that your end users can interact with your applications securely.</p>&#13;
			<p>Measuring these security layers, making sure they comply with various industry standards, and having concrete action plans for when new vulnerabilities arise, requires teams to work together at every stage of the software delivery life cycle. Security should not just be the <strong class="bold">Chief Information Security Officer's </strong>(<strong class="bold">CISO's</strong>) job. Done right, security is pervasive and designed into the platforms and software systems, with security professionals actively <a id="_idIndexMarker1999"/>participating as part of the core delivery team and not just being the Mr. No<span id="footnote-074-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-074">5</a></span> when penetration testing is carried out.</p>&#13;
			&#13;
			<p>Security is <a id="_idIndexMarker2000"/><a id="_idIndexMarker2001"/>one of those topics that needs its own book. By shifting security left, we will cover technical topics later in the book that include container image and vulnerability scanning, the container health index, CVE patching, OpenShift Compliance Operator, OpenShift Container Security Operator, and Security Policy enforcement with ACM. These tools can help you to complement and build out a continually compliant platform and application suite for yourself.</p>&#13;
			<h3 id="_idParaDest-320"><a id="_idTextAnchor420"/><a id="_idTextAnchor421"/>Measuring Performance</h3>&#13;
			<p>There is an adage that software<a id="_idIndexMarker2002"/><a id="_idIndexMarker2003"/> developers use—<em class="italics">first make it work, then make it work fast</em>. The truth to this statement is that some functionality must exist before it can be made to work fast! Qualities such as the performance and security of our applications are usually the most important non-functional requirements and they must be designed into the system from the very start in order to be successful. Let's define what we mean by performance.</p>&#13;
			<p><strong class="bold">Performance</strong> measures how fast the system processes a single transaction. This can be measured in isolation or under load. The system's performance has a major impact on its throughput. When end users talk about performance, what they are usually talking about is throughput; and they only care about the performance of their own transactions, not anyone else's. As far as they are concerned, if a system's response time exceeds their expectation, the system is down.</p>&#13;
			<p><strong class="bold">Throughput</strong> describes the<a id="_idIndexMarker2004"/><a id="_idIndexMarker2005"/> number of transactions the system can process in a given time span. The system's performance clearly affects its throughput, but not necessarily in a linear way. Throughput is always limited by a constraint in the system, what is known as a bottleneck. Trying to optimize or improve performance for any part of the system that is not a bottleneck will not increase throughput.</p>&#13;
			<div id="footnote-074" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-074-backlink">5</a>	<em class="italics">Mr. No</em> is a <em class="italics">Mr. Men</em> book only available in France. Mr. No always disagrees with everyone and everything. <em class="italics">Mr. No</em> is published under the title <em class="italics">Monsieur Non</em> in France. This is one of the two Mr. Men titles that were not published in English.</p>&#13;
			</div>&#13;
			<p>If we measure the number of end <a id="_idIndexMarker2006"/><a id="_idIndexMarker2007"/>user requests, the throughput will vary depending on how many requests there are. This is a measure of scalability. When a system is horizontally scalable, it means we can add capacity (more servers, more pods, or containers) to handle more throughput. In a shared-nothing architecture, we can add capacity until we reach a known bottleneck. For example, in OpenShift, this may be the number of pods per node, or the maximum number of nodes per cluster.</p>&#13;
			<p>It is also worth noting that for any given system, an "acceptable response time" may vary! For a mobile/web app, any response longer than a second or two means users will use their fingers to walk away to another website or app. For a trading system at a bank, the response time may be in the order of milliseconds or less. To understand the capacity a system requires, we need to understand the system as a whole, before breaking it down into its constituent parts. This is called <em class="italics">systems thinking</em>. By thinking holistically and then in detail about a system, we can determine the bottleneck in a system.</p>&#13;
			<p><a id="_idTextAnchor422"/>At any point in time, exactly one constraint determines the system's capacity. Let's imagine it is the database that limits the transaction throughput. Once we improve that bottleneck—for example, by using faster storage or adding indexes, or using better database technology<a id="_idTextAnchor423"/>—the next bottleneck in the system then becomes the performance constraint—the application server capacity is now limiting throughput, say.</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-321" class="Author-Heading"><a id="_idTextAnchor424"/>PetBattle Performance</h2>&#13;
			<p>When PetBattle is released to the world, users will gang up on us. Sometimes, users come in really, really big mobs. Picture the Instagram editors giggling as they point toward the PetBattle website, saying "Release the cat hoard!" Large mobs can trigger hangs, deadlocks, and obscure race conditions in our applications.</p>&#13;
			<p>The PetBattle team wants to run special stress <a id="_idIndexMarker2008"/><a id="_idIndexMarker2009"/>tests to hammer deep links or hot URLs within the UI and API layers. Currently, there is a direct coupling between users and the database, so the developers already know that some form of caching could be required when scaling.</p>&#13;
			<p>If we make monitoring a priority, we can refine our infrastructure and application monitoring systems to ensure we are collecting information on the right services and putting that information to good use. The visibility and transparency yielded by effective monitoring are invaluable. Proactive monitoring is a key part of a strong technical foundation.</p>&#13;
			<p>The team plans to deploy the PetBattle application suite into an environment where they can start performance testing parts of the system under load. They want to start simply by targeting various parts of the system, for example, the API to gain an understanding of system behavior under load. By doing this early and often, they can identify bottlenecks and from there work out a plan to fix them.</p>&#13;
			</div>&#13;
			<p>Like security, moving performance<a id="_idIndexMarker2010"/><a id="_idIndexMarker2011"/> testing left in the build and deployment process provides faster feedback and the opportunity to remediate and reveal issues early in the development life cycle. Complex system testing for<a id="_idIndexMarker2012"/><a id="_idIndexMarker2013"/> a full suite of applications is often not feasible until closer to the end of development; however, it is possible to test individual APIs, endpoints, or even parts of the UI often and early in an automated manner. One of the main benefits of doing this type of automated performance testing is to build up a baseline understanding of how the system performs. Any code changes that alter the system performance characteristics often go unnoticed if this type of testing is not being automated and you end up trying to firefight performance issues right before a go-live.</p>&#13;
			<h3 id="_idParaDest-322"><a id="_idTextAnchor425"/><a id="_idTextAnchor426"/>Measuring Deployment Pain</h3>&#13;
			<p>If you have ever had to work <a id="_idIndexMarker2014"/><a id="_idIndexMarker2015"/>night shifts or weekends to help deliver a Big Bang release of software products into a production environment, you will understand the feeling of anxiety and concern engineers and technical staff feel prior to a go-live event. In the literature, this is referred to as deployment pain, and it can highlight the disconnect that exists between common development and testing tasks when compared to release and operational tasks.</p>&#13;
			<p>Fundamentally, most deployment problems are caused by a complex, brittle deployment process where the following occurs:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>Software is often not written with deployability in mind.</li>&#13;
				<li>Manual steps are required to deploy to production.</li>&#13;
				<li>Multiple hand-offs are undertaken in deployment phases.</li>&#13;
			</ul>&#13;
			<p>Disruptions are a fact of life in all systems, OpenShift included. There are a host of Kubernetes primitives that will help our business services stay up and available for customers (including replication controllers, rolling deployments, health checks, pod disruption <a id="_idIndexMarker2016"/><a id="_idIndexMarker2017"/>budgets, horizontal pod autoscalers, and cluster <a id="_idIndexMarker2018"/><a id="_idIndexMarker2019"/>autoscalers). But even with the best infrastructure, failures can <a id="_idIndexMarker2020"/><a id="_idIndexMarker2021"/>still occur. Cloud service disruptions, hardware failures, resource exhaustion, and misconfigurations can still threaten the business service.</p>&#13;
			<h3 id="_idParaDest-323"><a id="_idTextAnchor427"/><a id="_idTextAnchor428"/>Measuring Culture</h3>&#13;
			<p>There is a link between <a id="_idIndexMarker2022"/><a id="_idIndexMarker2023"/>metrics and transformation. Measurements help inform the practices that are used. Gathering CI/CD metrics, for example, the time that builds and deployments take for the PetBattle apps, allows the team to adopt and develop streamlined CI/CD practices so they can release quickly, more often, and with confidence that failures will be rare. As the team becomes comfortable releasing software into production regularly, the business starts to trust that releases can be performed with little risk to end users. This leads to the faster releasing of new features, a tighter, shorter feedback loop, and ultimately, a culture that encourages rapid change and innovation. Now we have really started to unlock transformational change for the company:</p>&#13;
			<div>&#13;
				<div id="_idContainer431" class="IMG---Figure">&#13;
					<img src="../Images/figure-13-9.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 13.9: The link between metrics and transformation</p>&#13;
			<p>Operational metrics often measure failure in very <a id="_idIndexMarker2024"/><a id="_idIndexMarker2025"/>complex adaptive systems. When things fail, an authoritarian blame-like culture will look to find the "human error" and assign blame. This type of response to failure is not only bad but should be considered harmful.</p>&#13;
			<p>The goal of any failure analysis should be to discover how we can improve information flow so that people have better and more timely information, or to find better tools to help prevent catastrophic failures following apparently normal operations.</p>&#13;
			<h3 id="_idParaDest-324">M<a id="_idTextAnchor429"/><a id="_idTextAnchor430"/><a id="_idTextAnchor431"/>easuring Application Metrics</h3>&#13;
			<p>All of the metrics <a id="_idIndexMarker2026"/><a id="_idIndexMarker2027"/>collected so far are focused on<a id="_idIndexMarker2028"/><a id="_idIndexMarker2029"/> non-functional parameters. What about the business application? Can we get some real-time metrics about the application usage that we can radiate alongside these other metrics?</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-325" class="Author-Heading"><a id="_idTextAnchor432"/>PetBattle Application Metrics</h2>&#13;
			<p>The application developers have discovered how easy it is to generate custom application metrics by utilizing the MicroProfile metrics extension in the PetBattle API server. The founders wanted to know how many cats were being uploaded in real time. The developers added the metrics extension to their application, which provides default Java VM metrics such as heap, CPU, and threads. It also allows the team to expose custom metrics from the application. For example, by adding Java annotations to their code, they can easily measure the frequency of cats uploaded.</p>&#13;
			<p>These metrics can now be scraped and displayed on a dashboard using Prometheus and Grafana:</p>&#13;
			<div>&#13;
				<div id="_idContainer432" class="IMG---Figure">&#13;
					<img src="../Images/B16297_13_10.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Figure 13.10: Analyzing PetBattle application metrics using Prometheus and Grafana</p>&#13;
			<p>There are internal metrics we can monitor as well as external metrics. An external fake user (often called a synthetic user) can be set up to monitor the PetBattle app on a regular basis. This synthetic client experiences the same view of the system that real users experience. When that client cannot process a fake transaction, for example by trying to enter their fake cat in a Tournament, then there is a problem, whether the internal monitoring shows a problem or not!</p>&#13;
			</div>&#13;
			<p>There are lots of possible stability metrics the team thought they could easily measure and alert on if they were over a certain threshold. Slow responses tend to propagate upward from layer to layer in a gradual form of cascading failure. When the website becomes slow, users tend to hit the <span class="P---Screen-Text">Refresh</span> button more often, causing more and more traffic. If we give our system the ability to monitor its own performance (in other words, it becomes <a id="_idIndexMarker2030"/><a id="_idIndexMarker2031"/>observable), then the system can also tell the team when it isn't meeting its SL agreements. Some examples include the following:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>The number of users and active sessions</li>&#13;
				<li>Blocked threads in the API layer</li>&#13;
				<li>Out of memory events in the API or database layer</li>&#13;
				<li>Slow responses in the user interface</li>&#13;
				<li>High database CPU</li>&#13;
			</ul>&#13;
			<h3 id="_idParaDest-326"><a id="_idTextAnchor433"/><a id="_idTextAnchor434"/>Measuring Infrastructure Platform Costs and Utilization</h3>&#13;
			<p>OpenShift comes <a id="_idIndexMarker2032"/><a id="_idIndexMarker2033"/>with a cost management and metering application that can be used to show infrastructure usage. You deploy the Cost Management Metrics Operator in OpenShift and the reporting and APIs are delivered as part of a SaaS solution from <a href="http://cloud.redhat.com">cloud.redhat.com</a>.</p>&#13;
			<p>It allows the PetBattle team to do the following:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>Visualize, understand, and analyze the use of resources and costs.</li>&#13;
				<li>Forecast their future consumption and compare them with budgets.</li>&#13;
				<li>Optimize resources and consumption.</li>&#13;
				<li>Identify patterns of usage that should be investigated.</li>&#13;
				<li>Integrate with third-party tools that can benefit from cost and resourcing data.</li>&#13;
			</ul>&#13;
			<p>There are a lot of visualizations and dashboards available. <em class="italics">Figure 13.11</em> shows the overview dashboard in a large demo environment as an example. It is possible to track cost and usage at both an infrastructure and business level. Users can tag projects and applications to gain detailed breakdowns as well as historical trends. The dashboards can help answer common questions, for example:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>Show me the top project and top clusters by usage and cost:<ul style="list-style-type:disc;"><li>Which projects are costing me the most?</li>&#13;
<li>Which clusters are costing me the most?</li>&#13;
</ul></li>&#13;
				<li>Show how metrics contribute to the costs:<ul style="list-style-type:disc;"><li>What is driving costs? CPU, memory, storage?</li>&#13;
<li>What are the predicted costs for the next calendar month?</li>&#13;
</ul></li>&#13;
			</ul>&#13;
			<p>It is possible to change between accumulated and daily costs, as well as filter and drill down across<a id="_idIndexMarker2034"/><a id="_idIndexMarker2035"/> clusters, clouds, and projects:</p>&#13;
			<div>&#13;
				<div id="_idContainer433" class="IMG---Figure">&#13;
					<img src="../Images/B16297_13_11.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 13.11: Cost Management Overview dashboard at cloud.redhat.com</p>&#13;
			<p>You can check out the cost management product documentation,<span id="footnote-073-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-073">6</a></span> which has a lot more details about this service, including common configuration options across a hybrid cloud.</p>&#13;
			<div id="footnote-073" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-073-backlink">6</a>	<a href="https://access.redhat.com/documentation/en-us/cost_management_service/2021">https://access.redhat.com/documentation/en-us/cost_management_service/2021</a></p>&#13;
			</div>&#13;
			<h3 id="_idParaDest-327">M<a id="_idTextAnchor435"/><a id="_idTextAnchor436"/>easuring Resources and Services</h3>&#13;
			<p>There are a number of simple high-level checks that should be made for all of our resources and services. The <strong class="bold">USE</strong> method is a simple <a id="_idIndexMarker2036"/><a id="_idIndexMarker2037"/>checklist that can be summarized as <em class="italics">for every resource, monitor the following items</em>:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li><strong class="bold">Utilization</strong>: The percentage of time that the resource is busy.</li>&#13;
				<li><strong class="bold">Saturation</strong>: The amount of work a <a id="_idIndexMarker2038"/><a id="_idIndexMarker2039"/>resource has to do that is extra or overloaded, often a queue length or similar.</li>&#13;
				<li><strong class="bold">Errors</strong>: The count of error<a id="_idIndexMarker2040"/><a id="_idIndexMarker2041"/> events that occur.</li>&#13;
			</ul>&#13;
			<p>Errors should be investigated <a id="_idIndexMarker2042"/><a id="_idIndexMarker2043"/>because they can degrade performance, and may not be immediately noticed when the failure mode is recoverable. This includes operations that fail and are retried, and devices from a pool of redundant devices that fail:</p>&#13;
			<div>&#13;
				<div id="_idContainer434" class="IMG---Figure">&#13;
					<img src="../Images/B16297_13_12.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 13.12: USE metrics</p>&#13;
			<p>We can map the USE metrics against common resources in a tabulated form, as shown in <em class="italics">Figure 13.12</em>, allowing for quick identification of the types of issues that are occurring in our system. The OpenShift metrics stack supports pre-configured USE method dashboards at the cluster and node level:</p>&#13;
			<div>&#13;
				<div id="_idContainer435" class="IMG---Figure">&#13;
					<img src="../Images/B16297_13_13.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 13.13: OpenShift Monitoring USE dashboards</p>&#13;
			<p>Similarly, the <strong class="bold">RED</strong> method can be <a id="_idIndexMarker2044"/><a id="_idIndexMarker2045"/>summarized as <em class="italics">for every service, monitor the request</em>:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li><strong class="bold">Rate</strong>: The number of requests per second</li>&#13;
				<li><strong class="bold">Errors</strong>: The number of <a id="_idIndexMarker2046"/><a id="_idIndexMarker2047"/>requests that fail</li>&#13;
				<li><strong class="bold">Duration</strong>: The amount <a id="_idIndexMarker2048"/><a id="_idIndexMarker2049"/>of time requests take to complete</li>&#13;
			</ul>&#13;
			<p>The RED method is a good baseline<a id="_idIndexMarker2050"/><a id="_idIndexMarker2051"/> that can be applied to most request-based services. It reduces the cognitive load for SREs; in other words, they can think the same way about a large range of supported<a id="_idIndexMarker2052"/><a id="_idIndexMarker2053"/> services for baseline metrics. The RED method does break down for batch-oriented or streaming services. In the Google SRE Book, the original "Four Golden Signals" included the RED and Saturation metrics.</p>&#13;
			<h3 id="_idParaDest-328">Use<a id="_idTextAnchor437"/><a id="_idTextAnchor438"/>r Experience Analytics</h3>&#13;
			<p>There is a vast treasure trove of user <a id="_idIndexMarker2054"/><a id="_idIndexMarker2055"/>analytics that can be sourced from the PetBattle user interface. The most obvious analytics are to do with non-functional performance measures, for example, page load times and response latencies. Hopefully, by measuring and baselining basic user interface performance, the team can prevent the application services from being trampled by a flood of customers!</p>&#13;
			<p>Standard practice is to measure using histograms (percentiles), which allow for a better understanding of outlier performance. Reliably measuring and aggregating quantiles/percentiles of high-velocity metrics from multiple sources is no easy task.</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-329" class="Author-Heading"><a id="_idTextAnchor439"/>PetBattle User Experience Analytics</h2>&#13;
			<p>Given the primary goal of monetizing PetBattle, signing up to one of the many global businesses that provide web analytics to help sell targeted advertising to end users is likely a better use of resources than trying to build a custom analytic and advertising solution from the ground up. Analytics services can quickly give us page load times, click-through rates, advertisements, site search optimizations, and suggestions, as well as help identify upsell opportunities – for a fee.</p>&#13;
			<p>As part of testing the end user experience, an automated load testing of the user interface can give the team a preview of what operating the PetBattle V2 site would look like.</p>&#13;
			</div>&#13;
			<p>We have shown in this <a id="_idIndexMarker2056"/><a id="_idIndexMarker2057"/>section many different levels of quantitative analysis we can perform using metrics. Let's now see how these translate to our focus on outcomes.</p>&#13;
			<h2 id="_idParaDest-330"><a id="_idTextAnchor440"/><a id="_idTextAnchor441"/><a id="_idTextAnchor442"/>Visualize Measurable Outcomes</h2>&#13;
			<p>We have a lot of things we can now<a id="_idIndexMarker2058"/><a id="_idIndexMarker2059"/> wmeasure. How can we tell if we have <em class="italics">shifted the needle</em> and made an appreciable difference to the status quo? Quite often, the key metrics and data are not visible to everybody in the room; they are hidden away behind a login on a computer. </p>&#13;
			<p>To help solve this hidden data problem, we can make use of more information radiators, putting everything on walls around where the team works, using dashboards and large screens so we can visually represent all aspects of the delivery work. We can share all sorts of information that is useful to team members, stakeholders, and users alike. It can be physically presented on walls, windows, doors, and other flat surfaces and positioned in the line of sight of those people who will get value from consuming the information.</p>&#13;
			<p>There are some interesting consequences of making information more accessible. These include better speed and accuracy of teamwork. Because the information is now visible all of the time, people are frequently reminded of it. There is also less time wasted having to search for important information if it is constantly on display. It is also more likely that the information is accurate because people are continuously being reminded of it, and if it is inaccurate, questions get asked!</p>&#13;
			<p>When people who are not part of the core team come into the team space, information that is radiated on the walls can instantly be read. Stakeholders and people interested in the team's work can immediately gain a better understanding and awareness of the work that is going on. This activity is often referred to as <em class="italics">walking the walls</em>. Interested parties can inspect artifacts on the walls and have a conversation with team members about them. This is a hugely different experience from when information is hidden away in a system behind a login.</p>&#13;
			<h3 id="_idParaDest-331"><a id="_idTextAnchor443"/><a id="_idTextAnchor444"/>Proactive Notification</h3>&#13;
			<p>So, what happens <a id="_idIndexMarker2060"/><a id="_idIndexMarker2061"/>when things start to <a id="_idIndexMarker2062"/><a id="_idIndexMarker2063"/>go wrong? What happens when the system itself cannot resolve an issue by automatically restarting a pod, or scaling up cluster nodes? Enter the realm of <strong class="bold">Alerts</strong>.</p>&#13;
			<p>Alerting can take many forms. It could be that horrible text at 03:00 when things really start going wrong, or a more subtle alert, such as a Slack message to say something has updated successfully. The key here is that the information is being pushed, and not pulled. OpenShift has alerting capabilities built into it and we'll explore this in more detail in <em class="italics">Chapter 16</em>, <em class="italics">Own It</em>.</p>&#13;
			<p>The classic forms of alerting are when there are spikes in memory usage in applications. This could lead to an application failing or constantly having to restart. In these instances, the team might spot the spikes on their dashboards and go and investigate the issues. We can, of course, make this feedback loop even shorter by combining data from different sources and alert on that. For example, if our application memory spikes, if we could capture the logs from around that time and push both events to the team, it could help diagnose the problems quicker. The real power of smart notifications is being able to respond even more quickly to the event.</p>&#13;
			<p>Alerting the Development teams to things that have broken is extremely important. Notifications can <a id="_idIndexMarker2064"/><a id="_idIndexMarker2065"/>come from all layers of the system; it doesn't just need to be that terrible call in the dead of night to say the website is down! Whenever a job runs to build our code or deploy a new version of an application, sending a quick alert to the team's instant messaging software is a good way to notify the concerned stakeholders. If the information is timely, then we can respond more effectively. This could mean pulling the Andon cord and halting the production line while we gather together to fix the issue.</p>&#13;
			<h3 id="_idParaDest-332"><a id="_idTextAnchor445"/>Altering the Customers</h3>&#13;
			<p>The software we rely on daily sometimes experiences downtime. Notifying the developers and SREs of critical failures is vital to resolving issues. Notifying customers in a transparent way of any system failures and your path to resolution can set you apart from the crowd. Lots of <a id="_idIndexMarker2066"/><a id="_idIndexMarker2067"/>companies have status pages for their software to give an idea of whether something is up or down; for example, the Quay container repository status<span id="footnote-072-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-072">7</a></span> page gives you insight into what parts of the service are still working. This provides a great information radiator! Some of the best software companies in the world will not only publish the status of their downtimes, but also the steps they are taking to resolve issues in near real time. Companies such as <a id="_idIndexMarker2068"/><a id="_idIndexMarker2069"/>GitHub and GitLab will post messages to Twitter or open up a Google Doc to post updates and provide as close to real-time updates as they can to their user base. This level of transparency is great to see, especially when the issues they are facing can be discussed further by others using the same technology. They can become proactive, and not make the same mistakes on that next big critical upgrade!</p>&#13;
			<div id="footnote-072" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-072-backlink">7</a>	<a href="https://status.quay.io/">https://status.quay.io/</a></p>&#13;
			</div>&#13;
			<p>Alerting is only useful if it happens in a timely manner. It's no use alerting on a condition if no one can do anything about it because it's too late. It's often a bit of a balancing act to design alerting thresholds so that the humans who look after the product or systems can proactively take action, as opposed to pinging alerts too frequently by "crying wolf" when a limit is reached.</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-333" class="Author-Heading"><a id="_idTextAnchor446"/>Having Fun with Notifications and the Build!</h2>&#13;
			<div>&#13;
				<div id="_idContainer436" class="IMG---Figure" style="float: right; margin: 6px; hight:6cm; width:6cm;">&#13;
					<img src="../Images/Author_25.jpg" alt="" width="250" height="250"/>&#13;
				</div>&#13;
			</div>&#13;
			<p>As a self-confessed lazy developer, I know that when I push some code, it's unlikely that I will check the results of an automated build. I know this is arrogance, but hear me out. Usually, my thinking is, It works for me locally, what could possibly go wrong in the build?</p>&#13;
			<p>The truth is that often <a id="_idIndexMarker2070"/><a id="_idIndexMarker2071"/>something can, and does, go wrong, I've forgotten to link my code or I've forgotten to add a file before committing. Things go wrong. This is why wherever I work I love to have big dashboards from Jenkins, like the one shown in <em class="italics">Figure 13.14</em>, or monitors, so everyone can see the build status.</p>&#13;
			<p>They are such an important element of the feedback loop. I look at a dashboard like this and I can immediately see critical information that makes my job easier, such as what step in the build is running, who triggered the build, or how many tests are failing! It's simple information, but it's next to impossible for me as a lazy developer to ignore something if it's flashing red in my face like this!</p>&#13;
			<p>I love anything that can help shorten the feedback loop. I am particularly fond of a plugin that works well with a dashboard in Jenkins, called the Build Fail Analyzer. It parses the build log for a specific Regex and, if found, it can display a message. On the project, this screenshot is taken from the team who had gotten into the habit of trying to codify the failure issues, so when one was detected, it could prompt us to resolve it without looking through the log. </p>&#13;
			<p>On this dashboard, on the middle left side, I can see that the <span class="P---Screen-Text">dev-portal-fe-e2e-tests</span> testing suites have failed and the problem identified was selenium not started. With this information, I don't need to open Jenkins and read the log to see what happened – I can go straight to OpenShift to see why the pod did not start:</p>&#13;
			<div>&#13;
				<div id="_idContainer437" class="IMG---Figure">&#13;
					<img src="../Images/B16297_13_14.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Figure 13.14: The Build Fail Analyzer in Jenkins</p>&#13;
			<p>Failing the build is seen sometimes as the cursed state. We try to encourage a no-blame culture, but sometimes it can be a bit of fun to have a bit of a blame game. I am a fan of this one for sure! Whenever you fail the build, you're deemed to be the Cowboy Coder, the one who rides into town and shoots from the hip without a care for the consequences. Or maybe you just left a typo in your code!</p>&#13;
			<p>Either way, if the dashboard turns red, then you have to dress like the cowboy you are. This team took it one step further: not only did you have to wear the pink cowboy hat until you had fixed the problem, but you also had to ride around on the wooden hobby horse! Even if you had to go to the bathroom or get a coffee, the horse and hat went with you! You'd be amazed at the funny looks you get on the way to the canteen wearing this attire:</p>&#13;
			<div>&#13;
				<div id="_idContainer438" class="IMG---Figure">&#13;
					<img src="../Images/B16297_13_15.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Figure 13.15: The Cowboy Coders</p>&#13;
			<p>More silly examples come from a project I was working on years back. This was in the days prior to using a container platform, and we had several manually configured VMs that were critical for us when releasing software. It was a massive project, with seven Scrum teams globally distributed. We were building a suite of 50 product microservices so our build farm was always busy!</p>&#13;
			<p>On one occasion, we had to get the IT company that was managing our infrastructure to roll back to one of the previous backups, as someone had executed a large number of commands as root and broken several things for Jenkins. Raising a <strong class="inline">sev1</strong> ticket still took a few days to resolve! We couldn't let this kind of thing happen again as it was a massive drain on the team's morale and productivity. So, we decided to get inventive. We were using Slack as our messaging client and knew you could send messages to channels via a webhook. We<a id="_idIndexMarker2072"/><a id="_idIndexMarker2073"/> also knew that if anyone logged into a machine, we could execute a bash script. Tying these items together, we created the Kenny Loggins channel in our Slack instance...because when you log into a server as root, you're in the DANGER ZONE!</p>&#13;
			<div>&#13;
				<div id="_idContainer439" class="IMG---Figure">&#13;
					<img src="../Images/B16297_13_16.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Figure 13.16: The Kenny Loggins channel</p>&#13;
			</div>&#13;
			<p>This section has shown many different ways we can visualize outcomes and use metrics to trigger <a id="_idIndexMarker2074"/><a id="_idIndexMarker2075"/>proactive notifications to learn<a id="_idIndexMarker2076"/><a id="_idIndexMarker2077"/> faster. Let's see how this can be summarized with everything else we've learned in our Delivery Loop.</p>&#13;
			<h2 id="_idParaDest-334"><a id="_idTextAnchor447"/><a id="_idTextAnchor448"/>Creating a Delivery Map</h2>&#13;
			<p>We concluded <em class="italics">Section 3</em>, <em class="italics">Discover It</em>, with a Discovery Map, with a single information radiator that <a id="_idIndexMarker2078"/><a id="_idIndexMarker2079"/>summarized the Discovery Loop iteration. We concluded <em class="italics">Section 4</em>, <em class="italics">Prioritize It</em>, with an Options Map, which summarized the ideas and hypotheses we wanted to validate, how we would deliver the options, and what options we planned to work on first.</p>&#13;
			<p>We will conclude <em class="italics">Section 5</em>, <em class="italics">Deliver It</em>, with a Delivery Map. This is another open source artifact available in the Mobius Kit under Creative Commons that you can use to summarize all the learnings and decisions taken during your journey around the Delivery Loop.</p>&#13;
			<p>This map should slot neatly next to your Discovery Map and is used to summarize the following:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li><strong class="bold">Actions</strong>: What can we get done this week to improve the outcomes?</li>&#13;
				<li><strong class="bold">Doing</strong>: What is our current work in progress?</li>&#13;
				<li><strong class="bold">Done</strong>: What is ready to review?</li>&#13;
				<li><strong class="bold">Impact</strong>: What progress did we make toward the outcomes?</li>&#13;
				<li><strong class="bold">Learn</strong>: What did we learn?</li>&#13;
				<li><strong class="bold">Insights</strong>: What are our next steps?</li>&#13;
			</ul>&#13;
			<p>As we come out of the Delivery Loop and return to the Options Pivot in <em class="italics">Section 7</em>, <em class="italics">Improve It, Sustain It</em>, we will complete the final section of this map by asking, <em class="italics">What are our next steps?</em></p>&#13;
			<p>Now let's look at PetBattle's Delivery Map at the end of their first iteration of the Delivery Loop.</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-335" class="Author-Heading"><a id="_idTextAnchor449"/>PetBattle – the Delivery Map</h2>&#13;
			<p><em class="italics">Figure 13.17</em> has a lot of detail that may not all be readable in print. To explore it in full, you can access the image at the book's GitHub repository at <a href="https://github.com/PacktPublishing/DevOps-Culture-and-Practice-with-OpenShift">https://github.com/PacktPublishing/DevOps-Culture-and-Practice-with-OpenShift</a>:</p>&#13;
			<div>&#13;
				<div id="_idContainer440" class="IMG---Figure">&#13;
					<img src="../Images/B16297_13_17.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Figure 13.17: The PetBattle Delivery Map</p>&#13;
			</div>&#13;
			<p>The Delivery Map provides a powerful summary of the journey we've been round the Delivery Loop. Like all other artifacts, it is a living and breathing summary and should be revisted regularly and updated after every subsequent iteration.</p>&#13;
			<h2 id="_idParaDest-336"><a id="_idTextAnchor450"/><a id="_idTextAnchor451"/>Conclusion</h2>&#13;
			<p>We have now completed our journey around the Mobius Loop. In this chapter, we have focused on the measurements and learning we can take away from the features we launch, the experiment we run, and the research we conduct. A relentless focus on measurement enables us to take more concrete decisions that are backed by metrics-based evidence.</p>&#13;
			<p>The Showcase and Retrospective events that are often run by Scrum and other Agile teams provide ample opportunity to showcase metrics and highlight learnings. We take this opportunity to re-examine the experiments that we designed on the Options Pivot and investigate what actually happened. That often involves looking at the analytics provided by the advanced deployment capabilities offered by the platform – the results of A/B tests, canary launches, feature toggles, and dark launches.</p>&#13;
			<p>We also highlighted the importance of running usability tests with the full team involved, while being connected directly to end users to develop further empathy and see them testing the evolving application. Guerilla testing also provides a low-cost and simple way to gather learning from users:</p>&#13;
			<div>&#13;
				<div id="_idContainer441" class="IMG---Figure">&#13;
					<img src="../Images/B16297_13_18.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 13.18: The Delivery Loop</p>&#13;
			<p>We explored the many different metrics made available by the platform, the software, and our teams. Service Delivery and Operational Performance metrics popularized by DORA and Accelerate, and made available by open source tools such as Pelorus, provide leading indicators of the success of DevOps culture and practice. These can be supported by further metrics captured about security, performance, culture, the application itself, and the infrastructure. The importance of radiating these, in real time, in a very open and transparent manner cannot be underestimated, nor can putting the behaviors and practices in place to be reactive and responsive to changes in metrics.</p>&#13;
			<p>As we conclude <em class="italics">Section 5</em>, <em class="italics">Deliver It</em>, we can see just how many practices have allowed us to navigate the Mobius Loop, on top of our foundation of culture and technology:</p>&#13;
			<div>&#13;
				<div id="_idContainer442" class="IMG---Figure">&#13;
					<img src="../Images/B16297_13_19.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 13.19: The practices mapped onto the Mobius Loop</p>&#13;
			<p>While we have completed one revolution around the Mobius Loop, we have not completed the journey. We will never complete the journey until the whole product is turned off and decommissioned. This is because the Mobius Loop is infinite and will never end. As we come out of the Delivery Loop, we return to the Options Pivot. We will do this in <em class="italics">Chapter 17</em>, <em class="italics">Improve It</em>, when we explore the insights from our trip around the Loop and ask what we have learned, followed by what we are to do next.</p>&#13;
			<p>Before that, we are going to spend a few chapters diving a bit deeper into the technical solution. We have already started that in this chapter. In <em class="italics">Chapter 14</em>, <em class="italics">Build It</em>, we will look at other aspects of how we build the solution. In <em class="italics">Chapter 15</em>, <em class="italics">Run It</em>, we'll focus on running the solution. In <em class="italics">Chapter 16</em>, <em class="italics">Own It</em>, we'll explore what it means to own the solution. These three chapters form <em class="italics">Section 6</em> of our book and are all about how product teams <em class="italics">Build It</em>, <em class="italics">Run It</em>, <em class="italics">Own It</em>.</p>&#13;
		</div>&#13;
</body></html>