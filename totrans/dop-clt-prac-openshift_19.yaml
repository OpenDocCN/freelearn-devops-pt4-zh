- en: 13\. Measure and Learn
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13. 衡量与学习
- en: '*Startup success can be engineered by following the process, which means it
    can be learned, which means it can be taught.* – Eric Ries'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*通过遵循这一过程可以工程化创业成功，这意味着它可以被学习，这也意味着它可以被教授。* – 埃里克·里斯'
- en: In his book, *The Lean Startup*, Eric Ries describes a startup company as a
    *human institution designed to* *create a new product or service under conditions
    of extreme uncertainty*. He outlines a process to help deal with this uncertainty
    where a tight feedback loop is created around the creation of a **minimum viable
    product** (**MVP**). He argues that being able to react, fail fast, and use a
    data-driven approach to measurement assists in decision-making that is based on
    reason rather than emotion. This ability to learn from small experiments can be
    seen as a form of business agility – the ability to pivot quickly in the face
    of ever-changing circumstances. In lean terms, this feedback loop can be summarized
    as **Build, Measure, Learn**.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的书《精益创业》中，埃里克·里斯（Eric Ries）将初创公司描述为一个*旨在* *在极度不确定的条件下创造新产品或服务的人类机构*。他概述了一个应对这种不确定性的过程，在这个过程中围绕**最小可行产品**（**MVP**）的创建形成了一个紧密的反馈循环。他认为，能够迅速反应、快速失败并利用数据驱动的度量方法有助于基于理性而非情感做出决策。这种从小实验中学习的能力可以看作是一种商业敏捷性——在不断变化的环境面前快速调整的能力。在精益术语中，这个反馈循环可以总结为**构建、衡量、学习**。
- en: The cultural and human aspects of this process cannot be overlooked. Uncertainty
    and humanity are common bedfellows. Ron Westrum, an American sociologist, posits
    that organizations with better "information flow" function more effectively. He
    argues that a good culture requires trust and cooperation between people across
    the organization, and therefore it reflects the level of collaboration and trust
    inside the organization.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程中的文化和人文方面不容忽视。不确定性和人性往往是共生的。美国社会学家罗恩·韦斯特鲁姆（Ron Westrum）认为，具有更好“信息流”的组织运作更高效。他认为，一个良好的文化需要组织内各人之间的信任与合作，因此它反映了组织内部的协作与信任水平。
- en: Second, better organizational culture can indicate higher-quality decision-making.
    In a team with this type of culture, not only is better information available
    for making decisions, but those decisions are more easily reversed if they turn
    out to be wrong because the team is more likely to be open and transparent rather
    than closed and rigid.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，较好的组织文化可以表明更高质量的决策。在具有这种文化的团队中，不仅有更好的信息可以用来做决策，而且如果决策错误，团队也更容易反转这些决策，因为团队更可能是开放和透明的，而不是封闭和僵化的。
- en: So, how can we take these ideas and make them actionable in our delivery processes?
    When timelines for delivery are tight and deadlines are fast approaching, a team's
    ability to deliver and operate software systems is critical to the business performance.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何将这些想法应用到我们的交付过程中呢？当交付的时间表紧张，截止日期迫在眉睫时，团队交付和运营软件系统的能力对业务表现至关重要。
- en: 'With software, there are often two competing forces at work. Innovation, which
    inherently is accompanied by system change, and running software, which is serving
    end customers and implies that the system is stable. We can identify two important
    areas to focus on here:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件领域，通常有两股相互竞争的力量在起作用。创新本质上伴随着系统的变化，而运行软件则是为最终客户提供服务，意味着系统必须保持稳定。在这里，我们可以识别出两个需要关注的重要领域：
- en: To help measure the effectiveness of a team's development and delivery practices
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帮助衡量团队开发和交付实践的有效性
- en: To start measuring and monitoring activities that allow the rapid diagnosis
    of issues
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始衡量和监控能够快速诊断问题的活动
- en: In this chapter, we are going to explore different mechanisms and techniques
    to take measurements out of our Delivery Loops and use those measurements to drive
    decisions and next steps. We call this approach **metrics-driven transformation**.
    We will also learn what to measure and how we can take these measurements and
    make them visible, ultimately to help answer the question of whether or not we
    have moved the needle at all during our transformation journey.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将探讨不同的机制和技术，从我们的交付环节中提取度量数据，并利用这些数据推动决策和下一步行动。我们将这种方法称为**指标驱动的转型**。我们还将学习什么是值得衡量的，以及如何获取这些度量数据并使其可见，最终帮助解答我们在转型过程中是否取得了任何进展的问题。
- en: Metrics-Driven Transformation
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指标驱动的转型
- en: Metrics-driven transformation focuses on using value-based business metrics
    to understand how technology-related investments impact organizational performance
    and provide specific tools and guidance to help improve those metrics.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 基于度量的转型侧重于使用基于价值的业务指标来理解与技术相关的投资如何影响组织绩效，并提供特定的工具和指导，以帮助改善这些指标。
- en: In the previous chapter, we looked at different approaches to doing delivery,
    whether that be Waterfall, or using an Agile framework such as Scrum or Kanban.
    As we complete loops of delivery, we want to take value-based metrics to validate
    hypotheses, confirm the results of experiments, clarify the impact of our feature
    deliveries, determine whether we have moved toward the Target Outcomes we set
    out, and make decisions around what to do next.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们研究了不同的交付方法，无论是瀑布方法，还是使用敏捷框架如Scrum或Kanban。随着我们完成交付循环，我们希望采取基于价值的度量来验证假设，确认实验结果，澄清特性交付的影响，确定我们是否朝着设定的目标结果前进，并根据结果决定下一步的行动。
- en: There are many different levels of measurements we can take in our delivery
    ecosystem and a growing number of sources we can collect them from. In this chapter,
    we will explore metrics we can collect automatically from our software and our
    platform as well as practices we can use to collect metrics from our users, customers,
    employees, and the wider organization. Let's start by re-visiting some of the
    practices we've already used and see how we can use them to collect measurements
    and learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的交付生态系统中，我们可以采取许多不同层次的度量方法，并且可以从越来越多的来源收集这些数据。本章将探讨我们可以从软件和平台自动收集的指标，以及我们可以用来收集来自用户、客户、员工和更广泛组织的指标的实践。我们先从重新审视我们已经使用过的一些实践开始，看看我们如何利用这些实践收集度量数据和学习。
- en: Where to Measure and Learn
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在哪里进行衡量和学习
- en: In *Chapter 10*, *Setting Outcomes*, we introduced the practice of setting Target
    Outcomes based on all of the learning that came out of the practices on the Discovery
    Loop. We showed how to make these measurable and how to visualize them as an information
    radiator so everyone can inspect them. Using metrics, we can inspect where we
    are now in quantifiable terms, where we've been, and where we want to get to for
    each measurable outcome.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第10章*，*设定目标*中，我们介绍了基于从发现循环中的实践中获得的所有学习来设定目标结果的做法。我们展示了如何使这些目标具有可衡量性，以及如何将其可视化为信息辐射器，以便每个人都能检查它们。通过使用度量，我们可以检查当前的量化状态，我们的过往轨迹，以及我们希望达到的每个可衡量结果。
- en: 'We explained the difference between primary (business-focused) outcomes and
    supporting enabling outcomes, which are non-functional-based outcomes. Since then,
    we have organized all of our work around those Target Outcomes and have radiated
    them in other practice artifacts, including the Value Slice on the Options Pivot
    and Scrum boards in the Delivery Loop. Those outcomes should be our starting point
    for measurement, and re-visualizing them will allow us to measure the progress
    we have made (or not) toward the target value:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解释了主要（以业务为中心）结果和支持性促进性结果之间的区别，后者是基于非功能性结果的。从那时起，我们围绕这些目标结果组织了所有工作，并将其辐射到其他实践成果中，包括交付循环中的选项轴和Scrum看板上的价值切片。这些结果应该是我们衡量的起点，重新可视化它们将帮助我们衡量我们在实现目标价值过程中取得的进展（或没有进展）：
- en: '![](img/B16297_13_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16297_13_01.jpg)'
- en: 'Figure 13.1: Measuring targets'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1：衡量目标
- en: When measuring the outcomes, we need to be aware of any prior measurement that
    may form an historic baseline, and potentially what any estimate of value may
    be against the actual value. Thinking about these different forms of measurement
    leads us to ask the question, *Where and when should we take measures and learn,
    and where should we inspect them?*
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在衡量结果时，我们需要注意任何可能形成历史基线的先前测量，并且要考虑任何价值估算与实际价值之间的差异。思考这些不同形式的度量让我们提出了这样一个问题：*我们应该在哪里和何时进行度量和学习，又应该在哪里检查这些度量？*
- en: The Showcase
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 展示
- en: Ideally, we take measurements as soon as a delivery item is complete. That might
    be at the end of a sprint in Scrum or after delivering a completed feature in
    Kanban. Perhaps we can build in metrics data collection to the application code
    and run a report just before or during a Showcase. For example, if we have a Target
    Outcome around increasing the user base to 10,000 people, every Showcase could
    provide an update on what the current user base is and whether it is moving in
    the right direction toward the target.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们在交付项目完成后立即进行度量。这可能是在Scrum的一个冲刺结束时，或者是在Kanban中交付一个完成的特性后。或许我们可以将度量数据收集嵌入到应用程序代码中，并在展示会前或展示会期间运行报告。例如，如果我们的目标结果是将用户基数增加到10,000人，那么每个展示会都可以提供当前用户基数的更新，并展示其是否朝着目标的正确方向发展。
- en: There may be a lag in the availability of data that means it cannot be presented
    at Showcase events. In this case, there are two plans of attack. First, we can
    use the Showcase itself to collect some measurements and learning from stakeholders.
    Alternatively, we could collect measurements and learning from stakeholders and
    users on the latest Product Increment. But how?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据可用性存在延迟，可能无法在展示会活动中呈现。在这种情况下，有两种应对方案。首先，我们可以利用展示会本身收集一些度量和来自利益相关者的反馈。或者，我们也可以收集来自利益相关者和用户对最新产品增量的度量和反馈。但该如何进行呢？
- en: Perhaps we have an outcome around stakeholder confidence and/or user happiness
    in the application we are incrementally building. If so, then what better opportunity
    than to ask the stakeholders after seeing the Showcase of the latest increment
    how confident or happy they are with what they have seen? This can be quantitative,
    in that we can ask stakeholders to rate the product on a score of 1-10\. It can
    also be qualitative in the conversation that happens around this scoring and supplementary
    feedback from collecting feedback data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 或许我们的目标结果是围绕利益相关者的信心和/或用户在我们逐步构建的应用程序中的满意度。如果是这样，那在展示会后询问利益相关者他们对所见内容的信心或满意度，还有什么比这更好的机会呢？这可以是定量的，例如我们可以要求利益相关者在1-10的评分范围内对产品进行评分。也可以是定性的，通过围绕评分展开的对话以及收集反馈数据时的附加反馈。
- en: Perhaps we have an outcome around employee engagement and team skills. Again,
    what better opportunity than at the end of a Delivery Loop iteration to survey
    the team on their happiness level and ask them to rate themselves on different
    skills? This visualization will not only allow us to see trends, but also identify
    the positive and negative effects as a result of the team education and cultural
    activities that we are undertaking.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 或许我们的目标结果是围绕员工参与度和团队技能。再次强调，在交付循环的每次迭代结束时，调查团队成员的幸福感并让他们对不同的技能进行自我评分，又有什么比这更好的机会呢？这种可视化不仅可以让我们看到趋势，还能帮助我们识别由于团队教育和文化活动所产生的正负效应。
- en: 'Showcase events allow the opportunity to show many different kinds of metrics:
    Software Delivery metrics, Platform metrics, and Team Velocity metrics. We will
    explore the details of these shortly. If these are things that will be of interest
    to the audience and help them understand the impact of everything the team is
    doing, by all means, include them. However, the event where a more in-depth conversation
    can happen is the event that typically follows the Showcase, the Retrospective.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 展示会活动提供了展示各种度量标准的机会：软件交付度量、平台度量和团队速度度量。我们稍后会深入探讨这些内容。如果这些内容对观众有吸引力并帮助他们理解团队所做工作的影响，那么一定要包括它们。然而，能够进行更深入对话的活动通常是紧随展示会之后的回顾性。
- en: The Retrospective
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回顾性
- en: We introduced the Retrospective practice in the previous chapter and looked
    at many different formats of running them. Let's consider metrics a little further
    and also an engineer's perspective of Retros.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了回顾性实践并探讨了多种实施格式。接下来，让我们进一步考虑度量标准，并从工程师的角度看回顾性。
- en: The Retrospective – an Engineering Perspective
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回顾性 – 工程师视角
- en: '![](img/Author_4.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Author_4.jpg)'
- en: Back in the late 1980s, I vividly remember being introduced to **feedback loops**
    and **control theory** in chemical engineering classes, way before software engineering
    totally enveloped my career! If you wanted to control the level in a tank, the
    flow rate in a pipe, or nearly any dynamically changing system, you needed to
    learn how to keep that system stable. The software engineering practice of Retrospectives
    always makes me think of those second-order feedback loops, the physical connection
    back into our software design and development process that allows us to learn
    and adapt so that the system can become more stable.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 回到1980年代末，我清晰地记得在化学工程课程中第一次接触到**反馈回路**和**控制理论**，那时软件工程还没有完全占据我的职业生涯！如果你想控制油箱中的液位、管道中的流量，或者几乎任何动态变化的系统，你需要学习如何保持系统的稳定。软件工程中的回顾实践总是让我想起那些二阶反馈回路，它们是我们软件设计和开发过程中的物理连接，让我们能够学习和适应，从而使系统变得更加稳定。
- en: Retrospectives all have a common goal of allowing the team to inspect what has
    just happened, while allowing them to adapt and refine what goes on in the future.
    This is a critical function. If there is no feedback or feedback is not actioned,
    the team may start to lose faith in the whole delivery process. In the engineering
    world, if feedback fails, the tank overflows!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾会议的共同目标是让团队检查刚刚发生的事情，同时让他们能够调整和改进未来的工作。这是一个至关重要的功能。如果没有反馈或者反馈没有得到处理，团队可能会开始对整个交付过程失去信心。在工程领域，如果反馈失败，油箱就会溢出！
- en: It should come as no surprise that metrics should be included as hot topics
    in any Retro. Teams that discuss their poorly performing SDO or CI/CD metrics
    can gain a lot of insight into what is going wrong with the software delivery
    process. So, celebrate when the build starts to take 20 minutes to complete. This
    means that the metrics measurement and trending are in place and the team can
    now take action to find out why it has gotten so slow and seek to improve it.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么比度量指标应当作为回顾会议中的热议话题更不令人意外了。讨论表现不佳的SDO或CI/CD度量指标的团队，可以深入了解软件交付过程中出了什么问题。因此，当构建开始需要20分钟才能完成时，应该庆祝。这意味着度量指标的测量和趋势已到位，团队现在可以采取行动找出为何变得如此缓慢，并寻求改进。
- en: It's possible to save a lot of wasted time and resources by listening to timely
    feedback. If feedback is ignored, you can incur an opportunity cost by spending
    the next year carrying out remediation projects instead of rolling out new, revenue-generating
    features. The moral of the story is to Retro, Retro, Retro! You can really never
    have enough feedback.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通过及时听取反馈，能够节省大量浪费的时间和资源。如果忽视反馈，可能会在接下来的一年里，花时间做补救项目，而不是推出新的创收功能。这个故事的寓意是：回顾，回顾，再回顾！你真的永远不会得到足够的反馈。
- en: With all the tools and technology put in place when we created the technical
    foundation (in *Chapter 6, Open Technical Practices – Beginnings, Starting Right*,
    and *Chapter 7, Open Technical Practices – The Midpoint*), there are huge amounts
    of data, metrics, and analysis we can collect and conduct. If you run pub Retros
    (as introduced in the previous chapter), there is nothing better than taking a
    few print-outs of reports from these tools and taking them down the pub to analyze
    together over a pint of Guinness!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们创建技术基础（在*第6章，开放技术实践 - 开始，正确启动*，和*第7章，开放技术实践 - 中期*中提到）时，所有的工具和技术都已经到位，我们可以收集和进行大量的数据、度量和分析。如果你进行酒吧回顾（如前一章所介绍），没有什么比拿几份工具报告打印件，带到酒吧去，边喝着吉尼斯啤酒边分析它们更好的了！
- en: '![](img/B16297_13_02.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16297_13_02.jpg)'
- en: 'Figure 13.2: Pub Retrospective – discussing metrics'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2：酒吧回顾 – 讨论度量指标
- en: 'Export your build statistics, latest test results, static code analysis reports,
    Burndown charts, and whatever else you can find, lay them all out on the table,
    and ask yourselves: what does this data tell us about us as a team that we don''t
    know already? What can we learn about ourselves? What patterns exist? What can
    we do in the next sprint to make these measurements better?'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 导出你的构建统计数据、最新的测试结果、静态代码分析报告、燃尽图，以及你能找到的其他任何数据，把它们都摆在桌上，问问自己：这些数据告诉我们什么是我们之前不知道的？我们能学到什么？存在哪些模式？我们能在下一个迭代中做些什么来改进这些测量？
- en: Retrospective actions that are often taken include an increased focus on CI/CD
    infrastructure, increasing thresholds around code coverage for testing, and adding
    more or faster feedback loops for the team to learn from.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的回顾会议行动包括：更加关注CI/CD基础设施，提升代码覆盖率的阈值，并为团队增加更多或更快速的反馈循环，以便从中学习。
- en: Inspecting the Build Stats at Retrospectives
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在回顾会议中检查构建统计
- en: This is one of my favorite examples for highlighting the power and impact of
    the Retrospective. I collected this chart back in 2008 when delivering an access
    control management solution for a UK telecoms company. My team was building a
    Java-based application using Scrum and continuous delivery. They used Hudson for
    continuous integration and had a huge number of automated tests that were incrementally
    built into the app as part of the team's Definition of Done.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我最喜欢用来突出回顾会议（Retrospective）强大影响力的一个例子。我在2008年为一家英国电信公司提供访问控制管理解决方案时收集了这个图表。我的团队使用Scrum和持续交付方法，构建了一个基于Java的应用程序。他们使用Hudson进行持续集成，并且有大量的自动化测试，这些测试作为团队"完成定义"的一部分，逐步集成到应用程序中。
- en: 'Several build graphs were exported every 2 weeks and taken to the team''s Retrospective.
    The following chart shows the duration of the build as well as the success of
    the build (red means the build failed, yellow means the build succeeded but some
    automated tests failed, and blue means a successful build with all automated tests
    passing):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 每两周导出几个构建图表，并带到团队的回顾会议中。下图展示了构建的持续时间以及构建的成功情况（红色表示构建失败，黄色表示构建成功但部分自动化测试失败，蓝色表示构建成功且所有自动化测试通过）：
- en: '![](img/B16297_13_03.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16297_13_03.jpg)'
- en: 'Figure 13.3: Inspecting the build time statistics at Retrospectives (the graph
    has been annotated with a few labels to reflect the sprint that the data was a
    part of)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3：在回顾会议中检查构建时间统计（图表已加注标签，反映了数据所涉及的冲刺）
- en: As we can see, the build was very unstable in Sprint 1\. A much earlier version
    of this chart was taken to the pub Retro at the end of Sprint 1\. The team inspected
    it and agreed that, during Sprint 2, they would invest some time investigating
    the build stability issues.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，在第1次冲刺（Sprint 1）中，构建非常不稳定。该图表的一个早期版本在第1次冲刺结束时被带到了酒吧回顾会上。团队进行了检查，并一致认为，在第2次冲刺中，他们将花一些时间调查构建稳定性问题。
- en: Two weeks later, Sprint 2 was completed and the ScrumMaster brought another
    print-out of this chart from Hudson. The good news was that the Sprint 1 Retro
    action had paid off as the build was much more stable. However, it was noticed
    that the build was sometimes taking more than 15 minutes to complete. This was
    a much longer feedback loop than needed, so a further retrospective action was
    taken in Sprint 3 to address this. We can see that from Sprint 3 onward, the build
    was mostly stable and relatively quick.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 两周后，第2次冲刺完成，ScrumMaster带来了Hudson上打印出的这个图表。好消息是，第1次冲刺回顾的行动得到了回报，构建变得更加稳定。然而，团队注意到有时构建完成的时间超过了15分钟。这是一个比所需时间更长的反馈循环，因此在第3次冲刺中采取了进一步的回顾行动来解决这个问题。从第3次冲刺开始，我们可以看到构建大多数时候保持稳定并且相对快速。
- en: Imagine if we had not used the Retrospective to look at this data. Imagine if
    we had let the slow build just fester over time. Imagine how much time would have
    been lost. This is why we do metrics-driven Retrospectives.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，如果我们没有在回顾会议中查看这些数据。想象一下，如果我们让缓慢的构建问题继续拖延下去。想象一下，会浪费多少时间。这就是为什么我们要进行以指标为驱动的回顾会议。
- en: As we come out of the sprint, we now have the opportunity to learn the results
    of experiments we've designed.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们走出冲刺后，我们将有机会学习我们设计的实验结果。
- en: Experiments – the Results!
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实验——结果！
- en: In *Chapter 11*, *The Options Pivot*, when we introduced the Options Pivot,
    we introduced some of the advanced deployment strategies we could design in our
    experiments. If we decided to use one or more of these strategies when designing
    our experiments, now is the time to measure and learn from what actually happened.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第11章*，*选项枢纽（Options Pivot）*中，当我们介绍选项枢纽时，我们介绍了一些可以在实验中设计的高级部署策略。如果我们在设计实验时决定使用其中一种或多种策略，那么现在就是衡量和学习实际结果的时刻。
- en: If we designed an A/B test, we look at the data collected about the traffic,
    interaction, time spent, and other relevant metrics so we can judge the effectiveness
    of the two different versions based on the change in users' behavior.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们设计了A/B测试，我们会查看收集到的关于流量、互动、时间花费以及其他相关指标的数据，以便根据用户行为的变化来评估这两个版本的有效性。
- en: If we designed a Canary Release, have we learned from the behavior of the users
    that were a part of the Canary Release candidate to validate whether the feature
    should be rolled out to the wider population? Similarly, with dark launches and
    feature toggles, we collect metrics from usage to assess whether the feature release
    should be extended to a larger user group, or should be switched off and dark
    launches rolled back.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们设计了金丝雀发布，我们是否从参与金丝雀发布候选的用户行为中学到了经验，以验证是否应该将该功能推广到更广泛的用户群体？同样地，通过暗启动和功能切换，我们从使用中收集指标来评估是否应该将功能发布扩展到更大的用户群体，或者应该关闭并回滚暗启动。
- en: All of the learning captured from these experiments and the resulting analytics
    is quantitative. This means that you can study the data, observe trends, decide
    to extend the experiment, or create whole new experiments. As you run more experiments
    and gather more data, your decision-making capability becomes stronger and is
    based on metrics gathered from it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些实验和相应的分析中获得的所有学习都是定量的。这意味着你可以研究数据，观察趋势，决定是否延伸实验，或者创建全新的实验。随着你运行更多实验并收集更多数据，你的决策能力变得更加强大，并且是基于从中收集的度量标准。
- en: Of course, you can't validate everything through the use of numbers. Some commentary
    with further learning and understanding is needed by talking to end users, known
    as quantitative feedback. Let's explore a couple of practices to help us do this,
    starting with user testing.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，并不是所有事情都能通过数字验证。通过与最终用户交谈获得进一步的学习和理解是必要的，这称为定性反馈。让我们探索几种帮助我们做到这一点的实践，从用户测试开始。
- en: User Testing
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用户测试
- en: 'User-based testing is a technique that focuses on user interactions with a
    product. These types of evaluation directly involve end users and focus on the
    person. Let''s dive a little more into this by looking at two user testing practices:
    usability and guerilla testing.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 基于用户的测试是一种专注于用户与产品互动的技术。这些评估类型直接涉及最终用户，并专注于个人。让我们通过查看两种用户测试实践来深入了解一下：可用性测试和灵活测试。
- en: Usability Testing
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可用性测试
- en: In a usability testing session, the team observes real users interacting with
    the product. Typically, a facilitator sits with a user and asks them to complete
    tasks and explain their thinking as they go. The team sits in a separate room
    and observes the testing by video link.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在可用性测试会议中，团队观察真实用户与产品的互动。通常，一个主持人与用户坐在一起，要求他们完成任务并在操作过程中解释他们的思考过程。团队则坐在另一个房间，通过视频链接观察测试过程。
- en: A usability test is not a focus group; it's focused on what the user thinks
    and does in the real world. An Empathy Map, as introduced in *Chapter 8*, *Discovering
    the Why and Who*, can be a very useful supporting practice.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 可用性测试不是焦点小组；它专注于用户在真实世界中的想法和行为。如*第8章*“发现为什么和谁”，所介绍的同理地图，可以是一个非常有用的支持实践。
- en: A usability test can be run on an existing product, a prototype, or even a competitor's
    product. The prototype could be working code, or it could be something as simple
    as a few clickable images. Test early and often to create products that delight
    users and solve real needs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 可用性测试可以运行在现有产品、原型甚至竞争对手的产品上。原型可以是工作代码，也可以是几个可点击的图像。早期和频繁地测试，以创建让用户喜爱并解决真实需求的产品。
- en: Usability testing often highlights something that's obvious to someone who has
    been working on a product, but might be confusing to a user. What we think users
    need might not be what they actually need. Indeed, what users think they need
    may not be what they actually need! Usability testing can help answer questions
    such as, *Are we on the right track?* *What problems do we still need to solve?*,
    or *Which features should we build next?* With early feedback from real users,
    teams can avoid sinking time into a feature that's confusing or not useful.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 可用性测试经常会突出一些对于产品开发人员来说显而易见但对用户可能会令人困惑的事情。我们认为用户需要的可能并不是他们实际需要的。实际上，用户认为他们需要的也许并不是他们实际需要的！可用性测试可以帮助回答诸如，“我们是否在正确的轨道上？”“我们仍然需要解决哪些问题？”或者“我们接下来应该开发哪些功能？”通过从真实用户获得早期反馈，团队可以避免浪费时间在一个令人困惑或没有用处的功能上。
- en: '"We Are Not Our Users"'
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “我们不是我们的用户”
- en: '![](img/Donal.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Donal.jpg)'
- en: One of the best designers I've ever worked with once told me that *We are not
    our users*. Since I spent a fair amount of time in front of a code editor among
    my development team, I thought I got what he meant. I didn't fully appreciate
    what he meant until the following incident occurred sometime later.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾与一位非常出色的设计师共事，他曾告诉我：“*我们不是我们的用户*。”由于我在开发团队中花了相当多的时间在代码编辑器前，我认为我明白他的意思。但直到后来发生了一件事，我才真正理解他的意思。
- en: We were building a lightweight prototype for a mortgage application. The frontend
    was using an existing bank API for calculating mortgages and the application we
    were building on top asked a few initial questions before prompting the person
    to enter an estimate of their income. The text box initially filled in by the
    user had a placeholder saying, *e.g. 100,000.00*.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在为一个抵押贷款应用程序构建一个轻量级的原型。前端使用了一个现有的银行API来计算抵押贷款，我们正在构建的应用程序在提示用户输入收入估算之前，会先询问几个初步问题。用户最初填入的文本框中有一个占位符，内容是*例如
    100,000.00*。
- en: While doing some development and testing with the product owner and user experience
    designer, we must have filled out the form hundreds of times! Every time we used
    the form, we filled it out the exact same way, just by putting 100000 into that
    box. Not once had any of us ever put a decimal place in the box!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在与产品负责人和用户体验设计师一起进行开发和测试时，我们一定填写了这个表单上百次！每次使用表单时，我们都是以完全相同的方式填写的，只是将100000填入那个框中。我们从来没有任何人往框中填过小数点！
- en: Fast-forward a few weeks and we were doing some real-world testing. Of course,
    the very first person we have to use the application goes through the form, putting
    "120000.00" into the box and hitting submit. As you can probably imagine, we expected
    everything to work swimmingly and the user feedback session to continue; but instead,
    the application crashed.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 几周后，我们开始进行一些真实世界的测试。当然，第一个使用我们应用程序的人填写了表单，把"120000.00"输入到框中并点击了提交。正如你可能想象的那样，我们原本预期一切都会顺利进行，用户反馈环节会继续进行；但实际上，应用程序崩溃了。
- en: '![](img/B16297_13_04.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16297_13_04.jpg)'
- en: 'Figure 13.4: Mortgage application'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4：抵押贷款应用程序
- en: The business analyst who was running the usability test immediately phoned in
    to say that the app was broken. We replayed the exact thing the user did, only
    to discover that the bank's API was unable to accept a decimal point. But for
    us, the real surprise was that no one on our team had ever noticed this issue
    before. As a small team, we were quick to fix the issue by updating the placeholder
    text and sending only an integer to the API.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 进行可用性测试的业务分析师立即打电话告诉我们应用程序坏了。我们重新回放了用户的操作，结果发现银行的API无法接受小数点。但对我们来说，真正的惊讶是我们团队没有人注意到这个问题。作为一个小团队，我们迅速通过更新占位符文本并仅向API发送整数来解决了这个问题。
- en: This example always reminds me that we are not our users. Users do weird and
    wonderful things in an application. Even if you try to test all the scenarios,
    there is almost always going to be some edge case you have not thought of, and
    the chances are the first person to use your app will do the one weird thing!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子总是提醒我，我们不是我们的用户。用户在应用程序中会做出奇怪又神奇的事情。即使你试图测试所有的场景，几乎总会有一些你没有想到的边缘情况，而且很有可能第一个使用你应用程序的人会做出那个奇怪的事情！
- en: Read more about the usability testing practice, share your own experiences,
    ask questions, and improve the practice further in the Open Practice Library at
    [openpracticelibrary.com/practice/usability-testing](http://openpracticelibrary.com/practice/usability-testing).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在[openpracticelibrary.com/practice/usability-testing](http://openpracticelibrary.com/practice/usability-testing)的开放实践库中，您可以深入了解可用性测试实践，分享自己的经验，提出问题，并进一步改进这一实践。
- en: Some of the challenges you may encounter when trying to organize usability testing
    include getting actual customers to access legacy systems or not having the time,
    money, or experts to conduct this level of testing. Guerrilla testing offers a
    low-cost alternative or supplementary form of qualitative user testing.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在组织可用性测试时，您可能遇到的一些挑战包括让实际客户访问遗留系统，或者没有足够的时间、资金或专家来进行这种程度的测试。游击测试提供了一种低成本的替代方案或补充形式的定性用户测试。
- en: Guerrilla Testing
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 游击测试
- en: Guerrilla testing is a low-cost, lean, and Agile method of collecting data for
    testing and validating a hypothesis in a short session focused on specific tasks.
    Participants are not recruited in advance, but instead are approached in a number
    of environments by the team, where similar demographics are targeted; for example,
    customers in coffee shops, or administrators in an office environment.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 游击测试是一种低成本、精益和敏捷的方法，用于在短时间内针对特定任务收集数据来测试和验证假设。参与者不会提前招募，而是由团队在多种环境中接触，目标是类似的人群，例如在咖啡馆的顾客，或在办公室环境中的管理员。
- en: This testing provides a simple method for collecting enough data to make well-informed
    strategic design decisions. It can also assist senior stakeholders and product
    teams in understanding the importance of usability testing and customer feedback.
    Everyone on the team can facilitate without any research experts. It is a flexible
    approach that can be implemented at any stage of product development.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这种测试提供了一种简单的方法来收集足够的数据，以做出明智的战略设计决策。它还可以帮助高级利益相关者和产品团队理解可用性测试和客户反馈的重要性。团队中的每个人都可以在没有研究专家的情况下参与。它是一种灵活的方法，可以在产品开发的任何阶段实施。
- en: Guerrilla testing with a box of donuts in a busy Dublin bank!
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在繁忙的都柏林银行进行游击测试，带着一盒甜甜圈！
- en: '![](img/Donal.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Donal.jpg)'
- en: Doing any usability testing or generating feedback from a control group doesn't
    have to be a large costly exercise involving huge numbers of focus groups; it
    can be a simple activity! In the previous example, we undertook guerrilla testing
    after every sprint. It was an amazing way to generate real-world feedback and
    cost us next to nothing.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 进行任何可用性测试或从控制组生成反馈不必是一个涉及大量焦点小组的庞大昂贵的活动；它可以是一个简单的活动！在前一个示例中，我们在每个迭代后进行游击测试。这是一种产生真实世界反馈的惊人方式，而且几乎不花费什么。
- en: We would go into a branch of a bank, choosing a different one each week in different
    parts of the city to get a range of viewpoints. All we brought with us was the
    application and a massive box of donuts! The plan was simple. All we would do
    is lure people over to our stand and ask them to take part in a simple feedback
    session in exchange for free donuts! Users would road test the application and
    give us some valuable insight into things we had overlooked or failed to account
    for. The cost of the box of donuts was cheap but the feedback it gave us was invaluable.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会进入一家银行，每周选择不同的分行，分布在城市的不同区域，以获得不同的观点。我们带的东西非常简单——只有应用程序和一大盒甜甜圈！计划也很简单：我们会引诱人们来到我们的摊位，邀请他们参与一个简单的反馈环节，作为交换，我们提供免费的甜甜圈！用户会实际体验应用程序，并给我们一些宝贵的见解，帮助我们发现遗漏或未考虑到的地方。甜甜圈的盒子成本很低，但它为我们提供的反馈却是无价的。
- en: Read more about the guerilla testing practice, share your own experiences, ask
    questions, and improve the practice further in the Open Practice Library at [openpracticelibrary.com/practice/guerilla-testing/](http://openpracticelibrary.com/practice/guerilla-testing/).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 了解更多关于游击测试的实践，分享你自己的经验，提问，进一步完善这个实践，可以访问[openpracticelibrary.com/practice/guerilla-testing/](http://openpracticelibrary.com/practice/guerilla-testing/)。
- en: PetBattle Usability Testing
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 宠物对战可用性测试
- en: By looking at both the smooth delivery of changes and the overall stability
    of the production system, the PetBattle team hopes to avoid the types of failures
    seen when the hobbyist version of the application went viral and, at the same
    time, rapidly introduce new fixes and features to the software product.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察变更的顺利交付和生产系统的整体稳定性，宠物对战团队希望避免当应用程序的业余版本迅速传播时出现的失败类型，同时快速向软件产品中引入新的修复和功能。
- en: The first area the team would like to gather data from and measure is how long
    it takes to get new features into their testing environment. By automating and
    reducing the amount of time it takes to get fixes and features into an environment
    where testing can occur, this will shorten the feedback loop so the team can get
    fast feedback on their changes rather than having to wait for a release to happen.
    The PetBattle team is not sure exactly what the right metrics to measure here
    are, but thought that starting with this one would help them discover more software
    delivery metrics that could be useful.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 团队希望收集数据并测量的第一个领域是：将新特性投入测试环境所需的时间。通过自动化并减少将修复和特性投入可进行测试的环境所需的时间，将缩短反馈循环，使团队能够快速获得对其更改的反馈，而不是等待发布。PetBattle团队不确定该测量哪些正确的指标，但他们认为从这个开始有助于发现更多可能有用的软件交付指标。
- en: The second area of measurement the team would like to focus on is some simple
    performance testing. In the test environment, the plan is to concurrently load
    up the PetBattle application to see how it behaves and to try to identify any
    bugs or bottlenecks in the system. Some pre-emptive checking here will hopefully
    reveal any issues the team saw in the hobbyist version of the app. Again, the
    team is not quite sure exactly what parts of the system to focus on with their
    performance testing, or what metrics they should target yet, but deploying the
    PetBattle suite into an environment where this testing activity can happen is
    the right first step.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 团队希望关注的第二个测量领域是一些简单的性能测试。在测试环境中，计划是同时加载PetBattle应用程序，观察其表现并尝试识别系统中的任何错误或瓶颈。提前进行检查希望能够揭示团队在应用程序的业余版中看到的任何问题。同样，团队还不确定该将哪些系统部分作为性能测试的重点，或者应该关注哪些指标，但将PetBattle套件部署到可以进行此类测试的环境中是第一步。
- en: Taking a Measure and Learn approach to our Showcases, Retrospectives, experiments
    and user testing moves us into a continuous cycle of building something small,
    finding a way to measure from it, and gathering the data and learning from it,
    which then generates further ideas.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 采取度量与学习的方法来进行展示、回顾、实验和用户测试，使我们进入一个持续循环：构建一些小的东西，找出衡量方法，收集数据并从中学习，这将激发更多的想法。
- en: 'The *Build, Measure, Learn* feedback loop made famous in *The Lean Startup*
    is one of the most powerful tools and mindsets that is enabled by the platform,
    technology, and cultural tools we''ve been equipped with throughout this book:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在*精益创业*中闻名的*构建、衡量、学习*反馈循环，是平台、技术和文化工具赋予我们的一项最强大的工具和思维模式，这些工具贯穿了本书的内容：
- en: '![](img/B16297_13_05.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16297_13_05.jpg)'
- en: 'Figure 13.5: Lean – Build, Measure, Learn'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.5：精益 – 构建、衡量、学习
- en: Let's go into a little more detail about what to measure.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地探讨一下该测量什么。
- en: What to Measure?
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测量什么？
- en: '*What you measure is what you get.* – H. Thomas Johnson'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*你测量的就是你得到的。* – H. Thomas Johnson'
- en: In assessing what to measure as part of the *Build, Measure, Learn,* feedback
    loop, we are going to be standing on the shoulders of giants. There is a whole
    literature section on DevOps metrics available to us and we are going to call
    out our current favorites here. Top of that list is the DevOps DORA report[1](#footnote-078)
    and the book *Accelerate*,[2](#footnote-077) which are works where scientists
    described how they have taken a data-driven approach to measuring DevOps culture
    and practice. In the **DORA** report, effective IT delivery organizations take
    about an hour to get code from committed into trunk (in Git) to "running in production".
    This sounds great! So let's look at some of the detailed metrics that allow us
    to hone in on such a goal.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估作为*构建、衡量、学习*反馈循环的一部分时，我们将站在巨人的肩膀上。我们可以利用大量关于DevOps指标的文献，并将在这里指出我们当前的最爱。榜单最上面的是DevOps
    DORA报告[1](#footnote-078)和书籍《加速》[2](#footnote-077)，这两部作品描述了科学家们如何通过数据驱动的方法来衡量DevOps文化和实践。在**DORA**报告中，卓有成效的IT交付组织从代码提交到主干（Git中的代码）到“运行在生产环境中”大约需要一个小时。这听起来很棒！因此，让我们来看一下可以帮助我们集中精力实现这一目标的一些详细指标。
- en: Measuring Service Delivery and Operational Performance (SDO)
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 衡量服务交付和运营绩效（SDO）
- en: A key question for any team is, *What does good look like?*
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何团队来说，一个关键问题是，*什么是好的表现？*
- en: From the research in Accelerate, it appears that leading organizations update
    their software many times a day instead of once every few months, increasing their
    ability to use software to explore the market, respond to events, and release
    features faster than their competition. This huge increase in responsiveness does
    not come at a cost in stability or quality though, since failures are found and
    fixed quickly.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 根据《Accelerate》中的研究，领先的组织每天多次更新软件，而不是每几个月更新一次，这提高了他们利用软件探索市场、响应事件并比竞争对手更快发布功能的能力。然而，这种响应速度的巨大提升并没有以稳定性或质量为代价，因为失败被迅速发现并修复。
- en: 'Measuring software delivery performance is a difficult task. It would be easy
    to think that output-based measurements would suffice. Some often state that measurements
    might look similar to the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量软件交付性能是一项艰巨的任务。我们可能很容易认为基于输出的度量已经足够。一些人经常声称，度量可能看起来类似于以下内容：
- en: Lines of code written and committed by developers per day
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发人员每天编写并提交的代码行数
- en: Team utilization as a measure of team productivity
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以团队利用率作为衡量团队生产力的标准
- en: Team velocity (or the number of stories/features delivered per sprint)
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 团队的速度（或每个冲刺交付的故事/功能数量）
- en: Unfortunately, if we dig into these a little, we can quickly find problems with
    all of these measurements.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，如果我们稍微深入分析一下这些内容，我们可以迅速发现所有这些度量的缺陷。
- en: '[1](#footnote-078-backlink) [https://www.devops-research.com/research.html#reports](https://www.devops-research.com/research.html#reports)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[1](#footnote-078-backlink) [https://www.devops-research.com/research.html#reports](https://www.devops-research.com/research.html#reports)'
- en: '[2](#footnote-077-backlink) [https://itrevolution.com/book/accelerate](https://itrevolution.com/book/accelerate)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[2](#footnote-077-backlink) [https://itrevolution.com/book/accelerate](https://itrevolution.com/book/accelerate)'
- en: Is a solution that can be written with a 100-line piece of code better than
    a 200-line piece of code? On the surface it would seem so, but which solution
    is easier to maintain over time? Which solution is clearer and easier for new
    developers to discover and figure out? It may be that the 200-line piece of code
    is much easier to maintain and learn for the team rather than the expertly crafted
    bit of black magic code that only the expert understands. If a team is 100% utilized
    on feature development, when do they get time to learn, do unplanned work, or
    manage technical debt? Not having time for these activities ultimately slows delivery
    and innovation to a grinding halt. If a team delivers 10 stories per sprint, is
    this better than a team that delivers 5 stories per sprint? Can we really compare
    different teams' output when they work on unrelated work items? Probably not.
    Are the user stories the same size and value for our business and end customers?
    It is not easy to judge across teams with different products and ownership.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 是不是可以通过一段100行的代码写出比200行代码更好的解决方案？表面上看似如此，但哪种解决方案更容易随着时间推移进行维护呢？哪种方案更清晰且更容易让新开发者发现和理解？可能是200行的代码对于团队来说更易于维护和学习，而不是那段只有专家理解的精心编写的黑魔法代码。如果一个团队的资源100%都用于功能开发，那么他们何时才能有时间进行学习、处理临时工作或管理技术债务？没有时间做这些活动最终会导致交付和创新的停滞。如果一个团队每个冲刺交付10个故事，这比每个冲刺交付5个故事的团队更好吗？当不同团队在处理无关的工作项时，我们真的能比较他们的输出吗？可能不能。我们的用户故事是否对业务和最终客户的价值和大小相同？跨不同产品和所有权的团队之间很难做出这样的判断。
- en: These measurements are what we call output-based. We can instead shift the focus
    from what a team produces more toward the goals or Target Outcomes. By focusing
    on the global outcomes, the team members aren't pitted against each other by measuring
    the wrong thing. A classic example is rewarding Development Team members for throughput
    on new features and Operations Team members for service stability. Such measurements
    incentivize developers to throw poor-quality code into production as quickly as
    possible and operations to place painful change management processes in the way
    to slow down change.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这些度量标准我们称之为基于输出的度量。我们可以将重点从团队产生的内容转移到目标或目标结果上。通过专注于整体结果，团队成员不会因为度量错误的东西而互相竞争。一个经典的例子是奖励开发团队成员推动新特性上线的速度，而奖励运维团队成员的服务稳定性。这些度量标准鼓励开发人员尽可能快速地将低质量代码投入生产，而运维人员则通过设置繁琐的变更管理流程来阻碍变更。
- en: 'By not focusing on measurement or the right things to measure and focusing
    on outputs rather than outcomes is a quick way to get into trouble. Luckily, the
    DORA report starts to lay out some of the key metrics that a team can use for
    SDO:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不专注于测量或测量正确的内容，而是专注于输出而非结果，可能很快就会陷入困境。幸运的是，DORA 报告开始列出团队可以用来评估 SDO 的一些关键指标：
- en: '![](img/B16297_13_06.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16297_13_06.jpg)'
- en: 'Figure 13.6: DORA – Performance metrics'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.6：DORA – 性能指标
- en: '**Software Development Metrics** are metrics that capture the effectiveness
    of the development and delivery process by measuring the following:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**软件开发指标**是衡量开发和交付过程有效性的指标，通过以下方式进行测量：'
- en: '**Lead Time**: The time code is checked in to when it is released into production'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前置时间**：从代码提交到发布到生产环境的时间'
- en: '**Deployment Frequency**: How often the team can release code into production'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署频率**：团队将代码发布到生产环境的频率'
- en: 'Lead time is a key element of Lean Theory: by shortening the amount of time
    it takes to get product features to end users, the team can shorten the feedback
    loop for learning what their end users like or do not like. If the team is building
    the wrong thing, they can correct their course (or pivot) quickly when they have
    short lead times.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 前置时间是精益理论的一个关键元素：通过缩短将产品特性交付给最终用户的时间，团队可以缩短反馈循环，了解最终用户的喜好与不喜欢。如果团队构建了错误的东西，当前置时间较短时，他们可以迅速调整方向（或转变）以纠正问题。
- en: Another element to this is the size of the work delivered, or **Batch Size**.
    By delivering small incremental value to end users quickly, lead times are kept
    lower. With a much faster delivery cadence, how do we make sure system stability
    does not suffer?
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个因素是交付工作的大小，即**批量大小**。通过快速向最终用户交付小而增量的价值，可以保持较低的前置时间。随着交付节奏的加快，如何确保系统稳定性不受影响？
- en: '**Software Deployment Metrics** are the metrics that capture system stability
    and release quality. They are measured by the following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**软件部署指标**是衡量系统稳定性和发布质量的指标。它们通过以下方式进行测量：'
- en: '**Time to Restore**: The time taken from detecting a user-impacting incident
    to having it fixed or resolved'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**恢复时间**：从检测到影响用户的事件到修复或解决的时间'
- en: '**Change Failure Rate**: The number of released changes that fail or result
    in a user-impacting incident'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变更失败率**：已发布的变更中失败或导致用户影响事件的数量'
- en: What percentage of the changes made to production fail? How long does it take
    to restore service to end users in the event of failure? Development teams will
    have happier customers if outages are fixed quickly when changes occur and ideally
    have changes that do not fail at all!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 生产环境中有多少变更失败？发生故障时，恢复服务到最终用户的时间是多久？如果发生变更时故障能迅速修复，开发团队的客户会更满意，理想情况下变更应该完全没有失败！
- en: '**Service Operation Metrics** captures operational performance via metrics
    called **Service Availability**. Assume your product sales website has crashed.
    The company accountant might ask the question, *Is our application generating
    revenue right now?* Measuring service availability is a good way to link a technical
    goal with the desired business outcome.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**服务操作指标**通过称为**服务可用性**的指标捕捉操作性能。假设你的产品销售网站崩溃了，公司会计可能会问，*我们的应用现在有产生收入吗？* 测量服务可用性是将技术目标与期望的业务成果联系起来的好方法。'
- en: The metrics defined so far go a long way in helping a team understand key software,
    delivery, and operations metrics that are desirable for better organizational
    outcomes. To help capture and radiate these metrics, Red Hat has been investing
    in an open source project that has resulted in a dashboarding tool called Pelorus[3](#footnote-076).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 目前为止定义的指标在帮助团队理解关键的软件、交付和操作指标方面发挥了重要作用，这些指标有助于更好的组织结果。为了帮助捕捉和传播这些指标，Red Hat
    投资了一个开源项目，开发出了一个名为 Pelorus 的仪表板工具[3](#footnote-076)。
- en: '[3](#footnote-076-backlink) [https://github.com/konveyor/pelorus/](https://github.com/konveyor/pelorus/)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[3](#footnote-076-backlink) [https://github.com/konveyor/pelorus/](https://github.com/konveyor/pelorus/)'
- en: Pelorus
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pelorus
- en: 'Pelorus is an executive dashboard that helps visualize the progress we make
    on the SDO success metrics. It makes use of open source tools such as Prometheus
    and Grafana to track progress both locally (within a team) and globally (across
    the organization):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Pelorus 是一个执行仪表板，帮助可视化我们在 SDO 成功指标上的进展。它利用了 Prometheus 和 Grafana 等开源工具来跟踪进展，既可以在团队内部（局部）跟踪，也可以在组织内（全球）跟踪：
- en: '![](img/B16297_13_07.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16297_13_07.jpg)'
- en: 'Figure 13.7: Pelorus – SDO dashboard and metrics'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.7：Pelorus – SDO 仪表板和指标
- en: Pelorus consists of a set of exporters to customize data points to capture metrics
    from different providers. The sources from which exporters automate the collection
    of metrics are growing as more people contribute. This currently includes OpenShift
    (as Deploy time exporter), Git providers GitHub, GitLab, and Bitbucket (as Commit
    time exporters), and JIRA and ServiceNow (as established Issue trackers).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Pelorus由一组导出器组成，可以自定义数据点，从不同的提供者收集指标。随着更多人参与，导出器自动收集指标的来源在不断增加。目前包括OpenShift（作为部署时间导出器）、Git提供商GitHub、GitLab和Bitbucket（作为提交时间导出器），以及JIRA和ServiceNow（作为已建立的问题追踪器）。
- en: 'Using the data points that are collected from the providers, metric indicators
    are calculated to represent a measure. Each outcome is made measurable by a set
    of representative measures: **Lead Time for Change**, **Deployment Frequency**,
    **Mean Time to Restore**, and **Change Failure Rate**.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用从提供者收集的数据点，计算指标来表示一个衡量标准。每个结果通过一组代表性指标来衡量：**变更周期时间**、**部署频率**、**恢复时间平均值**和**变更失败率**。
- en: Pelorus offers a great opportunity to have real-time information radiators next
    to teams giving out these important metrics. Teams can also regularly inspect
    and discuss these at Showcase and Retrospective events and ask themselves what
    improvement actions or experiments they can run to try and improve these metrics
    further. What else should we measure?
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Pelorus为团队提供了一个绝佳的机会，可以在团队旁边展示这些重要指标的实时信息。团队还可以定期在展示会和回顾会议上检查和讨论这些指标，并问自己可以采取哪些改进措施或实验来进一步提升这些指标。我们还应该衡量什么？
- en: Measuring Lean Metrics
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测量精益指标
- en: 'The Lean movement also proposed a number of metrics to help measure software
    delivery performance. In their simplest form, you only need two pieces of information
    per work item or story: the start and finish date. This sounds easy to measure!
    But, of course, you need some policy around what those definitions are. With these
    two measurements we can start to measure a lot of different things, in particular:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 精益运动还提出了一些指标，帮助衡量软件交付绩效。最简单的形式下，你只需要每个工作项或故事的两个信息：开始日期和完成日期。这听起来很容易衡量！但当然，你需要一些关于这些定义的政策。通过这两个度量，我们可以开始衡量许多不同的事情，特别是：
- en: '**Time in Process**: The units of time per unit of work.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过程中的时间**：每单位工作所需的时间。'
- en: '**Lead Time**: The amount of time between when a feature is requested to when
    it is delivered into production.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交付周期时间**：从请求功能到其交付到生产环境之间的时间。'
- en: '**Flow Efficiency (or touch time/lead time)**: The time that the product is
    actually being worked on by the team, and value is being added.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流动效率（或触点时间/交付周期时间）**：产品实际被团队工作并且增加价值的时间。'
- en: '**Due Date Performance**: How often the feature gets delivered on time.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**截止日期绩效**：功能按时交付的频率。'
- en: Most of these metrics are part of what is known as **lagging** indicators for
    performance. In other words, they measure what has already happened. It is also
    possible to formulate **leading** indicators that can help predict future performance
    from this data. One such measurement example is based on the flow of work items
    or stories into and out of a team. This net flow of work items allows us to predict
    the confidence of due date delivery. As teams accept more and more work, this
    slows down their ability to deliver items on time. So, the net flow of stories
    in and out of a team becomes a lead indicator for measuring work item delivery
    performance.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标大多数是性能的**滞后**指标。换句话说，它们衡量的是已经发生的事情。也可以制定**前瞻**指标，利用这些数据帮助预测未来的绩效。其中一个例子是基于工作项或故事进出团队的流动情况。工作项的净流动量可以预测截止日期交付的信心。当团队接受越来越多的工作时，这会减慢他们按时交付项的能力。因此，团队内外工作项的净流动量成为衡量工作项交付绩效的前瞻性指标。
- en: It is surprising what start and end dates can tell us. The distribution of items
    with time can also help categorize the type of work undertaken by a team. For
    example, normal work items may look different to priority work items that come
    to the team from production failures. Another example may be work that requires
    long wait times for approval from, say, security or compliance teams. These would
    have large lead times compared to normal work items that don't require such approval.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Measuring SLOs, SLAs, and SLIs
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The goal of these **service level** (**SL**) metrics is to get customers, vendors,
    and users all on the same page regarding system behavior and performance. In particular,
    everyone needs to know and agree upon common SL questions, such as the following:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: How long for and how often will the system be available?
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there is an outage, how quickly will the response be to restore the service?
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How fast will the system respond if we make a single request?
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What about if we make many concurrent requests?
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Users of a service want to know the answer to these questions so they can plan
    and design how they consume any given service. For example, if you are building
    a solution that must be available all the time for your end users and it depends
    on a third-party service that is only available during business hours, you may
    need a different design, to implement a caching solution, or use another service
    with higher availability.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The SL acronyms can be broadly defined as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '**SLA**: An SL agreement that is normally a formal contract made between your
    organization and your clients, vendors, and users.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SLO**: SL objectives are the outcomes your team must deliver to meet the
    agreement.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SLI**: SL indicators are the actual metric numbers that are used to measure
    a team''s performance.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be very hard to measure SLAs properly. A service may be available but
    with degraded service performance, for example. It can also become more complicated
    when only some portion of your users experience a partial outage. Capturing this
    type of SLA complexity and measuring it accurately is hard to do.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: One benefit of SLAs is that they allow IT managers to quantitatively measure
    business outcomes. So rather than having to deal with a generic qualitative complaint,
    such as, "My application won't load and it is very slow", they can measure application
    availability (uptime) and percentile page load speed instead.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: An SLO is an agreement about a metric within a given SLA. A simple example may
    be that we agree to return search results quickly to our end users, with an average
    search latency of 200 milliseconds. In contrast, the SLI typically measures the
    SLO. So, we might define a target upper bound for our search, for example, by
    specifying that search latency for 99% of all performed searches must be less
    than 300 milliseconds.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: By quantitatively specifying and publishing SLAs, SLIs, and SLOs for our services,
    end users can set their expectations on how well the service will perform. This
    prevents qualitative complaints about the service being slow or over-reliance
    on a service where users expect it to be more available than it actually is.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: PetBattle Service Levels
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The current PetBattle hobbyist application has no SLAs – zero, zilch, nada.
    PetBattle V2 is going to be built for high availability. Given the app''s popularity
    and the fact it is the sole source of potential income for our fledgling company,
    it has been designated as "mission-critical". Redundancy is going to be built
    in at every level: at the cloud infrastructure layer (network, storage, and compute),
    and the application layer (scalable redundant highly available services). PetBattle
    V2 will be re-designed to have a shared-nothing architecture to allow maximum
    horizontal scalability without bottlenecks. PetBattle V2 will be required to meet
    a 99.99% availability SLA. That''s slightly more than 4 minutes of downtime allowed
    per month!'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Any external system that has a lower availability SLA will negatively impact
    the PetBattle V2 SLA when it fails. The PetBattle team writes the code and runs
    the service, and ideally there are as few external service providers in the Development
    and Delivery Loop as possible. Any new service that is incorporated into the application
    must be designed to meet these SLAs.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: By quantitatively specifying and publishing SLAs, SLIs, and SLOs for our services,
    end users can set their expectations on how well the service will perform. This
    prevents metrics-driven qualitative complaints about the service being slow or
    over-reliance on a service where users expect it to be more available than it
    actually is.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Security
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Malicious users are out there. How can we ensure that the user data that is
    kept in our applications is not misused? Or that our application services are
    not put to unintended usage, causing organizational or reputational harm? It is
    commonplace to see data leaks and security breaches related to software applications
    and services in the media today. Answering these questions is most often the primary
    concern of an **information security** (**InfoSec**) analyst or team of people.
    A modern approach to help tackle these security concerns is called **shifting
    security left**. The term is associated with teams that build InfoSec into the
    software delivery process instead of making it a separate phase that happens downstream
    of the development process.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'Building security into software development not only improves delivery performance,
    but also improves security quality. By designing and implementing security and
    compliance metrics into the system, it becomes possible to measure continual compliance
    against security standards:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_08.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.8: Standards-based system compliance'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: PetBattle Security
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The PetBattle team has asked the founders to hire an InfoSec professional who
    can be part of the development process. They are worried about potential data
    breaches of user details in PetBattle V2, especially if we need to start collecting
    payment details when we start monetizing the site. The PetBattle team has educated
    their developers of the common security risks, such as the OWASP Top 10 and how
    to prevent them.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The team plans to implement security scanning and testing measurement into their
    build pipeline. By using the OpenShift platform with a trusted software supply
    chain, the team can spend significantly less time remediating security issues
    that may arise.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: The team has identified that everyone needs to become very familiar with the
    detailed network design of the overall solution. This should help avert attacks
    by malicious actors. The team also wants to make sure all of the platforms and
    applications are easy to patch—so they can easily keep frameworks up to date when
    **Common Vulnerabilities and Exposures** (**CVEs**)[4](#footnote-075) arise.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[4](#footnote-075-backlink) [https://cve.mitre.org](https://cve.mitre.org)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Having a secure system involves making sure all of the layers in the stack are
    themselves secure – having a secure hardware environment, securing the operating
    system, securing the containers image layers being used, securing the dependencies
    that your application uses, securing your application code, securing the network
    that exposes your application services, and ultimately ensuring that your end
    users can interact with your applications securely.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Measuring these security layers, making sure they comply with various industry
    standards, and having concrete action plans for when new vulnerabilities arise,
    requires teams to work together at every stage of the software delivery life cycle.
    Security should not just be the **Chief Information Security Officer's** (**CISO's**)
    job. Done right, security is pervasive and designed into the platforms and software
    systems, with security professionals actively participating as part of the core
    delivery team and not just being the Mr. No[5](#footnote-074) when penetration
    testing is carried out.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Security is one of those topics that needs its own book. By shifting security
    left, we will cover technical topics later in the book that include container
    image and vulnerability scanning, the container health index, CVE patching, OpenShift
    Compliance Operator, OpenShift Container Security Operator, and Security Policy
    enforcement with ACM. These tools can help you to complement and build out a continually
    compliant platform and application suite for yourself.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Performance
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is an adage that software developers use—*first make it work, then make
    it work fast*. The truth to this statement is that some functionality must exist
    before it can be made to work fast! Qualities such as the performance and security
    of our applications are usually the most important non-functional requirements
    and they must be designed into the system from the very start in order to be successful.
    Let's define what we mean by performance.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 软件开发人员常用一句格言——*先让它工作，再让它工作得更快*。这句话的真理在于，某些功能必须先存在，才能做得更快！性能和安全性等特性通常是我们应用程序最重要的非功能性需求，必须从一开始就设计到系统中，才能确保成功。让我们来定义一下什么是性能。
- en: '**Performance** measures how fast the system processes a single transaction.
    This can be measured in isolation or under load. The system''s performance has
    a major impact on its throughput. When end users talk about performance, what
    they are usually talking about is throughput; and they only care about the performance
    of their own transactions, not anyone else''s. As far as they are concerned, if
    a system''s response time exceeds their expectation, the system is down.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**性能** 衡量系统处理单个事务的速度。这可以在孤立状态下或者负载下进行测量。系统的性能对其吞吐量有重大影响。当最终用户谈论性能时，他们通常指的是吞吐量；他们只关心自己事务的性能，而不关心其他人的事务。对他们来说，如果系统的响应时间超过了他们的预期，那么系统就算是崩溃了。'
- en: '**Throughput** describes the number of transactions the system can process
    in a given time span. The system''s performance clearly affects its throughput,
    but not necessarily in a linear way. Throughput is always limited by a constraint
    in the system, what is known as a bottleneck. Trying to optimize or improve performance
    for any part of the system that is not a bottleneck will not increase throughput.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**吞吐量** 描述了系统在给定时间内能够处理的事务数量。系统的性能显然会影响其吞吐量，但不一定是线性关系。吞吐量总是受到系统中某个约束的限制，这就是所谓的瓶颈。试图优化或提高系统中非瓶颈部分的性能并不会增加吞吐量。'
- en: '[5](#footnote-074-backlink) *Mr. No* is a *Mr. Men* book only available in
    France. Mr. No always disagrees with everyone and everything. *Mr. No* is published
    under the title *Monsieur Non* in France. This is one of the two Mr. Men titles
    that were not published in English.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[5](#footnote-074-backlink) *Mr. No* 是一本仅在法国发行的 *Mr. Men* 系列书籍。Mr. No 总是与每个人和一切事物意见相左。*Mr.
    No* 在法国出版时的标题是 *Monsieur Non*。这是两本未以英文出版的 *Mr. Men* 书籍之一。'
- en: If we measure the number of end user requests, the throughput will vary depending
    on how many requests there are. This is a measure of scalability. When a system
    is horizontally scalable, it means we can add capacity (more servers, more pods,
    or containers) to handle more throughput. In a shared-nothing architecture, we
    can add capacity until we reach a known bottleneck. For example, in OpenShift,
    this may be the number of pods per node, or the maximum number of nodes per cluster.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们衡量最终用户请求的数量，吞吐量将根据请求的多少而变化。这是一个衡量可扩展性的标准。当一个系统具有水平可扩展性时，意味着我们可以通过增加容量（更多的服务器、更多的
    pods 或容器）来处理更多的吞吐量。在无共享架构中，我们可以增加容量，直到达到已知的瓶颈。例如，在 OpenShift 中，这可能是每个节点的 pods
    数量，或者每个集群的最大节点数量。
- en: It is also worth noting that for any given system, an "acceptable response time"
    may vary! For a mobile/web app, any response longer than a second or two means
    users will use their fingers to walk away to another website or app. For a trading
    system at a bank, the response time may be in the order of milliseconds or less.
    To understand the capacity a system requires, we need to understand the system
    as a whole, before breaking it down into its constituent parts. This is called
    *systems thinking*. By thinking holistically and then in detail about a system,
    we can determine the bottleneck in a system.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得注意的是，对于任何给定的系统，“可接受的响应时间”可能是不同的！对于移动/网页应用程序来说，任何超过一两秒的响应时间都会让用户用手指离开，去浏览其他网站或应用。对于银行的交易系统来说，响应时间可能在毫秒级别甚至更短。要了解一个系统所需的容量，我们需要先全面了解系统，再将其拆解成各个部分。这被称为*系统思维*。通过从整体到细节地思考一个系统，我们可以找出系统中的瓶颈。
- en: At any point in time, exactly one constraint determines the system's capacity.
    Let's imagine it is the database that limits the transaction throughput. Once
    we improve that bottleneck—for example, by using faster storage or adding indexes,
    or using better database technology—the next bottleneck in the system then becomes
    the performance constraint—the application server capacity is now limiting throughput,
    say.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: PetBattle Performance
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When PetBattle is released to the world, users will gang up on us. Sometimes,
    users come in really, really big mobs. Picture the Instagram editors giggling
    as they point toward the PetBattle website, saying "Release the cat hoard!" Large
    mobs can trigger hangs, deadlocks, and obscure race conditions in our applications.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: The PetBattle team wants to run special stress tests to hammer deep links or
    hot URLs within the UI and API layers. Currently, there is a direct coupling between
    users and the database, so the developers already know that some form of caching
    could be required when scaling.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: If we make monitoring a priority, we can refine our infrastructure and application
    monitoring systems to ensure we are collecting information on the right services
    and putting that information to good use. The visibility and transparency yielded
    by effective monitoring are invaluable. Proactive monitoring is a key part of
    a strong technical foundation.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: The team plans to deploy the PetBattle application suite into an environment
    where they can start performance testing parts of the system under load. They
    want to start simply by targeting various parts of the system, for example, the
    API to gain an understanding of system behavior under load. By doing this early
    and often, they can identify bottlenecks and from there work out a plan to fix
    them.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Like security, moving performance testing left in the build and deployment process
    provides faster feedback and the opportunity to remediate and reveal issues early
    in the development life cycle. Complex system testing for a full suite of applications
    is often not feasible until closer to the end of development; however, it is possible
    to test individual APIs, endpoints, or even parts of the UI often and early in
    an automated manner. One of the main benefits of doing this type of automated
    performance testing is to build up a baseline understanding of how the system
    performs. Any code changes that alter the system performance characteristics often
    go unnoticed if this type of testing is not being automated and you end up trying
    to firefight performance issues right before a go-live.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Deployment Pain
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have ever had to work night shifts or weekends to help deliver a Big
    Bang release of software products into a production environment, you will understand
    the feeling of anxiety and concern engineers and technical staff feel prior to
    a go-live event. In the literature, this is referred to as deployment pain, and
    it can highlight the disconnect that exists between common development and testing
    tasks when compared to release and operational tasks.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'Fundamentally, most deployment problems are caused by a complex, brittle deployment
    process where the following occurs:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Software is often not written with deployability in mind.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manual steps are required to deploy to production.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple hand-offs are undertaken in deployment phases.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disruptions are a fact of life in all systems, OpenShift included. There are
    a host of Kubernetes primitives that will help our business services stay up and
    available for customers (including replication controllers, rolling deployments,
    health checks, pod disruption budgets, horizontal pod autoscalers, and cluster
    autoscalers). But even with the best infrastructure, failures can still occur.
    Cloud service disruptions, hardware failures, resource exhaustion, and misconfigurations
    can still threaten the business service.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Culture
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There is a link between metrics and transformation. Measurements help inform
    the practices that are used. Gathering CI/CD metrics, for example, the time that
    builds and deployments take for the PetBattle apps, allows the team to adopt and
    develop streamlined CI/CD practices so they can release quickly, more often, and
    with confidence that failures will be rare. As the team becomes comfortable releasing
    software into production regularly, the business starts to trust that releases
    can be performed with little risk to end users. This leads to the faster releasing
    of new features, a tighter, shorter feedback loop, and ultimately, a culture that
    encourages rapid change and innovation. Now we have really started to unlock transformational
    change for the company:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/figure-13-9.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.9: The link between metrics and transformation'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Operational metrics often measure failure in very complex adaptive systems.
    When things fail, an authoritarian blame-like culture will look to find the "human
    error" and assign blame. This type of response to failure is not only bad but
    should be considered harmful.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: The goal of any failure analysis should be to discover how we can improve information
    flow so that people have better and more timely information, or to find better
    tools to help prevent catastrophic failures following apparently normal operations.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Application Metrics
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All of the metrics collected so far are focused on non-functional parameters.
    What about the business application? Can we get some real-time metrics about the
    application usage that we can radiate alongside these other metrics?
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: PetBattle Application Metrics
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The application developers have discovered how easy it is to generate custom
    application metrics by utilizing the MicroProfile metrics extension in the PetBattle
    API server. The founders wanted to know how many cats were being uploaded in real
    time. The developers added the metrics extension to their application, which provides
    default Java VM metrics such as heap, CPU, and threads. It also allows the team
    to expose custom metrics from the application. For example, by adding Java annotations
    to their code, they can easily measure the frequency of cats uploaded.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'These metrics can now be scraped and displayed on a dashboard using Prometheus
    and Grafana:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_10.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.10: Analyzing PetBattle application metrics using Prometheus and
    Grafana'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: There are internal metrics we can monitor as well as external metrics. An external
    fake user (often called a synthetic user) can be set up to monitor the PetBattle
    app on a regular basis. This synthetic client experiences the same view of the
    system that real users experience. When that client cannot process a fake transaction,
    for example by trying to enter their fake cat in a Tournament, then there is a
    problem, whether the internal monitoring shows a problem or not!
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'There are lots of possible stability metrics the team thought they could easily
    measure and alert on if they were over a certain threshold. Slow responses tend
    to propagate upward from layer to layer in a gradual form of cascading failure.
    When the website becomes slow, users tend to hit the Refresh button more often,
    causing more and more traffic. If we give our system the ability to monitor its
    own performance (in other words, it becomes observable), then the system can also
    tell the team when it isn''t meeting its SL agreements. Some examples include
    the following:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: The number of users and active sessions
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blocked threads in the API layer
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Out of memory events in the API or database layer
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slow responses in the user interface
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High database CPU
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring Infrastructure Platform Costs and Utilization
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenShift comes with a cost management and metering application that can be
    used to show infrastructure usage. You deploy the Cost Management Metrics Operator
    in OpenShift and the reporting and APIs are delivered as part of a SaaS solution
    from [cloud.redhat.com](http://cloud.redhat.com).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'It allows the PetBattle team to do the following:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Visualize, understand, and analyze the use of resources and costs.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forecast their future consumption and compare them with budgets.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimize resources and consumption.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify patterns of usage that should be investigated.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate with third-party tools that can benefit from cost and resourcing data.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are a lot of visualizations and dashboards available. *Figure 13.11*
    shows the overview dashboard in a large demo environment as an example. It is
    possible to track cost and usage at both an infrastructure and business level.
    Users can tag projects and applications to gain detailed breakdowns as well as
    historical trends. The dashboards can help answer common questions, for example:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'Show me the top project and top clusters by usage and cost:'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which projects are costing me the most?
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Which clusters are costing me the most?
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Show how metrics contribute to the costs:'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is driving costs? CPU, memory, storage?
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the predicted costs for the next calendar month?
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is possible to change between accumulated and daily costs, as well as filter
    and drill down across clusters, clouds, and projects:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_11.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.11: Cost Management Overview dashboard at cloud.redhat.com'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: You can check out the cost management product documentation,[6](#footnote-073)
    which has a lot more details about this service, including common configuration
    options across a hybrid cloud.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[6](#footnote-073-backlink) [https://access.redhat.com/documentation/en-us/cost_management_service/2021](https://access.redhat.com/documentation/en-us/cost_management_service/2021)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Resources and Services
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are a number of simple high-level checks that should be made for all
    of our resources and services. The **USE** method is a simple checklist that can
    be summarized as *for every resource, monitor the following items*:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '**Utilization**: The percentage of time that the resource is busy.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Saturation**: The amount of work a resource has to do that is extra or overloaded,
    often a queue length or similar.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Errors**: The count of error events that occur.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Errors should be investigated because they can degrade performance, and may
    not be immediately noticed when the failure mode is recoverable. This includes
    operations that fail and are retried, and devices from a pool of redundant devices
    that fail:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_12.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.12: USE metrics'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'We can map the USE metrics against common resources in a tabulated form, as
    shown in *Figure 13.12*, allowing for quick identification of the types of issues
    that are occurring in our system. The OpenShift metrics stack supports pre-configured
    USE method dashboards at the cluster and node level:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_13.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.13: OpenShift Monitoring USE dashboards'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the **RED** method can be summarized as *for every service, monitor
    the request*:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '**Rate**: The number of requests per second'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Errors**: The number of requests that fail'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Duration**: The amount of time requests take to complete'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The RED method is a good baseline that can be applied to most request-based
    services. It reduces the cognitive load for SREs; in other words, they can think
    the same way about a large range of supported services for baseline metrics. The
    RED method does break down for batch-oriented or streaming services. In the Google
    SRE Book, the original "Four Golden Signals" included the RED and Saturation metrics.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: User Experience Analytics
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is a vast treasure trove of user analytics that can be sourced from the
    PetBattle user interface. The most obvious analytics are to do with non-functional
    performance measures, for example, page load times and response latencies. Hopefully,
    by measuring and baselining basic user interface performance, the team can prevent
    the application services from being trampled by a flood of customers!
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Standard practice is to measure using histograms (percentiles), which allow
    for a better understanding of outlier performance. Reliably measuring and aggregating
    quantiles/percentiles of high-velocity metrics from multiple sources is no easy
    task.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: PetBattle User Experience Analytics
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given the primary goal of monetizing PetBattle, signing up to one of the many
    global businesses that provide web analytics to help sell targeted advertising
    to end users is likely a better use of resources than trying to build a custom
    analytic and advertising solution from the ground up. Analytics services can quickly
    give us page load times, click-through rates, advertisements, site search optimizations,
    and suggestions, as well as help identify upsell opportunities – for a fee.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: As part of testing the end user experience, an automated load testing of the
    user interface can give the team a preview of what operating the PetBattle V2
    site would look like.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: We have shown in this section many different levels of quantitative analysis
    we can perform using metrics. Let's now see how these translate to our focus on
    outcomes.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Visualize Measurable Outcomes
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have a lot of things we can now wmeasure. How can we tell if we have *shifted
    the needle* and made an appreciable difference to the status quo? Quite often,
    the key metrics and data are not visible to everybody in the room; they are hidden
    away behind a login on a computer.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: To help solve this hidden data problem, we can make use of more information
    radiators, putting everything on walls around where the team works, using dashboards
    and large screens so we can visually represent all aspects of the delivery work.
    We can share all sorts of information that is useful to team members, stakeholders,
    and users alike. It can be physically presented on walls, windows, doors, and
    other flat surfaces and positioned in the line of sight of those people who will
    get value from consuming the information.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: There are some interesting consequences of making information more accessible.
    These include better speed and accuracy of teamwork. Because the information is
    now visible all of the time, people are frequently reminded of it. There is also
    less time wasted having to search for important information if it is constantly
    on display. It is also more likely that the information is accurate because people
    are continuously being reminded of it, and if it is inaccurate, questions get
    asked!
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: When people who are not part of the core team come into the team space, information
    that is radiated on the walls can instantly be read. Stakeholders and people interested
    in the team's work can immediately gain a better understanding and awareness of
    the work that is going on. This activity is often referred to as *walking the
    walls*. Interested parties can inspect artifacts on the walls and have a conversation
    with team members about them. This is a hugely different experience from when
    information is hidden away in a system behind a login.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Proactive Notification
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So, what happens when things start to go wrong? What happens when the system
    itself cannot resolve an issue by automatically restarting a pod, or scaling up
    cluster nodes? Enter the realm of **Alerts**.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Alerting can take many forms. It could be that horrible text at 03:00 when things
    really start going wrong, or a more subtle alert, such as a Slack message to say
    something has updated successfully. The key here is that the information is being
    pushed, and not pulled. OpenShift has alerting capabilities built into it and
    we'll explore this in more detail in *Chapter 16*, *Own It*.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: The classic forms of alerting are when there are spikes in memory usage in applications.
    This could lead to an application failing or constantly having to restart. In
    these instances, the team might spot the spikes on their dashboards and go and
    investigate the issues. We can, of course, make this feedback loop even shorter
    by combining data from different sources and alert on that. For example, if our
    application memory spikes, if we could capture the logs from around that time
    and push both events to the team, it could help diagnose the problems quicker.
    The real power of smart notifications is being able to respond even more quickly
    to the event.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Alerting the Development teams to things that have broken is extremely important.
    Notifications can come from all layers of the system; it doesn't just need to
    be that terrible call in the dead of night to say the website is down! Whenever
    a job runs to build our code or deploy a new version of an application, sending
    a quick alert to the team's instant messaging software is a good way to notify
    the concerned stakeholders. If the information is timely, then we can respond
    more effectively. This could mean pulling the Andon cord and halting the production
    line while we gather together to fix the issue.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Altering the Customers
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The software we rely on daily sometimes experiences downtime. Notifying the
    developers and SREs of critical failures is vital to resolving issues. Notifying
    customers in a transparent way of any system failures and your path to resolution
    can set you apart from the crowd. Lots of companies have status pages for their
    software to give an idea of whether something is up or down; for example, the
    Quay container repository status[7](#footnote-072) page gives you insight into
    what parts of the service are still working. This provides a great information
    radiator! Some of the best software companies in the world will not only publish
    the status of their downtimes, but also the steps they are taking to resolve issues
    in near real time. Companies such as GitHub and GitLab will post messages to Twitter
    or open up a Google Doc to post updates and provide as close to real-time updates
    as they can to their user base. This level of transparency is great to see, especially
    when the issues they are facing can be discussed further by others using the same
    technology. They can become proactive, and not make the same mistakes on that
    next big critical upgrade!
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[7](#footnote-072-backlink) [https://status.quay.io/](https://status.quay.io/)'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Alerting is only useful if it happens in a timely manner. It's no use alerting
    on a condition if no one can do anything about it because it's too late. It's
    often a bit of a balancing act to design alerting thresholds so that the humans
    who look after the product or systems can proactively take action, as opposed
    to pinging alerts too frequently by "crying wolf" when a limit is reached.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Having Fun with Notifications and the Build!
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](img/Author_25.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
- en: As a self-confessed lazy developer, I know that when I push some code, it's
    unlikely that I will check the results of an automated build. I know this is arrogance,
    but hear me out. Usually, my thinking is, It works for me locally, what could
    possibly go wrong in the build?
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: The truth is that often something can, and does, go wrong, I've forgotten to
    link my code or I've forgotten to add a file before committing. Things go wrong.
    This is why wherever I work I love to have big dashboards from Jenkins, like the
    one shown in *Figure 13.14*, or monitors, so everyone can see the build status.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: They are such an important element of the feedback loop. I look at a dashboard
    like this and I can immediately see critical information that makes my job easier,
    such as what step in the build is running, who triggered the build, or how many
    tests are failing! It's simple information, but it's next to impossible for me
    as a lazy developer to ignore something if it's flashing red in my face like this!
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: I love anything that can help shorten the feedback loop. I am particularly fond
    of a plugin that works well with a dashboard in Jenkins, called the Build Fail
    Analyzer. It parses the build log for a specific Regex and, if found, it can display
    a message. On the project, this screenshot is taken from the team who had gotten
    into the habit of trying to codify the failure issues, so when one was detected,
    it could prompt us to resolve it without looking through the log.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'On this dashboard, on the middle left side, I can see that the dev-portal-fe-e2e-tests
    testing suites have failed and the problem identified was selenium not started.
    With this information, I don''t need to open Jenkins and read the log to see what
    happened – I can go straight to OpenShift to see why the pod did not start:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_14.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.14: The Build Fail Analyzer in Jenkins'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Failing the build is seen sometimes as the cursed state. We try to encourage
    a no-blame culture, but sometimes it can be a bit of fun to have a bit of a blame
    game. I am a fan of this one for sure! Whenever you fail the build, you're deemed
    to be the Cowboy Coder, the one who rides into town and shoots from the hip without
    a care for the consequences. Or maybe you just left a typo in your code!
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'Either way, if the dashboard turns red, then you have to dress like the cowboy
    you are. This team took it one step further: not only did you have to wear the
    pink cowboy hat until you had fixed the problem, but you also had to ride around
    on the wooden hobby horse! Even if you had to go to the bathroom or get a coffee,
    the horse and hat went with you! You''d be amazed at the funny looks you get on
    the way to the canteen wearing this attire:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_15.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.15: The Cowboy Coders'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: More silly examples come from a project I was working on years back. This was
    in the days prior to using a container platform, and we had several manually configured
    VMs that were critical for us when releasing software. It was a massive project,
    with seven Scrum teams globally distributed. We were building a suite of 50 product
    microservices so our build farm was always busy!
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: On one occasion, we had to get the IT company that was managing our infrastructure
    to roll back to one of the previous backups, as someone had executed a large number
    of commands as root and broken several things for Jenkins. Raising a `sev1` ticket
    still took a few days to resolve! We couldn't let this kind of thing happen again
    as it was a massive drain on the team's morale and productivity. So, we decided
    to get inventive. We were using Slack as our messaging client and knew you could
    send messages to channels via a webhook. We also knew that if anyone logged into
    a machine, we could execute a bash script. Tying these items together, we created
    the Kenny Loggins channel in our Slack instance...because when you log into a
    server as root, you're in the DANGER ZONE!
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_16.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.16: The Kenny Loggins channel'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: This section has shown many different ways we can visualize outcomes and use
    metrics to trigger proactive notifications to learn faster. Let's see how this
    can be summarized with everything else we've learned in our Delivery Loop.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Delivery Map
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We concluded *Section 3*, *Discover It*, with a Discovery Map, with a single
    information radiator that summarized the Discovery Loop iteration. We concluded
    *Section 4*, *Prioritize It*, with an Options Map, which summarized the ideas
    and hypotheses we wanted to validate, how we would deliver the options, and what
    options we planned to work on first.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: We will conclude *Section 5*, *Deliver It*, with a Delivery Map. This is another
    open source artifact available in the Mobius Kit under Creative Commons that you
    can use to summarize all the learnings and decisions taken during your journey
    around the Delivery Loop.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'This map should slot neatly next to your Discovery Map and is used to summarize
    the following:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '**Actions**: What can we get done this week to improve the outcomes?'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Doing**: What is our current work in progress?'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Done**: What is ready to review?'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Impact**: What progress did we make toward the outcomes?'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learn**: What did we learn?'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Insights**: What are our next steps?'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we come out of the Delivery Loop and return to the Options Pivot in *Section
    7*, *Improve It, Sustain It*, we will complete the final section of this map by
    asking, *What are our next steps?*
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at PetBattle's Delivery Map at the end of their first iteration
    of the Delivery Loop.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: PetBattle – the Delivery Map
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Figure 13.17* has a lot of detail that may not all be readable in print. To
    explore it in full, you can access the image at the book''s GitHub repository
    at [https://github.com/PacktPublishing/DevOps-Culture-and-Practice-with-OpenShift](https://github.com/PacktPublishing/DevOps-Culture-and-Practice-with-OpenShift):'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_17.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.17: The PetBattle Delivery Map'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: The Delivery Map provides a powerful summary of the journey we've been round
    the Delivery Loop. Like all other artifacts, it is a living and breathing summary
    and should be revisted regularly and updated after every subsequent iteration.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have now completed our journey around the Mobius Loop. In this chapter, we
    have focused on the measurements and learning we can take away from the features
    we launch, the experiment we run, and the research we conduct. A relentless focus
    on measurement enables us to take more concrete decisions that are backed by metrics-based
    evidence.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: The Showcase and Retrospective events that are often run by Scrum and other
    Agile teams provide ample opportunity to showcase metrics and highlight learnings.
    We take this opportunity to re-examine the experiments that we designed on the
    Options Pivot and investigate what actually happened. That often involves looking
    at the analytics provided by the advanced deployment capabilities offered by the
    platform – the results of A/B tests, canary launches, feature toggles, and dark
    launches.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 'We also highlighted the importance of running usability tests with the full
    team involved, while being connected directly to end users to develop further
    empathy and see them testing the evolving application. Guerilla testing also provides
    a low-cost and simple way to gather learning from users:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_18.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.18: The Delivery Loop'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: We explored the many different metrics made available by the platform, the software,
    and our teams. Service Delivery and Operational Performance metrics popularized
    by DORA and Accelerate, and made available by open source tools such as Pelorus,
    provide leading indicators of the success of DevOps culture and practice. These
    can be supported by further metrics captured about security, performance, culture,
    the application itself, and the infrastructure. The importance of radiating these,
    in real time, in a very open and transparent manner cannot be underestimated,
    nor can putting the behaviors and practices in place to be reactive and responsive
    to changes in metrics.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'As we conclude *Section 5*, *Deliver It*, we can see just how many practices
    have allowed us to navigate the Mobius Loop, on top of our foundation of culture
    and technology:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_19.jpg)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.19: The practices mapped onto the Mobius Loop'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: While we have completed one revolution around the Mobius Loop, we have not completed
    the journey. We will never complete the journey until the whole product is turned
    off and decommissioned. This is because the Mobius Loop is infinite and will never
    end. As we come out of the Delivery Loop, we return to the Options Pivot. We will
    do this in *Chapter 17*, *Improve It*, when we explore the insights from our trip
    around the Loop and ask what we have learned, followed by what we are to do next.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Before that, we are going to spend a few chapters diving a bit deeper into the
    technical solution. We have already started that in this chapter. In *Chapter
    14*, *Build It*, we will look at other aspects of how we build the solution. In
    *Chapter 15*, *Run It*, we'll focus on running the solution. In *Chapter 16*,
    *Own It*, we'll explore what it means to own the solution. These three chapters
    form *Section 6* of our book and are all about how product teams *Build It*, *Run
    It*, *Own It*.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
