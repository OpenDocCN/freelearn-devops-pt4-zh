- en: 13\. Measure and Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Startup success can be engineered by following the process, which means it
    can be learned, which means it can be taught.* – Eric Ries'
  prefs: []
  type: TYPE_NORMAL
- en: In his book, *The Lean Startup*, Eric Ries describes a startup company as a
    *human institution designed to* *create a new product or service under conditions
    of extreme uncertainty*. He outlines a process to help deal with this uncertainty
    where a tight feedback loop is created around the creation of a **minimum viable
    product** (**MVP**). He argues that being able to react, fail fast, and use a
    data-driven approach to measurement assists in decision-making that is based on
    reason rather than emotion. This ability to learn from small experiments can be
    seen as a form of business agility – the ability to pivot quickly in the face
    of ever-changing circumstances. In lean terms, this feedback loop can be summarized
    as **Build, Measure, Learn**.
  prefs: []
  type: TYPE_NORMAL
- en: The cultural and human aspects of this process cannot be overlooked. Uncertainty
    and humanity are common bedfellows. Ron Westrum, an American sociologist, posits
    that organizations with better "information flow" function more effectively. He
    argues that a good culture requires trust and cooperation between people across
    the organization, and therefore it reflects the level of collaboration and trust
    inside the organization.
  prefs: []
  type: TYPE_NORMAL
- en: Second, better organizational culture can indicate higher-quality decision-making.
    In a team with this type of culture, not only is better information available
    for making decisions, but those decisions are more easily reversed if they turn
    out to be wrong because the team is more likely to be open and transparent rather
    than closed and rigid.
  prefs: []
  type: TYPE_NORMAL
- en: So, how can we take these ideas and make them actionable in our delivery processes?
    When timelines for delivery are tight and deadlines are fast approaching, a team's
    ability to deliver and operate software systems is critical to the business performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'With software, there are often two competing forces at work. Innovation, which
    inherently is accompanied by system change, and running software, which is serving
    end customers and implies that the system is stable. We can identify two important
    areas to focus on here:'
  prefs: []
  type: TYPE_NORMAL
- en: To help measure the effectiveness of a team's development and delivery practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To start measuring and monitoring activities that allow the rapid diagnosis
    of issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we are going to explore different mechanisms and techniques
    to take measurements out of our Delivery Loops and use those measurements to drive
    decisions and next steps. We call this approach **metrics-driven transformation**.
    We will also learn what to measure and how we can take these measurements and
    make them visible, ultimately to help answer the question of whether or not we
    have moved the needle at all during our transformation journey.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics-Driven Transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Metrics-driven transformation focuses on using value-based business metrics
    to understand how technology-related investments impact organizational performance
    and provide specific tools and guidance to help improve those metrics.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we looked at different approaches to doing delivery,
    whether that be Waterfall, or using an Agile framework such as Scrum or Kanban.
    As we complete loops of delivery, we want to take value-based metrics to validate
    hypotheses, confirm the results of experiments, clarify the impact of our feature
    deliveries, determine whether we have moved toward the Target Outcomes we set
    out, and make decisions around what to do next.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different levels of measurements we can take in our delivery
    ecosystem and a growing number of sources we can collect them from. In this chapter,
    we will explore metrics we can collect automatically from our software and our
    platform as well as practices we can use to collect metrics from our users, customers,
    employees, and the wider organization. Let's start by re-visiting some of the
    practices we've already used and see how we can use them to collect measurements
    and learning.
  prefs: []
  type: TYPE_NORMAL
- en: Where to Measure and Learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *Chapter 10*, *Setting Outcomes*, we introduced the practice of setting Target
    Outcomes based on all of the learning that came out of the practices on the Discovery
    Loop. We showed how to make these measurable and how to visualize them as an information
    radiator so everyone can inspect them. Using metrics, we can inspect where we
    are now in quantifiable terms, where we've been, and where we want to get to for
    each measurable outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'We explained the difference between primary (business-focused) outcomes and
    supporting enabling outcomes, which are non-functional-based outcomes. Since then,
    we have organized all of our work around those Target Outcomes and have radiated
    them in other practice artifacts, including the Value Slice on the Options Pivot
    and Scrum boards in the Delivery Loop. Those outcomes should be our starting point
    for measurement, and re-visualizing them will allow us to measure the progress
    we have made (or not) toward the target value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.1: Measuring targets'
  prefs: []
  type: TYPE_NORMAL
- en: When measuring the outcomes, we need to be aware of any prior measurement that
    may form an historic baseline, and potentially what any estimate of value may
    be against the actual value. Thinking about these different forms of measurement
    leads us to ask the question, *Where and when should we take measures and learn,
    and where should we inspect them?*
  prefs: []
  type: TYPE_NORMAL
- en: The Showcase
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ideally, we take measurements as soon as a delivery item is complete. That might
    be at the end of a sprint in Scrum or after delivering a completed feature in
    Kanban. Perhaps we can build in metrics data collection to the application code
    and run a report just before or during a Showcase. For example, if we have a Target
    Outcome around increasing the user base to 10,000 people, every Showcase could
    provide an update on what the current user base is and whether it is moving in
    the right direction toward the target.
  prefs: []
  type: TYPE_NORMAL
- en: There may be a lag in the availability of data that means it cannot be presented
    at Showcase events. In this case, there are two plans of attack. First, we can
    use the Showcase itself to collect some measurements and learning from stakeholders.
    Alternatively, we could collect measurements and learning from stakeholders and
    users on the latest Product Increment. But how?
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps we have an outcome around stakeholder confidence and/or user happiness
    in the application we are incrementally building. If so, then what better opportunity
    than to ask the stakeholders after seeing the Showcase of the latest increment
    how confident or happy they are with what they have seen? This can be quantitative,
    in that we can ask stakeholders to rate the product on a score of 1-10\. It can
    also be qualitative in the conversation that happens around this scoring and supplementary
    feedback from collecting feedback data.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps we have an outcome around employee engagement and team skills. Again,
    what better opportunity than at the end of a Delivery Loop iteration to survey
    the team on their happiness level and ask them to rate themselves on different
    skills? This visualization will not only allow us to see trends, but also identify
    the positive and negative effects as a result of the team education and cultural
    activities that we are undertaking.
  prefs: []
  type: TYPE_NORMAL
- en: 'Showcase events allow the opportunity to show many different kinds of metrics:
    Software Delivery metrics, Platform metrics, and Team Velocity metrics. We will
    explore the details of these shortly. If these are things that will be of interest
    to the audience and help them understand the impact of everything the team is
    doing, by all means, include them. However, the event where a more in-depth conversation
    can happen is the event that typically follows the Showcase, the Retrospective.'
  prefs: []
  type: TYPE_NORMAL
- en: The Retrospective
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We introduced the Retrospective practice in the previous chapter and looked
    at many different formats of running them. Let's consider metrics a little further
    and also an engineer's perspective of Retros.
  prefs: []
  type: TYPE_NORMAL
- en: The Retrospective – an Engineering Perspective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](img/Author_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Back in the late 1980s, I vividly remember being introduced to **feedback loops**
    and **control theory** in chemical engineering classes, way before software engineering
    totally enveloped my career! If you wanted to control the level in a tank, the
    flow rate in a pipe, or nearly any dynamically changing system, you needed to
    learn how to keep that system stable. The software engineering practice of Retrospectives
    always makes me think of those second-order feedback loops, the physical connection
    back into our software design and development process that allows us to learn
    and adapt so that the system can become more stable.
  prefs: []
  type: TYPE_NORMAL
- en: Retrospectives all have a common goal of allowing the team to inspect what has
    just happened, while allowing them to adapt and refine what goes on in the future.
    This is a critical function. If there is no feedback or feedback is not actioned,
    the team may start to lose faith in the whole delivery process. In the engineering
    world, if feedback fails, the tank overflows!
  prefs: []
  type: TYPE_NORMAL
- en: It should come as no surprise that metrics should be included as hot topics
    in any Retro. Teams that discuss their poorly performing SDO or CI/CD metrics
    can gain a lot of insight into what is going wrong with the software delivery
    process. So, celebrate when the build starts to take 20 minutes to complete. This
    means that the metrics measurement and trending are in place and the team can
    now take action to find out why it has gotten so slow and seek to improve it.
  prefs: []
  type: TYPE_NORMAL
- en: It's possible to save a lot of wasted time and resources by listening to timely
    feedback. If feedback is ignored, you can incur an opportunity cost by spending
    the next year carrying out remediation projects instead of rolling out new, revenue-generating
    features. The moral of the story is to Retro, Retro, Retro! You can really never
    have enough feedback.
  prefs: []
  type: TYPE_NORMAL
- en: With all the tools and technology put in place when we created the technical
    foundation (in *Chapter 6, Open Technical Practices – Beginnings, Starting Right*,
    and *Chapter 7, Open Technical Practices – The Midpoint*), there are huge amounts
    of data, metrics, and analysis we can collect and conduct. If you run pub Retros
    (as introduced in the previous chapter), there is nothing better than taking a
    few print-outs of reports from these tools and taking them down the pub to analyze
    together over a pint of Guinness!
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.2: Pub Retrospective – discussing metrics'
  prefs: []
  type: TYPE_NORMAL
- en: 'Export your build statistics, latest test results, static code analysis reports,
    Burndown charts, and whatever else you can find, lay them all out on the table,
    and ask yourselves: what does this data tell us about us as a team that we don''t
    know already? What can we learn about ourselves? What patterns exist? What can
    we do in the next sprint to make these measurements better?'
  prefs: []
  type: TYPE_NORMAL
- en: Retrospective actions that are often taken include an increased focus on CI/CD
    infrastructure, increasing thresholds around code coverage for testing, and adding
    more or faster feedback loops for the team to learn from.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting the Build Stats at Retrospectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is one of my favorite examples for highlighting the power and impact of
    the Retrospective. I collected this chart back in 2008 when delivering an access
    control management solution for a UK telecoms company. My team was building a
    Java-based application using Scrum and continuous delivery. They used Hudson for
    continuous integration and had a huge number of automated tests that were incrementally
    built into the app as part of the team's Definition of Done.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several build graphs were exported every 2 weeks and taken to the team''s Retrospective.
    The following chart shows the duration of the build as well as the success of
    the build (red means the build failed, yellow means the build succeeded but some
    automated tests failed, and blue means a successful build with all automated tests
    passing):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.3: Inspecting the build time statistics at Retrospectives (the graph
    has been annotated with a few labels to reflect the sprint that the data was a
    part of)'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the build was very unstable in Sprint 1\. A much earlier version
    of this chart was taken to the pub Retro at the end of Sprint 1\. The team inspected
    it and agreed that, during Sprint 2, they would invest some time investigating
    the build stability issues.
  prefs: []
  type: TYPE_NORMAL
- en: Two weeks later, Sprint 2 was completed and the ScrumMaster brought another
    print-out of this chart from Hudson. The good news was that the Sprint 1 Retro
    action had paid off as the build was much more stable. However, it was noticed
    that the build was sometimes taking more than 15 minutes to complete. This was
    a much longer feedback loop than needed, so a further retrospective action was
    taken in Sprint 3 to address this. We can see that from Sprint 3 onward, the build
    was mostly stable and relatively quick.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine if we had not used the Retrospective to look at this data. Imagine if
    we had let the slow build just fester over time. Imagine how much time would have
    been lost. This is why we do metrics-driven Retrospectives.
  prefs: []
  type: TYPE_NORMAL
- en: As we come out of the sprint, we now have the opportunity to learn the results
    of experiments we've designed.
  prefs: []
  type: TYPE_NORMAL
- en: Experiments – the Results!
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *Chapter 11*, *The Options Pivot*, when we introduced the Options Pivot,
    we introduced some of the advanced deployment strategies we could design in our
    experiments. If we decided to use one or more of these strategies when designing
    our experiments, now is the time to measure and learn from what actually happened.
  prefs: []
  type: TYPE_NORMAL
- en: If we designed an A/B test, we look at the data collected about the traffic,
    interaction, time spent, and other relevant metrics so we can judge the effectiveness
    of the two different versions based on the change in users' behavior.
  prefs: []
  type: TYPE_NORMAL
- en: If we designed a Canary Release, have we learned from the behavior of the users
    that were a part of the Canary Release candidate to validate whether the feature
    should be rolled out to the wider population? Similarly, with dark launches and
    feature toggles, we collect metrics from usage to assess whether the feature release
    should be extended to a larger user group, or should be switched off and dark
    launches rolled back.
  prefs: []
  type: TYPE_NORMAL
- en: All of the learning captured from these experiments and the resulting analytics
    is quantitative. This means that you can study the data, observe trends, decide
    to extend the experiment, or create whole new experiments. As you run more experiments
    and gather more data, your decision-making capability becomes stronger and is
    based on metrics gathered from it.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you can't validate everything through the use of numbers. Some commentary
    with further learning and understanding is needed by talking to end users, known
    as quantitative feedback. Let's explore a couple of practices to help us do this,
    starting with user testing.
  prefs: []
  type: TYPE_NORMAL
- en: User Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'User-based testing is a technique that focuses on user interactions with a
    product. These types of evaluation directly involve end users and focus on the
    person. Let''s dive a little more into this by looking at two user testing practices:
    usability and guerilla testing.'
  prefs: []
  type: TYPE_NORMAL
- en: Usability Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a usability testing session, the team observes real users interacting with
    the product. Typically, a facilitator sits with a user and asks them to complete
    tasks and explain their thinking as they go. The team sits in a separate room
    and observes the testing by video link.
  prefs: []
  type: TYPE_NORMAL
- en: A usability test is not a focus group; it's focused on what the user thinks
    and does in the real world. An Empathy Map, as introduced in *Chapter 8*, *Discovering
    the Why and Who*, can be a very useful supporting practice.
  prefs: []
  type: TYPE_NORMAL
- en: A usability test can be run on an existing product, a prototype, or even a competitor's
    product. The prototype could be working code, or it could be something as simple
    as a few clickable images. Test early and often to create products that delight
    users and solve real needs.
  prefs: []
  type: TYPE_NORMAL
- en: Usability testing often highlights something that's obvious to someone who has
    been working on a product, but might be confusing to a user. What we think users
    need might not be what they actually need. Indeed, what users think they need
    may not be what they actually need! Usability testing can help answer questions
    such as, *Are we on the right track?* *What problems do we still need to solve?*,
    or *Which features should we build next?* With early feedback from real users,
    teams can avoid sinking time into a feature that's confusing or not useful.
  prefs: []
  type: TYPE_NORMAL
- en: '"We Are Not Our Users"'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](img/Donal.jpg)'
  prefs: []
  type: TYPE_IMG
- en: One of the best designers I've ever worked with once told me that *We are not
    our users*. Since I spent a fair amount of time in front of a code editor among
    my development team, I thought I got what he meant. I didn't fully appreciate
    what he meant until the following incident occurred sometime later.
  prefs: []
  type: TYPE_NORMAL
- en: We were building a lightweight prototype for a mortgage application. The frontend
    was using an existing bank API for calculating mortgages and the application we
    were building on top asked a few initial questions before prompting the person
    to enter an estimate of their income. The text box initially filled in by the
    user had a placeholder saying, *e.g. 100,000.00*.
  prefs: []
  type: TYPE_NORMAL
- en: While doing some development and testing with the product owner and user experience
    designer, we must have filled out the form hundreds of times! Every time we used
    the form, we filled it out the exact same way, just by putting 100000 into that
    box. Not once had any of us ever put a decimal place in the box!
  prefs: []
  type: TYPE_NORMAL
- en: Fast-forward a few weeks and we were doing some real-world testing. Of course,
    the very first person we have to use the application goes through the form, putting
    "120000.00" into the box and hitting submit. As you can probably imagine, we expected
    everything to work swimmingly and the user feedback session to continue; but instead,
    the application crashed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.4: Mortgage application'
  prefs: []
  type: TYPE_NORMAL
- en: The business analyst who was running the usability test immediately phoned in
    to say that the app was broken. We replayed the exact thing the user did, only
    to discover that the bank's API was unable to accept a decimal point. But for
    us, the real surprise was that no one on our team had ever noticed this issue
    before. As a small team, we were quick to fix the issue by updating the placeholder
    text and sending only an integer to the API.
  prefs: []
  type: TYPE_NORMAL
- en: This example always reminds me that we are not our users. Users do weird and
    wonderful things in an application. Even if you try to test all the scenarios,
    there is almost always going to be some edge case you have not thought of, and
    the chances are the first person to use your app will do the one weird thing!
  prefs: []
  type: TYPE_NORMAL
- en: Read more about the usability testing practice, share your own experiences,
    ask questions, and improve the practice further in the Open Practice Library at
    [openpracticelibrary.com/practice/usability-testing](http://openpracticelibrary.com/practice/usability-testing).
  prefs: []
  type: TYPE_NORMAL
- en: Some of the challenges you may encounter when trying to organize usability testing
    include getting actual customers to access legacy systems or not having the time,
    money, or experts to conduct this level of testing. Guerrilla testing offers a
    low-cost alternative or supplementary form of qualitative user testing.
  prefs: []
  type: TYPE_NORMAL
- en: Guerrilla Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Guerrilla testing is a low-cost, lean, and Agile method of collecting data for
    testing and validating a hypothesis in a short session focused on specific tasks.
    Participants are not recruited in advance, but instead are approached in a number
    of environments by the team, where similar demographics are targeted; for example,
    customers in coffee shops, or administrators in an office environment.
  prefs: []
  type: TYPE_NORMAL
- en: This testing provides a simple method for collecting enough data to make well-informed
    strategic design decisions. It can also assist senior stakeholders and product
    teams in understanding the importance of usability testing and customer feedback.
    Everyone on the team can facilitate without any research experts. It is a flexible
    approach that can be implemented at any stage of product development.
  prefs: []
  type: TYPE_NORMAL
- en: Guerrilla testing with a box of donuts in a busy Dublin bank!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](img/Donal.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Doing any usability testing or generating feedback from a control group doesn't
    have to be a large costly exercise involving huge numbers of focus groups; it
    can be a simple activity! In the previous example, we undertook guerrilla testing
    after every sprint. It was an amazing way to generate real-world feedback and
    cost us next to nothing.
  prefs: []
  type: TYPE_NORMAL
- en: We would go into a branch of a bank, choosing a different one each week in different
    parts of the city to get a range of viewpoints. All we brought with us was the
    application and a massive box of donuts! The plan was simple. All we would do
    is lure people over to our stand and ask them to take part in a simple feedback
    session in exchange for free donuts! Users would road test the application and
    give us some valuable insight into things we had overlooked or failed to account
    for. The cost of the box of donuts was cheap but the feedback it gave us was invaluable.
  prefs: []
  type: TYPE_NORMAL
- en: Read more about the guerilla testing practice, share your own experiences, ask
    questions, and improve the practice further in the Open Practice Library at [openpracticelibrary.com/practice/guerilla-testing/](http://openpracticelibrary.com/practice/guerilla-testing/).
  prefs: []
  type: TYPE_NORMAL
- en: PetBattle Usability Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By looking at both the smooth delivery of changes and the overall stability
    of the production system, the PetBattle team hopes to avoid the types of failures
    seen when the hobbyist version of the application went viral and, at the same
    time, rapidly introduce new fixes and features to the software product.
  prefs: []
  type: TYPE_NORMAL
- en: The first area the team would like to gather data from and measure is how long
    it takes to get new features into their testing environment. By automating and
    reducing the amount of time it takes to get fixes and features into an environment
    where testing can occur, this will shorten the feedback loop so the team can get
    fast feedback on their changes rather than having to wait for a release to happen.
    The PetBattle team is not sure exactly what the right metrics to measure here
    are, but thought that starting with this one would help them discover more software
    delivery metrics that could be useful.
  prefs: []
  type: TYPE_NORMAL
- en: The second area of measurement the team would like to focus on is some simple
    performance testing. In the test environment, the plan is to concurrently load
    up the PetBattle application to see how it behaves and to try to identify any
    bugs or bottlenecks in the system. Some pre-emptive checking here will hopefully
    reveal any issues the team saw in the hobbyist version of the app. Again, the
    team is not quite sure exactly what parts of the system to focus on with their
    performance testing, or what metrics they should target yet, but deploying the
    PetBattle suite into an environment where this testing activity can happen is
    the right first step.
  prefs: []
  type: TYPE_NORMAL
- en: Taking a Measure and Learn approach to our Showcases, Retrospectives, experiments
    and user testing moves us into a continuous cycle of building something small,
    finding a way to measure from it, and gathering the data and learning from it,
    which then generates further ideas.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *Build, Measure, Learn* feedback loop made famous in *The Lean Startup*
    is one of the most powerful tools and mindsets that is enabled by the platform,
    technology, and cultural tools we''ve been equipped with throughout this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.5: Lean – Build, Measure, Learn'
  prefs: []
  type: TYPE_NORMAL
- en: Let's go into a little more detail about what to measure.
  prefs: []
  type: TYPE_NORMAL
- en: What to Measure?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*What you measure is what you get.* – H. Thomas Johnson'
  prefs: []
  type: TYPE_NORMAL
- en: In assessing what to measure as part of the *Build, Measure, Learn,* feedback
    loop, we are going to be standing on the shoulders of giants. There is a whole
    literature section on DevOps metrics available to us and we are going to call
    out our current favorites here. Top of that list is the DevOps DORA report[1](#footnote-078)
    and the book *Accelerate*,[2](#footnote-077) which are works where scientists
    described how they have taken a data-driven approach to measuring DevOps culture
    and practice. In the **DORA** report, effective IT delivery organizations take
    about an hour to get code from committed into trunk (in Git) to "running in production".
    This sounds great! So let's look at some of the detailed metrics that allow us
    to hone in on such a goal.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Service Delivery and Operational Performance (SDO)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A key question for any team is, *What does good look like?*
  prefs: []
  type: TYPE_NORMAL
- en: From the research in Accelerate, it appears that leading organizations update
    their software many times a day instead of once every few months, increasing their
    ability to use software to explore the market, respond to events, and release
    features faster than their competition. This huge increase in responsiveness does
    not come at a cost in stability or quality though, since failures are found and
    fixed quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Measuring software delivery performance is a difficult task. It would be easy
    to think that output-based measurements would suffice. Some often state that measurements
    might look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Lines of code written and committed by developers per day
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Team utilization as a measure of team productivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Team velocity (or the number of stories/features delivered per sprint)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unfortunately, if we dig into these a little, we can quickly find problems with
    all of these measurements.
  prefs: []
  type: TYPE_NORMAL
- en: '[1](#footnote-078-backlink) [https://www.devops-research.com/research.html#reports](https://www.devops-research.com/research.html#reports)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2](#footnote-077-backlink) [https://itrevolution.com/book/accelerate](https://itrevolution.com/book/accelerate)'
  prefs: []
  type: TYPE_NORMAL
- en: Is a solution that can be written with a 100-line piece of code better than
    a 200-line piece of code? On the surface it would seem so, but which solution
    is easier to maintain over time? Which solution is clearer and easier for new
    developers to discover and figure out? It may be that the 200-line piece of code
    is much easier to maintain and learn for the team rather than the expertly crafted
    bit of black magic code that only the expert understands. If a team is 100% utilized
    on feature development, when do they get time to learn, do unplanned work, or
    manage technical debt? Not having time for these activities ultimately slows delivery
    and innovation to a grinding halt. If a team delivers 10 stories per sprint, is
    this better than a team that delivers 5 stories per sprint? Can we really compare
    different teams' output when they work on unrelated work items? Probably not.
    Are the user stories the same size and value for our business and end customers?
    It is not easy to judge across teams with different products and ownership.
  prefs: []
  type: TYPE_NORMAL
- en: These measurements are what we call output-based. We can instead shift the focus
    from what a team produces more toward the goals or Target Outcomes. By focusing
    on the global outcomes, the team members aren't pitted against each other by measuring
    the wrong thing. A classic example is rewarding Development Team members for throughput
    on new features and Operations Team members for service stability. Such measurements
    incentivize developers to throw poor-quality code into production as quickly as
    possible and operations to place painful change management processes in the way
    to slow down change.
  prefs: []
  type: TYPE_NORMAL
- en: 'By not focusing on measurement or the right things to measure and focusing
    on outputs rather than outcomes is a quick way to get into trouble. Luckily, the
    DORA report starts to lay out some of the key metrics that a team can use for
    SDO:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.6: DORA – Performance metrics'
  prefs: []
  type: TYPE_NORMAL
- en: '**Software Development Metrics** are metrics that capture the effectiveness
    of the development and delivery process by measuring the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lead Time**: The time code is checked in to when it is released into production'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment Frequency**: How often the team can release code into production'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lead time is a key element of Lean Theory: by shortening the amount of time
    it takes to get product features to end users, the team can shorten the feedback
    loop for learning what their end users like or do not like. If the team is building
    the wrong thing, they can correct their course (or pivot) quickly when they have
    short lead times.'
  prefs: []
  type: TYPE_NORMAL
- en: Another element to this is the size of the work delivered, or **Batch Size**.
    By delivering small incremental value to end users quickly, lead times are kept
    lower. With a much faster delivery cadence, how do we make sure system stability
    does not suffer?
  prefs: []
  type: TYPE_NORMAL
- en: '**Software Deployment Metrics** are the metrics that capture system stability
    and release quality. They are measured by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Time to Restore**: The time taken from detecting a user-impacting incident
    to having it fixed or resolved'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Change Failure Rate**: The number of released changes that fail or result
    in a user-impacting incident'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What percentage of the changes made to production fail? How long does it take
    to restore service to end users in the event of failure? Development teams will
    have happier customers if outages are fixed quickly when changes occur and ideally
    have changes that do not fail at all!
  prefs: []
  type: TYPE_NORMAL
- en: '**Service Operation Metrics** captures operational performance via metrics
    called **Service Availability**. Assume your product sales website has crashed.
    The company accountant might ask the question, *Is our application generating
    revenue right now?* Measuring service availability is a good way to link a technical
    goal with the desired business outcome.'
  prefs: []
  type: TYPE_NORMAL
- en: The metrics defined so far go a long way in helping a team understand key software,
    delivery, and operations metrics that are desirable for better organizational
    outcomes. To help capture and radiate these metrics, Red Hat has been investing
    in an open source project that has resulted in a dashboarding tool called Pelorus[3](#footnote-076).
  prefs: []
  type: TYPE_NORMAL
- en: '[3](#footnote-076-backlink) [https://github.com/konveyor/pelorus/](https://github.com/konveyor/pelorus/)'
  prefs: []
  type: TYPE_NORMAL
- en: Pelorus
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Pelorus is an executive dashboard that helps visualize the progress we make
    on the SDO success metrics. It makes use of open source tools such as Prometheus
    and Grafana to track progress both locally (within a team) and globally (across
    the organization):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.7: Pelorus – SDO dashboard and metrics'
  prefs: []
  type: TYPE_NORMAL
- en: Pelorus consists of a set of exporters to customize data points to capture metrics
    from different providers. The sources from which exporters automate the collection
    of metrics are growing as more people contribute. This currently includes OpenShift
    (as Deploy time exporter), Git providers GitHub, GitLab, and Bitbucket (as Commit
    time exporters), and JIRA and ServiceNow (as established Issue trackers).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the data points that are collected from the providers, metric indicators
    are calculated to represent a measure. Each outcome is made measurable by a set
    of representative measures: **Lead Time for Change**, **Deployment Frequency**,
    **Mean Time to Restore**, and **Change Failure Rate**.'
  prefs: []
  type: TYPE_NORMAL
- en: Pelorus offers a great opportunity to have real-time information radiators next
    to teams giving out these important metrics. Teams can also regularly inspect
    and discuss these at Showcase and Retrospective events and ask themselves what
    improvement actions or experiments they can run to try and improve these metrics
    further. What else should we measure?
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Lean Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Lean movement also proposed a number of metrics to help measure software
    delivery performance. In their simplest form, you only need two pieces of information
    per work item or story: the start and finish date. This sounds easy to measure!
    But, of course, you need some policy around what those definitions are. With these
    two measurements we can start to measure a lot of different things, in particular:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Time in Process**: The units of time per unit of work.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lead Time**: The amount of time between when a feature is requested to when
    it is delivered into production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flow Efficiency (or touch time/lead time)**: The time that the product is
    actually being worked on by the team, and value is being added.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Due Date Performance**: How often the feature gets delivered on time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of these metrics are part of what is known as **lagging** indicators for
    performance. In other words, they measure what has already happened. It is also
    possible to formulate **leading** indicators that can help predict future performance
    from this data. One such measurement example is based on the flow of work items
    or stories into and out of a team. This net flow of work items allows us to predict
    the confidence of due date delivery. As teams accept more and more work, this
    slows down their ability to deliver items on time. So, the net flow of stories
    in and out of a team becomes a lead indicator for measuring work item delivery
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: It is surprising what start and end dates can tell us. The distribution of items
    with time can also help categorize the type of work undertaken by a team. For
    example, normal work items may look different to priority work items that come
    to the team from production failures. Another example may be work that requires
    long wait times for approval from, say, security or compliance teams. These would
    have large lead times compared to normal work items that don't require such approval.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring SLOs, SLAs, and SLIs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The goal of these **service level** (**SL**) metrics is to get customers, vendors,
    and users all on the same page regarding system behavior and performance. In particular,
    everyone needs to know and agree upon common SL questions, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How long for and how often will the system be available?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there is an outage, how quickly will the response be to restore the service?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How fast will the system respond if we make a single request?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What about if we make many concurrent requests?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Users of a service want to know the answer to these questions so they can plan
    and design how they consume any given service. For example, if you are building
    a solution that must be available all the time for your end users and it depends
    on a third-party service that is only available during business hours, you may
    need a different design, to implement a caching solution, or use another service
    with higher availability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The SL acronyms can be broadly defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SLA**: An SL agreement that is normally a formal contract made between your
    organization and your clients, vendors, and users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SLO**: SL objectives are the outcomes your team must deliver to meet the
    agreement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SLI**: SL indicators are the actual metric numbers that are used to measure
    a team''s performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be very hard to measure SLAs properly. A service may be available but
    with degraded service performance, for example. It can also become more complicated
    when only some portion of your users experience a partial outage. Capturing this
    type of SLA complexity and measuring it accurately is hard to do.
  prefs: []
  type: TYPE_NORMAL
- en: One benefit of SLAs is that they allow IT managers to quantitatively measure
    business outcomes. So rather than having to deal with a generic qualitative complaint,
    such as, "My application won't load and it is very slow", they can measure application
    availability (uptime) and percentile page load speed instead.
  prefs: []
  type: TYPE_NORMAL
- en: An SLO is an agreement about a metric within a given SLA. A simple example may
    be that we agree to return search results quickly to our end users, with an average
    search latency of 200 milliseconds. In contrast, the SLI typically measures the
    SLO. So, we might define a target upper bound for our search, for example, by
    specifying that search latency for 99% of all performed searches must be less
    than 300 milliseconds.
  prefs: []
  type: TYPE_NORMAL
- en: By quantitatively specifying and publishing SLAs, SLIs, and SLOs for our services,
    end users can set their expectations on how well the service will perform. This
    prevents qualitative complaints about the service being slow or over-reliance
    on a service where users expect it to be more available than it actually is.
  prefs: []
  type: TYPE_NORMAL
- en: PetBattle Service Levels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The current PetBattle hobbyist application has no SLAs – zero, zilch, nada.
    PetBattle V2 is going to be built for high availability. Given the app''s popularity
    and the fact it is the sole source of potential income for our fledgling company,
    it has been designated as "mission-critical". Redundancy is going to be built
    in at every level: at the cloud infrastructure layer (network, storage, and compute),
    and the application layer (scalable redundant highly available services). PetBattle
    V2 will be re-designed to have a shared-nothing architecture to allow maximum
    horizontal scalability without bottlenecks. PetBattle V2 will be required to meet
    a 99.99% availability SLA. That''s slightly more than 4 minutes of downtime allowed
    per month!'
  prefs: []
  type: TYPE_NORMAL
- en: Any external system that has a lower availability SLA will negatively impact
    the PetBattle V2 SLA when it fails. The PetBattle team writes the code and runs
    the service, and ideally there are as few external service providers in the Development
    and Delivery Loop as possible. Any new service that is incorporated into the application
    must be designed to meet these SLAs.
  prefs: []
  type: TYPE_NORMAL
- en: By quantitatively specifying and publishing SLAs, SLIs, and SLOs for our services,
    end users can set their expectations on how well the service will perform. This
    prevents metrics-driven qualitative complaints about the service being slow or
    over-reliance on a service where users expect it to be more available than it
    actually is.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Malicious users are out there. How can we ensure that the user data that is
    kept in our applications is not misused? Or that our application services are
    not put to unintended usage, causing organizational or reputational harm? It is
    commonplace to see data leaks and security breaches related to software applications
    and services in the media today. Answering these questions is most often the primary
    concern of an **information security** (**InfoSec**) analyst or team of people.
    A modern approach to help tackle these security concerns is called **shifting
    security left**. The term is associated with teams that build InfoSec into the
    software delivery process instead of making it a separate phase that happens downstream
    of the development process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Building security into software development not only improves delivery performance,
    but also improves security quality. By designing and implementing security and
    compliance metrics into the system, it becomes possible to measure continual compliance
    against security standards:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.8: Standards-based system compliance'
  prefs: []
  type: TYPE_NORMAL
- en: PetBattle Security
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The PetBattle team has asked the founders to hire an InfoSec professional who
    can be part of the development process. They are worried about potential data
    breaches of user details in PetBattle V2, especially if we need to start collecting
    payment details when we start monetizing the site. The PetBattle team has educated
    their developers of the common security risks, such as the OWASP Top 10 and how
    to prevent them.
  prefs: []
  type: TYPE_NORMAL
- en: The team plans to implement security scanning and testing measurement into their
    build pipeline. By using the OpenShift platform with a trusted software supply
    chain, the team can spend significantly less time remediating security issues
    that may arise.
  prefs: []
  type: TYPE_NORMAL
- en: The team has identified that everyone needs to become very familiar with the
    detailed network design of the overall solution. This should help avert attacks
    by malicious actors. The team also wants to make sure all of the platforms and
    applications are easy to patch—so they can easily keep frameworks up to date when
    **Common Vulnerabilities and Exposures** (**CVEs**)[4](#footnote-075) arise.
  prefs: []
  type: TYPE_NORMAL
- en: '[4](#footnote-075-backlink) [https://cve.mitre.org](https://cve.mitre.org)'
  prefs: []
  type: TYPE_NORMAL
- en: Having a secure system involves making sure all of the layers in the stack are
    themselves secure – having a secure hardware environment, securing the operating
    system, securing the containers image layers being used, securing the dependencies
    that your application uses, securing your application code, securing the network
    that exposes your application services, and ultimately ensuring that your end
    users can interact with your applications securely.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring these security layers, making sure they comply with various industry
    standards, and having concrete action plans for when new vulnerabilities arise,
    requires teams to work together at every stage of the software delivery life cycle.
    Security should not just be the **Chief Information Security Officer's** (**CISO's**)
    job. Done right, security is pervasive and designed into the platforms and software
    systems, with security professionals actively participating as part of the core
    delivery team and not just being the Mr. No[5](#footnote-074) when penetration
    testing is carried out.
  prefs: []
  type: TYPE_NORMAL
- en: Security is one of those topics that needs its own book. By shifting security
    left, we will cover technical topics later in the book that include container
    image and vulnerability scanning, the container health index, CVE patching, OpenShift
    Compliance Operator, OpenShift Container Security Operator, and Security Policy
    enforcement with ACM. These tools can help you to complement and build out a continually
    compliant platform and application suite for yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is an adage that software developers use—*first make it work, then make
    it work fast*. The truth to this statement is that some functionality must exist
    before it can be made to work fast! Qualities such as the performance and security
    of our applications are usually the most important non-functional requirements
    and they must be designed into the system from the very start in order to be successful.
    Let's define what we mean by performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance** measures how fast the system processes a single transaction.
    This can be measured in isolation or under load. The system''s performance has
    a major impact on its throughput. When end users talk about performance, what
    they are usually talking about is throughput; and they only care about the performance
    of their own transactions, not anyone else''s. As far as they are concerned, if
    a system''s response time exceeds their expectation, the system is down.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Throughput** describes the number of transactions the system can process
    in a given time span. The system''s performance clearly affects its throughput,
    but not necessarily in a linear way. Throughput is always limited by a constraint
    in the system, what is known as a bottleneck. Trying to optimize or improve performance
    for any part of the system that is not a bottleneck will not increase throughput.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5](#footnote-074-backlink) *Mr. No* is a *Mr. Men* book only available in
    France. Mr. No always disagrees with everyone and everything. *Mr. No* is published
    under the title *Monsieur Non* in France. This is one of the two Mr. Men titles
    that were not published in English.'
  prefs: []
  type: TYPE_NORMAL
- en: If we measure the number of end user requests, the throughput will vary depending
    on how many requests there are. This is a measure of scalability. When a system
    is horizontally scalable, it means we can add capacity (more servers, more pods,
    or containers) to handle more throughput. In a shared-nothing architecture, we
    can add capacity until we reach a known bottleneck. For example, in OpenShift,
    this may be the number of pods per node, or the maximum number of nodes per cluster.
  prefs: []
  type: TYPE_NORMAL
- en: It is also worth noting that for any given system, an "acceptable response time"
    may vary! For a mobile/web app, any response longer than a second or two means
    users will use their fingers to walk away to another website or app. For a trading
    system at a bank, the response time may be in the order of milliseconds or less.
    To understand the capacity a system requires, we need to understand the system
    as a whole, before breaking it down into its constituent parts. This is called
    *systems thinking*. By thinking holistically and then in detail about a system,
    we can determine the bottleneck in a system.
  prefs: []
  type: TYPE_NORMAL
- en: At any point in time, exactly one constraint determines the system's capacity.
    Let's imagine it is the database that limits the transaction throughput. Once
    we improve that bottleneck—for example, by using faster storage or adding indexes,
    or using better database technology—the next bottleneck in the system then becomes
    the performance constraint—the application server capacity is now limiting throughput,
    say.
  prefs: []
  type: TYPE_NORMAL
- en: PetBattle Performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When PetBattle is released to the world, users will gang up on us. Sometimes,
    users come in really, really big mobs. Picture the Instagram editors giggling
    as they point toward the PetBattle website, saying "Release the cat hoard!" Large
    mobs can trigger hangs, deadlocks, and obscure race conditions in our applications.
  prefs: []
  type: TYPE_NORMAL
- en: The PetBattle team wants to run special stress tests to hammer deep links or
    hot URLs within the UI and API layers. Currently, there is a direct coupling between
    users and the database, so the developers already know that some form of caching
    could be required when scaling.
  prefs: []
  type: TYPE_NORMAL
- en: If we make monitoring a priority, we can refine our infrastructure and application
    monitoring systems to ensure we are collecting information on the right services
    and putting that information to good use. The visibility and transparency yielded
    by effective monitoring are invaluable. Proactive monitoring is a key part of
    a strong technical foundation.
  prefs: []
  type: TYPE_NORMAL
- en: The team plans to deploy the PetBattle application suite into an environment
    where they can start performance testing parts of the system under load. They
    want to start simply by targeting various parts of the system, for example, the
    API to gain an understanding of system behavior under load. By doing this early
    and often, they can identify bottlenecks and from there work out a plan to fix
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Like security, moving performance testing left in the build and deployment process
    provides faster feedback and the opportunity to remediate and reveal issues early
    in the development life cycle. Complex system testing for a full suite of applications
    is often not feasible until closer to the end of development; however, it is possible
    to test individual APIs, endpoints, or even parts of the UI often and early in
    an automated manner. One of the main benefits of doing this type of automated
    performance testing is to build up a baseline understanding of how the system
    performs. Any code changes that alter the system performance characteristics often
    go unnoticed if this type of testing is not being automated and you end up trying
    to firefight performance issues right before a go-live.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Deployment Pain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have ever had to work night shifts or weekends to help deliver a Big
    Bang release of software products into a production environment, you will understand
    the feeling of anxiety and concern engineers and technical staff feel prior to
    a go-live event. In the literature, this is referred to as deployment pain, and
    it can highlight the disconnect that exists between common development and testing
    tasks when compared to release and operational tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fundamentally, most deployment problems are caused by a complex, brittle deployment
    process where the following occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: Software is often not written with deployability in mind.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manual steps are required to deploy to production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple hand-offs are undertaken in deployment phases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disruptions are a fact of life in all systems, OpenShift included. There are
    a host of Kubernetes primitives that will help our business services stay up and
    available for customers (including replication controllers, rolling deployments,
    health checks, pod disruption budgets, horizontal pod autoscalers, and cluster
    autoscalers). But even with the best infrastructure, failures can still occur.
    Cloud service disruptions, hardware failures, resource exhaustion, and misconfigurations
    can still threaten the business service.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Culture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There is a link between metrics and transformation. Measurements help inform
    the practices that are used. Gathering CI/CD metrics, for example, the time that
    builds and deployments take for the PetBattle apps, allows the team to adopt and
    develop streamlined CI/CD practices so they can release quickly, more often, and
    with confidence that failures will be rare. As the team becomes comfortable releasing
    software into production regularly, the business starts to trust that releases
    can be performed with little risk to end users. This leads to the faster releasing
    of new features, a tighter, shorter feedback loop, and ultimately, a culture that
    encourages rapid change and innovation. Now we have really started to unlock transformational
    change for the company:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/figure-13-9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.9: The link between metrics and transformation'
  prefs: []
  type: TYPE_NORMAL
- en: Operational metrics often measure failure in very complex adaptive systems.
    When things fail, an authoritarian blame-like culture will look to find the "human
    error" and assign blame. This type of response to failure is not only bad but
    should be considered harmful.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of any failure analysis should be to discover how we can improve information
    flow so that people have better and more timely information, or to find better
    tools to help prevent catastrophic failures following apparently normal operations.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Application Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All of the metrics collected so far are focused on non-functional parameters.
    What about the business application? Can we get some real-time metrics about the
    application usage that we can radiate alongside these other metrics?
  prefs: []
  type: TYPE_NORMAL
- en: PetBattle Application Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The application developers have discovered how easy it is to generate custom
    application metrics by utilizing the MicroProfile metrics extension in the PetBattle
    API server. The founders wanted to know how many cats were being uploaded in real
    time. The developers added the metrics extension to their application, which provides
    default Java VM metrics such as heap, CPU, and threads. It also allows the team
    to expose custom metrics from the application. For example, by adding Java annotations
    to their code, they can easily measure the frequency of cats uploaded.
  prefs: []
  type: TYPE_NORMAL
- en: 'These metrics can now be scraped and displayed on a dashboard using Prometheus
    and Grafana:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.10: Analyzing PetBattle application metrics using Prometheus and
    Grafana'
  prefs: []
  type: TYPE_NORMAL
- en: There are internal metrics we can monitor as well as external metrics. An external
    fake user (often called a synthetic user) can be set up to monitor the PetBattle
    app on a regular basis. This synthetic client experiences the same view of the
    system that real users experience. When that client cannot process a fake transaction,
    for example by trying to enter their fake cat in a Tournament, then there is a
    problem, whether the internal monitoring shows a problem or not!
  prefs: []
  type: TYPE_NORMAL
- en: 'There are lots of possible stability metrics the team thought they could easily
    measure and alert on if they were over a certain threshold. Slow responses tend
    to propagate upward from layer to layer in a gradual form of cascading failure.
    When the website becomes slow, users tend to hit the Refresh button more often,
    causing more and more traffic. If we give our system the ability to monitor its
    own performance (in other words, it becomes observable), then the system can also
    tell the team when it isn''t meeting its SL agreements. Some examples include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of users and active sessions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blocked threads in the API layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Out of memory events in the API or database layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slow responses in the user interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High database CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring Infrastructure Platform Costs and Utilization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenShift comes with a cost management and metering application that can be
    used to show infrastructure usage. You deploy the Cost Management Metrics Operator
    in OpenShift and the reporting and APIs are delivered as part of a SaaS solution
    from [cloud.redhat.com](http://cloud.redhat.com).
  prefs: []
  type: TYPE_NORMAL
- en: 'It allows the PetBattle team to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Visualize, understand, and analyze the use of resources and costs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forecast their future consumption and compare them with budgets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimize resources and consumption.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify patterns of usage that should be investigated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate with third-party tools that can benefit from cost and resourcing data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are a lot of visualizations and dashboards available. *Figure 13.11*
    shows the overview dashboard in a large demo environment as an example. It is
    possible to track cost and usage at both an infrastructure and business level.
    Users can tag projects and applications to gain detailed breakdowns as well as
    historical trends. The dashboards can help answer common questions, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Show me the top project and top clusters by usage and cost:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which projects are costing me the most?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Which clusters are costing me the most?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Show how metrics contribute to the costs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is driving costs? CPU, memory, storage?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the predicted costs for the next calendar month?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is possible to change between accumulated and daily costs, as well as filter
    and drill down across clusters, clouds, and projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.11: Cost Management Overview dashboard at cloud.redhat.com'
  prefs: []
  type: TYPE_NORMAL
- en: You can check out the cost management product documentation,[6](#footnote-073)
    which has a lot more details about this service, including common configuration
    options across a hybrid cloud.
  prefs: []
  type: TYPE_NORMAL
- en: '[6](#footnote-073-backlink) [https://access.redhat.com/documentation/en-us/cost_management_service/2021](https://access.redhat.com/documentation/en-us/cost_management_service/2021)'
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Resources and Services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are a number of simple high-level checks that should be made for all
    of our resources and services. The **USE** method is a simple checklist that can
    be summarized as *for every resource, monitor the following items*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Utilization**: The percentage of time that the resource is busy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Saturation**: The amount of work a resource has to do that is extra or overloaded,
    often a queue length or similar.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Errors**: The count of error events that occur.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Errors should be investigated because they can degrade performance, and may
    not be immediately noticed when the failure mode is recoverable. This includes
    operations that fail and are retried, and devices from a pool of redundant devices
    that fail:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.12: USE metrics'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can map the USE metrics against common resources in a tabulated form, as
    shown in *Figure 13.12*, allowing for quick identification of the types of issues
    that are occurring in our system. The OpenShift metrics stack supports pre-configured
    USE method dashboards at the cluster and node level:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.13: OpenShift Monitoring USE dashboards'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the **RED** method can be summarized as *for every service, monitor
    the request*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rate**: The number of requests per second'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Errors**: The number of requests that fail'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Duration**: The amount of time requests take to complete'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The RED method is a good baseline that can be applied to most request-based
    services. It reduces the cognitive load for SREs; in other words, they can think
    the same way about a large range of supported services for baseline metrics. The
    RED method does break down for batch-oriented or streaming services. In the Google
    SRE Book, the original "Four Golden Signals" included the RED and Saturation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: User Experience Analytics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is a vast treasure trove of user analytics that can be sourced from the
    PetBattle user interface. The most obvious analytics are to do with non-functional
    performance measures, for example, page load times and response latencies. Hopefully,
    by measuring and baselining basic user interface performance, the team can prevent
    the application services from being trampled by a flood of customers!
  prefs: []
  type: TYPE_NORMAL
- en: Standard practice is to measure using histograms (percentiles), which allow
    for a better understanding of outlier performance. Reliably measuring and aggregating
    quantiles/percentiles of high-velocity metrics from multiple sources is no easy
    task.
  prefs: []
  type: TYPE_NORMAL
- en: PetBattle User Experience Analytics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given the primary goal of monetizing PetBattle, signing up to one of the many
    global businesses that provide web analytics to help sell targeted advertising
    to end users is likely a better use of resources than trying to build a custom
    analytic and advertising solution from the ground up. Analytics services can quickly
    give us page load times, click-through rates, advertisements, site search optimizations,
    and suggestions, as well as help identify upsell opportunities – for a fee.
  prefs: []
  type: TYPE_NORMAL
- en: As part of testing the end user experience, an automated load testing of the
    user interface can give the team a preview of what operating the PetBattle V2
    site would look like.
  prefs: []
  type: TYPE_NORMAL
- en: We have shown in this section many different levels of quantitative analysis
    we can perform using metrics. Let's now see how these translate to our focus on
    outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Visualize Measurable Outcomes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have a lot of things we can now wmeasure. How can we tell if we have *shifted
    the needle* and made an appreciable difference to the status quo? Quite often,
    the key metrics and data are not visible to everybody in the room; they are hidden
    away behind a login on a computer.
  prefs: []
  type: TYPE_NORMAL
- en: To help solve this hidden data problem, we can make use of more information
    radiators, putting everything on walls around where the team works, using dashboards
    and large screens so we can visually represent all aspects of the delivery work.
    We can share all sorts of information that is useful to team members, stakeholders,
    and users alike. It can be physically presented on walls, windows, doors, and
    other flat surfaces and positioned in the line of sight of those people who will
    get value from consuming the information.
  prefs: []
  type: TYPE_NORMAL
- en: There are some interesting consequences of making information more accessible.
    These include better speed and accuracy of teamwork. Because the information is
    now visible all of the time, people are frequently reminded of it. There is also
    less time wasted having to search for important information if it is constantly
    on display. It is also more likely that the information is accurate because people
    are continuously being reminded of it, and if it is inaccurate, questions get
    asked!
  prefs: []
  type: TYPE_NORMAL
- en: When people who are not part of the core team come into the team space, information
    that is radiated on the walls can instantly be read. Stakeholders and people interested
    in the team's work can immediately gain a better understanding and awareness of
    the work that is going on. This activity is often referred to as *walking the
    walls*. Interested parties can inspect artifacts on the walls and have a conversation
    with team members about them. This is a hugely different experience from when
    information is hidden away in a system behind a login.
  prefs: []
  type: TYPE_NORMAL
- en: Proactive Notification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So, what happens when things start to go wrong? What happens when the system
    itself cannot resolve an issue by automatically restarting a pod, or scaling up
    cluster nodes? Enter the realm of **Alerts**.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting can take many forms. It could be that horrible text at 03:00 when things
    really start going wrong, or a more subtle alert, such as a Slack message to say
    something has updated successfully. The key here is that the information is being
    pushed, and not pulled. OpenShift has alerting capabilities built into it and
    we'll explore this in more detail in *Chapter 16*, *Own It*.
  prefs: []
  type: TYPE_NORMAL
- en: The classic forms of alerting are when there are spikes in memory usage in applications.
    This could lead to an application failing or constantly having to restart. In
    these instances, the team might spot the spikes on their dashboards and go and
    investigate the issues. We can, of course, make this feedback loop even shorter
    by combining data from different sources and alert on that. For example, if our
    application memory spikes, if we could capture the logs from around that time
    and push both events to the team, it could help diagnose the problems quicker.
    The real power of smart notifications is being able to respond even more quickly
    to the event.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting the Development teams to things that have broken is extremely important.
    Notifications can come from all layers of the system; it doesn't just need to
    be that terrible call in the dead of night to say the website is down! Whenever
    a job runs to build our code or deploy a new version of an application, sending
    a quick alert to the team's instant messaging software is a good way to notify
    the concerned stakeholders. If the information is timely, then we can respond
    more effectively. This could mean pulling the Andon cord and halting the production
    line while we gather together to fix the issue.
  prefs: []
  type: TYPE_NORMAL
- en: Altering the Customers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The software we rely on daily sometimes experiences downtime. Notifying the
    developers and SREs of critical failures is vital to resolving issues. Notifying
    customers in a transparent way of any system failures and your path to resolution
    can set you apart from the crowd. Lots of companies have status pages for their
    software to give an idea of whether something is up or down; for example, the
    Quay container repository status[7](#footnote-072) page gives you insight into
    what parts of the service are still working. This provides a great information
    radiator! Some of the best software companies in the world will not only publish
    the status of their downtimes, but also the steps they are taking to resolve issues
    in near real time. Companies such as GitHub and GitLab will post messages to Twitter
    or open up a Google Doc to post updates and provide as close to real-time updates
    as they can to their user base. This level of transparency is great to see, especially
    when the issues they are facing can be discussed further by others using the same
    technology. They can become proactive, and not make the same mistakes on that
    next big critical upgrade!
  prefs: []
  type: TYPE_NORMAL
- en: '[7](#footnote-072-backlink) [https://status.quay.io/](https://status.quay.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: Alerting is only useful if it happens in a timely manner. It's no use alerting
    on a condition if no one can do anything about it because it's too late. It's
    often a bit of a balancing act to design alerting thresholds so that the humans
    who look after the product or systems can proactively take action, as opposed
    to pinging alerts too frequently by "crying wolf" when a limit is reached.
  prefs: []
  type: TYPE_NORMAL
- en: Having Fun with Notifications and the Build!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](img/Author_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As a self-confessed lazy developer, I know that when I push some code, it's
    unlikely that I will check the results of an automated build. I know this is arrogance,
    but hear me out. Usually, my thinking is, It works for me locally, what could
    possibly go wrong in the build?
  prefs: []
  type: TYPE_NORMAL
- en: The truth is that often something can, and does, go wrong, I've forgotten to
    link my code or I've forgotten to add a file before committing. Things go wrong.
    This is why wherever I work I love to have big dashboards from Jenkins, like the
    one shown in *Figure 13.14*, or monitors, so everyone can see the build status.
  prefs: []
  type: TYPE_NORMAL
- en: They are such an important element of the feedback loop. I look at a dashboard
    like this and I can immediately see critical information that makes my job easier,
    such as what step in the build is running, who triggered the build, or how many
    tests are failing! It's simple information, but it's next to impossible for me
    as a lazy developer to ignore something if it's flashing red in my face like this!
  prefs: []
  type: TYPE_NORMAL
- en: I love anything that can help shorten the feedback loop. I am particularly fond
    of a plugin that works well with a dashboard in Jenkins, called the Build Fail
    Analyzer. It parses the build log for a specific Regex and, if found, it can display
    a message. On the project, this screenshot is taken from the team who had gotten
    into the habit of trying to codify the failure issues, so when one was detected,
    it could prompt us to resolve it without looking through the log.
  prefs: []
  type: TYPE_NORMAL
- en: 'On this dashboard, on the middle left side, I can see that the dev-portal-fe-e2e-tests
    testing suites have failed and the problem identified was selenium not started.
    With this information, I don''t need to open Jenkins and read the log to see what
    happened – I can go straight to OpenShift to see why the pod did not start:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.14: The Build Fail Analyzer in Jenkins'
  prefs: []
  type: TYPE_NORMAL
- en: Failing the build is seen sometimes as the cursed state. We try to encourage
    a no-blame culture, but sometimes it can be a bit of fun to have a bit of a blame
    game. I am a fan of this one for sure! Whenever you fail the build, you're deemed
    to be the Cowboy Coder, the one who rides into town and shoots from the hip without
    a care for the consequences. Or maybe you just left a typo in your code!
  prefs: []
  type: TYPE_NORMAL
- en: 'Either way, if the dashboard turns red, then you have to dress like the cowboy
    you are. This team took it one step further: not only did you have to wear the
    pink cowboy hat until you had fixed the problem, but you also had to ride around
    on the wooden hobby horse! Even if you had to go to the bathroom or get a coffee,
    the horse and hat went with you! You''d be amazed at the funny looks you get on
    the way to the canteen wearing this attire:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.15: The Cowboy Coders'
  prefs: []
  type: TYPE_NORMAL
- en: More silly examples come from a project I was working on years back. This was
    in the days prior to using a container platform, and we had several manually configured
    VMs that were critical for us when releasing software. It was a massive project,
    with seven Scrum teams globally distributed. We were building a suite of 50 product
    microservices so our build farm was always busy!
  prefs: []
  type: TYPE_NORMAL
- en: On one occasion, we had to get the IT company that was managing our infrastructure
    to roll back to one of the previous backups, as someone had executed a large number
    of commands as root and broken several things for Jenkins. Raising a `sev1` ticket
    still took a few days to resolve! We couldn't let this kind of thing happen again
    as it was a massive drain on the team's morale and productivity. So, we decided
    to get inventive. We were using Slack as our messaging client and knew you could
    send messages to channels via a webhook. We also knew that if anyone logged into
    a machine, we could execute a bash script. Tying these items together, we created
    the Kenny Loggins channel in our Slack instance...because when you log into a
    server as root, you're in the DANGER ZONE!
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.16: The Kenny Loggins channel'
  prefs: []
  type: TYPE_NORMAL
- en: This section has shown many different ways we can visualize outcomes and use
    metrics to trigger proactive notifications to learn faster. Let's see how this
    can be summarized with everything else we've learned in our Delivery Loop.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Delivery Map
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We concluded *Section 3*, *Discover It*, with a Discovery Map, with a single
    information radiator that summarized the Discovery Loop iteration. We concluded
    *Section 4*, *Prioritize It*, with an Options Map, which summarized the ideas
    and hypotheses we wanted to validate, how we would deliver the options, and what
    options we planned to work on first.
  prefs: []
  type: TYPE_NORMAL
- en: We will conclude *Section 5*, *Deliver It*, with a Delivery Map. This is another
    open source artifact available in the Mobius Kit under Creative Commons that you
    can use to summarize all the learnings and decisions taken during your journey
    around the Delivery Loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'This map should slot neatly next to your Discovery Map and is used to summarize
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Actions**: What can we get done this week to improve the outcomes?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Doing**: What is our current work in progress?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Done**: What is ready to review?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Impact**: What progress did we make toward the outcomes?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learn**: What did we learn?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Insights**: What are our next steps?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we come out of the Delivery Loop and return to the Options Pivot in *Section
    7*, *Improve It, Sustain It*, we will complete the final section of this map by
    asking, *What are our next steps?*
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at PetBattle's Delivery Map at the end of their first iteration
    of the Delivery Loop.
  prefs: []
  type: TYPE_NORMAL
- en: PetBattle – the Delivery Map
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Figure 13.17* has a lot of detail that may not all be readable in print. To
    explore it in full, you can access the image at the book''s GitHub repository
    at [https://github.com/PacktPublishing/DevOps-Culture-and-Practice-with-OpenShift](https://github.com/PacktPublishing/DevOps-Culture-and-Practice-with-OpenShift):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.17: The PetBattle Delivery Map'
  prefs: []
  type: TYPE_NORMAL
- en: The Delivery Map provides a powerful summary of the journey we've been round
    the Delivery Loop. Like all other artifacts, it is a living and breathing summary
    and should be revisted regularly and updated after every subsequent iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have now completed our journey around the Mobius Loop. In this chapter, we
    have focused on the measurements and learning we can take away from the features
    we launch, the experiment we run, and the research we conduct. A relentless focus
    on measurement enables us to take more concrete decisions that are backed by metrics-based
    evidence.
  prefs: []
  type: TYPE_NORMAL
- en: The Showcase and Retrospective events that are often run by Scrum and other
    Agile teams provide ample opportunity to showcase metrics and highlight learnings.
    We take this opportunity to re-examine the experiments that we designed on the
    Options Pivot and investigate what actually happened. That often involves looking
    at the analytics provided by the advanced deployment capabilities offered by the
    platform – the results of A/B tests, canary launches, feature toggles, and dark
    launches.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also highlighted the importance of running usability tests with the full
    team involved, while being connected directly to end users to develop further
    empathy and see them testing the evolving application. Guerilla testing also provides
    a low-cost and simple way to gather learning from users:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.18: The Delivery Loop'
  prefs: []
  type: TYPE_NORMAL
- en: We explored the many different metrics made available by the platform, the software,
    and our teams. Service Delivery and Operational Performance metrics popularized
    by DORA and Accelerate, and made available by open source tools such as Pelorus,
    provide leading indicators of the success of DevOps culture and practice. These
    can be supported by further metrics captured about security, performance, culture,
    the application itself, and the infrastructure. The importance of radiating these,
    in real time, in a very open and transparent manner cannot be underestimated,
    nor can putting the behaviors and practices in place to be reactive and responsive
    to changes in metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we conclude *Section 5*, *Deliver It*, we can see just how many practices
    have allowed us to navigate the Mobius Loop, on top of our foundation of culture
    and technology:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_13_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.19: The practices mapped onto the Mobius Loop'
  prefs: []
  type: TYPE_NORMAL
- en: While we have completed one revolution around the Mobius Loop, we have not completed
    the journey. We will never complete the journey until the whole product is turned
    off and decommissioned. This is because the Mobius Loop is infinite and will never
    end. As we come out of the Delivery Loop, we return to the Options Pivot. We will
    do this in *Chapter 17*, *Improve It*, when we explore the insights from our trip
    around the Loop and ask what we have learned, followed by what we are to do next.
  prefs: []
  type: TYPE_NORMAL
- en: Before that, we are going to spend a few chapters diving a bit deeper into the
    technical solution. We have already started that in this chapter. In *Chapter
    14*, *Build It*, we will look at other aspects of how we build the solution. In
    *Chapter 15*, *Run It*, we'll focus on running the solution. In *Chapter 16*,
    *Own It*, we'll explore what it means to own the solution. These three chapters
    form *Section 6* of our book and are all about how product teams *Build It*, *Run
    It*, *Own It*.
  prefs: []
  type: TYPE_NORMAL
