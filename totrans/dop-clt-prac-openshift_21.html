<html><head></head><body>
		<div>&#13;
			<div id="_idContainer448" class="Content">&#13;
			</div>&#13;
		</div>&#13;
		<div id="_idContainer449" class="Content">&#13;
			<h1 id="_idParaDest-338">14. <a id="_idTextAnchor453"/>Build It</h1>&#13;
		</div>&#13;
		<div id="_idContainer499" class="Content">&#13;
			<p>"<em class="italics">It works on my machine</em>"—a phrase heard time and time again by developers, testers, and operators as they write, test, and verify their code. <em class="italics">It works on my machine</em> is a phrase rooted in siloed teams where ownership of the problem moves around like a tennis ball on a court at Wimbledon. The metaphorical wall that exists between teams that have operated in silos, passing work over the wall and not taking full responsibility for the end-to-end journey is a problem that has been around for decades. We need to break away from this behavior! From now on, it's not, "<em class="italics">It's working on my machine</em>" but rather, "<em class="italics">How has your code progressed in the build system</em>?" The build and deployment pipeline our code runs through is a shared responsibility. In order for this to be the case, all team members must contribute to the pipeline and be ready to fix it when it breaks.</p>&#13;
			<div>&#13;
				<div id="_idContainer450" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_01.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.1: It works on my machine</p>&#13;
			<p>If your code has failed the build because you forgot to check in a dependency, or if your tests are not passing, then it is your responsibility to fix it! The purpose of the deployment pipeline is to create a<a id="_idIndexMarker2082"/><a id="_idIndexMarker2083"/> repeatable process that our code will pass through and accelerate releases while also de-risking them. If we know that on every commit to a repository all of our tests are executed, we're going to have a large amount of confidence that the resulting application will work well. If we're continuously increasing the test volume as the application's complexity increases, that too should grow our confidence. It's critical for teams to want to own their software pipelines.</p>&#13;
			<p>Having a platform such as OpenShift is a bit like the Beatles singing on the rooftop of the Apple Corps building on Saville Row about people <em class="italics">coming together</em>. Developers, operations, testers, architects, designers, database administrators, and analysts, everyone coming together and using a platform like OpenShift provides a shared space to collaborate upon. Building applications and business services on the platform where developers can self-service all of their requirements in a safe, access-controlled manner, bringing down the walls between teams, and removing bottlenecks such as having to wait for permissions to deploy an application—this gets everyone speaking the same language to deliver business outcomes through modern application delivery and technological solutions.</p>&#13;
			<h2 id="_idParaDest-339"><a id="_idTextAnchor454"/>C<a id="_idTextAnchor455"/><a id="_idTextAnchor456"/>luster Resources</h2>&#13;
			<p>This <a id="_idIndexMarker2084"/><a id="_idIndexMarker2085"/>section of the book will be one of the most technical. As described in the <em class="italics">Appendix</em>, the minimum<a id="_idIndexMarker2086"/><a id="_idIndexMarker2087"/> requirements for running the code examples using <strong class="bold">CodeReady Containers</strong> (<strong class="bold">CRCs</strong>) in this chapter are as follows:</p>&#13;
			&#13;
			<div>&#13;
				<div id="_idContainer1005" class="IMG---Figure">&#13;
					<img src="../Images/B16297_Table_14.1.png" alt="" style="height:100px; width:600px;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Table 14.1: Minimum requirements for running code examples using CRCs<a id="_idTextAnchor457"/></p>&#13;
			<p>With the amount of memory <a id="_idIndexMarker2088"/><a id="_idIndexMarker2089"/>required to follow, and the technical content out of the way, let's dive into things in more detail. We'll start by looking over the components of the existing PetBattle applications as they move from a hobby weekend project into a highly available, production-based setup that is built and maintained by a strong, cross-functional team.</p>&#13;
			<h2 id="_idParaDest-340"><a id="_idTextAnchor458"/>Existing PetBattle Architecture</h2>&#13;
			<p>The initial PetBattle architecture was pretty basic and revolved around deploying application components <a id="_idIndexMarker2090"/><a id="_idIndexMarker2091"/>running in a single <strong class="bold">virtual machine</strong> (<strong class="bold">VM</strong>). The initial architecture had three major components: a JavaScript frontend; a Java-based backend, providing an API and a database; and a single instance of MongoDB. Nothing here was too exciting or complex but hidden inside was a minefield of technical debt and poor implementation that caused all sorts of problems when the site usage took off.</p>&#13;
			<p>The issues with this architecture<a id="_idIndexMarker2092"/><a id="_idIndexMarker2093"/> seemed to include:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>A monolith—Everything had to be deployed and scaled as one unit, there were no independent moving parts.</li>&#13;
				<li>Authentication and access control were non-existent.</li>&#13;
				<li>Tests? Unit tests? But seriously, there weren't many.</li>&#13;
				<li>It required a lot of data maintenance as everything was stored in the database.</li>&#13;
				<li>Bad actors adding inappropriate images to our family-friendly cat application.</li>&#13;
				<li>Fragile application—If something went wrong, the application would crash and everything had to be restarted.</li>&#13;
			</ul>&#13;
			<p>Back in <em class="italics">Chapter 9</em>, <em class="italics">Discovering the How</em>, we went through an Event Storming exercise that helped drive a newly proposed architecture. It consisted of a UI component and a backing service that provided different REST-based APIs to the UI, as shown in <em class="italics">Figure 14.2</em>:</p>&#13;
			<div>&#13;
				<div id="_idContainer451" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_02.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.2: PetBattle's initial hobbyist architecture</p>&#13;
			<p>Let's now take a look at the individual PetBattle components.</p>&#13;
			<h3 id="_idParaDest-341">P<a id="_idTextAnchor459"/>etBattle Components</h3>&#13;
			<p>In the coming chapters, we will explore <a id="_idIndexMarker2094"/><a id="_idIndexMarker2095"/>automation, testing, and the extension of PetBattle to<a id="_idIndexMarker2096"/><a id="_idIndexMarker2097"/> include aspects such as monitoring and alerting, Knative Serving, and Service Mesh. But first, let's imagine that the PetBattle team has completed a few sprints of development. They have been building assets from the Event Storms and now have the components and architecture as seen in <em class="italics">Figure 14.3</em>. Through the Event Storm, we also identified a need for authentication to manage users. The tool of choice for that aspect was Keycloak.</p>&#13;
			<div>&#13;
				<div id="_idContainer452" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_03.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure"><a id="_idTextAnchor460"/>Fi<a id="_idTextAnchor461"/>gure 14.3: PetBattle's evolving architecture</p>&#13;
			<p>PetBattle's <a id="_idIndexMarker2098"/><a id="_idIndexMarker2099"/>architecture is now increasing in complexity. It has a UI that connects to two services to provide it with <a id="_idIndexMarker2100"/><a id="_idIndexMarker2101"/>data. <strong class="bold">Single Sign-On</strong> (<strong class="bold">SSO</strong>) and user management are provided by Keycloak. Let's take a look at each component of the architecture in more detail.</p>&#13;
			<h3>Us<a id="_idTextAnchor462"/><a id="_idTextAnchor463"/>er Interface</h3>&#13;
			<p>The UI is written <a id="_idIndexMarker2102"/><a id="_idIndexMarker2103"/>in Angular<span id="footnote-093-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-093">1</a></span> v12, a complete JavaScript framework from Google for building web and mobile applications. The application is transpiled and the static site code is then served from a container <a id="_idIndexMarker2104"/><a id="_idIndexMarker2105"/>running Nginx (a webserver) instance provided by Red Hat. The application is set up to pull its configuration <a id="_idIndexMarker2106"/><a id="_idIndexMarker2107"/>on startup, which sets up endpoints for all of the dependent services, such as Keycloak and the APIs. This configuration is managed as a ConfigMap in OpenShift.</p>&#13;
			&#13;
			<h3>Pe<a id="_idTextAnchor464"/><a id="_idTextAnchor465"/>t Service</h3>&#13;
			<p>The Pet service is a<a id="_idIndexMarker2108"/><a id="_idIndexMarker2109"/> straightforward service that uses Java Quarkus<span id="footnote-092-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-092">2</a></span> as the framework, backed by a MongoDB database to retrieve and store details of the pets uploaded to partake in a tournament.</p>&#13;
			&#13;
			<h3>To<a id="_idTextAnchor466"/><a id="_idTextAnchor467"/>urnament Service</h3>&#13;
			<p>The Tournament <a id="_idIndexMarker2110"/><a id="_idIndexMarker2111"/>service also uses the Quarkus framework and stores the state in both MongoDB and an Infinispan distributed <a id="_idIndexMarker2112"/><a id="_idIndexMarker2113"/>cache. MongoDB is used to store the details of the tournament such as which pet won the tournament—but why did we use a cache? </p>&#13;
			<p>Well, the answer is that a tournament only exists for a finite period of time and using a database to store temporal data is not a great fit for our use case. Also, Infinispan stores the <a id="_idIndexMarker2114"/><a id="_idIndexMarker2115"/>cache data in memory, which is much faster to access than data on disk. The drawback of this is that if the Infinispan pod dies/crashes, then the data is lost. However, we plan to circumvent this in production by having at least two replicas, with the data being replicated between the pods.</p>&#13;
			<h3>Us<a id="_idTextAnchor468"/><a id="_idTextAnchor469"/>er Management</h3>&#13;
			<p>User management, authentication, and access <a id="_idIndexMarker2116"/><a id="_idIndexMarker2117"/>control are a few other critical parts of the architecture that need to be addressed. We're using Keycloak,<span id="footnote-091-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-091">3</a></span> an open source identity and access management tool, to provide this functionality. We could have written some code ourselves for this functionality, but security is an area that requires a lot of expertise to get it right, and Keycloak does a great job of using open standards to do this job correctly.</p>&#13;
			<div id="footnote-093" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-093-backlink">1</a>	<a href="https://angular.io/">https://angular.io/</a></p>&#13;
			</div>&#13;
			<div id="footnote-092" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-092-backlink">2</a>	<a href="https://quarkus.io/">https://quarkus.io/</a></p>&#13;
			</div>&#13;
			<div id="footnote-091" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-091-backlink">3</a>	<a href="https://www.keycloak.org/">https://www.keycloak.org/</a></p>&#13;
			</div>&#13;
			<h3 id="_idParaDest-342">Pl<a id="_idTextAnchor470"/><a id="_idTextAnchor471"/>an of Attack</h3>&#13;
			<p>Initially, we are going to get the <a id="_idIndexMarker2118"/><a id="_idIndexMarker2119"/>core PetBattle application components and services up and running on OpenShift in a fairly manual way. We want to be able to develop locally, adding new functionality to show how easy it is to combine Helm and OpenShift to repeatedly deploy our code. Once that is completed, we are going to automate the setup and deployment process using various tools, including Tekton/Jenkins, Argo CD, and GitOps. We will <a id="_idIndexMarker2120"/><a id="_idIndexMarker2121"/>explore how to add new components to our <a id="_idIndexMarker2122"/><a id="_idIndexMarker2123"/>architecture using Knative and experiment with some of the more advanced deployment capabilities that we can utilize. Finally, in <em class="italics">Chapter 16</em>, <em class="italics">Own It</em>, we will look at application monitoring and alerting along with<a id="_idIndexMarker2124"/><a id="_idIndexMarker2125"/> Service Mesh for traceability. <em class="italics">Figure 14.4 </em>shows the additional components added to the architecture, including the Knative Cat Detector Service being proxied via the Service Mesh.</p>&#13;
			<div>&#13;
				<div id="_idContainer453" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_04.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.4: PetBattle's target architecture, including final OpenShift deployment</p>&#13;
			<p>We will be using the command line as <a id="_idIndexMarker2126"/><a id="_idIndexMarker2127"/>much as possible to show and explain the commands involved. Each step can also be performed via the OpenShift web console. If you're new to OpenShift, the web console is a great place to get started as it's full of tips and tutorials!</p>&#13;
			<h2 id="_idParaDest-343">Run<a id="_idTextAnchor472"/><a id="_idTextAnchor473"/>ning PetBattle</h2>&#13;
			<p>In <em class="italics">Chapter 6</em>, <em class="italics">Open Technical Practices</em> – <em class="italics">Beginnings</em>,<em class="italics"> Starting Right,</em> we talked about Helm and its use as an application lifecycle manager for installing, upgrading, and rolling back application<a id="_idIndexMarker2128"/><a id="_idIndexMarker2129"/> deployments. We are going to start with the command line, but you can skip to the end of this section if you would like to follow the web console method. If you need help installing the Helm command-line tool, take a look at <em class="italics">Chapter 6</em> as a refresher. Now let's see how we can easily deploy the PetBattle suite of applications as Helm charts into a single project on OpenShift. On your terminal, add the PetBattle Helm repositories:</p>&#13;
			<p class="snippet">$ helm repo add petbattle \</p>&#13;
			<p class="snippet">     https://petbattle.github.io/helm-charts</p>&#13;
			<p>There are three main applications that make up PetBattle and are searchable in the Helm repository:</p>&#13;
			&#13;
			<div>&#13;
				<div id="_idContainer1006" class="IMG---Figure">&#13;
					<img src="../Images/B16297_Table_14.2.png" alt="" style="height:150px; width:600px;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Table 14.2: The three main applications making up PetBattle</p>&#13;
			<p>The infrastructure Helm chart is normally deployed as a dependency of the Tournament Helm chart but can optionally be deployed alone. This can be useful for debugging purposes. The <strong class="bold">Not Safe For Families</strong> (<strong class="bold">NSFF</strong>) component is an optional chart, adding a feature whereby the API checks uploaded images for safe content for our family-friendly application.</p>&#13;
			&#13;
			<div>&#13;
				<div id="_idContainer1007" class="IMG---Figure">&#13;
					<img src="../Images/B16297_Table_14.3.png" alt="" style="height:150px; width:600px;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Table 14.3: The infrastructure and NSFF Helm charts</p>&#13;
			<p>We can search for the latest versions of these charts using the following command: </p>&#13;
			<p class="snippet">$ helm search repo pet-battle</p>&#13;
			<p>Let's now deploy <a id="_idIndexMarker2130"/><a id="_idIndexMarker2131"/>the main PetBattle application into our OpenShift cluster. We need to update a local copy of the PetBattle frontend's Helm <strong class="inline">values.yaml</strong> file to match our cluster URLs. This is needed to connect the frontend when deployed to the correct collection of backend services. We can provide these values to our Helm charts when deploying the suite of PetBattle applications. Let's download an example of the <strong class="inline">values.yaml</strong> file for us to edit:</p>&#13;
			<p class="snippet">$ wget https://raw.githubusercontent.com/petbattle/pet-battle/master/chart/values.yaml/tmp/values.yaml</p>&#13;
			<p>Open the <strong class="inline">values.yaml</strong> file and replace the five URLs listed in the <strong class="inline">config_map</strong> to match your OpenShift cluster (change the <strong class="inline">apps.cluster.com</strong> domain to <strong class="inline">apps-crc.testing</strong>, for example, if you are using a CRC). For example:</p>&#13;
			<p class="snippet"># custom end point injected by config map</p>&#13;
			<p class="snippet">config_map: '{</p>&#13;
			<p class="snippet">  "catsUrl": "https://pet-battle-api-petbattle.apps.cluster.com",</p>&#13;
			<p class="snippet">  "tournamentsUrl": "https://pet-battle-tournament-petbattle.apps.cluster.com",</p>&#13;
			<p class="snippet">  "matomoUrl": "https://matomo-labs-ci-cd.apps.cluster.com/",</p>&#13;
			<p class="snippet">  "keycloak": {</p>&#13;
			<p class="snippet">    "url": "https://keycloak-petbattle.apps.cluster.com/auth/",</p>&#13;
			<p class="snippet">    "realm": "pbrealm",</p>&#13;
			<p class="snippet">    "clientId": "pbclient",</p>&#13;
			<p class="snippet">    "redirectUri": "https://pet-battle-petbattle.apps.cluster.com/*",</p>&#13;
			<p class="snippet">    "enableLogging": true</p>&#13;
			<p class="snippet">   }</p>&#13;
			<p class="snippet">}'</p>&#13;
			<p>Gather the <strong class="bold">latest chart version</strong> for each of the PetBattle applications from the preceding Helm search command and install the three applications <strong class="inline">pet-battle</strong>, <strong class="inline">pet-battle-api</strong>, and<strong class="inline"> pet-battle-tournament</strong> into <a id="_idIndexMarker2132"/><a id="_idIndexMarker2133"/>your cluster. To do this, you will need to be logged in to your OpenShift cluster. For example:</p>&#13;
			<p class="snippet"># Login to OpenShift</p>&#13;
			<p class="snippet">$ oc login -u &lt;username&gt; --server=&lt;server api url&gt;</p>&#13;
			<p class="snippet">$ helm upgrade --install pet-battle-api \</p>&#13;
			<p class="snippet">petbattle/pet-battle-api --version=1.0.15 \</p>&#13;
			<p class="snippet">--namespace petbattle --create-namespace</p>&#13;
			<p class="snippet">$ helm upgrade --install pet-battle \</p>&#13;
			<p class="snippet">petbattle/pet-battle --version=1.0.6 \</p>&#13;
			<p class="snippet">-f /tmp/values.yaml --namespace petbattle</p>&#13;
			<p class="snippet">$ helm upgrade --install pet-battle-tournament \</p>&#13;
			<p class="snippet">petbattle/pet-battle-tournament --version=1.0.39 \</p>&#13;
			<p class="snippet">--set pet-battle-infra.install_cert_util=true \</p>&#13;
			<p class="snippet">--timeout=10m \</p>&#13;
			<p class="snippet">--namespace petbattle</p>&#13;
			<p>If the <strong class="inline">pet-battle-tournament</strong> install times out, just run it again.</p>&#13;
			<p>Each Helm install chart command should return a message similar to the following:</p>&#13;
			<p class="snippet">    NAME: pet-battle-api</p>&#13;
			<p class="snippet">    LAST DEPLOYED: Thu Feb 25 19:37:38 2021</p>&#13;
			<p class="snippet">    NAMESPACE: petbattle</p>&#13;
			<p class="snippet">    STATUS: deployed</p>&#13;
			<p class="snippet">    REVISION: 1</p>&#13;
			<p class="snippet">    TEST SUITE: None</p>&#13;
			<p>Using <strong class="inline">helm list</strong> should give you a list of the installed charts. You should see the following pods running in your <strong class="inline">petbattle</strong> project. An example is shown in <em class="italics">Figure 14.5</em>:</p>&#13;
			<div>&#13;
				<div id="_idContainer454" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_05.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.5: PetBattle pods</p>&#13;
			<p>The Tournament service <a id="_idIndexMarker2134"/><a id="_idIndexMarker2135"/>will take several minutes to deploy and stabilize. This is because its dependent infrastructure chart is deploying<a id="_idIndexMarker2136"/><a id="_idIndexMarker2137"/> operator subscriptions for Keycloak, Infinispan, and Grafana. Navigate to the OpenShift web console and you should now be able to explore the PetBattle application suite as shown in <em class="italics">Figure 14.6</em>. Browse to the PetBattle frontend to play with the applications.</p>&#13;
			<div>&#13;
				<div id="_idContainer455" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_06.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.6: PetBattle Helm charts deployed in the OpenShift Developer view</p>&#13;
			<p>You have now been shown how to install PetBattle Helm charts using the command line—some may say the hard way! We are now going to demonstrate some of the integrated features of Helm in OpenShift—some may say the easier way! We can create a <strong class="inline">HelmChartRepository</strong> Custom <a id="_idIndexMarker2138"/><a id="_idIndexMarker2139"/>Resource object that points to our PetBattle Helm chart repository; think of it as <strong class="inline">helm repo add</strong> for OpenShift. Run this command to install the chart repository:</p>&#13;
			<p class="snippet">cat &lt;&lt;EOF | oc apply -f -</p>&#13;
			<p class="snippet">apiVersion: helm.openshift.io/v1beta1</p>&#13;
			<p class="snippet">kind: HelmChartRepository</p>&#13;
			<p class="snippet">metadata:</p>&#13;
			<p class="snippet">  name: petbattle-charts</p>&#13;
			<p class="snippet">spec:</p>&#13;
			<p class="snippet">  name: petbattle</p>&#13;
			<p class="snippet">  connectionConfig:</p>&#13;
			<p class="snippet">    url: https://petbattle.github.io/helm-charts</p>&#13;
			<p class="snippet">EOF</p>&#13;
			<p>With this in place, we can browse to the Developer view in OpenShift and select Add Helm Charts, and see a menu and a form-driven approach to installing our Helm charts—just select a chart and install it:</p>&#13;
			<div>&#13;
				<div id="_idContainer456" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_07.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.7: Adding PetBattle via the HelmChartRepository view in OpenShift</p>&#13;
			<p>This can provide a great developer experience for teams sharing services with each other. A backend team can produce a new app to the repository and a downstream team can deploy it to their <a id="_idIndexMarker2140"/><a id="_idIndexMarker2141"/>developer environment in a single click. In fact, if you add a Helm values schema file, OpenShift will build a <strong class="bold">What You See Is What You Get</strong> (<strong class="bold">WYSIWYG</strong>) form for easy <a id="_idIndexMarker2142"/><a id="_idIndexMarker2143"/>configuration of the values file.</p>&#13;
			<h2 id="_idParaDest-344">Arg<a id="_idTextAnchor474"/><a id="_idTextAnchor475"/>o CD</h2>&#13;
			<p>When we established our foundations in <em class="italics">Section 2</em>, <em class="italics">Establishing the Foundation</em>, we bootstrapped all of our builds, deployment, and tooling using Helm and Argo CD. We made some opinionated choices when running that bootstrap automation and it's worth discussing some of the<a id="_idIndexMarker2144"/><a id="_idIndexMarker2145"/> trade-offs we made in a bit more detail. We followed our call to action when establishing our technical foundation and planned out what worked for us as the PetBattle <a id="_idIndexMarker2146"/><a id="_idIndexMarker2147"/>product team and reviewed and discussed what was working and not working so well.</p>&#13;
			<p>It turned out that for our development team, bootstrapping all of the CI/CD tools became an extremely important task. We had been given an arbitrary (but necessary) constraint that our development OpenShift cluster needed to be rebuilt from scratch every two weeks. So we needed to be confident that our CI and CD could stand up quickly and repeatedly. By following our everything-as-code practice, all of our OpenShift infrastructure definitions, CI/CD tooling, and pipeline definitions are stored in Git. The declared CI/CD tooling state is continuously synced to our development cluster by Argo CD, so updating the SonarQube Helm chart version, for example, is as easy as changing one line and pushing it to Git. The change is synchronized and rolled out to our cluster a minute later.</p>&#13;
			<p>Being able to effectively lifecycle-manage all of the supporting tools involved with building your applications takes effort and attention to detail, but it is worth it in the long run, as you will have built a system that can handle change easily and repeatedly. We have optimized our application lifecycle around the cost of change, making the cost (in man hours) as small as possible. Human time is our biggest resource cost after all!</p>&#13;
			<p>The versions of all our tooling are checked into Git using <strong class="bold">Semantic Versioning</strong><span id="footnote-090-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-090">4</a></span> (<strong class="bold">SemVer</strong>). There is a good <a id="_idIndexMarker2148"/><a id="_idIndexMarker2149"/>lesson to be learned here in terms of version control; nearly all modern open-source software uses this pattern for it. Often you can be surprised with the resulting deployment when a chart or operator references the <em class="italics">latest</em> images from external sources. The <em class="italics">latest</em> tag is a moving target and is updated often. Referencing tags for your versions in Git is like walking a tightrope, there is a balancing act to be performed between wanting to easily accept your toolchain and wanting to update it for <a id="_idIndexMarker2150"/><a id="_idIndexMarker2151"/>bugs and security fixes—hence the use of tags and knowing with confidence that a specific version is working. Normally in SemVer <strong class="inline">MAJOR.MINOR</strong> versions are tags that move with small bug fixes and security patches. <strong class="inline">MAJOR.MINOR.PATCH</strong> versions are not tags and yet they specify a fixed version (ideally!). Choose a strategy that does not incur too much technical debt that strands the team on old and unsupported versions forever. This is balanced with not having to constantly update version numbers all the time. Of course, if you have optimized for a small cost of change through automation, this problem of changing versions becomes much less of an issue!</p>&#13;
			<div id="footnote-090" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-090-backlink">4</a>	<a href="https://semver.org/">https://semver.org/</a></p>&#13;
			</div>&#13;
			<p>We have chosen a <em class="italics">push</em> (CI) and <em class="italics">pull</em> (CD) model for our software delivery lifecycle. The job of building images and artifacts (Helm charts and configuration), as well as unit and integration testing, is part of a <em class="italics">push</em> CI model. On every code commit, a build pipeline trigger (Tekton or Jenkins) fires. It is the job of the Argo CD controller to keep what we have deployed in our OpenShift cluster in sync with the declared application state in our Git repositories. This is a GitOps pull model for CI. The key thing here is that Git is the single source of truth and everything can be recreated from this source.</p>&#13;
			<div>&#13;
				<div id="_idContainer457" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_08.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.8: A GitOps push and pull model for continuous delivery</p>&#13;
			<p>The main benefit we see with this approach is that it is developer-centric. Any change in the code base triggers a pipeline build and deployment. This gives the team fast feedback for any breakages, since automated tests are always run against the new code. The pull CD model decouples the synchronous nature of a build and testing pipeline. Built artifacts (container images and configuration) can be built once, then tagged and promoted through a lifecycle, all of which is controlled from Git. This is great for auditability and discovering who changed what and when. We can easily trace code committed and pushed with builds, tests, and deployments. It is also a flexible approach in that not all artifacts need to be built per se. Configuration can be changed and deployed using the <a id="_idIndexMarker2152"/><a id="_idIndexMarker2153"/>same model. The model is also very flexible in its ability to support different development workflow <a id="_idIndexMarker2154"/><a id="_idIndexMarker2155"/>models. For example, Gitflow and Trunk-based development can easily be catered for, depending on how the team chooses to work.</p>&#13;
			<h2 id="_idParaDest-345"><a id="_idTextAnchor476"/>Trunk-Based Development and Environments</h2>&#13;
			<p>When <a id="_idIndexMarker2156"/><a id="_idIndexMarker2157"/>designing our <a id="_idIndexMarker2158"/><a id="_idIndexMarker2159"/>initial pipelines, we mapped out the <a id="_idIndexMarker2160"/><a id="_idIndexMarker2161"/>basic build, bake, deploy, integration testing, tag, and promotion stages. In <em class="italics">Figure 14.9</em>, we can see the MultiBranchPipeline Plugin, branches, and namespaces.</p>&#13;
			<div>&#13;
				<div id="_idContainer458" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_09.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.9: Branches and namespaces</p>&#13;
			<p>This helped us to clarify where the responsibilities lie between our Git branches, our continuous integration tasks, continuous delivery tasks, and which OpenShift projects these would occur in.</p>&#13;
			<div>&#13;
				<div id="_idContainer459" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_10.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.10: Branches and activities modeling</p>&#13;
			<p>Because we are following trunk-based development,<span id="footnote-089-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-089">5</a></span> our main/master branch is built, tagged, and promoted through the full lifecycle, that is, images are built, unit and functionally tested, and deployed into <strong class="inline">labs-test</strong> with end-to-end testing prior to deployment within the <strong class="inline">labs-staging</strong> project. For any short-lived feature branches or pull requests, we decided to only unit test, build, and deploy these sources into our <strong class="inline">labs-dev</strong> OpenShift project. That <a id="_idIndexMarker2162"/><a id="_idIndexMarker2163"/>way we can customize which pipeline tasks happen on specific code branches. There is a trade-off between the time and resources used for every code commit in our pipelines, and this must be adjusted according to what goes into our pipelines to help improve the overall product quality.</p>&#13;
			<div id="footnote-089" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-089-backlink">5</a>	<a href="https://trunkbaseddevelopment.com">https://trunkbaseddevelopment.com</a></p>&#13;
			</div>&#13;
			<h2 id="_idParaDest-346">The An<a id="_idTextAnchor477"/><a id="_idTextAnchor478"/>atomy of the App-of-Apps Pattern</h2>&#13;
			<p>We choose to use Helm; remember, at its<a id="_idIndexMarker2164"/><a id="_idIndexMarker2165"/> most basic, Helm is just templating language for packaging our Kubernetes-based application resources. Each PetBattle application has its own Git repository and Helm chart, making <a id="_idIndexMarker2166"/><a id="_idIndexMarker2167"/>it easier to code independently of other apps. This inner <em class="italics">Helm chart per application</em> box is depicted in <em class="italics">Figure 14.11</em>. A developer can get the same experience and end result installing an application chart using a <strong class="inline">helm install</strong> as our fully automated pipeline. This is important from a useability perspective. Argo CD has great support for all sorts of packaging formats that suit Kubernetes deployments, Kustomize, Helm, as well as <a id="_idIndexMarker2168"/><a id="_idIndexMarker2169"/>just raw YAML files. Because Helm is a templating language, we can mutate the Helm chart templates and their generated Kubernetes objects with various values.</p>&#13;
			<div>&#13;
				<div id="_idContainer460" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_11.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.11: Application packaging, Helm, and Argo CD with the app-of-apps pattern</p>&#13;
			<p>One strict view of GitOps is that mutating a state is not as <em class="italics">pure</em> as just checking in the filled-in templates with the values themselves. Kustomize, for example, has no templating and follows this approach. We <a id="_idIndexMarker2170"/><a id="_idIndexMarker2171"/>use Kustomize for deploying our CI/CD automation with Argo CD because we think it fits that use case better. This means that we are less likely to have a large number of CI/CD environments for our PetBattle product—at the moment there is just one.</p>&#13;
			<p>The trade-off here is that while we use GitOps to synchronize the Helm chart itself, the supply of application values may come from multiple places, so you have to be careful to understand where <a id="_idIndexMarker2172"/><a id="_idIndexMarker2173"/>overriding values and precedence occurs, as follows:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li><strong class="bold">Lowest precedence</strong>: <strong class="inline">values.yaml</strong> provided with the chart (or its sub chart dependencies)—these are kept in sync by the Argo CD controller.</li>&#13;
				<li><strong class="bold">Higher precedence</strong>: Override values specified when Argo CD creates the Helm application. These have higher precedence than value files, such as <strong class="inline">helm template --set</strong> on the command line. These can be specified in a template or a trigger, depending on how the pipeline is run.</li>&#13;
			</ul>&#13;
			<p>We deploy each of our applications using an Argo CD application definition. We use one Argo CD application definition for every environment in which we wish to deploy the application. This is the red box depicted in <em class="italics">Figure 14.11</em>. We make use of Argo CD with the app-of-apps pattern<span id="footnote-088-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-088">6</a></span> to bundle these all up; some might call this an application suite! In PetBattle we generate the app-of-apps definitions using a Helm chart. This is the third, outer green box in <em class="italics">Figure 14.11</em>. The configuration for this outer box is kept in a separate Git repository to our application. </p>&#13;
			&#13;
			<p>The app-of-apps pattern is where we declaratively specify one Argo CD app that consists only of other apps. In our case, this is the <strong class="inline">pet-battle-suite</strong> application. We have chosen to put all of our applications that are built from the main/master under this <strong class="inline">pet-battle-suite</strong> umbrella. We have a PetBattle suite for <em class="italics">testing</em> and <em class="italics">stage</em> environments. <em class="italics">Figure 14.12</em> shows the app-of-apps for the stage environment:</p>&#13;
			<div>&#13;
				<div id="_idContainer461" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_12.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.12: Argo CD, a deployed application suite</p>&#13;
			<div id="footnote-088" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-088-backlink">6</a>	<a href="https://argoproj.github.io/argo-cd/operator-manual/cluster-bootstrapping/#app-of-apps-pattern">https://argoproj.github.io/argo-cd/operator-manual/cluster-bootstrapping/#app-of-apps-pattern</a></p>&#13;
			</div>&#13;
			<p>In Git, we model different branches using the following patterns:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li><strong class="bold">HEAD</strong>/<strong class="bold">main</strong>/<strong class="bold">master</strong>: An <a id="_idIndexMarker2174"/><a id="_idIndexMarker2175"/>app-of-apps pattern (full suite of apps deployed and synced to all our environments, <strong class="inline">labs-test</strong>, <strong class="inline">labs-staging</strong>)</li>&#13;
				<li><strong class="bold">Branches</strong>/<strong class="bold">PRs</strong>: A single Helm chart-deployed application in our <strong class="inline">labs-dev</strong> namespace only</li>&#13;
			</ul>&#13;
			<p>The Argo CD sync policy for our <a id="_idIndexMarker2176"/><a id="_idIndexMarker2177"/>applications is set to <em class="italics">automated</em> + <em class="italics">prune</em>, so that child apps are automatically created, synced, and deleted when the manifest is changed. You can change or disable this if you need to. We also configure<a id="_idIndexMarker2178"/><a id="_idIndexMarker2179"/> a webhook against the CI/CD Git repository so that any changes trigger Argo CD to sync all applications; this avoids having to wait for the three-minute sync cycle when CI/CD code changes.</p>&#13;
			<p>The Git revision can be set to a specific Git commit <strong class="bold">Secure Hash Algorithm</strong> (<strong class="bold">SHA</strong>) for each child application. A Git SHA is a unique 40-character code computed for every commit to the repository, and is therefore not movable, unlike a tag. This ensures that even if the child app's repository<a id="_idIndexMarker2180"/><a id="_idIndexMarker2181"/> changes, the app will only change when the parent app changes that revision. Alternatively, you can set it to HEAD/master/main or a branch name to keep in sync with that particular branch. It's a good idea to use Git commit SHAs to manage your application versions the closer you are to production environments. Pinning to an exact version for production ensures easier traceability when things go wrong. The structure here is flexible to suit your product team's needs.</p>&#13;
			<h2 id="_idParaDest-347">Build It<a id="_idTextAnchor479"/><a id="_idTextAnchor480"/> – CI/CD for PetBattle</h2>&#13;
			<p>Let's get our hands <a id="_idIndexMarker2182"/><a id="_idIndexMarker2183"/>dirty now by getting down into the weeds with some more techie stuff. How do we go from code to running in production in a repeatable and safe way? If you remember all the way back in <em class="italics">Section 2</em>, <em class="italics">Establishing the Foundation</em>, we learned about Derek the DevOps dinosaur and the obstacles we put him through to test his fearsomeness. We will now do the same thing with our PetBattle apps, beginning with the frontend.</p>&#13;
			<h3 id="_idParaDest-348"><a id="_idTextAnchor481"/>The Big Picture</h3>&#13;
			<p>Before we<a id="_idIndexMarker2184"/><a id="_idIndexMarker2185"/> touch a line of code, we always like to keep an eye on the Big Picture. This helps us frame what the tool is and why we are using it, and to scaffold out our pipeline from a very high level. Let's build the details of how we manage the PetBattle source code in this same way. As a subtle<a id="_idIndexMarker2186"/><a id="_idIndexMarker2187"/> reminder, the PetBattle frontend is <a id="_idIndexMarker2188"/><a id="_idIndexMarker2189"/>an Angular application. It was built using Node.js v12 and is deployed to a Red Hat Nginx image.</p>&#13;
			<p>Continuing our Big Picture from the foundation, let's add the steps we will consider implementing for the PetBattle frontend so we can get it ready for a high frequency of change being pushed through it.</p>&#13;
			<div>&#13;
				<div id="_idContainer462" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_13.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.13: The Big Picture including the tools the team thinks they will use</p>&#13;
			<p>As a quick reminder, our Big Picture from <em class="italics">Section 2</em>, <em class="italics">Establishing the Foundation</em>, identified all the tools we might use as depicted in <em class="italics">Figure 14.13</em>, which include:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>Jenkins: To automate the building and testing of our software</li>&#13;
				<li>Nexus: To host our binaries and Helm charts</li>&#13;
				<li>Argo CD: To manage our deployments</li>&#13;
				<li>SonarQube: To <a id="_idIndexMarker2190"/><a id="_idIndexMarker2191"/>assess our code quality</li>&#13;
				<li>Zalenium: For automated browser testing</li>&#13;
			</ul>&#13;
			<p>Now that the tools are in <a id="_idIndexMarker2192"/><a id="_idIndexMarker2193"/>place, let's think about the stages our code should move through for being deployed. A team should start small—what is the minimum amount of automation we need to get code compiling and deployed? It's very important for teams to start small with a basic end-to-end flow of their code; otherwise, things become messy quite quickly, leading to unnecessary complexity and potentially not delivering anything. It's also<a id="_idIndexMarker2194"/><a id="_idIndexMarker2195"/> important because the feedback loop we are creating needs to be fast. We don't want a brilliant, complex process that thinks of everything but takes hours to run! It's not the kind of feedback loop we're trying to create.</p>&#13;
			<p>We always use three<a id="_idIndexMarker2196"/><a id="_idIndexMarker2197"/> simple stages: Build &gt; Bake &gt; Deploy. A good pattern for engineers to take is to keep to an abstract definition of their pipeline so they can get greater reuse of the pattern across any of their apps, irrespective of the technology they use. Each stage should have a well-defined interface with input and output. Reusing a pipeline definition in this way can lower the context switch when moving between backend and frontend. With this in mind, we can define the stages of our build in the following manner.</p>&#13;
			<h3 id="_idParaDest-349"><a id="_idTextAnchor482"/>The Build</h3>&#13;
			<p><em class="italics">Input</em>: <em class="italics">The code base</em></p>&#13;
			<p><em class="italics">Output</em>: <em class="italics">A "compiled" and unit-tested software artifact</em></p>&#13;
			<div>&#13;
				<div id="_idContainer463" class="IMG---Figure">&#13;
					<img src="../Images/figure-14-14.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.14: The BUILD component from the Big Picture</p>&#13;
			<p>The build should always take our<a id="_idIndexMarker2198"/><a id="_idIndexMarker2199"/> source code, compile it, and run some unit tests before producing some kind of artifact that will be stored in Nexus. By defining the interface of the build process as lightly as this, it means we can substitute the implementation of what that looks like in each technology or application type. So, for example, when building the PetBattle frontend Angular app, we will use the <strong class="bold">Node Package Manager</strong> (<strong class="bold">npm</strong>) to<a id="_idIndexMarker2200"/><a id="_idIndexMarker2201"/> complete these steps within the Build stage, but a Java application would likely use Gradle or Maven to achieve the same effect. The hardest work will happen in this stage and it is usually what has the highest dependency on the framework or language that's being used. We will see this in later stages; the technology originally being used becomes less important, so higher reuse of code can occur.</p>&#13;
			<h3 id="_idParaDest-350"><a id="_idTextAnchor483"/>The Bake</h3>&#13;
			<p><em class="italics">Input</em>: <em class="italics">A "compiled" software artifact</em></p>&#13;
			<p><em class="italics">Output</em>: <em class="italics">A tagged </em><em class="italics"><a id="_idIndexMarker2202"/><a id="_idIndexMarker2203"/></em><em class="italics">container image</em></p>&#13;
			<div>&#13;
				<div id="_idContainer464" class="IMG---Figure">&#13;
					<img src="../Images/figure-14-15.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.15: The BAKE component from the Big Picture</p>&#13;
			<p>This is the act of taking our software artifact that <a id="_idIndexMarker2204"/><a id="_idIndexMarker2205"/>was created as an output in the previous step and packaging it into a<a id="_idIndexMarker2206"/><a id="_idIndexMarker2207"/> box, that is, a Linux Container Image. This image is then tagged and stored in a container registry, either one built into OpenShift or an external one. In OpenShift there are many different ways we can<a id="_idIndexMarker2208"/><a id="_idIndexMarker2209"/> achieve this, such as using source-2-image, binary<a id="_idIndexMarker2210"/><a id="_idIndexMarker2211"/> build, or providing <a id="_idIndexMarker2212"/><a id="_idIndexMarker2213"/>a Containerfile/Dockerfile.</p>&#13;
			<h3 id="_idParaDest-351"><a id="_idTextAnchor484"/>The Deploy</h3>&#13;
			<p><em class="italics">Input</em>: <em class="italics">A tagged image</em></p>&#13;
			<p><em class="italics">Output</em>: <em class="italics">A running app in a given environment</em></p>&#13;
			<div>&#13;
				<div id="_idContainer465" class="IMG---Figure">&#13;
					<img src="../Images/figure-14-16.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.16: The DEPLOY component from the Big Picture</p>&#13;
			<p>Take the image that has <a id="_idIndexMarker2214"/><a id="_idIndexMarker2215"/>just been pushed to the registry and deploy it along with any other services or configuration required for it to run. Our applications will be packaged as Helm charts, so the deployment will likely have to patch the image referenced in our app's chart. We want our pipeline to support multiple workflows. For feature development, we can just <strong class="inline">helm install</strong> into the development namespace. But for release candidates, we should be committing new release information to Git for it to trigger the rollout of changes. The implementation of this workflow is the responsibility of the steps, the lower level of what is being executed. The abstract view of a Deploy should result in a <em class="italics">verified</em> app deployed on our cluster (and ultimately promoted all the way to production).</p>&#13;
			<p>The team captures these stages for the applications they're building by adding some nice doodles to their Big Picture. Next, they begin thinking about promoting the application across the environment from test to production. When building applications in containers, we want to ensure the app can run in any environment, so controlling application configuration <a id="_idIndexMarker2216"/><a id="_idIndexMarker2217"/>separately is vital. The team will not want to rebuild the application to target different environments either, so once an image is baked and deployed it needs to be verified before promotion. Let's explore these stages further.</p>&#13;
			<h3 id="_idParaDest-352"><a id="_idTextAnchor485"/>System Test</h3>&#13;
			<p><em class="italics">Input</em>: <em class="italics">The app name and version under test</em></p>&#13;
			<p><em class="italics">Output</em>: <em class="italics">A successful test </em><em class="italics"><a id="_idIndexMarker2218"/><a id="_idIndexMarker2219"/></em><em class="italics">report and verified app</em></p>&#13;
			<div>&#13;
				<div id="_idContainer466" class="IMG---Figure">&#13;
					<img src="../Images/figure-14-17.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.17: The SYSTEM TEST component from the Big Picture</p>&#13;
			<p>Drive the user behavior within<a id="_idIndexMarker2220"/><a id="_idIndexMarker2221"/> the application via the frontend by verifying whether the app is behaving as expected. If all the connected parts that make up the application are behaving as expected (the microservices, authentication, and frontend) then the app can be signed off and will not need to be rebuilt. Our system test cases for PetBattle will be the acceptance criteria the team has agreed upon. Because of this, we can sign off the application as ready for real-world users. Any component that has changed in the stack should trigger this stage; it is not just the responsibility of the frontend.</p>&#13;
			<h3 id="_idParaDest-353"><a id="_idTextAnchor486"/>Promote</h3>&#13;
			<p><em class="italics">Input</em>: <em class="italics">A verified image name and version</em></p>&#13;
			<p><em class="italics">Output</em>: <em class="italics">Running app in production environment</em></p>&#13;
			<div>&#13;
				<div id="_idContainer467" class="IMG---Figure">&#13;
					<img src="../Images/figure-14-18.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.18: The PROMOTE component from the Big Picture</p>&#13;
			<p>With the application<a id="_idIndexMarker2222"/><a id="_idIndexMarker2223"/> working as expected (based on our <a id="_idIndexMarker2224"/><a id="_idIndexMarker2225"/>passing system test cases), we can now promote the images that make up our app to the new environment, along with their configuration. Of course, in the world of GitOps, this is not a manual rollout of a new deployment but committing the new version and any custom configuration to our configuration repositories, where they will be picked up by Argo CD and deployed.</p>&#13;
			<div>&#13;
				<div id="_idContainer468" class="IMG---Figure">&#13;
					<img src="../Images/figure-14-19.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.19:<a id="_idTextAnchor487"/> The Big Picture including all the stages of the pipeline in place</p>&#13;
			<p>In<em class="italics"> Figure 14.19<a id="_idTextAnchor488"/><a id="_idTextAnchor489"/></em>, we can see the<a id="_idIndexMarker2226"/><a id="_idIndexMarker2227"/> Big Picture with the stages of the pipeline drawn in. Now that the team knows the stages that their software will pass through on the way across the cluster, they can fill in the lower-level details, the steps. At this stage, the team is looking to see how they can build common pipeline steps, irrespective of the technology they're using. This will provide greater reuse across their software stack but, more importantly, reduce the cognitive load for engineers writing software in multiple technologies. For this, it's a good idea to put on the Big Picture the technology being used. In PetBattle's case, it is Angular and <a id="_idIndexMarker2228"/><a id="_idIndexMarker2229"/>Quarkus (Node.js and<a id="_idIndexMarker2230"/><a id="_idIndexMarker2231"/> Maven for the build tools). They use a new color sticky to write the steps that each service will go through in order to fulfill the interface defined at each stage.</p>&#13;
			<p>In<em class="italics"> Figure 14.20<a id="_idTextAnchor490"/><a id="_idTextAnchor491"/></em>, we detail what these steps could look like for the Build stage of our pipeline. First, we install the application dependencies. Following this, we test, lint, and compile the code. Finally, we <a id="_idIndexMarker2232"/><a id="_idIndexMarker2233"/>store the <a id="_idIndexMarker2234"/><a id="_idIndexMarker2235"/>successful artifacts in the Nexus repository to use in the next stage, the Bake.</p>&#13;
			<div>&#13;
				<div id="_idContainer469" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_20.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.20<a id="_idTextAnchor492"/>: <a id="_idTextAnchor493"/>Th<a id="_idTextAnchor494"/><a id="_idTextAnchor495"/>e Build stage and the breakdown of its steps</p>&#13;
			<p>The team continues to flesh out the steps across all the stages. Finally, they add some example containers deployed to each namespace at each stage to give a view of all the components deployed for the PetBattle system to work. This is detailed in <em class="italics">Figure 14.21</em>:</p>&#13;
			<div>&#13;
				<div id="_idContainer470" class="IMG---Figure">&#13;
					<img src="../Images/figure-14-21.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.21: The complete Big Picture for our software delivery process</p>&#13;
			<p>The Big Picture <a id="_idTextAnchor496"/>is a <a id="_idIndexMarker2236"/><a id="_idIndexMarker2237"/>helpful practice for getting team alignment on what's in our toolchain and how we use it. It can be a great thing to play back to non-technical team members too, giving them an idea of the complexity and usefulness of being able to repeatedly build and test code. As with all our practices, it's also never done; when a new tool enters our toolchain or we add a new stage in our pipeline, we add it to the Big Picture first. It is the living and breathing documentation of our software delivery process. With the Big Picture complete for now, let's move on to implementing the components it describes.</p>&#13;
			<h3 id="_idParaDest-354"><a id="_idTextAnchor497"/>Choose Your Own Adventure</h3>&#13;
			<p>We understand that there are many ways to do anything and in software development, there are usually hundreds or even more. With this in mind, we hope to meet you, dear reader, where you are now. By this, we mean that the next section will identify two ways of doing the same thing, so take the approach that better fits your own context.</p>&#13;
			<p>Jenkins is the build tool of choice for lots of companies and developers alike. It has been around for some time and it has<a id="_idIndexMarker2238"/><a id="_idIndexMarker2239"/> its set of quirks for sure. It was never intended to be deployed as a container when it was first conceived. In order to keep things current and have an eye on the future, we've decided to write the code for the Big Picture using both Tekton and Jenkins. Both can easily be tweaked for both frontend and backend development, but for the purposes of this book we will use Jenkins to automate the parts of the Big Picture for our Angular application. For the API, written in Java (Quarkus), we will use Tekton, and go through setting up the same things in a more Kubernetes native way. Both paths are available for the avid reader to play with and get working, but we'll split the narrative this way for illustrative purposes.</p>&#13;
			<p>So, like you would in a <em class="italics">choose your own adventure</em> book, you can now pick the path that you would like to follow next. If you're not interested in Jenkins automation, then skip ahead to the Tekton section directly. The code for both options is available in the Git repositories for the book.</p>&#13;
			<p>Before attempting the pieces in this chapter, make sure to have completed the bootstrap steps in <em class="italics">Chapter 7,</em> <em class="italics">Open Technical Practices –</em> <em class="italics">The Midpoint</em>, under the <em class="italics">Implementing GitOps – Let's Build the Big Picture With Some Real Working Code!</em> section. These steps deploy the CI/CD tooling into your cluster using GitOps. The main tools we are going to use in the next sections include Jenkins, Argo CD, and Tekton.</p>&#13;
			<h2 id="_idParaDest-355">Jenkins–The Front<a id="_idTextAnchor498"/><a id="_idTextAnchor499"/>end</h2>&#13;
			<p>Jenkins is our trusty <a id="_idIndexMarker2240"/><a id="_idIndexMarker2241"/>friend who will do the hard crunching of code—compiling, testing, and so on—on our behalf. In order to get the best out of all the tools in our kit bag, there are a few items we need to <a id="_idIndexMarker2242"/><a id="_idIndexMarker2243"/>configure first. This includes, among other things, managing secrets and <a id="_idIndexMarker2244"/><a id="_idIndexMarker2245"/>adding webhooks to trigger our Jenkins automation as soon as a developer commits their code.</p>&#13;
			<h3 id="_idParaDest-356">Connect Argo CD t<a id="_idTextAnchor500"/><a id="_idTextAnchor501"/>o Git</h3>&#13;
			<p>Let's talk about GitOps. We <a id="_idIndexMarker2246"/><a id="_idIndexMarker2247"/>want our Git repositories to be the single source of truth and the Argo CD controller to analyze the differences between what is currently deployed to our cluster and what is stored in our Git repositories. Argo CD can do things based on the difference it sees between the desired state (in Git) and the actual state (in the cluster) such as automatically synchronizing them or sending a notification to say that these two states are not as expected. For example, in Git we may have set version 123 of our application but the cluster currently has version 122 deployed.</p>&#13;
			<p>To create this connectivity between our configuration repository and Argo CD, we need to create an Argo CD app-of-apps to point to the repository. The<a id="_idIndexMarker2248"/><a id="_idIndexMarker2249"/> app-of-apps pattern is a neat way to describe all elements of a system. Imagine we have an app, named <strong class="inline">App-1</strong>, which is our full system. This <strong class="inline">App-1</strong> is made up of independently deployable services such as <strong class="inline">App-1a</strong>, <strong class="inline">App-1b</strong>, <strong class="inline">App-1c</strong>, and so on. For PetBattle, we have the whole system that is all of our frontend, APIs, and other services. We also have one of these for our staging and test environments; this allows us to think of our app-of-apps as a suite of applications.</p>&#13;
			<p>If we clone the <strong class="inline">ubiquitous-journey</strong><span id="footnote-087-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-087">7</a></span> project <a id="_idIndexMarker2250"/><a id="_idIndexMarker2251"/>that we set up in <em class="italics">Chapter 7</em>, <em class="italics">Open Technical Practices</em> – <em class="italics">The Midpoint,</em> to bootstrap our cluster, there is another set of charts in here for our application stacks located in <em class="italics">applications</em>/<em class="italics">deployments</em>. When applied, these definitions will create our Argo CD application Custom Resource pointing to our Helm charts that will be created by the running builds on either Tekton or Jenkins.</p>&#13;
			&#13;
			<p>The values files (<strong class="inline">values-applications-stage.yaml</strong>) contain the Helm chart version and application version that will be updated by Jenkins on successful builds. We want Argo CD to monitor these values when applying changes to the cluster. These values files also contain our overrides to the base Helm chart for specific environments, for example, the config map that the frontend is configured with to communicate with the services it requires to work properly (<strong class="inline">tournament-svc</strong>, <strong class="inline">cats-svc</strong>, and so on). The following snippet shows the definition of this. These values will differ between development, testing, and staging, so this pattern gives us the ability to version control the configuration we want the application to use on startup.</p>&#13;
			<p class="snippet">pet_battle_stage:</p>&#13;
			<p class="snippet">    name: pet-battle-stage</p>&#13;
			<p class="snippet">    enabled: true</p>&#13;
			<p class="snippet">    source: *helm_repo</p>&#13;
			<p class="snippet">    chart_name: pet-battle</p>&#13;
			<p class="snippet">    sync_policy_automated: true</p>&#13;
			<p class="snippet">    destination: labs-staging</p>&#13;
			<p class="snippet">    source_ref: 1.0.6</p>&#13;
			<p class="snippet">    values:</p>&#13;
			<p class="snippet">      fullnameOverride: pet-battle</p>&#13;
			<p class="snippet">      image_repository: quay.io</p>&#13;
			<p class="snippet">      image_name: pet-battle</p>&#13;
			<p class="snippet">      image_namespace: petbattle</p>&#13;
			<p class="snippet">      config_map: '{ "catsUrl": "https://pet-battle-api-labs-staging.apps.hivec.sandbox1405.opentlc.com", "tournamentsUrl": "https://pet-battle-tournament-labs-staging.apps.hivec.sandbox1405.opentlc.com", "matomoUrl": "https://matomo-labs-ci-cd.apps.hivec.sandbox1405.opentlc.com/", "keycloak": { "url": "https://keycloak-labs-staging.apps.hivec.sandbox1405.opentlc.com/auth/", "realm": "pbrealm", "clientId": "pbclient", "redirectUri": "https://pet-battle-labs-staging.apps.hivec.sandbox1405.opentlc.com/*", "enableLogging": true } }'</p>&#13;
			<p class="snippet">      image_version: "master"</p>&#13;
			<p class="snippet">    project:</p>&#13;
			<p class="snippet">      name: pet-battle-stage</p>&#13;
			<p class="snippet">      enabled: true</p>&#13;
			<div id="footnote-087" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-087-backlink">7</a>	<a href="https://github.com/petbattle/ubiquitous-journey">https://github.com/petbattle/ubiquitous-journey</a></p>&#13;
			</div>&#13;
			&#13;
			<p>So, when we deploy an Argo CD <a id="_idIndexMarker2252"/><a id="_idIndexMarker2253"/>application pointing to this Git repository, it will find additional apps and <a id="_idIndexMarker2254"/><a id="_idIndexMarker2255"/>so create our app-of-apps pattern. The structure of the repository is trimmed, but you can see that the chart is very basic, having just two templates for creating a project in Argo CD and the application definitions to put inside the project.</p>&#13;
			<p class="snippet">ubiquitous-journey/applications</p>&#13;
			<p class="snippet">├── README.md</p>&#13;
			<p class="snippet">├── alerting</p>&#13;
			<p class="snippet">│   └── ....</p>&#13;
			<p class="snippet">├── build</p>&#13;
			<p class="snippet">│   ├── ...</p>&#13;
			<p class="snippet">└── deployment</p>&#13;
			<p class="snippet">    ├── Chart.yaml</p>&#13;
			<p class="snippet">    ├── argo-app-of-apps-stage.yaml</p>&#13;
			<p class="snippet">    ├── argo-app-of-apps-test.yaml</p>&#13;
			<p class="snippet">    ├── templates</p>&#13;
			<p class="snippet">    │   ├── _helpers.tpl</p>&#13;
			<p class="snippet">    │   ├── argoapplicationdeploy.yaml</p>&#13;
			<p class="snippet">    │   └── argocd-project.yaml</p>&#13;
			<p class="snippet">    ├── values-applications-stage.yaml</p>&#13;
			<p class="snippet">    └── values-applications-test.yaml </p>&#13;
			<p>We could go to the Argo CD UI and connect it to this repository manually, or use the Argo CD CLI to create the Argo CD application Custom Resource, but let's just run this handy one-liner to connect things up for both our staging and test app-of-apps:</p>&#13;
			<p class="snippet"># from the root of ubiquitous-journey</p>&#13;
			<p class="snippet">$ cd applications/deployment</p>&#13;
			<p class="snippet"># install an app-of-apps for each test and staging</p>&#13;
			<p class="snippet">$ helm upgrade --install pet-battle-suite-stage -f \</p>&#13;
			<p class="snippet">argo-app-of-apps-stage.yaml \--namespace labs-ci-cd .</p>&#13;
			<p class="snippet">$ helm upgrade --install pet-battle-suite-test -f \</p>&#13;
			<p class="snippet">argo-app-of-apps-test.yaml \--namespace labs-ci-cd .</p>&#13;
			<p>With these in place, we should see Argo CD <a id="_idIndexMarker2256"/><a id="_idIndexMarker2257"/>create the app-of-apps definitions in the UI, but it will be unable to sync with the child applications. This is because we have not built them yet! Once they are available, Argo CD will kick in and sync them up for us.</p>&#13;
			<div>&#13;
				<div id="_idContainer471" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_22.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.22: Argo CD sync of the pet-battle application suite for the staging environment</p>&#13;
			<p>To extend this app-of-apps pattern now is very simple. We only need to connect Git to Argo CD this <a id="_idIndexMarker2258"/><a id="_idIndexMarker2259"/>one time. If, after the next few Sprints, the PetBattle team realizes they need to add a new component or service, they can simply extend the values YAML, that is, <strong class="inline">values-applications-stage.yaml</strong> or <strong class="inline">values-applications-test.yaml</strong> for their staging or test environment, with a reference to the new component chart location and version. For example, for <strong class="inline">cool-new-svc</strong>:</p>&#13;
			<p class="snippet">cool_new_svc_stage:</p>&#13;
			<p class="snippet">    name: cool-new-svc-stage</p>&#13;
			<p class="snippet">    enabled: true</p>&#13;
			<p class="snippet">    source: *helm_repo</p>&#13;
			<p class="snippet">    chart_name: cool-new-svc</p>&#13;
			<p class="snippet">    sync_policy_automated: true</p>&#13;
			<p class="snippet">    destination: labs-staging</p>&#13;
			<p class="snippet">    source_ref: 1.0.1 # version of the helm chart</p>&#13;
			<p class="snippet">    values:</p>&#13;
			<p class="snippet">      fullnameOverride: cool-new-svc</p>&#13;
			<p class="snippet">      image_repository: quay.io</p>&#13;
			<p class="snippet">      image_name: pet-battle</p>&#13;
			<p class="snippet">      image_namespace: petbattle</p>&#13;
			<p class="snippet">      image_version: "2.1.3" # version of the application image</p>&#13;
			<p class="snippet">    project:</p>&#13;
			<p class="snippet">      name: pet-battle-stage</p>&#13;
			<p class="snippet">      enabled: true</p>&#13;
			<h3 id="_idParaDest-357">Secrets in Our Pip<a id="_idTextAnchor502"/><a id="_idTextAnchor503"/>eline</h3>&#13;
			<p>Jenkins <a id="_idIndexMarker2260"/><a id="_idIndexMarker2261"/>is going to be responsible for compiling our code, pushing the image to a registry, and writing values to Git. This means Jenkins is going to need some secrets! In our case, we're using Quay.io for hosting our images so Jenkins will need the access to be able to push our packaged Container Images to this repository, which requires authentication. If you're following along with forks of the PetBattle repositories and want to create your own running <strong class="inline">pet-battle</strong> instance, go ahead<a id="_idIndexMarker2262"/><a id="_idIndexMarker2263"/> and sign up for a free account on <a href="https://quay.io/">https://quay.io/</a>. You can log in with GitHub, Google, or your Red Hat account.</p>&#13;
			<h3>Quay.io</h3>&#13;
			<p>When on Qu<a id="_idTextAnchor504"/><a id="_idTextAnchor505"/>ay, create three new<a id="_idIndexMarker2264"/><a id="_idIndexMarker2265"/> repositories, one for each of the application components that we will be building. You can mark them as public, as private repositories cost money.</p>&#13;
			<div>&#13;
				<div id="_idContainer472" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_23.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.23: PetBattle images in Quay.io</p>&#13;
			<p>These repositories serve as empty image stores for us to push our images to from within the pipeline. But we need to provide Jenkins with the correct access to be able to push them, so go ahead and hit the robot icon on the UI to create a new service account that Jenkins can use. Give it a sensible name and description for readability.</p>&#13;
			<div>&#13;
				<div id="_idContainer473" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_24.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.24: Robots in Quay.io</p>&#13;
			<p>We are going to mark all the repositories we created previously as <span class="P---Screen-Text">Write</span> by this robot. Hit <span class="P---Screen-Text">Add permissions</span>:</p>&#13;
			<div>&#13;
				<div id="_idContainer474" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_25.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.25: Robot RBAC in Quay.io</p>&#13;
			<p>Now that the repositories<a id="_idIndexMarker2266"/><a id="_idIndexMarker2267"/> and robot account have been created, we can download the secret to be <a id="_idIndexMarker2268"/><a id="_idIndexMarker2269"/>used in our pipelines! Hit the cog on the side of the secret name and select <span class="P---Screen-Text">View Credentials</span>.</p>&#13;
			<div>&#13;
				<div id="_idContainer475" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_26.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.26: How to view the robot credentials in Quay.io</p>&#13;
			<p>On the page that pops <a id="_idIndexMarker2270"/><a id="_idIndexMarker2271"/>up, download the Kubernetes YAML and store it in your fork of <strong class="inline">pet-battle</strong>.</p>&#13;
			<div>&#13;
				<div id="_idContainer476" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_27.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.27: Downloading the Kubernetes secret</p>&#13;
			<p>We can apply it to our cluster (ensure you are logged in first):</p>&#13;
			<p class="snippet">$ oc apply -n labs-ci-cd -f petbattle-jenkinspb-secret.yml</p>&#13;
			<h3>GitHub</h3>&#13;
			<p>A secret is also<a id="_idTextAnchor506"/><a id="_idTextAnchor507"/><a id="_idTextAnchor508"/> required for Jenkins to be able to push updates to our Helm values files stored in Git. The values<a id="_idIndexMarker2272"/><a id="_idIndexMarker2273"/> files for our applications will contain the properties we want to pass to our templates, such as ConfigMap variables, or locations of images, such as Quay.io. Our values files for the deployment of our applications will also hold a reference to the image version (that is, the SemVer of our app, such as 1.0.1) to be deployed by patching our DeploymentConfigs. We don't want to manually update this but have a robot (Jenkins) update this when there has been a successful build. Therefore, this secret will be needed to write these changes in versions to our configured repositories, which are being pointed out by Argo CD. We track version changes across all our environments in this way because, after all, if it's not in Git, it's not real.</p>&#13;
			<p>To create a secret for<a id="_idIndexMarker2274"/><a id="_idIndexMarker2275"/> GitHub, simply go to the Developer Settings view. While logged into GitHub, that's <strong class="bold">Settings &gt; Developer Settings &gt; Personal access tokens</strong> or <a href="https://github.com/settings/tokens">https://github.com/settings/tokens</a> for the lazy. Create a new <strong class="bold">Personal Access Token</strong> (<strong class="bold">PAT</strong>); this can <a id="_idIndexMarker2276"/><a id="_idIndexMarker2277"/>be used to authenticate and push code to the repository. Give it a sensible name and allow it to have repository access.</p>&#13;
			<div>&#13;
				<div id="_idContainer477" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_28.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.28: GitHub personal access token permissions</p>&#13;
			<p>Save the token's value, as you won't be able to access it again without generating a new one. With the token in place, we can create a secret in Jenkins by adding it to a <strong class="inline">basic-auth</strong> secret. In order for Jenkins, which is running in the same namespace as where this secret will be created, to be able to consume the value of the secret, we can apply a special annotation, <strong class="inline">credential.sync.jenkins.openshift.io: "true"</strong>. This little piece of magic will allow any credentials to be updated in Jenkins by only updating the secret! </p>&#13;
			<p>Update the secret with your values for <strong class="inline">GITHUB_TOKEN</strong> and <strong class="inline">GITHUB_USERNAME</strong> if you're following along in your own<a id="_idIndexMarker2278"/><a id="_idIndexMarker2279"/> fork and apply them to the cluster:</p>&#13;
			<p class="snippet">$ cat &lt;&lt;EOF | oc apply -f- </p>&#13;
			<p class="snippet">apiVersion: v1</p>&#13;
			<p class="snippet">stringData:</p>&#13;
			<p class="snippet">  password: GITHUB_TOKEN</p>&#13;
			<p class="snippet">  username: GITHUB_USERNAME</p>&#13;
			<p class="snippet">kind: Secret</p>&#13;
			<p class="snippet">metadata:</p>&#13;
			<p class="snippet">  labels:</p>&#13;
			<p class="snippet">    credential.sync.jenkins.openshift.io: "true"</p>&#13;
			<p class="snippet">  name: git-auth</p>&#13;
			<p class="snippet">  namespace: labs-ci-cd</p>&#13;
			<p class="snippet">type: kubernetes.io/basic-auth</p>&#13;
			<p class="snippet">EOF</p>&#13;
			<h3>SealedSecrets</h3>&#13;
			<p>You might <a id="_idTextAnchor509"/><a id="_idTextAnchor510"/><a id="_idTextAnchor511"/>be<a id="_idIndexMarker2280"/><a id="_idIndexMarker2281"/> thinking that these secrets should probably be stored somewhere safe—and you're right! If you want to explore the idea of storing the secrets in Git so they too <a id="_idIndexMarker2282"/><a id="_idIndexMarker2283"/>are <em class="italics">GitOpsy</em> (yes, I did just invent a word there), then we could use SealedSecrets by Bitnami. It provides a<a id="_idIndexMarker2284"/><a id="_idIndexMarker2285"/> controller for encrypting secrets, allowing us to store them as plain text. This means we <a id="_idIndexMarker2286"/><a id="_idIndexMarker2287"/>can commit them to Git! Through the magic of the SealedSecret Custom Resource, it decrypts the SealedSecret, and creates a regular Kubernetes secret on your behalf. We've written our Jenkins Helm chart to accept SealedSecrets for this very reason!</p>&#13;
			<p>You can deploy SealedSecrets <a id="_idIndexMarker2288"/><a id="_idIndexMarker2289"/>to the cluster by enabling it in the Ubiquitous Journey Git project. Open up <strong class="inline">bootstrap/values-bootstrap.yaml</strong>. It's as simple as changing the <strong class="inline">enabled</strong> flag to <strong class="inline">true</strong> and, of course, Git committing the changes. This will resync with Argo CD and create an instance of Bitnami SealedSecrets in your cluster, by default in the <strong class="inline">labs-ci-cd</strong> namespace. Because this is a new component we're adding to our tooling, we should of course also update our Big Picture with the tool and a sentence to describe how we use it.</p>&#13;
			<p class="snippet">sealed-secrets:</p>&#13;
			<p class="snippet">  # Disabled by default</p>&#13;
			<p class="snippet">  enabled: true</p>&#13;
			<p class="snippet">  nameOverride: sealed-secrets</p>&#13;
			<p>Once the controller has been created, we can seal our secrets by following these few steps:</p>&#13;
			<ol>&#13;
				<li>Install <strong class="inline">kubeseal</strong> using the instructions found on their GitHub releases page: <a href="https://github.com/bitnami-labs/sealed-secrets/releases">https://github.com/bitnami-labs/sealed-secrets/releases</a>.</li>&#13;
				<li>Log in to the cluster <a id="_idIndexMarker2290"/><a id="_idIndexMarker2291"/>where SealedSecrets is deployed and take note of the namespace (in our case this defaults to <strong class="inline">labs-ci-cd</strong>).</li>&#13;
				<li>Process your existing<a id="_idIndexMarker2292"/><a id="_idIndexMarker2293"/> secret using the <strong class="inline">kubeseal</strong> command-line utility. It is important to set the correct namespace otherwise the secret will not be unsealed. In this case, we're going to seal it as <strong class="inline">super-dooper-secret</strong>. It should look something like this:<p class="snippet"># create secret file from step 3</p><p class="snippet">$ cat &lt;&lt; EOF &gt; /tmp/super-dooper.yaml</p><p class="snippet">---</p><p class="snippet">apiVersion: v1</p><p class="snippet">kind: Secret</p><p class="snippet">metadata:</p><p class="snippet">  name: super-dooper</p><p class="snippet">  labels:</p><p class="snippet">    credential.sync.jenkins.openshift.io: "true"</p><p class="snippet">type: "kubernetes.io/basic-auth"</p><p class="snippet">stringData:</p><p class="snippet">  password: "<span class="P-_-Code-Highlight">myGitHubToken</span>"</p><p class="snippet">  username:  "<span class="P-_-Code-Highlight">donal</span>"</p><p class="snippet">EOF </p><p class="snippet"># encrypt the secret $ kubeseal &lt; /tmp/super-dooper.yaml &gt; /tmp/sealed-super-dooper.yaml \</p><p class="snippet">  -n labs-ci-cd \</p><p class="snippet">  --controller-namespace labs-ci-cd \</p><p class="snippet">  --controller-name sealed-secrets \</p><p class="snippet">  -o yaml</p></li>&#13;
				<li>You can now apply that secret straight to the cluster for validation, but you <em class="italics">should</em> add it to the cluster using Argo CD by committing it to Git. If it's not in Git, it's not real. Here, we can see what the SealedSecret looks like before it's applied to the cluster. As you <a id="_idIndexMarker2294"/><a id="_idIndexMarker2295"/>can see, it's a very large, encrypted string for each variable we sealed:<p class="snippet"># have a look at the sealed secret</p><p class="snippet">$ cat /tmp/sealed-super-dooper.yaml </p><p class="snippet">apiVersion: bitnami.com/v1alpha1</p><p class="snippet">kind: SealedSecret</p><p class="snippet">metadata:</p><p class="snippet">  creationTimestamp: null</p><p class="snippet">  name: super-dooper</p><p class="snippet">  namespace: labs-ci-cd</p><p class="snippet">spec:</p><p class="snippet">  encryptedData:</p><p class="snippet">    password: AgC6NyZa2to2MtKbXYxJBCOfxmnSQ4PJgV8KGdDRawWstj24FIEm5YCyH6n/BXq9DEPIJL4IshLb2+/kONKHMhKy0CW5iGVadi13GcvO7lxZpVLeVr4T3nc/AqDwPrZ2KdzAI62h/7O4o2htRWrYnKqzmUqdESzMxWCK9dl7HZyArGadvwrH98iR48avsaNWJRvhMEDD6EMjS5yQ2vJYFMcMzOVyMtbD4f8e3jK7OO+vqoXsHtiuHk4HB63BZZqreiDcFgZMGaD6Bo6FyMSs/tbkBjttiRvP5zZJ5fqC8IEgbZeuwhJ1eVOeKs/2xGBUMoEiYo6cKaUOqV9k130K2wcdXgN8B25phkRK9DpO23LoF/7/uLwNn01pCcxAxm1/2kvX24uPLtirmg1rQ03E9qrnlvykyJ+9G3QBNtIlsiuoYmEYogZCSRZX29Cm0GWLolYPhlhMDDN6VQI6ktKCH6ubMcbh888Gn2KF8NzpQvV5wN9mQVFMR8+wNVkLGsaN+EEdgAc2CmiajIXur3zu4Menq3iWzJcWHdyTNlROpJeFH9qyfJLzbkWinPyzyBZEXeiZVKZ/ZAYEvXpyHAUngbnNnUO8HBwsLHb//uYEzWRufIJezCy9PYxUVSBNIdfPybuCSeb87Bgry/+5D5aUjrqLuKJUhsLWIL3waHyvQswUjCQlcgFA7OZ9lwMqkDUYy9SnYatIZ98kf1Z6DA==</p><p class="snippet">    username: AgDY4NgxKug07A+jZ63h0Rdisfm6o7kVaKaiaPek9ZOiHsox1A0P4klYaK/7cTEyOCpFVC/2nx00TX6F2KbA1GsRHkjnU/79nOkYWqsWWTU32c/0Re8sSEIPX7aVgR/sMXYeWyRediRogA23xFcFzIFSvw4fZ2XpeX0BZNPbMdwZv2b+j/cjW8Po75B5gqbjwhMyH36QUApnjmoWmutLONVgAnHVM2rBr1Kx4wgxyy+hdmj+6ZkgMBckd53lMVX0unRVW93Ij2eDcxTwN+HvVY7nBDmxVHuYAt6t31+DXpqBew10kNDxd8Xw2MpUFDb3JpMwIVtTntmgeoyCHmo7nCYzQkGhwdrEYzoLVQBq+jf0Wmu3YRpEzZbegdTU3QfS1J7XM+86pAF6gcgbmrhpguGkU+PwnzPMxGNkq445oEPpvRemftjyFf7A8C+bZ90lrvVzZsfOue8WdXKm66vZoYuMPqA2o2HQV0IraaNGYPt9FmiAuXqWhzKsSVsbURXUUOaZIPAyX1z5V1reRz+gs/cGHYKbmUua7XOFQr32siANI1IkRPi9cT+9iP9GGdq5RzZL75cJGFV8BorZ3CMADGC+skrFKOExFvSrvofBnODB/xnPuirzsnQPcxtdvIz+sCv4M8qG2j0ASH1DBLLF7vMP9rLBgA1sPtzqX0CBakjuOjYDqpbXaKqHrM6kdTuBvO7tTDpAYA==</p><p class="snippet">  template:</p><p class="snippet">    metadata:</p><p class="snippet">      creationTimestamp: null</p><p class="snippet">      labels:</p><p class="snippet">        credential.sync.jenkins.openshift.io: "true"</p><p class="snippet">      name: super-dooper</p><p class="snippet">      namespace: labs-ci-cd</p><p class="snippet">    type: kubernetes.io/basic-auth</p><p class="snippet"># apply it to the cluster</p><p class="snippet">$ cat /tmp/sealed-super-dooper.yaml | oc apply -f- -n labs-ci-cd sealedsecret.bitnami.com/super-dooper configured</p></li>&#13;
				<li>To<a id="_idIndexMarker2296"/><a id="_idIndexMarker2297"/> <em class="italics">GitOpsify</em> (yes, again I did just make that up), open up your Jenkins configuration in <strong class="inline">ubiquitous-journey/values-tooling.yaml</strong>. Set your values on Jenkins <strong class="inline">sealed_secrets</strong> as follows using the output of the secret generation step to add the encrypted information to each key. The example here is trimmed for readability:<p class="snippet">- name: jenkins</p><p class="snippet">  enabled: true</p><p class="snippet">  source: https://github.com/redhat-cop/helm-charts.git</p><p class="snippet">  ...</p><p class="snippet">  values:</p><p class="snippet">    ...</p><p class="snippet">    sealed_secrets:</p><p class="snippet">      - name: super-dooper</p><p class="snippet">        password: AgAD+uOI5aCI9YKU2NYt2p7as.....</p><p class="snippet">        username: AgCmeFkNTa0tOvXdI+lEjdJmV5u7FVUcn86SFxiUAF6y.....</p></li>&#13;
				<li>If you've <a id="_idIndexMarker2298"/><a id="_idIndexMarker2299"/>already manually applied the secret in <em class="italics">Step 4</em>, delete it by running <strong class="inline">cat /tmp/sealed-super-dooper.yaml | oc delete -f- -n labs-ci-cd</strong>. Then <strong class="inline">Git commit</strong> these changes so they are available to Jenkins and, more importantly, stored in Git. In Argo CD, we should see that the SealedSecret generated a regular secret.<div id="_idContainer478" class="IMG---Figure"><img src="../Images/B16297_14_29.jpg" alt=""/></div><p class="figure">Figure 14.29: SealedSecrets from Argo CD</p></li>&#13;
				<li>In Jenkins, we should see all that the synchronized secrets using the <strong class="inline">magic</strong> annotation (<strong class="inline">credential.sync.jenkins.openshift.io: "true"</strong>) have become available.<div id="_idContainer479" class="IMG---Figure"><img src="../Images/B16297_14_30.jpg" alt=""/></div></li>&#13;
			</ol>&#13;
			<p class="figure">Figure 14.30: Jenkins secrets automatically loaded from Kubernetes</p>&#13;
			<p>For simplicity here, we will <a id="_idIndexMarker2300"/><a id="_idIndexMarker2301"/>continue without having sealed the secrets; the topic of secrets and GitOps has been included only for illustrative purposes.</p>&#13;
			<h3 id="_idParaDest-358">The Anatomy of a Jenkinsfi<a id="_idTextAnchor512"/><a id="_idTextAnchor513"/>le</h3>&#13;
			<p>Some of you may be <a id="_idIndexMarker2302"/><a id="_idIndexMarker2303"/>familiar with a <strong class="inline">Jenkinsfile</strong>, but for those who are not, let's take a look at the anatomy of one. A <strong class="inline">Jenkinsfile</strong> is just a simple <strong class="bold">domain-specific language</strong> (<strong class="bold">DSL</strong>) that Jenkins knows how to interpret and build our pipelines from. It's the blueprint for the tasks we want to carry out and the order in which they come. Historically, people manually configured<a id="_idIndexMarker2304"/><a id="_idIndexMarker2305"/> Jenkins by creating jobs in the UI, but as people started to do these things at scale, repeatability and maintenance became an issue. I remember when on my first project, one of the developers' code kept failing the build, so he just edited the job to remove that step. Because we had no authorization or traceability around who could change what on Jenkins, this change was not noticed for some time, until things began to break because of it. Fast-forward to now, and we have the <strong class="inline">Jenkinsfile</strong>. It is an <em class="italics">everything-as-code</em> practice that defines the sequence of things we want our pipeline to execute in order.</p>&#13;
			<p>The <strong class="inline">Jenkinsfile</strong> is made up of a pipeline definition with a collection of blocks, as the following is from our PetBattle frontend. If <a id="_idIndexMarker2306"/><a id="_idIndexMarker2307"/>you're curious as to where this file is, you will find it at the root of the project in Git. <em class="italics">Figure 14.31</em> is trimmed a little for simplicity:</p>&#13;
			<div>&#13;
				<div id="_idContainer480" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_31.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.31: Anatomy of a Jenkinsfile</p>&#13;
			<p>The key aspects of a <strong class="inline">Jenkinsfile</strong> DSL are:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li><strong class="inline">pipeline {}</strong> is how all declarative Jenkins pipelines begin.</li>&#13;
				<li><strong class="inline">environment {}</strong> defines the environment <a id="_idIndexMarker2308"/><a id="_idIndexMarker2309"/>variables to be used across all Build stages. Global variables can be defined here.</li>&#13;
				<li><strong class="inline">options {}</strong> contains specific job specs you want to run globally across jobs; for example, setting the terminal color or the default timeout.</li>&#13;
				<li><strong class="inline">stages {}</strong> encapsulates the collection of blocks our pipeline will go through, that is, the <strong class="inline">stage</strong>.</li>&#13;
				<li><strong class="inline">stage {}</strong>: All jobs must have at least one stage. This is the logical part of the build that will be executed, such as <strong class="inline">bake-image</strong>, and contains the steps, agents, and other stage-specific configurations.</li>&#13;
				<li><strong class="inline">agent {}</strong> specifies the node that the build should be run on, for example, <strong class="inline">jenkins-agent-npm</strong>.</li>&#13;
				<li><strong class="inline">steps {}</strong>: Each stage has one or more steps involved. These could be executing shell commands, scripts, Git checkout, and so on.</li>&#13;
				<li><strong class="inline">post {}</strong> is used to specify the post-build actions. Jenkins' declarative pipeline syntax provides very useful callbacks for <strong class="inline">success</strong>, <strong class="inline">failure</strong>, and <strong class="inline">always</strong>, which are useful for controlling the job flow or processing reports after a command is executed.</li>&#13;
				<li><strong class="inline">when {}</strong> is used for flow control. It can be used at the stage level, as well as to stop the pipeline from entering that stage; for example, when branch is master, deploy to test environment.</li>&#13;
				<li><strong class="inline">parallel {}</strong> is used to execute some blocks simultaneously. By default, Jenkins executes each stage sequentially. If things can be done in parallel, then they should, as it will accelerate the feedback loop for the development team.</li>&#13;
			</ul>&#13;
			<p>For us, we are creating the components in the Big Picture, which are Build &gt; Bake &gt; Deploy.</p>&#13;
			<p>The Build should always take the source code, compile it, run <a id="_idIndexMarker2310"/><a id="_idIndexMarker2311"/>some linting (static code checking) and testing before producing a package, and store it in Nexus. We should produce test reports and have them interpreted by Jenkins when deciding to fail the build or not. We are building an Angular application, but Jenkins does not know how to execute <strong class="inline">npm</strong> or other JavaScript-based commands, so we need to tell it to use the agent that contains the <strong class="inline">npm</strong> binary. This is where the agents that we bootstrapped to the Jenkins deployment will come in handy. Each agent that is built extends the base agent image with the binary we need (that is, <strong class="inline">npm</strong>) and is pushed to the cluster. This ImageStream is then labeled <strong class="inline">role=jenkins-slave</strong> to make it automatically discoverable by Jenkins if they are running in the same namespace. This means that for us to use this, we just need to configure our Jenkins stage to use <strong class="inline">agent { label "jenkins-agent-npm" }</strong>.</p>&#13;
			<div>&#13;
				<div id="_idContainer481" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_32.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.32: Jenkins agent discovery magic</p>&#13;
			<p>The Build stage will use this agent <a id="_idIndexMarker2312"/><a id="_idIndexMarker2313"/>and execute some steps. The first thing is to capture the app's version to be used throughout the pipeline by reading the app's manifest (<strong class="inline">pom.xml</strong> for Java or <strong class="inline">package.json</strong> for Node). This version is then used on all generated artifacts, including our image and Helm chart version, and should follow SemVer (for example, <strong class="inline">&lt;major&gt;.&lt;minor&gt;.&lt;patch&gt; = 1.0.1</strong>). We will then pull our dependencies, run our tests, lint, and build our code before publishing the results to Jenkins and the package to Nexus. </p>&#13;
			<p>This will display in the Jenkins <a id="_idIndexMarker2314"/><a id="_idIndexMarker2315"/>declarative pipeline like so:</p>&#13;
			<p class="snippet">stage("Build (Compile App)") {</p>&#13;
			<p class="snippet">  agent { label "jenkins-agent-npm" }</p>&#13;
			<p class="snippet">  steps {</p>&#13;
			<p class="snippet">    script {</p>&#13;
			<p class="snippet">        env.VERSION = sh(returnStdout: true, script: "npm run version --silent").trim()</p>&#13;
			<p class="snippet">        env.PACKAGE = "${APP_NAME}-${VERSION}.tar.gz"</p>&#13;
			<p class="snippet">    }</p>&#13;
			<p class="snippet">    sh 'printenv'</p>&#13;
			<p class="snippet">    echo '### Install deps ###'</p>&#13;
			<p class="snippet">    // sh 'npm install'</p>&#13;
			<p class="snippet">    sh 'npm ci --registry http://sonatype-nexus-service:${SONATYPE_NEXUS_SERVICE_SERVICE_PORT}/repository/labs-npm'</p>&#13;
			<p class="snippet">    echo '### Running linter ###'</p>&#13;
			<p class="snippet">    sh 'npm run lint'</p>&#13;
			<p class="snippet">    echo '### Running tests ###'</p>&#13;
			<p class="snippet">    sh 'npm run test:ci'</p>&#13;
			<p class="snippet">    echo '### Running build ###'</p>&#13;
			<p class="snippet">    sh '''</p>&#13;
			<p class="snippet">        npm run build</p>&#13;
			<p class="snippet">    '''</p>&#13;
			<p class="snippet">    echo '### Packaging App for Nexus ###'</p>&#13;
			<p class="snippet">    sh '''</p>&#13;
			<p class="snippet">        tar -zcvf ${PACKAGE} dist Dockerfile nginx.conf</p>&#13;
			<p class="snippet">        curl -v -f -u ${NEXUS_CREDS} --upload-file ${PACKAGE} http://${SONATYPE_NEXUS_SERVICE_SERVICE_HOST}:${SONATYPE_NEXUS_SERVICE_SERVICE_PORT}/repository/${NEXUS_REPO_NAME}/${APP_NAME}/${PACKAGE}</p>&#13;
			<p class="snippet">    '''</p>&#13;
			<p class="snippet">  }</p>&#13;
			<p class="snippet">  post {</p>&#13;
			<p class="snippet">    always {</p>&#13;
			<p class="snippet">      junit 'junit.xml'</p>&#13;
			<p class="snippet">      publishHTML target: [</p>&#13;
			<p class="snippet">        allowMissing: true,</p>&#13;
			<p class="snippet">        alwaysLinkToLastBuild: false,</p>&#13;
			<p class="snippet">        keepAll: false,</p>&#13;
			<p class="snippet">        reportDir: 'reports/lcov-report',</p>&#13;
			<p class="snippet">        reportFiles: 'index.html',</p>&#13;
			<p class="snippet">        reportName: 'Code Coverage'</p>&#13;
			<p class="snippet">      ]</p>&#13;
			<p class="snippet">    }</p>&#13;
			<p class="snippet">  }</p>&#13;
			<p class="snippet">}</p>&#13;
			<p>Our Bake will always take the output of the previous step, in this case, the package stored in Nexus, and pop it into a container. In our case, we will be running an OpenShift build. This will result in the package being added to the base container and pushed to a repository. If we are executing a sandbox build, say some new feature on a branch, then we are not concerned with pushing the image externally—so we can use the internal registry for OpenShift. If this build is a release candidate then we'll push into Quay.io (our external registry for storing images). The breakdown of the steps for a Bake is found in the Git repository<a id="_idIndexMarker2316"/><a id="_idIndexMarker2317"/> that accompanies this book: <a href="https://github.com/petbattle/pet-battle/blob/master/Jenkinsfile">https://github.com/petbattle/pet-battle/blob/master/Jenkinsfile</a>.</p>&#13;
			<p>From a bird's-eye view, the idea is to get the package from Nexus and then create an OpenShift <strong class="inline">BuildConfig</strong> with a binary build and pass the package to it. You should then see the build execute in the OpenShift cluster.</p>&#13;
			<div>&#13;
				<div id="_idContainer482" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_33.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.33: Jenkins stages outline for Bake and Deploy</p>&#13;
			<p>The deployment will take the application that has just been packaged up with its dependencies and deploy it to our cluster. Initially, we will push the application to our <strong class="inline">labs-test</strong> environment. We want to package the application and its Kubernetes resources as a Helm chart, so for the deployment we will patch the version of the application referenced in the values file with the latest release. For this reason, our Deploy stage is broken down into two parts.</p>&#13;
			<p>The first one patches the Helm chart with the new image information, as well as any repository configuration, such as where to find the image we just Baked! This is then stored in Nexus, which can be used as a Helm chart repository.</p>&#13;
			<p>Secondly, it will install this <a id="_idIndexMarker2318"/><a id="_idIndexMarker2319"/>Helm chart. Depending on what branch we're on, this behavior of how the application will be deployed differs. If we're building on <strong class="inline">master</strong> or <strong class="inline">main</strong>, it is a release candidate, so there is no more <strong class="inline">oc</strong> applying some configuration—this is GitOps land! Instead, we can commit the latest changes to our Argo CD config repository (Ubiquitous Journey). The commits on this repository should be mostly automated if we're doing this the right way. Managing our apps this way makes rollback easy—all we have to do is Git revert!</p>&#13;
			<div>&#13;
				<div id="_idContainer483" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_34.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.34: Jenkins automated commit of the new version from a pipeline run</p>&#13;
			<h3 id="_idParaDest-359"><a id="_idTextAnchor514"/>Branching</h3>&#13;
			<p>Our pipeline is <a id="_idIndexMarker2320"/><a id="_idIndexMarker2321"/>desi<a id="_idTextAnchor515"/><a id="_idTextAnchor516"/>gned to work on <em class="italics">multibranch</em>, creating new pipeline instances for every branch that is committed to in Git. It is intended to have slightly different behavior on <a id="_idIndexMarker2322"/><a id="_idIndexMarker2323"/>each branch. In our world, anything that gets merged to <strong class="inline">master</strong> or <strong class="inline">main</strong> is deemed to be a release candidate. This means that when a developer is ready to merge their code, they would amend the <strong class="inline">package.json</strong> version (or <strong class="inline">pom.xml</strong> version for Java projects) with the new release they want to try and get all the way through the pipeline to production. We could automate the version management, but because our workflow has always been easier, a developer will do this management, as they are best placed to decide whether it's a patch, a minor, or a major release.</p>&#13;
			<p>This means that anything not on the <strong class="inline">main</strong> or <strong class="inline">master</strong> branch is deemed to be a sandbox execution of the pipeline. If something is a sandbox build, it is there to provide fast feedback to the developers of the current state of development in that feature. It can also act as a warning to other engineers that something is not ready to be merged if it's failing. The<a id="_idIndexMarker2324"/><a id="_idIndexMarker2325"/> sandbox builds should be thought of as ephemeral—we're not interested in keeping them hanging around—hence we make some key changes to the pipeline to accommodate this:</p>&#13;
			<ol>&#13;
				<li value="1"><strong class="bold">Internal registry</strong>: If our built image is pushed to<a id="_idIndexMarker2326"/><a id="_idIndexMarker2327"/> our external repository, it will become clogged up and messy with unnecessary images. Every time a developer commits to any branch it would create new images, so it can introduce a cleanup headache; hence we use the internal registry, which automatically prunes old images for us. We only use the external registry when we know a release could go all the way to production.</li>&#13;
				<li><strong class="bold">Helm install</strong>: For our deployments, we're <a id="_idIndexMarker2328"/><a id="_idIndexMarker2329"/>not interested in bringing in a heavyweight tool like Argo CD to manage the development/sandbox deployments. It's unnecessary, so we just use Jenkins to execute a Helm install instead. This will verify that our app can deploy as we expect. We use Argo CD and GitOps to manage the deployments in test and staging environments, but any lower environments we should also treat as ephemeral (as we should test and staging too).</li>&#13;
			</ol>&#13;
			<p>This approach allows us to support many different types of Git workflow. We can support GitHub Flow, Gitflow, and Trunk, all via the same consistent approach to the pipelines.</p>&#13;
			<h3 id="_idParaDest-360"><a id="_idTextAnchor517"/>Webhooks</h3>&#13;
			<p>Before we actually tr<a id="_idTextAnchor518"/><a id="_idTextAnchor519"/>igger <a id="_idIndexMarker2330"/><a id="_idIndexMarker2331"/>Jenkins to build things for us, it's important to add a few webhooks to make our development faster. We need two, one for the Argo CD config repo and one for Jenkins in our source code repository.</p>&#13;
			<p>When we commit a new change to the Git repositories that Argo CD is watching for, it polls. The poll time is configurable, but who can be bothered to wait at all? Argo CD allows you to configure a webhook to tell it to initiate a sync when a change has been made.</p>&#13;
			<p>This is particularly important if we want things to happen after Argo CD has worked its magic, such as in a system test. Our pipeline in Jenkins runs synchronously, but Argo CD is asynchronous and therefore anything we can do to reduce the wait between these behaviors is critical.</p>&#13;
			<p>On GitHub, we can configure the webhook for Ubiquitous Journey to trigger Argo CD whenever the repository updates. On GitHub, add the webhook with the address of our Argo CD server followed by <strong class="inline">/api/webhook</strong>.</p>&#13;
			<div>&#13;
				<div id="_idContainer484" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_35.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.35: Webhook to trigger Argo CD on Git commit</p>&#13;
			<h3 id="_idParaDest-361"><a id="_idTextAnchor520"/>Jenkins</h3>&#13;
			<p>Every time we commit <a id="_idIndexMarker2332"/><a id="_idIndexMarker2333"/>to our source code repository, we want Jenkins to run a build. We're using the multibranch plugin for Jenkins, so this means that when we commit to the repository, the webhook will trigger a branch scan, which should bring back any new feature branches to build pipelines or create builds for any new code commits on any branch.</p>&#13;
			<p>Configuring the Jenkins webhook for the <strong class="inline">pet-battle</strong> frontend is simple. On GitHub's <em class="italics">Hooks</em> page, add the URL to our Jenkins instance in the following form, where the trigger token is the name of our GitHub project. As a convention, I tend to use the name of the Git project as the token, so the same would apply for the backend if you were building it using Jenkins too:<strong class="inline">JENKINS_URL/multibranch-webhook-trigger/invoke?token=[Trigger token]</strong></p>&#13;
			<p>For example, the frontend application's webhook URL would look something like this:</p>&#13;
			<p><a href="https://jenkins-labs-ci-cd.apps.petbattle.com/multibranch-webhook-trigger/invoke?token=pet-battle ">https://jenkins-labs-ci-cd.apps.petbattle.com/multibranch-webhook-trigger/invoke?token=pet-battle</a></p>&#13;
			<h3 id="_idParaDest-362"><a id="_idTextAnchor521"/>Bringing It All Together</h3>&#13;
			<p>We hav<a id="_idTextAnchor522"/><a id="_idTextAnchor523"/>e now gone through what a Jenkins file is and what it does for us. We've spoken about branching and what we mean for a build to be a release candidate (that is, a version bump and on master/main). We've touched on deploying using Helm and GitOps to commit our change and have Argo CD roll out the change for us…but how do we connect Jenkins up to all this magic?</p>&#13;
			<p>As with all these things, there are several ways. We <em class="italics">could</em> open up Jenkins and hit <strong class="bold">New Job &gt; Multibranch Pipeline</strong>, configure it to point to our Git repository, and set it to be triggered by a webhook, but that feels like the old way of doing things. I want a repeatable process so I don't have to do this step each time. Enter our <strong class="inline">seed-multibranch-pipelines</strong> job! Some of you may have noticed that Jenkins was configured to point to our organization's GitHub for PetBattle when we deployed the Helm chart from Ubiquitous Journey. We set some environment variables on the image (in <strong class="inline">ubiquitous-journey/values-tooling.yaml</strong>) to point to our GitHub organization as follows:</p>&#13;
			<p class="snippet">- name: GITHUB_ACCOUNT</p>&#13;
			<p class="snippet">value: 'petbattle'</p>&#13;
			<p class="snippet">- name: GITHUB_ORG</p>&#13;
			<p class="snippet">value: 'true'</p>&#13;
			<p>If you’re following along with a fork of the Ubiquitous Journey and want to see the pipeline run end to end, update both ARGOCD_CONFIG_REPO to point to your fork and QUAY_ACCOUNT to resolve to your user on Quay.io.</p>&#13;
			<p>These are used by the <strong class="inline">seed-multibranch-pipelines</strong> job that is baked into the Jenkins image to scan the organization for repositories that contain a <strong class="inline">Jenkinsfile</strong> and are not archived. If it finds any, it <a id="_idIndexMarker2334"/><a id="_idIndexMarker2335"/>will automatically scaffold out multibranch Jenkins jobs for us. In our case, we have a <strong class="inline">Jenkinsfile</strong> for both the Cats API and the PetBattle frontend, so jobs are created for us without having to configure anything! If you're following along and not using <a id="_idIndexMarker2336"/><a id="_idIndexMarker2337"/>GitHub but GitLab, you can set <strong class="inline">GITLAB_*</strong> environment variables to achieve the same effect. </p>&#13;
			<div>&#13;
				<div id="_idContainer485" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_36.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.36: Jenkins seed to scaffold out our Jenkins jobs</p>&#13;
			<p>If you open Jenkins and drill down into the <strong class="inline">pet-battle</strong> folder for the frontend code base, you should see builds; for example, a Git branch called <strong class="inline">cool-new-cat</strong> and the <strong class="inline">master</strong> with pipeline executions for each of them. Opening the Blue Ocean view, we get a much better understanding of the flow control we built, as previously discussed.</p>&#13;
			<div>&#13;
				<div id="_idContainer486" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_37.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.37: Jenkins release candidate pipeline</p>&#13;
			<p>For the master branch, which we deem to be a release candidate, the artifacts that are built could go all the way. If we are updating our application, we bump the manifest version along with any changes we're bringing in and Git commit, which should trigger the build. From this point, our build environment is configured, and the pipeline should execute. We target an external repository and the image that's built will be pushed to Quay.io for portability across multiple clusters. Our Helm chart's values are patched and pushed to Nexus for storage. If we need to update our Helm chart itself, for example, to add some new configuration to the chart or add a new Kubernetes resource, we should of course bump the chart version too. For our deployment, we patch the Argo CD config repository (Ubiquitous Journey) with the new release information, and it should sync automatically for us, deploying our application to the <strong class="inline">labs-test</strong> namespace! We <a id="_idIndexMarker2338"/><a id="_idIndexMarker2339"/>then run a verify step to check that the version being rolled out matches the new version (based on the labels) and has been successful.</p>&#13;
			<div>&#13;
				<div id="_idContainer487" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_38.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.38: Jenkins feature development pipeline</p>&#13;
			<p>For our feature branches, the idea is much the same, but without the need for an external repository. Our charts are also manipulated to override the name to include the branch. This means that on each commit to a feature branch, we get a new application deployed containing the branch name in the route. So, for our <strong class="inline">cool-new-cat</strong> branch, the application is deployed as <strong class="inline">cool-new-cat-pet-battle</strong> and is available in the developmental environment.</p>&#13;
			<p>The remaining stages that were added to the Big Picture, System Test and Promote, will be covered in the next chapter, when we look in more detail at the testing for PetBattle.</p>&#13;
			<h3 id="_idParaDest-363"><a id="_idTextAnchor524"/>What's Next for Jenkinsfile</h3>&#13;
			<p>Jenkin<a id="_idTextAnchor525"/><a id="_idTextAnchor526"/>s has been around <a id="_idIndexMarker2340"/><a id="_idIndexMarker2341"/>for quite some time. It is not the most container-native approach to building software but there is a rich ecosystem surrounding it. It's lasted a long time because people like it! Hopefully, this gives you a taste of what can be done with Jenkins for our PetBattle applications, but it's by no means the end. There are a few plot holes in the story, as some of you may have noticed. For example, once a build has been successfully deployed to the test environment, how do I promote it onward? Should I do more testing? Well, the answer will come in the next chapter when we explore system tests and extend our pipeline further to include promoting images. At the end of a successful pipeline execution, the values file in our repository is not updated; we should be thinking about writing the successful build artifact details back to the repository, so it's always got a sensible default set to what is currently deployed.</p>&#13;
			<p>The stages we have written here are fairly massive and do have some <strong class="inline">bash</strong> and other logic inside of them. If you were to build a non-frontend application, for example, you would want to build<a id="_idIndexMarker2342"/><a id="_idIndexMarker2343"/> something in Golang. For the most part, the only thing that needs to change is the Build stage, as the act of putting something in a box, and how we package our Helm chart and deploy the app, remains the same. Once the app, in any language or framework, is in a container, then how we ship it remains the same. This means there is a high potential for reusing the code in the Bake and Deploy stages again and again, thus lowering the bar for adopting new technologies on a platform such as OpenShift. But be careful – copying and pasting the same steps across many jobs in a large estate of apps can lead to one mistake being copied around. Changes to the pipeline can become costly too, as you have to update each <strong class="inline">Jenkinsfile</strong> in each repository.</p>&#13;
			<div>&#13;
				<div id="_idContainer488" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_39.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.39: A lurking dragon, watch out!</p>&#13;
			<p>Jenkins does tackle these problems with the use of shared libraries, and more recently the <strong class="bold">Jenkins Templating Engine</strong> (<strong class="bold">JTE</strong>). The JTE tackles the problem by enforcing pipeline approaches from a governance point of view. While this might seem like a great way to standardize<a id="_idIndexMarker2344"/><a id="_idIndexMarker2345"/> across an enterprise—here be dragons!</p>&#13;
			<p>Applying a standard <a id="_idIndexMarker2346"/><a id="_idIndexMarker2347"/>pipeline without justification or the ability for teams to pull requests and make changes for their own specific use case is the same as having Dev and Ops in separate rooms. We've worked with plenty of customers who have tried approaches like this and ultimately it makes them go slower, rather than faster. The teams putting the pipelines in place think they're helping and providing a great service, but when things go wrong, they are the bottleneck to fixing it. For some teams, the hammer approach<a id="_idIndexMarker2348"/><a id="_idIndexMarker2349"/> might not be applicable for their use case and so the pipeline becomes something in the way for them to go faster.</p>&#13;
			<p>Tekton is another way for us to get greater pipeline reusability and also honor more of our GitOps landscape. Let's explore it now for our Java microservices.</p>&#13;
			<h2 id="_idParaDest-364"><a id="_idTextAnchor527"/>Tekton–The Backend</h2>&#13;
			<p>Tekton<span id="footnote-086-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-086">8</a></span> is an open source cloud-native CI/CD tool<a id="_idIndexMarker2350"/><a id="_idIndexMarker2351"/> that forms the basis for OpenShift Pipelines.<span id="footnote-085-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-085">9</a></span></p>&#13;
			&#13;
			<h3 id="_idParaDest-365"><a id="_idTextAnchor528"/>Tekton Basics</h3>&#13;
			<p>There are many similarities <a id="_idIndexMarker2352"/><a id="_idIndexMarker2353"/>between what Jenkins does and what Tekton does. For example, both can be used to store pipeline definitions as code in a Git repository. Tekton is deployed as an operator in our cluster and allows users to define in YAML <strong class="inline">Pipeline</strong> and <strong class="inline">Task</strong> definitions. Tekton Hub<span id="footnote-084-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-084">10</a></span> is a <a id="_idIndexMarker2354"/><a id="_idIndexMarker2355"/>repository for sharing these YAML resources among the community, giving great reusability to standard workflows.</p>&#13;
			<div id="footnote-086" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-086-backlink">8</a>	<a href="https://tekton.dev">https://tekton.dev</a></p>&#13;
				<div id="footnote-085" class="_idFootnote" epub:type="footnote">&#13;
					<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-085-backlink">9</a>	<a href="https://docs.openshift.com/container-platform/4.7/cicd/pipelines/understanding-openshift-pipelines.html">https://docs.openshift.com/container-platform/4.7/cicd/pipelines/understanding-openshift-pipelines.html</a></p>&#13;
				</div>&#13;
			</div>&#13;
			<div id="footnote-084" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-084-backlink">10</a>	<a href="https://hub.tekton.dev">https://hub.tekton.dev</a></p>&#13;
			</div>&#13;
			<div>&#13;
				<div id="_idContainer489" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_40.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.40: Tekton Hub and OpenShift Cluster tasks</p>&#13;
			<p>OpenShift also<a id="_idIndexMarker2356"/><a id="_idIndexMarker2357"/> makes these available<a id="_idIndexMarker2358"/><a id="_idIndexMarker2359"/> globally as <strong class="inline">ClusterTasks</strong>. To write a pipeline you can wire together these task definitions. OpenShift provides a guided <strong class="inline">Pipeline</strong> builder UI for just this task. You link various tasks together and define parameters and outputs as specified in each task definition.</p>&#13;
			<div>&#13;
				<div id="_idContainer490" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_41.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.41: OpenShift Pipeline bui<a id="_idTextAnchor529"/><a id="_idTextAnchor530"/>lder UI</p>&#13;
			<p>There are numerous task activities in our pipeline definitions that require persistent storage. When <a id="_idIndexMarker2360"/><a id="_idIndexMarker2361"/>building our backend PetBattle API and Tournament applications using Maven, we pull our <a id="_idIndexMarker2362"/><a id="_idIndexMarker2363"/>Java dependencies via our Nexus repository manager. To speed up this process, we can perform the same caching we might do on our laptops and store these locally between builds in a <strong class="inline">.m2/repository</strong> folder and share this between builds. We also use persistent storage for built artifacts so they can be shared between different steps in our pipeline. Another use case is to mount Kubernetes secrets into our pipelines:</p>&#13;
			<p class="snippet"># maven pipeline </p>&#13;
			<p class="snippet">apiVersion: tekton.d<a id="_idTextAnchor531"/>ev/v1beta1</p>&#13;
			<p class="snippet">kind: Pipeline</p>&#13;
			<p class="snippet">metadata:</p>&#13;
			<p class="snippet">  name: maven-pipeline</p>&#13;
			<p class="snippet">  labels:</p>&#13;
			<p class="snippet">    petbattle.app/uj: ubiquitous-journey</p>&#13;
			<p class="snippet">spec:</p>&#13;
			<p class="snippet">  workspaces:</p>&#13;
			<p class="snippet">    - name: shared-workspace</p>&#13;
			<p class="snippet">    - name: maven-settings</p>&#13;
			<p class="snippet">    - name: argocd-env-secret</p>&#13;
			<p class="snippet">    - name: maven-m2</p>&#13;
			<p class="snippet">    - name: git-auth-secret </p>&#13;
			<p class="snippet"># binding the workspace in the PipelineRun object</p>&#13;
			<p class="snippet">  resourcetemplates:</p>&#13;
			<p class="snippet">    - apiVersion: tekton.dev/v1beta1</p>&#13;
			<p class="snippet">      kind: PipelineRun</p>&#13;
			<p class="snippet">      metadata:</p>&#13;
			<p class="snippet">      ...        workspaces:</p>&#13;
			<p class="snippet">        - name: shared-workspace</p>&#13;
			<p class="snippet">          persistentVolumeClaim:</p>&#13;
			<p class="snippet">            claimName: build-images</p>&#13;
			<p class="snippet">        - name: maven-settings</p>&#13;
			<p class="snippet">          persistentVolumeClaim:</p>&#13;
			<p class="snippet">            claimName: maven-source</p>&#13;
			<p class="snippet">        - name: argocd-env-secret</p>&#13;
			<p class="snippet">          secret:</p>&#13;
			<p class="snippet">            secretName: argocd-token</p>&#13;
			<p class="snippet">        - name: maven-m2</p>&#13;
			<p class="snippet">          persistentVolumeClaim:</p>&#13;
			<p class="snippet">            claimName: maven-m2</p>&#13;
			<p class="snippet">        - name: git-auth-secret</p>&#13;
			<p class="snippet">          secret:</p>&#13;
			<p class="snippet">            secretName: git-auth</p>&#13;
			<p>In Tekton, we link these Kubernetes objects with the named <strong class="inline">workspaces</strong> when we create what is called the <strong class="inline">PipelineRun</strong>, a piece of code that represents one run of a pipeline. Similarly, the<a id="_idIndexMarker2364"/><a id="_idIndexMarker2365"/> execution of a single task is a <strong class="inline">TaskRun</strong>. Each <strong class="inline">workspace</strong> is then made available for the tasks in that <strong class="inline">PipelineRun</strong> as shown.</p>&#13;
			<h3 id="_idParaDest-366"><a id="_idTextAnchor532"/>Reusable Pipelines</h3>&#13;
			<p>There are some <a id="_idIndexMarker2366"/><a id="_idIndexMarker2367"/>choi<a id="_idTextAnchor533"/><a id="_idTextAnchor534"/>ces to be made before you start writing and designing your Tekton pipeline. The first is to choose whether you write a pipeline for each application, or whether you write reusable pipelines that can be used for applications that are similar.</p>&#13;
			<p>In PetBattle, we started with one pipeline per application; this is similar to having a <strong class="inline">Jenkinsfile</strong> in each application Git repository. Both the API and Tournament PetBattle applications are built using Java, Quarkus, and Maven, so it makes sense to consolidate the pipeline code and write a reusable parameterized pipeline for these two applications because they will always have similar tasks. We use our <strong class="inline">maven-pipeline</strong> in PetBattle to do this.</p>&#13;
			<div>&#13;
				<div id="_idContainer491" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_42.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.42: PetBattle's Tekton pipelines</p>&#13;
			<p>Of course, you could keep the reuse to the <strong class="inline">Task</strong> level only but we share common tasks across the PetBattle UI, API, and Tournament applications. Ultimately, the development team has to balance the benefits of maintaining one pipeline over application pipeline autonomy. There is no one-size-fits-all answer.</p>&#13;
			<h3 id="_idParaDest-367"><a id="_idTextAnchor535"/>Build, Bake, Deploy with Tekton</h3>&#13;
			<p>The nex<a id="_idTextAnchor536"/><a id="_idTextAnchor537"/>t step is to <a id="_idIndexMarker2368"/><a id="_idIndexMarker2369"/>start designing what we put into our pipeline. This is a very iterative process! In our Big Picture, we talked about the Build, Bake, and Deploy process, so it makes sense to add pipeline task steps that follow this methodology.</p>&#13;
			<div>&#13;
				<div id="_idContainer492" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_43.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.43: The list of task definitions used by PetBattle's Tekton Pipelines</p>&#13;
			<p>The <strong class="inline">maven-pipeline</strong> starts by cloning the application and CI/CD (Ubiquitous Journey) repositories into the shared <strong class="inline">workspace</strong>. We check the<a id="_idIndexMarker2370"/><a id="_idIndexMarker2371"/> code quality by invoking Maven to build and test the application, with quality reports being uploaded to our SonarQube image.</p>&#13;
			<p>We check that the quality gate<a id="_idIndexMarker2372"/><a id="_idIndexMarker2373"/> in SonarQube has passed and then invoke Maven to package our application. Tekton offers us useful constructs to retry a task step if it fails by specifying the number of <strong class="inline">retries</strong> as well as the ordering of task steps using the <strong class="inline">runAfter</strong> task name list.</p>&#13;
			<p class="snippet">    - name: quality-gate-check</p>&#13;
			<p class="snippet">      retries: 1</p>&#13;
			<p class="snippet">      taskRef:</p>&#13;
			<p class="snippet">        name: sonarqube-quality-gate-check</p>&#13;
			<p class="snippet">      workspaces:</p>&#13;
			<p class="snippet">        - name: output</p>&#13;
			<p class="snippet">          workspace: shared-workspace</p>&#13;
			<p class="snippet">      params:</p>&#13;
			<p class="snippet">      - name: WORK_DIRECTORY</p>&#13;
			<p class="snippet">        value: "$(params.APPLICATION_NAME)/$(params.GIT_BRANCH)"</p>&#13;
			<p class="snippet">      runAfter:</p>&#13;
			<p class="snippet">      - save-test-results</p>&#13;
			<p>In Java Quarkus, the packaging <a id="_idIndexMarker2374"/><a id="_idIndexMarker2375"/>format could be a fat JAR, an exploded fast JAR, or a native GraalVM-based image. There are various trade-offs with each of these formats.<span id="footnote-083-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-083">11</a></span> However, we are using the exploded fast JAR in PetBattle, which allows us to trade off between faster build times or faster startup times. This is the end of the Build stage. We have moved the unit testing left in our pipeline, so we get fast feedback on any code quality issues before we move on to the Bake and Deploy pipeline phases.</p>&#13;
			&#13;
			<div>&#13;
				<div id="_idContainer493" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_44.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.44: The view of a PipelineRun in OpenShift showing the tasks being executed</p>&#13;
			<div id="footnote-083" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-083-backlink">11</a>	<a href="https://quarkus.io/guides/maven-tooling">https://quarkus.io/guides/maven-tooling</a></p>&#13;
			</div>&#13;
			<p>The Bake stage is next. We use<a id="_idIndexMarker2376"/><a id="_idIndexMarker2377"/> a standard OpenShift BuildConfig object, which <a id="_idIndexMarker2378"/><a id="_idIndexMarker2379"/>is loaded using Kustomize, as we do not package that with our Helm chart. We perform a binary build using the <strong class="inline">oc start build</strong> command on the packaged application. We decided not to upload the built application package to Nexus because we want to work with container images as <a id="_idIndexMarker2380"/><a id="_idIndexMarker2381"/>our unit of deployment. If we were building libraries that needed to support our services, then they should be captured in Nexus at this stage. It is worth pointing out that we could also push the image to an external registry at this point in time so it can be easily shared between OpenShift clusters.</p>&#13;
			<div>&#13;
				<div id="_idContainer494" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_45.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.45: Bake part of the pipeline</p>&#13;
			<p>The next step is to lint and package the application Helm chart. The versioned chart is then uploaded to Nexus. If we were on an application branch, the next pipeline step would be a <strong class="inline">helm install</strong> into the <strong class="inline">labs-dev</strong> project. We can make use of <strong class="inline">when</strong> statements in our Tekton pipeline to configure such behavior:</p>&#13;
			<p class="snippet">- name: helm-install-apps-dev # branches only deploy to dev</p>&#13;
			<p class="snippet">      when:</p>&#13;
			<p class="snippet">        - Input: "$(params.GIT_BRANCH)"</p>&#13;
			<p class="snippet">          Operator: notin</p>&#13;
			<p class="snippet">          Values: ["master","trunk","main"]</p>&#13;
			<p class="snippet">      taskRef:</p>&#13;
			<p class="snippet">        name: helm-install-from-chartrepo</p>&#13;
			<p>When on trunk/HEAD, the ImageStream is versioned and tagged into the namespaces we are going to deploy our application to (<strong class="inline">labs-test</strong>, <strong class="inline">labs-staging</strong>). Because we are practicing GitOps, the applications are deployed using Argo CD and Git. The Argo CD app-of-apps values files are updated with the new chart and image versions. This is checked into source code by the pipeline and <strong class="inline">git commit</strong> is executed. Argo CD is configured to automatically<a id="_idIndexMarker2382"/><a id="_idIndexMarker2383"/> sync our applications in <strong class="inline">labs-test</strong> and <strong class="inline">labs-staging</strong>, and the last step of the pipeline is to make sure the sync task was successful.</p>&#13;
			<div>&#13;
				<div id="_idContainer495" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_46.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.46: Deploy part of the pipeline</p>&#13;
			<p>There is a lot of pipeline information available to the developer in the OpenShift web console and all of the pipeline task logs can be easily seen.</p>&#13;
			<div>&#13;
				<div id="_idContainer496" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_47.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.47: Tekton pipeline progress and status hover</p>&#13;
			<p>Tekton also has a great command-line tool called <strong class="inline">tkn</strong>, which <a id="_idIndexMarker2384"/><a id="_idIndexMarker2385"/>can be used to perform all of the pipeline actions available in the OpenShift console, such as viewing logs, starting pipeline runs, and defining Tekton objects.</p>&#13;
			<p class="snippet">$ tkn pr list -n labs-ci-cd</p>&#13;
			<p class="snippet">&#13;
NAME                          STARTED    DURATION     STATUS</p>&#13;
			<p class="snippet">pet-battle-tournament-44vg5   1 days ago 27 minutes   Failed</p>&#13;
			<p class="snippet">pet-battle-9d9c7              2 days ago 19 minutes   Succeeded</p>&#13;
			<p class="snippet">pet-battle-api-jcgn2          2 days ago 21 minutes   Succeeded</p>&#13;
			<p class="snippet">pet-battle-kfch9              2 days ago 5 minutes    Failed</p>&#13;
			<p class="snippet">pet-battle-tournament-br5xd   2 days ago 24 minutes   Failed</p>&#13;
			<p class="snippet">pet-battle-api-5l4sd          2 days ago 23 minutes   Failed</p>&#13;
			<p>Let's now take a look <a id="_idIndexMarker2386"/><a id="_idIndexMarker2387"/>at how we can trigger a build.</p>&#13;
			<h3 id="_idParaDest-368"><a id="_idTextAnchor538"/>Triggers and Webhooks</h3>&#13;
			<p>On every developer <a id="_idIndexMarker2388"/><a id="_idIndexMarker2389"/>pus<a id="_idTextAnchor539"/><a id="_idTextAnchor540"/>h to Git, we wish to trigger a build. This ensures we get the fastest feedback for all of our <a id="_idIndexMarker2390"/><a id="_idIndexMarker2391"/>code changes. In Tekton this is achieved by using an <strong class="inline">EventListener</strong> pod object. When created, a pod is deployed, exposing our defined trigger actions.</p>&#13;
			<div>&#13;
				<div id="_idContainer497" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_48.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.48: Tekton Triggers flow</p>&#13;
			<p>Tekton Triggers work by having <strong class="inline">EventListener</strong> objects receive incoming webhook notifications, processing them using an interceptor, and creating Kubernetes resources from templates if the interceptor allows it, with the extraction of fields from the body of the webhook (there's an assumption that the body is a JSON file):</p>&#13;
			<p class="snippet">apiVersion: triggers.tekton.dev/v1alpha1</p>&#13;
			<p class="snippet">kind: EventListener</p>&#13;
			<p class="snippet">metadata:</p>&#13;
			<p class="snippet">  name: github-webhook</p>&#13;
			<p class="snippet">  labels:</p>&#13;
			<p class="snippet">    app: github</p>&#13;
			<p class="snippet">spec:</p>&#13;
			<p class="snippet">  serviceAccountName: pipeline</p>&#13;
			<p class="snippet">  triggers:</p>&#13;
			<p class="snippet">    - name: pet-battle-api-webhook-all-branches ...</p>&#13;
			<p class="snippet">    - name: pet-battle-api-webhook-pr ...</p>&#13;
			<p class="snippet">    - name: pet-battle-tournament-webhook-all-branches ...</p>&#13;
			<p class="snippet">    - name: pet-battle-tournament-webhook-pr ...</p>&#13;
			<p class="snippet">    - name: pet-battle-webhook-all-branches ...</p>&#13;
			<p class="snippet">    - name: pet-battle-webhook-pr ...</p>&#13;
			<p>In OpenShift, we expose the <strong class="inline">EventListener</strong> webhook endpoint as a route so that it can be wired into Git. Different types of<a id="_idIndexMarker2392"/><a id="_idIndexMarker2393"/> <strong class="bold">source control managers</strong> (<strong class="bold">SCMs</strong>) define different webhook payloads. We are using GitHub, so it is those webhook payloads<span id="footnote-082-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-082">12</a></span> that we need to use to help define the parameters used to create the <strong class="inline">TriggerBinding</strong> and pass to our <strong class="inline">TriggerTemplate</strong>. The <strong class="inline">TriggerTemplate</strong><strong class="inline"><a id="_idIndexMarker2394"/><a id="_idIndexMarker2395"/></strong> then defines the Tekton resources to create. In our case, this is a <strong class="inline">PipelineRun</strong> or <strong class="inline">TaskRun</strong> definition.</p>&#13;
			&#13;
			<p class="snippet">triggers:</p>&#13;
			<p class="snippet">    - name: pet-battle-api-webhook-all-branches</p>&#13;
			<p class="snippet">      interceptors: # fixme add secret.ref</p>&#13;
			<p class="snippet">        - cel:</p>&#13;
			<p class="snippet">            filter: &gt;-</p>&#13;
			<p class="snippet">              (header.match('X-GitHub-Event', 'push') &amp;&amp;</p>&#13;
			<p class="snippet">                body.repository.full_name ==</p>&#13;
			<p class="snippet">                                        'petbattle/pet-battle-api')</p>&#13;
			<p class="snippet">            overlays:</p>&#13;
			<p class="snippet">            - key: truncated_sha</p>&#13;
			<p class="snippet">              expression: "body.head_commit.id.truncate(7)"</p>&#13;
			<p class="snippet">            - key: branch_name</p>&#13;
			<p class="snippet">              expression: "body.ref.split('/')[2]"</p>&#13;
			<p class="snippet">            - key: app_of_apps_key</p>&#13;
			<p class="snippet">              expression: "body.repository.name.replace('-','_',-1)"</p>&#13;
			<p class="snippet">      bindings:</p>&#13;
			<p class="snippet">      - kind: TriggerBinding</p>&#13;
			<p class="snippet">        ref: github-trigger-binding</p>&#13;
			<p class="snippet">      template:</p>&#13;
			<p class="snippet">        ref: pet-battle-api-maven-trigger-template</p>&#13;
			<p>Tekton uses an expression language, known as the <strong class="bold">Common Expression Language</strong> (<strong class="bold">CEL</strong>),<span id="footnote-081-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-081">13</a></span> to parse and filter <a id="_idIndexMarker2396"/><a id="_idIndexMarker2397"/>requests based on JSON bodies and request headers. This is necessary because of the differing webhook payloads and potentially different Git workflows. For example, we are using GitHub and treat a pull request differently from changes to our main/HEAD. One customization we make that you can see above is to define the Argo CD app-of-apps key in the trigger binding based on the Git repository name. This allows <a id="_idIndexMarker2398"/><a id="_idIndexMarker2399"/>us to check the synchronization of just the one application that changed and not the whole application suite during the Deploy phase of our pipeline. While triggering seems complex, the flexibility is required when dealing with all the various Git SCMs and workflows that are available to development teams.</p>&#13;
			<div id="footnote-082" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-082-backlink">12</a>	<a href="https://docs.github.com/en/developers/webhooks-and-events/webhook-events-and-payloads">https://docs.github.com/en/developers/webhooks-and-events/webhook-events-and-payloads</a></p>&#13;
			</div>&#13;
			<div id="footnote-081" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-081-backlink">13</a>	<a href="https://github.com/google/cel-go">https://github.com/google/cel-go</a></p>&#13;
			</div>&#13;
			<p>There are some convenience templates loaded into the <strong class="inline">labs-ci-cd</strong> project by Ubiquitous Journey that can be used to manually trigger a <strong class="inline">PipelineRun</strong>—this is handy if you have not configured the GitHub webhook yet.</p>&#13;
			<p class="snippet">$ oc -n labs-ci-cd process pet-battle-api | oc -n labs-ci-cd create -f-</p>&#13;
			<p class="snippet">$ oc -n labs-ci-cd process pet-battle | oc -n labs-ci-cd create -f-</p>&#13;
			<p class="snippet">$ oc -n labs-ci-cd process pet-battle-tournament | oc -n labs-ci-cd create -f-</p>&#13;
			<p>You can manually add webhooks to your GitHub projects<span id="footnote-080-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-080">14</a></span> that point to the <strong class="inline">EventListener</strong> route exposed in the <strong class="inline">labs-ci-cd</strong> project.</p>&#13;
			&#13;
			<p class="snippet">$ oc -n labs-ci-cd get route webhook \     -o custom-columns=ROUTE:.spec.host --no-headers</p>&#13;
			<p>Otherwise, check <a id="_idIndexMarker2400"/><a id="_idIndexMarker2401"/>out the PetBattle Ubiquitous Journey documentation for Tekton tasks that can be run to automatically add these webhooks to your Git repositories.</p>&#13;
			<h3 id="_idParaDest-369"><a id="_idTextAnchor541"/>GitOps our Pipelines</h3>&#13;
			<p>Our <a id="_idIndexMarker2402"/><a id="_idIndexMarker2403"/>pipeline, task, trig<a id="_idTextAnchor542"/><a id="_idTextAnchor543"/>ger, workspace, and volume definitions are themselves applied to our <strong class="inline">labs-ci-cd</strong> project using GitOps. The idea here is to minimize how hard it is to adapt our pipelines. We may want to <a id="_idIndexMarker2404"/><a id="_idIndexMarker2405"/>add some more security checks into our pipeline steps, for example. If there are testing failures, or even service failures in production, then we need to adapt our pipelines to cater for further quality controls or testing steps. Adding new tools or modifying task steps becomes nothing more than pushing the pipeline as code definitions to Git.</p>&#13;
			<p class="snippet"># PetBattle Tekton objects</p>&#13;
			<p class="snippet">  - name: tekton-pipelines</p>&#13;
			<p class="snippet">    destination: labs-ci-cd</p>&#13;
			<p class="snippet">    enabled: true</p>&#13;
			<p class="snippet">    source: https://github.com/petbattle/ubiquitous-journey.git</p>&#13;
			<p class="snippet">    source_path: tekton</p>&#13;
			<p class="snippet">    source_ref: main</p>&#13;
			<p class="snippet">    sync_policy: *sync_policy_true</p>&#13;
			<p class="snippet">    no_helm: true</p>&#13;
			<p>Within our Tekton source folder, we use Kustomize to apply all of the YAML files that define our Tekton objects. These pipeline objects are kept in sync by Argo CD.</p>&#13;
			<div id="footnote-080" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-080-backlink">14</a>	<a href="https://docs.github.com/en/developers/webhooks-and-events/creating-webhooks">https://docs.github.com/en/developers/webhooks-and-events/creating-webhooks</a></p>&#13;
			</div>&#13;
			<h2 id="_idParaDest-370"><a id="_idTextAnchor544"/>Which One Should I Use?</h2>&#13;
			<p>The CI/CD too<a id="_idTextAnchor545"/><a id="_idTextAnchor546"/>ling landscape is massive<span id="footnote-079-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-079">15</a></span> and also extremely vibrant and healthy. The CNCF landscape for tools in this category <a id="_idIndexMarker2406"/><a id="_idIndexMarker2407"/>has no less than 36 products and projects today. In trying to answer the question of which one you should use; it is best to consider multiple factors:</p>&#13;
			&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>Does your team have previous skills in a certain tooling or language? For example, pipelines as code in Jenkins use the Groovy language, so if your team has Groovy or JavaScript skills, this could be a good choice.</li>&#13;
				<li>Does the tool integrate with the platform easily? Most of the tools in CNCF have good integration with Kubernetes already and have a cloud-native pedigree. That does not mean that all tools are the same in terms of deployment, platform integration, or lifecycle management—some <a id="_idIndexMarker2408"/><a id="_idIndexMarker2409"/>may be <strong class="bold">Software as a Service</strong> (<strong class="bold">SaaS</strong>)-only offerings with agents, whereas some can be deployed per team using namespace isolation on your cluster. Others, such as Argo CD and Tekton, can be deployed at <a id="_idIndexMarker2410"/><a id="_idIndexMarker2411"/>cluster scope using the operator pattern, and have their lifecycle managed via the <strong class="bold">Operator Lifecycle Manager</strong> (<strong class="bold">OLM</strong>). Tekton has great web console integration with <a id="_idIndexMarker2412"/><a id="_idIndexMarker2413"/>OpenShift because of the OpenShift Pipelines operator.</li>&#13;
				<li>Tool deployment model: Jenkins and Argo CD both have a client-server model for deployment. This can be problematic at larger scales, such as when looking after thousands of pipelines or hundreds of applications. It may be necessary to use multiple deployments to scale across teams and clusters. Argo CD and Tekton extend Kubernetes using CRDs <a id="_idIndexMarker2414"/><a id="_idIndexMarker2415"/>and operator patterns, so deployment is more Kubernetes-native in its scaling model. </li>&#13;
				<li>Enterprise support: Most, but not all, the tools have vendor support. This is important for enterprise organizations that need a relationship with a vendor to cover certification, training, security fixes, and product lifecycles.</li>&#13;
			</ul>&#13;
			<div id="footnote-079" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-079-backlink">15</a>	<a href="https://landscape.cncf.io/card-mode?category=continuous-integration-delivery&amp;grouping=category">https://landscape.cncf.io/card-mode?category=continuous-integration-delivery&amp;grouping=category</a></p>&#13;
			</div>&#13;
			<div>&#13;
				<div id="_idContainer498" class="IMG---Figure">&#13;
					<img src="../Images/B16297_14_49.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 14.49: CNCF CI/CD tooling landscape</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>Active open-source <a id="_idIndexMarker2416"/><a id="_idIndexMarker2417"/>communities: A vibrant upstream community is important as a place to collaborate and share code and knowledge. Rapid development of features and plugins often occurs in a community based on real user problems and requests.</li>&#13;
				<li>One tool for both CI and CD or use different tools? As we have shown with PetBattle, sometimes it makes sense for CI to be a push-type model, and CD to be a pull-type model using different tools.</li>&#13;
				<li>Extensibility model: This is important for the ecosystem around the tooling. Jenkins has a great plugin model that allows lots of different extensions to the core. Tekton has a similar model, but it is different in that users have the ability to use any container in a task. It is important to weigh up these extensions as they offer a lot of value on top of the core tool itself. A good example is that Tekton does not manage test dashboards and results as well as Jenkins and its plugins do, so we might lean on<a id="_idIndexMarker2418"/><a id="_idIndexMarker2419"/> Allure to do <a id="_idIndexMarker2420"/><a id="_idIndexMarker2421"/>this. Reporting and dashboarding extensions are important to make the feedback loop as short as possible during CI/CD.</li>&#13;
			</ul>&#13;
			<p>Once you have <a id="_idIndexMarker2422"/><a id="_idIndexMarker2423"/>considered a few of these ideals, hopefully you will agree on the right set of tools for your product and team. A measure of design and planning is required to answer the question of where the various steps in your continuous deployment happen and what application packaging approach should be used (templated or not templated, for example). By now, we have instilled an experiment-driven approach to answering these types of questions, where it is not one or the other tool, but about choosing the right tool for the job at hand!</p>&#13;
			<h2 id="_idParaDest-371"><a id="_idTextAnchor547"/>Conclusion </h2>&#13;
			<p>In this chapter we introduced how we use Git as the single source of truth. We covered taking our source code and packaging it using either</p>&#13;
			<p>Tekton or Jenkins. In the next chapter, we will focus on testing, introducing a new component to our app using Knative, running A/B tests, and capturing user metrics using some of the advanced deployment capabilities within OpenShift.</p>&#13;
		</div>&#13;
</body></html>