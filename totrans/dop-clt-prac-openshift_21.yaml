- en: 14\. Build It
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"*It works on my machine*"—a phrase heard time and time again by developers,
    testers, and operators as they write, test, and verify their code. *It works on
    my machine* is a phrase rooted in siloed teams where ownership of the problem
    moves around like a tennis ball on a court at Wimbledon. The metaphorical wall
    that exists between teams that have operated in silos, passing work over the wall
    and not taking full responsibility for the end-to-end journey is a problem that
    has been around for decades. We need to break away from this behavior! From now
    on, it''s not, "*It''s working on my machine*" but rather, "*How has your code
    progressed in the build system*?" The build and deployment pipeline our code runs
    through is a shared responsibility. In order for this to be the case, all team
    members must contribute to the pipeline and be ready to fix it when it breaks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.1: It works on my machine'
  prefs: []
  type: TYPE_NORMAL
- en: If your code has failed the build because you forgot to check in a dependency,
    or if your tests are not passing, then it is your responsibility to fix it! The
    purpose of the deployment pipeline is to create a repeatable process that our
    code will pass through and accelerate releases while also de-risking them. If
    we know that on every commit to a repository all of our tests are executed, we're
    going to have a large amount of confidence that the resulting application will
    work well. If we're continuously increasing the test volume as the application's
    complexity increases, that too should grow our confidence. It's critical for teams
    to want to own their software pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Having a platform such as OpenShift is a bit like the Beatles singing on the
    rooftop of the Apple Corps building on Saville Row about people *coming together*.
    Developers, operations, testers, architects, designers, database administrators,
    and analysts, everyone coming together and using a platform like OpenShift provides
    a shared space to collaborate upon. Building applications and business services
    on the platform where developers can self-service all of their requirements in
    a safe, access-controlled manner, bringing down the walls between teams, and removing
    bottlenecks such as having to wait for permissions to deploy an application—this
    gets everyone speaking the same language to deliver business outcomes through
    modern application delivery and technological solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section of the book will be one of the most technical. As described in
    the *Appendix*, the minimum requirements for running the code examples using **CodeReady
    Containers** (**CRCs**) in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_Table_14.1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 14.1: Minimum requirements for running code examples using CRCs'
  prefs: []
  type: TYPE_NORMAL
- en: With the amount of memory required to follow, and the technical content out
    of the way, let's dive into things in more detail. We'll start by looking over
    the components of the existing PetBattle applications as they move from a hobby
    weekend project into a highly available, production-based setup that is built
    and maintained by a strong, cross-functional team.
  prefs: []
  type: TYPE_NORMAL
- en: Existing PetBattle Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The initial PetBattle architecture was pretty basic and revolved around deploying
    application components running in a single **virtual machine** (**VM**). The initial
    architecture had three major components: a JavaScript frontend; a Java-based backend,
    providing an API and a database; and a single instance of MongoDB. Nothing here
    was too exciting or complex but hidden inside was a minefield of technical debt
    and poor implementation that caused all sorts of problems when the site usage
    took off.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The issues with this architecture seemed to include:'
  prefs: []
  type: TYPE_NORMAL
- en: A monolith—Everything had to be deployed and scaled as one unit, there were
    no independent moving parts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authentication and access control were non-existent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tests? Unit tests? But seriously, there weren't many.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It required a lot of data maintenance as everything was stored in the database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bad actors adding inappropriate images to our family-friendly cat application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fragile application—If something went wrong, the application would crash and
    everything had to be restarted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Back in *Chapter 9*, *Discovering the How*, we went through an Event Storming
    exercise that helped drive a newly proposed architecture. It consisted of a UI
    component and a backing service that provided different REST-based APIs to the
    UI, as shown in *Figure 14.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.2: PetBattle''s initial hobbyist architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Let's now take a look at the individual PetBattle components.
  prefs: []
  type: TYPE_NORMAL
- en: PetBattle Components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the coming chapters, we will explore automation, testing, and the extension
    of PetBattle to include aspects such as monitoring and alerting, Knative Serving,
    and Service Mesh. But first, let's imagine that the PetBattle team has completed
    a few sprints of development. They have been building assets from the Event Storms
    and now have the components and architecture as seen in *Figure 14.3*. Through
    the Event Storm, we also identified a need for authentication to manage users.
    The tool of choice for that aspect was Keycloak.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.3: PetBattle''s evolving architecture'
  prefs: []
  type: TYPE_NORMAL
- en: PetBattle's architecture is now increasing in complexity. It has a UI that connects
    to two services to provide it with data. **Single Sign-On** (**SSO**) and user
    management are provided by Keycloak. Let's take a look at each component of the
    architecture in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: User Interface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The UI is written in Angular[1](#footnote-093) v12, a complete JavaScript framework
    from Google for building web and mobile applications. The application is transpiled
    and the static site code is then served from a container running Nginx (a webserver)
    instance provided by Red Hat. The application is set up to pull its configuration
    on startup, which sets up endpoints for all of the dependent services, such as
    Keycloak and the APIs. This configuration is managed as a ConfigMap in OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: Pet Service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Pet service is a straightforward service that uses Java Quarkus[2](#footnote-092)
    as the framework, backed by a MongoDB database to retrieve and store details of
    the pets uploaded to partake in a tournament.
  prefs: []
  type: TYPE_NORMAL
- en: Tournament Service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Tournament service also uses the Quarkus framework and stores the state
    in both MongoDB and an Infinispan distributed cache. MongoDB is used to store
    the details of the tournament such as which pet won the tournament—but why did
    we use a cache?
  prefs: []
  type: TYPE_NORMAL
- en: Well, the answer is that a tournament only exists for a finite period of time
    and using a database to store temporal data is not a great fit for our use case.
    Also, Infinispan stores the cache data in memory, which is much faster to access
    than data on disk. The drawback of this is that if the Infinispan pod dies/crashes,
    then the data is lost. However, we plan to circumvent this in production by having
    at least two replicas, with the data being replicated between the pods.
  prefs: []
  type: TYPE_NORMAL
- en: User Management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: User management, authentication, and access control are a few other critical
    parts of the architecture that need to be addressed. We're using Keycloak,[3](#footnote-091)
    an open source identity and access management tool, to provide this functionality.
    We could have written some code ourselves for this functionality, but security
    is an area that requires a lot of expertise to get it right, and Keycloak does
    a great job of using open standards to do this job correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '[1](#footnote-093-backlink) [https://angular.io/](https://angular.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2](#footnote-092-backlink) [https://quarkus.io/](https://quarkus.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3](#footnote-091-backlink) [https://www.keycloak.org/](https://www.keycloak.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: Plan of Attack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Initially, we are going to get the core PetBattle application components and
    services up and running on OpenShift in a fairly manual way. We want to be able
    to develop locally, adding new functionality to show how easy it is to combine
    Helm and OpenShift to repeatedly deploy our code. Once that is completed, we are
    going to automate the setup and deployment process using various tools, including
    Tekton/Jenkins, Argo CD, and GitOps. We will explore how to add new components
    to our architecture using Knative and experiment with some of the more advanced
    deployment capabilities that we can utilize. Finally, in *Chapter 16*, *Own It*,
    we will look at application monitoring and alerting along with Service Mesh for
    traceability. *Figure 14.4* shows the additional components added to the architecture,
    including the Knative Cat Detector Service being proxied via the Service Mesh.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.4: PetBattle''s target architecture, including final OpenShift deployment'
  prefs: []
  type: TYPE_NORMAL
- en: We will be using the command line as much as possible to show and explain the
    commands involved. Each step can also be performed via the OpenShift web console.
    If you're new to OpenShift, the web console is a great place to get started as
    it's full of tips and tutorials!
  prefs: []
  type: TYPE_NORMAL
- en: Running PetBattle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In *Chapter 6*, *Open Technical Practices* – *Beginnings*, *Starting Right,*
    we talked about Helm and its use as an application lifecycle manager for installing,
    upgrading, and rolling back application deployments. We are going to start with
    the command line, but you can skip to the end of this section if you would like
    to follow the web console method. If you need help installing the Helm command-line
    tool, take a look at *Chapter 6* as a refresher. Now let''s see how we can easily
    deploy the PetBattle suite of applications as Helm charts into a single project
    on OpenShift. On your terminal, add the PetBattle Helm repositories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'There are three main applications that make up PetBattle and are searchable
    in the Helm repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_Table_14.2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 14.2: The three main applications making up PetBattle'
  prefs: []
  type: TYPE_NORMAL
- en: The infrastructure Helm chart is normally deployed as a dependency of the Tournament
    Helm chart but can optionally be deployed alone. This can be useful for debugging
    purposes. The **Not Safe For Families** (**NSFF**) component is an optional chart,
    adding a feature whereby the API checks uploaded images for safe content for our
    family-friendly application.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_Table_14.3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 14.3: The infrastructure and NSFF Helm charts'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can search for the latest versions of these charts using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now deploy the main PetBattle application into our OpenShift cluster.
    We need to update a local copy of the PetBattle frontend''s Helm `values.yaml`
    file to match our cluster URLs. This is needed to connect the frontend when deployed
    to the correct collection of backend services. We can provide these values to
    our Helm charts when deploying the suite of PetBattle applications. Let''s download
    an example of the `values.yaml` file for us to edit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Open the `values.yaml` file and replace the five URLs listed in the `config_map`
    to match your OpenShift cluster (change the `apps.cluster.com` domain to `apps-crc.testing`,
    for example, if you are using a CRC). For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Gather the `pet-battle`, `pet-battle-api`, and `pet-battle-tournament` into
    your cluster. To do this, you will need to be logged in to your OpenShift cluster.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: If the `pet-battle-tournament` install times out, just run it again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each Helm install chart command should return a message similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `helm list` should give you a list of the installed charts. You should
    see the following pods running in your `petbattle` project. An example is shown
    in *Figure 14.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.5: PetBattle pods'
  prefs: []
  type: TYPE_NORMAL
- en: The Tournament service will take several minutes to deploy and stabilize. This
    is because its dependent infrastructure chart is deploying operator subscriptions
    for Keycloak, Infinispan, and Grafana. Navigate to the OpenShift web console and
    you should now be able to explore the PetBattle application suite as shown in
    *Figure 14.6*. Browse to the PetBattle frontend to play with the applications.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.6: PetBattle Helm charts deployed in the OpenShift Developer view'
  prefs: []
  type: TYPE_NORMAL
- en: 'You have now been shown how to install PetBattle Helm charts using the command
    line—some may say the hard way! We are now going to demonstrate some of the integrated
    features of Helm in OpenShift—some may say the easier way! We can create a `HelmChartRepository`
    Custom Resource object that points to our PetBattle Helm chart repository; think
    of it as `helm repo add` for OpenShift. Run this command to install the chart
    repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'With this in place, we can browse to the Developer view in OpenShift and select
    Add Helm Charts, and see a menu and a form-driven approach to installing our Helm
    charts—just select a chart and install it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.7: Adding PetBattle via the HelmChartRepository view in OpenShift'
  prefs: []
  type: TYPE_NORMAL
- en: This can provide a great developer experience for teams sharing services with
    each other. A backend team can produce a new app to the repository and a downstream
    team can deploy it to their developer environment in a single click. In fact,
    if you add a Helm values schema file, OpenShift will build a **What You See Is
    What You Get** (**WYSIWYG**) form for easy configuration of the values file.
  prefs: []
  type: TYPE_NORMAL
- en: Argo CD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we established our foundations in *Section 2*, *Establishing the Foundation*,
    we bootstrapped all of our builds, deployment, and tooling using Helm and Argo
    CD. We made some opinionated choices when running that bootstrap automation and
    it's worth discussing some of the trade-offs we made in a bit more detail. We
    followed our call to action when establishing our technical foundation and planned
    out what worked for us as the PetBattle product team and reviewed and discussed
    what was working and not working so well.
  prefs: []
  type: TYPE_NORMAL
- en: It turned out that for our development team, bootstrapping all of the CI/CD
    tools became an extremely important task. We had been given an arbitrary (but
    necessary) constraint that our development OpenShift cluster needed to be rebuilt
    from scratch every two weeks. So we needed to be confident that our CI and CD
    could stand up quickly and repeatedly. By following our everything-as-code practice,
    all of our OpenShift infrastructure definitions, CI/CD tooling, and pipeline definitions
    are stored in Git. The declared CI/CD tooling state is continuously synced to
    our development cluster by Argo CD, so updating the SonarQube Helm chart version,
    for example, is as easy as changing one line and pushing it to Git. The change
    is synchronized and rolled out to our cluster a minute later.
  prefs: []
  type: TYPE_NORMAL
- en: Being able to effectively lifecycle-manage all of the supporting tools involved
    with building your applications takes effort and attention to detail, but it is
    worth it in the long run, as you will have built a system that can handle change
    easily and repeatedly. We have optimized our application lifecycle around the
    cost of change, making the cost (in man hours) as small as possible. Human time
    is our biggest resource cost after all!
  prefs: []
  type: TYPE_NORMAL
- en: The versions of all our tooling are checked into Git using `MAJOR.MINOR` versions
    are tags that move with small bug fixes and security patches. `MAJOR.MINOR.PATCH`
    versions are not tags and yet they specify a fixed version (ideally!). Choose
    a strategy that does not incur too much technical debt that strands the team on
    old and unsupported versions forever. This is balanced with not having to constantly
    update version numbers all the time. Of course, if you have optimized for a small
    cost of change through automation, this problem of changing versions becomes much
    less of an issue!
  prefs: []
  type: TYPE_NORMAL
- en: '[4](#footnote-090-backlink) [https://semver.org/](https://semver.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: We have chosen a *push* (CI) and *pull* (CD) model for our software delivery
    lifecycle. The job of building images and artifacts (Helm charts and configuration),
    as well as unit and integration testing, is part of a *push* CI model. On every
    code commit, a build pipeline trigger (Tekton or Jenkins) fires. It is the job
    of the Argo CD controller to keep what we have deployed in our OpenShift cluster
    in sync with the declared application state in our Git repositories. This is a
    GitOps pull model for CI. The key thing here is that Git is the single source
    of truth and everything can be recreated from this source.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.8: A GitOps push and pull model for continuous delivery'
  prefs: []
  type: TYPE_NORMAL
- en: The main benefit we see with this approach is that it is developer-centric.
    Any change in the code base triggers a pipeline build and deployment. This gives
    the team fast feedback for any breakages, since automated tests are always run
    against the new code. The pull CD model decouples the synchronous nature of a
    build and testing pipeline. Built artifacts (container images and configuration)
    can be built once, then tagged and promoted through a lifecycle, all of which
    is controlled from Git. This is great for auditability and discovering who changed
    what and when. We can easily trace code committed and pushed with builds, tests,
    and deployments. It is also a flexible approach in that not all artifacts need
    to be built per se. Configuration can be changed and deployed using the same model.
    The model is also very flexible in its ability to support different development
    workflow models. For example, Gitflow and Trunk-based development can easily be
    catered for, depending on how the team chooses to work.
  prefs: []
  type: TYPE_NORMAL
- en: Trunk-Based Development and Environments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When designing our initial pipelines, we mapped out the basic build, bake, deploy,
    integration testing, tag, and promotion stages. In *Figure 14.9*, we can see the
    MultiBranchPipeline Plugin, branches, and namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.9: Branches and namespaces'
  prefs: []
  type: TYPE_NORMAL
- en: This helped us to clarify where the responsibilities lie between our Git branches,
    our continuous integration tasks, continuous delivery tasks, and which OpenShift
    projects these would occur in.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.10: Branches and activities modeling'
  prefs: []
  type: TYPE_NORMAL
- en: Because we are following trunk-based development,[5](#footnote-089) our main/master
    branch is built, tagged, and promoted through the full lifecycle, that is, images
    are built, unit and functionally tested, and deployed into `labs-test` with end-to-end
    testing prior to deployment within the `labs-staging` project. For any short-lived
    feature branches or pull requests, we decided to only unit test, build, and deploy
    these sources into our `labs-dev` OpenShift project. That way we can customize
    which pipeline tasks happen on specific code branches. There is a trade-off between
    the time and resources used for every code commit in our pipelines, and this must
    be adjusted according to what goes into our pipelines to help improve the overall
    product quality.
  prefs: []
  type: TYPE_NORMAL
- en: '[5](#footnote-089-backlink) [https://trunkbaseddevelopment.com](https://trunkbaseddevelopment.com)'
  prefs: []
  type: TYPE_NORMAL
- en: The Anatomy of the App-of-Apps Pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We choose to use Helm; remember, at its most basic, Helm is just templating
    language for packaging our Kubernetes-based application resources. Each PetBattle
    application has its own Git repository and Helm chart, making it easier to code
    independently of other apps. This inner *Helm chart per application* box is depicted
    in *Figure 14.11*. A developer can get the same experience and end result installing
    an application chart using a `helm install` as our fully automated pipeline. This
    is important from a useability perspective. Argo CD has great support for all
    sorts of packaging formats that suit Kubernetes deployments, Kustomize, Helm,
    as well as just raw YAML files. Because Helm is a templating language, we can
    mutate the Helm chart templates and their generated Kubernetes objects with various
    values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.11: Application packaging, Helm, and Argo CD with the app-of-apps
    pattern'
  prefs: []
  type: TYPE_NORMAL
- en: One strict view of GitOps is that mutating a state is not as *pure* as just
    checking in the filled-in templates with the values themselves. Kustomize, for
    example, has no templating and follows this approach. We use Kustomize for deploying
    our CI/CD automation with Argo CD because we think it fits that use case better.
    This means that we are less likely to have a large number of CI/CD environments
    for our PetBattle product—at the moment there is just one.
  prefs: []
  type: TYPE_NORMAL
- en: 'The trade-off here is that while we use GitOps to synchronize the Helm chart
    itself, the supply of application values may come from multiple places, so you
    have to be careful to understand where overriding values and precedence occurs,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`values.yaml` provided with the chart (or its sub chart dependencies)—these
    are kept in sync by the Argo CD controller.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`helm template --set` on the command line. These can be specified in a template
    or a trigger, depending on how the pipeline is run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We deploy each of our applications using an Argo CD application definition.
    We use one Argo CD application definition for every environment in which we wish
    to deploy the application. This is the red box depicted in *Figure 14.11*. We
    make use of Argo CD with the app-of-apps pattern[6](#footnote-088) to bundle these
    all up; some might call this an application suite! In PetBattle we generate the
    app-of-apps definitions using a Helm chart. This is the third, outer green box
    in *Figure 14.11*. The configuration for this outer box is kept in a separate
    Git repository to our application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The app-of-apps pattern is where we declaratively specify one Argo CD app that
    consists only of other apps. In our case, this is the `pet-battle-suite` application.
    We have chosen to put all of our applications that are built from the main/master
    under this `pet-battle-suite` umbrella. We have a PetBattle suite for *testing*
    and *stage* environments. *Figure 14.12* shows the app-of-apps for the stage environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.12: Argo CD, a deployed application suite'
  prefs: []
  type: TYPE_NORMAL
- en: '[6](#footnote-088-backlink) [https://argoproj.github.io/argo-cd/operator-manual/cluster-bootstrapping/#app-of-apps-pattern](https://argoproj.github.io/argo-cd/operator-manual/cluster-bootstrapping/#app-of-apps-pattern)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Git, we model different branches using the following patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`labs-test`, `labs-staging`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labs-dev` namespace only'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Argo CD sync policy for our applications is set to *automated* + *prune*,
    so that child apps are automatically created, synced, and deleted when the manifest
    is changed. You can change or disable this if you need to. We also configure a
    webhook against the CI/CD Git repository so that any changes trigger Argo CD to
    sync all applications; this avoids having to wait for the three-minute sync cycle
    when CI/CD code changes.
  prefs: []
  type: TYPE_NORMAL
- en: The Git revision can be set to a specific Git commit **Secure Hash Algorithm**
    (**SHA**) for each child application. A Git SHA is a unique 40-character code
    computed for every commit to the repository, and is therefore not movable, unlike
    a tag. This ensures that even if the child app's repository changes, the app will
    only change when the parent app changes that revision. Alternatively, you can
    set it to HEAD/master/main or a branch name to keep in sync with that particular
    branch. It's a good idea to use Git commit SHAs to manage your application versions
    the closer you are to production environments. Pinning to an exact version for
    production ensures easier traceability when things go wrong. The structure here
    is flexible to suit your product team's needs.
  prefs: []
  type: TYPE_NORMAL
- en: Build It – CI/CD for PetBattle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's get our hands dirty now by getting down into the weeds with some more
    techie stuff. How do we go from code to running in production in a repeatable
    and safe way? If you remember all the way back in *Section 2*, *Establishing the
    Foundation*, we learned about Derek the DevOps dinosaur and the obstacles we put
    him through to test his fearsomeness. We will now do the same thing with our PetBattle
    apps, beginning with the frontend.
  prefs: []
  type: TYPE_NORMAL
- en: The Big Picture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we touch a line of code, we always like to keep an eye on the Big Picture.
    This helps us frame what the tool is and why we are using it, and to scaffold
    out our pipeline from a very high level. Let's build the details of how we manage
    the PetBattle source code in this same way. As a subtle reminder, the PetBattle
    frontend is an Angular application. It was built using Node.js v12 and is deployed
    to a Red Hat Nginx image.
  prefs: []
  type: TYPE_NORMAL
- en: Continuing our Big Picture from the foundation, let's add the steps we will
    consider implementing for the PetBattle frontend so we can get it ready for a
    high frequency of change being pushed through it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.13: The Big Picture including the tools the team thinks they will
    use'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a quick reminder, our Big Picture from *Section 2*, *Establishing the Foundation*,
    identified all the tools we might use as depicted in *Figure 14.13*, which include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Jenkins: To automate the building and testing of our software'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nexus: To host our binaries and Helm charts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Argo CD: To manage our deployments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SonarQube: To assess our code quality'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zalenium: For automated browser testing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that the tools are in place, let's think about the stages our code should
    move through for being deployed. A team should start small—what is the minimum
    amount of automation we need to get code compiling and deployed? It's very important
    for teams to start small with a basic end-to-end flow of their code; otherwise,
    things become messy quite quickly, leading to unnecessary complexity and potentially
    not delivering anything. It's also important because the feedback loop we are
    creating needs to be fast. We don't want a brilliant, complex process that thinks
    of everything but takes hours to run! It's not the kind of feedback loop we're
    trying to create.
  prefs: []
  type: TYPE_NORMAL
- en: 'We always use three simple stages: Build > Bake > Deploy. A good pattern for
    engineers to take is to keep to an abstract definition of their pipeline so they
    can get greater reuse of the pattern across any of their apps, irrespective of
    the technology they use. Each stage should have a well-defined interface with
    input and output. Reusing a pipeline definition in this way can lower the context
    switch when moving between backend and frontend. With this in mind, we can define
    the stages of our build in the following manner.'
  prefs: []
  type: TYPE_NORMAL
- en: The Build
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Input*: *The code base*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Output*: *A "compiled" and unit-tested software artifact*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/figure-14-14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.14: The BUILD component from the Big Picture'
  prefs: []
  type: TYPE_NORMAL
- en: The build should always take our source code, compile it, and run some unit
    tests before producing some kind of artifact that will be stored in Nexus. By
    defining the interface of the build process as lightly as this, it means we can
    substitute the implementation of what that looks like in each technology or application
    type. So, for example, when building the PetBattle frontend Angular app, we will
    use the **Node Package Manager** (**npm**) to complete these steps within the
    Build stage, but a Java application would likely use Gradle or Maven to achieve
    the same effect. The hardest work will happen in this stage and it is usually
    what has the highest dependency on the framework or language that's being used.
    We will see this in later stages; the technology originally being used becomes
    less important, so higher reuse of code can occur.
  prefs: []
  type: TYPE_NORMAL
- en: The Bake
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Input*: *A "compiled" software artifact*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Output*: *A tagged* *container image*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/figure-14-15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.15: The BAKE component from the Big Picture'
  prefs: []
  type: TYPE_NORMAL
- en: This is the act of taking our software artifact that was created as an output
    in the previous step and packaging it into a box, that is, a Linux Container Image.
    This image is then tagged and stored in a container registry, either one built
    into OpenShift or an external one. In OpenShift there are many different ways
    we can achieve this, such as using source-2-image, binary build, or providing
    a Containerfile/Dockerfile.
  prefs: []
  type: TYPE_NORMAL
- en: The Deploy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Input*: *A tagged image*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Output*: *A running app in a given environment*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/figure-14-16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.16: The DEPLOY component from the Big Picture'
  prefs: []
  type: TYPE_NORMAL
- en: Take the image that has just been pushed to the registry and deploy it along
    with any other services or configuration required for it to run. Our applications
    will be packaged as Helm charts, so the deployment will likely have to patch the
    image referenced in our app's chart. We want our pipeline to support multiple
    workflows. For feature development, we can just `helm install` into the development
    namespace. But for release candidates, we should be committing new release information
    to Git for it to trigger the rollout of changes. The implementation of this workflow
    is the responsibility of the steps, the lower level of what is being executed.
    The abstract view of a Deploy should result in a *verified* app deployed on our
    cluster (and ultimately promoted all the way to production).
  prefs: []
  type: TYPE_NORMAL
- en: The team captures these stages for the applications they're building by adding
    some nice doodles to their Big Picture. Next, they begin thinking about promoting
    the application across the environment from test to production. When building
    applications in containers, we want to ensure the app can run in any environment,
    so controlling application configuration separately is vital. The team will not
    want to rebuild the application to target different environments either, so once
    an image is baked and deployed it needs to be verified before promotion. Let's
    explore these stages further.
  prefs: []
  type: TYPE_NORMAL
- en: System Test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Input*: *The app name and version under test*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Output*: *A successful test* *report and verified app*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/figure-14-17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.17: The SYSTEM TEST component from the Big Picture'
  prefs: []
  type: TYPE_NORMAL
- en: Drive the user behavior within the application via the frontend by verifying
    whether the app is behaving as expected. If all the connected parts that make
    up the application are behaving as expected (the microservices, authentication,
    and frontend) then the app can be signed off and will not need to be rebuilt.
    Our system test cases for PetBattle will be the acceptance criteria the team has
    agreed upon. Because of this, we can sign off the application as ready for real-world
    users. Any component that has changed in the stack should trigger this stage;
    it is not just the responsibility of the frontend.
  prefs: []
  type: TYPE_NORMAL
- en: Promote
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Input*: *A verified image name and version*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Output*: *Running app in production environment*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/figure-14-18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.18: The PROMOTE component from the Big Picture'
  prefs: []
  type: TYPE_NORMAL
- en: With the application working as expected (based on our passing system test cases),
    we can now promote the images that make up our app to the new environment, along
    with their configuration. Of course, in the world of GitOps, this is not a manual
    rollout of a new deployment but committing the new version and any custom configuration
    to our configuration repositories, where they will be picked up by Argo CD and
    deployed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/figure-14-19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.19: The Big Picture including all the stages of the pipeline in place'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 14.19*, we can see the Big Picture with the stages of the pipeline
    drawn in. Now that the team knows the stages that their software will pass through
    on the way across the cluster, they can fill in the lower-level details, the steps.
    At this stage, the team is looking to see how they can build common pipeline steps,
    irrespective of the technology they're using. This will provide greater reuse
    across their software stack but, more importantly, reduce the cognitive load for
    engineers writing software in multiple technologies. For this, it's a good idea
    to put on the Big Picture the technology being used. In PetBattle's case, it is
    Angular and Quarkus (Node.js and Maven for the build tools). They use a new color
    sticky to write the steps that each service will go through in order to fulfill
    the interface defined at each stage.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 14.20*, we detail what these steps could look like for the Build
    stage of our pipeline. First, we install the application dependencies. Following
    this, we test, lint, and compile the code. Finally, we store the successful artifacts
    in the Nexus repository to use in the next stage, the Bake.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.20: The Build stage and the breakdown of its steps'
  prefs: []
  type: TYPE_NORMAL
- en: 'The team continues to flesh out the steps across all the stages. Finally, they
    add some example containers deployed to each namespace at each stage to give a
    view of all the components deployed for the PetBattle system to work. This is
    detailed in *Figure 14.21*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/figure-14-21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.21: The complete Big Picture for our software delivery process'
  prefs: []
  type: TYPE_NORMAL
- en: The Big Picture is a helpful practice for getting team alignment on what's in
    our toolchain and how we use it. It can be a great thing to play back to non-technical
    team members too, giving them an idea of the complexity and usefulness of being
    able to repeatedly build and test code. As with all our practices, it's also never
    done; when a new tool enters our toolchain or we add a new stage in our pipeline,
    we add it to the Big Picture first. It is the living and breathing documentation
    of our software delivery process. With the Big Picture complete for now, let's
    move on to implementing the components it describes.
  prefs: []
  type: TYPE_NORMAL
- en: Choose Your Own Adventure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We understand that there are many ways to do anything and in software development,
    there are usually hundreds or even more. With this in mind, we hope to meet you,
    dear reader, where you are now. By this, we mean that the next section will identify
    two ways of doing the same thing, so take the approach that better fits your own
    context.
  prefs: []
  type: TYPE_NORMAL
- en: Jenkins is the build tool of choice for lots of companies and developers alike.
    It has been around for some time and it has its set of quirks for sure. It was
    never intended to be deployed as a container when it was first conceived. In order
    to keep things current and have an eye on the future, we've decided to write the
    code for the Big Picture using both Tekton and Jenkins. Both can easily be tweaked
    for both frontend and backend development, but for the purposes of this book we
    will use Jenkins to automate the parts of the Big Picture for our Angular application.
    For the API, written in Java (Quarkus), we will use Tekton, and go through setting
    up the same things in a more Kubernetes native way. Both paths are available for
    the avid reader to play with and get working, but we'll split the narrative this
    way for illustrative purposes.
  prefs: []
  type: TYPE_NORMAL
- en: So, like you would in a *choose your own adventure* book, you can now pick the
    path that you would like to follow next. If you're not interested in Jenkins automation,
    then skip ahead to the Tekton section directly. The code for both options is available
    in the Git repositories for the book.
  prefs: []
  type: TYPE_NORMAL
- en: Before attempting the pieces in this chapter, make sure to have completed the
    bootstrap steps in *Chapter 7,* *Open Technical Practices –* *The Midpoint*, under
    the *Implementing GitOps – Let's Build the Big Picture With Some Real Working
    Code!* section. These steps deploy the CI/CD tooling into your cluster using GitOps.
    The main tools we are going to use in the next sections include Jenkins, Argo
    CD, and Tekton.
  prefs: []
  type: TYPE_NORMAL
- en: Jenkins–The Frontend
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Jenkins is our trusty friend who will do the hard crunching of code—compiling,
    testing, and so on—on our behalf. In order to get the best out of all the tools
    in our kit bag, there are a few items we need to configure first. This includes,
    among other things, managing secrets and adding webhooks to trigger our Jenkins
    automation as soon as a developer commits their code.
  prefs: []
  type: TYPE_NORMAL
- en: Connect Argo CD to Git
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's talk about GitOps. We want our Git repositories to be the single source
    of truth and the Argo CD controller to analyze the differences between what is
    currently deployed to our cluster and what is stored in our Git repositories.
    Argo CD can do things based on the difference it sees between the desired state
    (in Git) and the actual state (in the cluster) such as automatically synchronizing
    them or sending a notification to say that these two states are not as expected.
    For example, in Git we may have set version 123 of our application but the cluster
    currently has version 122 deployed.
  prefs: []
  type: TYPE_NORMAL
- en: To create this connectivity between our configuration repository and Argo CD,
    we need to create an Argo CD app-of-apps to point to the repository. The app-of-apps
    pattern is a neat way to describe all elements of a system. Imagine we have an
    app, named `App-1`, which is our full system. This `App-1` is made up of independently
    deployable services such as `App-1a`, `App-1b`, `App-1c`, and so on. For PetBattle,
    we have the whole system that is all of our frontend, APIs, and other services.
    We also have one of these for our staging and test environments; this allows us
    to think of our app-of-apps as a suite of applications.
  prefs: []
  type: TYPE_NORMAL
- en: If we clone the `ubiquitous-journey`[7](#footnote-087) project that we set up
    in *Chapter 7*, *Open Technical Practices* – *The Midpoint,* to bootstrap our
    cluster, there is another set of charts in here for our application stacks located
    in *applications*/*deployments*. When applied, these definitions will create our
    Argo CD application Custom Resource pointing to our Helm charts that will be created
    by the running builds on either Tekton or Jenkins.
  prefs: []
  type: TYPE_NORMAL
- en: The values files (`values-applications-stage.yaml`) contain the Helm chart version
    and application version that will be updated by Jenkins on successful builds.
    We want Argo CD to monitor these values when applying changes to the cluster.
    These values files also contain our overrides to the base Helm chart for specific
    environments, for example, the config map that the frontend is configured with
    to communicate with the services it requires to work properly (`tournament-svc`,
    `cats-svc`, and so on). The following snippet shows the definition of this. These
    values will differ between development, testing, and staging, so this pattern
    gives us the ability to version control the configuration we want the application
    to use on startup.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[7](#footnote-087-backlink) [https://github.com/petbattle/ubiquitous-journey](https://github.com/petbattle/ubiquitous-journey)'
  prefs: []
  type: TYPE_NORMAL
- en: So, when we deploy an Argo CD application pointing to this Git repository, it
    will find additional apps and so create our app-of-apps pattern. The structure
    of the repository is trimmed, but you can see that the chart is very basic, having
    just two templates for creating a project in Argo CD and the application definitions
    to put inside the project.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'We could go to the Argo CD UI and connect it to this repository manually, or
    use the Argo CD CLI to create the Argo CD application Custom Resource, but let''s
    just run this handy one-liner to connect things up for both our staging and test
    app-of-apps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: With these in place, we should see Argo CD create the app-of-apps definitions
    in the UI, but it will be unable to sync with the child applications. This is
    because we have not built them yet! Once they are available, Argo CD will kick
    in and sync them up for us.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.22: Argo CD sync of the pet-battle application suite for the staging
    environment'
  prefs: []
  type: TYPE_NORMAL
- en: 'To extend this app-of-apps pattern now is very simple. We only need to connect
    Git to Argo CD this one time. If, after the next few Sprints, the PetBattle team
    realizes they need to add a new component or service, they can simply extend the
    values YAML, that is, `values-applications-stage.yaml` or `values-applications-test.yaml`
    for their staging or test environment, with a reference to the new component chart
    location and version. For example, for `cool-new-svc`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: Secrets in Our Pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Jenkins is going to be responsible for compiling our code, pushing the image
    to a registry, and writing values to Git. This means Jenkins is going to need
    some secrets! In our case, we're using Quay.io for hosting our images so Jenkins
    will need the access to be able to push our packaged Container Images to this
    repository, which requires authentication. If you're following along with forks
    of the PetBattle repositories and want to create your own running `pet-battle`
    instance, go ahead and sign up for a free account on [https://quay.io/](https://quay.io/).
    You can log in with GitHub, Google, or your Red Hat account.
  prefs: []
  type: TYPE_NORMAL
- en: Quay.io
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When on Quay, create three new repositories, one for each of the application
    components that we will be building. You can mark them as public, as private repositories
    cost money.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.23: PetBattle images in Quay.io'
  prefs: []
  type: TYPE_NORMAL
- en: These repositories serve as empty image stores for us to push our images to
    from within the pipeline. But we need to provide Jenkins with the correct access
    to be able to push them, so go ahead and hit the robot icon on the UI to create
    a new service account that Jenkins can use. Give it a sensible name and description
    for readability.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.24: Robots in Quay.io'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to mark all the repositories we created previously as Write by
    this robot. Hit Add permissions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.25: Robot RBAC in Quay.io'
  prefs: []
  type: TYPE_NORMAL
- en: Now that the repositories and robot account have been created, we can download
    the secret to be used in our pipelines! Hit the cog on the side of the secret
    name and select View Credentials.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.26: How to view the robot credentials in Quay.io'
  prefs: []
  type: TYPE_NORMAL
- en: On the page that pops up, download the Kubernetes YAML and store it in your
    fork of `pet-battle`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.27: Downloading the Kubernetes secret'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can apply it to our cluster (ensure you are logged in first):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: GitHub
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A secret is also required for Jenkins to be able to push updates to our Helm
    values files stored in Git. The values files for our applications will contain
    the properties we want to pass to our templates, such as ConfigMap variables,
    or locations of images, such as Quay.io. Our values files for the deployment of
    our applications will also hold a reference to the image version (that is, the
    SemVer of our app, such as 1.0.1) to be deployed by patching our DeploymentConfigs.
    We don't want to manually update this but have a robot (Jenkins) update this when
    there has been a successful build. Therefore, this secret will be needed to write
    these changes in versions to our configured repositories, which are being pointed
    out by Argo CD. We track version changes across all our environments in this way
    because, after all, if it's not in Git, it's not real.
  prefs: []
  type: TYPE_NORMAL
- en: To create a secret for GitHub, simply go to the Developer Settings view. While
    logged into GitHub, that's **Settings > Developer Settings > Personal access tokens**
    or [https://github.com/settings/tokens](https://github.com/settings/tokens) for
    the lazy. Create a new **Personal Access Token** (**PAT**); this can be used to
    authenticate and push code to the repository. Give it a sensible name and allow
    it to have repository access.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.28: GitHub personal access token permissions'
  prefs: []
  type: TYPE_NORMAL
- en: 'Save the token''s value, as you won''t be able to access it again without generating
    a new one. With the token in place, we can create a secret in Jenkins by adding
    it to a `basic-auth` secret. In order for Jenkins, which is running in the same
    namespace as where this secret will be created, to be able to consume the value
    of the secret, we can apply a special annotation, `credential.sync.jenkins.openshift.io:
    "true"`. This little piece of magic will allow any credentials to be updated in
    Jenkins by only updating the secret!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Update the secret with your values for `GITHUB_TOKEN` and `GITHUB_USERNAME`
    if you''re following along in your own fork and apply them to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: SealedSecrets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You might be thinking that these secrets should probably be stored somewhere
    safe—and you're right! If you want to explore the idea of storing the secrets
    in Git so they too are *GitOpsy* (yes, I did just invent a word there), then we
    could use SealedSecrets by Bitnami. It provides a controller for encrypting secrets,
    allowing us to store them as plain text. This means we can commit them to Git!
    Through the magic of the SealedSecret Custom Resource, it decrypts the SealedSecret,
    and creates a regular Kubernetes secret on your behalf. We've written our Jenkins
    Helm chart to accept SealedSecrets for this very reason!
  prefs: []
  type: TYPE_NORMAL
- en: You can deploy SealedSecrets to the cluster by enabling it in the Ubiquitous
    Journey Git project. Open up `bootstrap/values-bootstrap.yaml`. It's as simple
    as changing the `enabled` flag to `true` and, of course, Git committing the changes.
    This will resync with Argo CD and create an instance of Bitnami SealedSecrets
    in your cluster, by default in the `labs-ci-cd` namespace. Because this is a new
    component we're adding to our tooling, we should of course also update our Big
    Picture with the tool and a sentence to describe how we use it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the controller has been created, we can seal our secrets by following
    these few steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install `kubeseal` using the instructions found on their GitHub releases page:
    [https://github.com/bitnami-labs/sealed-secrets/releases](https://github.com/bitnami-labs/sealed-secrets/releases).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log in to the cluster where SealedSecrets is deployed and take note of the namespace
    (in our case this defaults to `labs-ci-cd`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Process your existing secret using the `kubeseal` command-line utility. It
    is important to set the correct namespace otherwise the secret will not be unsealed.
    In this case, we''re going to seal it as `super-dooper-secret`. It should look
    something like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can now apply that secret straight to the cluster for validation, but you
    *should* add it to the cluster using Argo CD by committing it to Git. If it''s
    not in Git, it''s not real. Here, we can see what the SealedSecret looks like
    before it''s applied to the cluster. As you can see, it''s a very large, encrypted
    string for each variable we sealed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To *GitOpsify* (yes, again I did just make that up), open up your Jenkins configuration
    in `ubiquitous-journey/values-tooling.yaml`. Set your values on Jenkins `sealed_secrets`
    as follows using the output of the secret generation step to add the encrypted
    information to each key. The example here is trimmed for readability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you've already manually applied the secret in *Step 4*, delete it by running
    `cat /tmp/sealed-super-dooper.yaml | oc delete -f- -n labs-ci-cd`. Then `Git commit`
    these changes so they are available to Jenkins and, more importantly, stored in
    Git. In Argo CD, we should see that the SealedSecret generated a regular secret.![](img/B16297_14_29.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 14.29: SealedSecrets from Argo CD'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In Jenkins, we should see all that the synchronized secrets using the `magic`
    annotation (`credential.sync.jenkins.openshift.io: "true"`) have become available.![](img/B16297_14_30.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 14.30: Jenkins secrets automatically loaded from Kubernetes'
  prefs: []
  type: TYPE_NORMAL
- en: For simplicity here, we will continue without having sealed the secrets; the
    topic of secrets and GitOps has been included only for illustrative purposes.
  prefs: []
  type: TYPE_NORMAL
- en: The Anatomy of a Jenkinsfile
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some of you may be familiar with a `Jenkinsfile`, but for those who are not,
    let's take a look at the anatomy of one. A `Jenkinsfile` is just a simple `Jenkinsfile`.
    It is an *everything-as-code* practice that defines the sequence of things we
    want our pipeline to execute in order.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Jenkinsfile` is made up of a pipeline definition with a collection of
    blocks, as the following is from our PetBattle frontend. If you''re curious as
    to where this file is, you will find it at the root of the project in Git. *Figure
    14.31* is trimmed a little for simplicity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.31: Anatomy of a Jenkinsfile'
  prefs: []
  type: TYPE_NORMAL
- en: 'The key aspects of a `Jenkinsfile` DSL are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pipeline {}` is how all declarative Jenkins pipelines begin.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`environment {}` defines the environment variables to be used across all Build
    stages. Global variables can be defined here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`options {}` contains specific job specs you want to run globally across jobs;
    for example, setting the terminal color or the default timeout.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stages {}` encapsulates the collection of blocks our pipeline will go through,
    that is, the `stage`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stage {}`: All jobs must have at least one stage. This is the logical part
    of the build that will be executed, such as `bake-image`, and contains the steps,
    agents, and other stage-specific configurations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`agent {}` specifies the node that the build should be run on, for example,
    `jenkins-agent-npm`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`steps {}`: Each stage has one or more steps involved. These could be executing
    shell commands, scripts, Git checkout, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`post {}` is used to specify the post-build actions. Jenkins'' declarative
    pipeline syntax provides very useful callbacks for `success`, `failure`, and `always`,
    which are useful for controlling the job flow or processing reports after a command
    is executed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`when {}` is used for flow control. It can be used at the stage level, as well
    as to stop the pipeline from entering that stage; for example, when branch is
    master, deploy to test environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`parallel {}` is used to execute some blocks simultaneously. By default, Jenkins
    executes each stage sequentially. If things can be done in parallel, then they
    should, as it will accelerate the feedback loop for the development team.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For us, we are creating the components in the Big Picture, which are Build >
    Bake > Deploy.
  prefs: []
  type: TYPE_NORMAL
- en: The Build should always take the source code, compile it, run some linting (static
    code checking) and testing before producing a package, and store it in Nexus.
    We should produce test reports and have them interpreted by Jenkins when deciding
    to fail the build or not. We are building an Angular application, but Jenkins
    does not know how to execute `npm` or other JavaScript-based commands, so we need
    to tell it to use the agent that contains the `npm` binary. This is where the
    agents that we bootstrapped to the Jenkins deployment will come in handy. Each
    agent that is built extends the base agent image with the binary we need (that
    is, `npm`) and is pushed to the cluster. This ImageStream is then labeled `role=jenkins-slave`
    to make it automatically discoverable by Jenkins if they are running in the same
    namespace. This means that for us to use this, we just need to configure our Jenkins
    stage to use `agent { label "jenkins-agent-npm" }`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.32: Jenkins agent discovery magic'
  prefs: []
  type: TYPE_NORMAL
- en: The Build stage will use this agent and execute some steps. The first thing
    is to capture the app's version to be used throughout the pipeline by reading
    the app's manifest (`pom.xml` for Java or `package.json` for Node). This version
    is then used on all generated artifacts, including our image and Helm chart version,
    and should follow SemVer (for example, `<major>.<minor>.<patch> = 1.0.1`). We
    will then pull our dependencies, run our tests, lint, and build our code before
    publishing the results to Jenkins and the package to Nexus.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will display in the Jenkins declarative pipeline like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE159]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE162]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE163]'
  prefs: []
  type: TYPE_PRE
- en: 'Our Bake will always take the output of the previous step, in this case, the
    package stored in Nexus, and pop it into a container. In our case, we will be
    running an OpenShift build. This will result in the package being added to the
    base container and pushed to a repository. If we are executing a sandbox build,
    say some new feature on a branch, then we are not concerned with pushing the image
    externally—so we can use the internal registry for OpenShift. If this build is
    a release candidate then we''ll push into Quay.io (our external registry for storing
    images). The breakdown of the steps for a Bake is found in the Git repository
    that accompanies this book: [https://github.com/petbattle/pet-battle/blob/master/Jenkinsfile](https://github.com/petbattle/pet-battle/blob/master/Jenkinsfile).'
  prefs: []
  type: TYPE_NORMAL
- en: From a bird's-eye view, the idea is to get the package from Nexus and then create
    an OpenShift `BuildConfig` with a binary build and pass the package to it. You
    should then see the build execute in the OpenShift cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.33: Jenkins stages outline for Bake and Deploy'
  prefs: []
  type: TYPE_NORMAL
- en: The deployment will take the application that has just been packaged up with
    its dependencies and deploy it to our cluster. Initially, we will push the application
    to our `labs-test` environment. We want to package the application and its Kubernetes
    resources as a Helm chart, so for the deployment we will patch the version of
    the application referenced in the values file with the latest release. For this
    reason, our Deploy stage is broken down into two parts.
  prefs: []
  type: TYPE_NORMAL
- en: The first one patches the Helm chart with the new image information, as well
    as any repository configuration, such as where to find the image we just Baked!
    This is then stored in Nexus, which can be used as a Helm chart repository.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, it will install this Helm chart. Depending on what branch we're on,
    this behavior of how the application will be deployed differs. If we're building
    on `master` or `main`, it is a release candidate, so there is no more `oc` applying
    some configuration—this is GitOps land! Instead, we can commit the latest changes
    to our Argo CD config repository (Ubiquitous Journey). The commits on this repository
    should be mostly automated if we're doing this the right way. Managing our apps
    this way makes rollback easy—all we have to do is Git revert!
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.34: Jenkins automated commit of the new version from a pipeline run'
  prefs: []
  type: TYPE_NORMAL
- en: Branching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our pipeline is designed to work on *multibranch*, creating new pipeline instances
    for every branch that is committed to in Git. It is intended to have slightly
    different behavior on each branch. In our world, anything that gets merged to
    `master` or `main` is deemed to be a release candidate. This means that when a
    developer is ready to merge their code, they would amend the `package.json` version
    (or `pom.xml` version for Java projects) with the new release they want to try
    and get all the way through the pipeline to production. We could automate the
    version management, but because our workflow has always been easier, a developer
    will do this management, as they are best placed to decide whether it's a patch,
    a minor, or a major release.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that anything not on the `main` or `master` branch is deemed to
    be a sandbox execution of the pipeline. If something is a sandbox build, it is
    there to provide fast feedback to the developers of the current state of development
    in that feature. It can also act as a warning to other engineers that something
    is not ready to be merged if it''s failing. The sandbox builds should be thought
    of as ephemeral—we''re not interested in keeping them hanging around—hence we
    make some key changes to the pipeline to accommodate this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Internal registry**: If our built image is pushed to our external repository,
    it will become clogged up and messy with unnecessary images. Every time a developer
    commits to any branch it would create new images, so it can introduce a cleanup
    headache; hence we use the internal registry, which automatically prunes old images
    for us. We only use the external registry when we know a release could go all
    the way to production.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Helm install**: For our deployments, we''re not interested in bringing in
    a heavyweight tool like Argo CD to manage the development/sandbox deployments.
    It''s unnecessary, so we just use Jenkins to execute a Helm install instead. This
    will verify that our app can deploy as we expect. We use Argo CD and GitOps to
    manage the deployments in test and staging environments, but any lower environments
    we should also treat as ephemeral (as we should test and staging too).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This approach allows us to support many different types of Git workflow. We
    can support GitHub Flow, Gitflow, and Trunk, all via the same consistent approach
    to the pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Webhooks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we actually trigger Jenkins to build things for us, it's important to
    add a few webhooks to make our development faster. We need two, one for the Argo
    CD config repo and one for Jenkins in our source code repository.
  prefs: []
  type: TYPE_NORMAL
- en: When we commit a new change to the Git repositories that Argo CD is watching
    for, it polls. The poll time is configurable, but who can be bothered to wait
    at all? Argo CD allows you to configure a webhook to tell it to initiate a sync
    when a change has been made.
  prefs: []
  type: TYPE_NORMAL
- en: This is particularly important if we want things to happen after Argo CD has
    worked its magic, such as in a system test. Our pipeline in Jenkins runs synchronously,
    but Argo CD is asynchronous and therefore anything we can do to reduce the wait
    between these behaviors is critical.
  prefs: []
  type: TYPE_NORMAL
- en: On GitHub, we can configure the webhook for Ubiquitous Journey to trigger Argo
    CD whenever the repository updates. On GitHub, add the webhook with the address
    of our Argo CD server followed by `/api/webhook`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.35: Webhook to trigger Argo CD on Git commit'
  prefs: []
  type: TYPE_NORMAL
- en: Jenkins
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Every time we commit to our source code repository, we want Jenkins to run a
    build. We're using the multibranch plugin for Jenkins, so this means that when
    we commit to the repository, the webhook will trigger a branch scan, which should
    bring back any new feature branches to build pipelines or create builds for any
    new code commits on any branch.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the Jenkins webhook for the `pet-battle` frontend is simple. On
    GitHub's *Hooks* page, add the URL to our Jenkins instance in the following form,
    where the trigger token is the name of our GitHub project. As a convention, I
    tend to use the name of the Git project as the token, so the same would apply
    for the backend if you were building it using Jenkins too:`JENKINS_URL/multibranch-webhook-trigger/invoke?token=[Trigger
    token]`
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the frontend application''s webhook URL would look something like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://jenkins-labs-ci-cd.apps.petbattle.com/multibranch-webhook-trigger/invoke?token=pet-battle](https://jenkins-labs-ci-cd.apps.petbattle.com/multibranch-webhook-trigger/invoke?token=pet-battle
    )'
  prefs: []
  type: TYPE_NORMAL
- en: Bringing It All Together
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have now gone through what a Jenkins file is and what it does for us. We've
    spoken about branching and what we mean for a build to be a release candidate
    (that is, a version bump and on master/main). We've touched on deploying using
    Helm and GitOps to commit our change and have Argo CD roll out the change for
    us…but how do we connect Jenkins up to all this magic?
  prefs: []
  type: TYPE_NORMAL
- en: 'As with all these things, there are several ways. We *could* open up Jenkins
    and hit `seed-multibranch-pipelines` job! Some of you may have noticed that Jenkins
    was configured to point to our organization''s GitHub for PetBattle when we deployed
    the Helm chart from Ubiquitous Journey. We set some environment variables on the
    image (in `ubiquitous-journey/values-tooling.yaml`) to point to our GitHub organization
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE164]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE165]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE167]'
  prefs: []
  type: TYPE_PRE
- en: If you’re following along with a fork of the Ubiquitous Journey and want to
    see the pipeline run end to end, update both ARGOCD_CONFIG_REPO to point to your
    fork and QUAY_ACCOUNT to resolve to your user on Quay.io.
  prefs: []
  type: TYPE_NORMAL
- en: These are used by the `seed-multibranch-pipelines` job that is baked into the
    Jenkins image to scan the organization for repositories that contain a `Jenkinsfile`
    and are not archived. If it finds any, it will automatically scaffold out multibranch
    Jenkins jobs for us. In our case, we have a `Jenkinsfile` for both the Cats API
    and the PetBattle frontend, so jobs are created for us without having to configure
    anything! If you're following along and not using GitHub but GitLab, you can set
    `GITLAB_*` environment variables to achieve the same effect.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.36: Jenkins seed to scaffold out our Jenkins jobs'
  prefs: []
  type: TYPE_NORMAL
- en: If you open Jenkins and drill down into the `pet-battle` folder for the frontend
    code base, you should see builds; for example, a Git branch called `cool-new-cat`
    and the `master` with pipeline executions for each of them. Opening the Blue Ocean
    view, we get a much better understanding of the flow control we built, as previously
    discussed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.37: Jenkins release candidate pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: For the master branch, which we deem to be a release candidate, the artifacts
    that are built could go all the way. If we are updating our application, we bump
    the manifest version along with any changes we're bringing in and Git commit,
    which should trigger the build. From this point, our build environment is configured,
    and the pipeline should execute. We target an external repository and the image
    that's built will be pushed to Quay.io for portability across multiple clusters.
    Our Helm chart's values are patched and pushed to Nexus for storage. If we need
    to update our Helm chart itself, for example, to add some new configuration to
    the chart or add a new Kubernetes resource, we should of course bump the chart
    version too. For our deployment, we patch the Argo CD config repository (Ubiquitous
    Journey) with the new release information, and it should sync automatically for
    us, deploying our application to the `labs-test` namespace! We then run a verify
    step to check that the version being rolled out matches the new version (based
    on the labels) and has been successful.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.38: Jenkins feature development pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: For our feature branches, the idea is much the same, but without the need for
    an external repository. Our charts are also manipulated to override the name to
    include the branch. This means that on each commit to a feature branch, we get
    a new application deployed containing the branch name in the route. So, for our
    `cool-new-cat` branch, the application is deployed as `cool-new-cat-pet-battle`
    and is available in the developmental environment.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining stages that were added to the Big Picture, System Test and Promote,
    will be covered in the next chapter, when we look in more detail at the testing
    for PetBattle.
  prefs: []
  type: TYPE_NORMAL
- en: What's Next for Jenkinsfile
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Jenkins has been around for quite some time. It is not the most container-native
    approach to building software but there is a rich ecosystem surrounding it. It's
    lasted a long time because people like it! Hopefully, this gives you a taste of
    what can be done with Jenkins for our PetBattle applications, but it's by no means
    the end. There are a few plot holes in the story, as some of you may have noticed.
    For example, once a build has been successfully deployed to the test environment,
    how do I promote it onward? Should I do more testing? Well, the answer will come
    in the next chapter when we explore system tests and extend our pipeline further
    to include promoting images. At the end of a successful pipeline execution, the
    values file in our repository is not updated; we should be thinking about writing
    the successful build artifact details back to the repository, so it's always got
    a sensible default set to what is currently deployed.
  prefs: []
  type: TYPE_NORMAL
- en: The stages we have written here are fairly massive and do have some `bash` and
    other logic inside of them. If you were to build a non-frontend application, for
    example, you would want to build something in Golang. For the most part, the only
    thing that needs to change is the Build stage, as the act of putting something
    in a box, and how we package our Helm chart and deploy the app, remains the same.
    Once the app, in any language or framework, is in a container, then how we ship
    it remains the same. This means there is a high potential for reusing the code
    in the Bake and Deploy stages again and again, thus lowering the bar for adopting
    new technologies on a platform such as OpenShift. But be careful – copying and
    pasting the same steps across many jobs in a large estate of apps can lead to
    one mistake being copied around. Changes to the pipeline can become costly too,
    as you have to update each `Jenkinsfile` in each repository.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.39: A lurking dragon, watch out!'
  prefs: []
  type: TYPE_NORMAL
- en: Jenkins does tackle these problems with the use of shared libraries, and more
    recently the **Jenkins Templating Engine** (**JTE**). The JTE tackles the problem
    by enforcing pipeline approaches from a governance point of view. While this might
    seem like a great way to standardize across an enterprise—here be dragons!
  prefs: []
  type: TYPE_NORMAL
- en: Applying a standard pipeline without justification or the ability for teams
    to pull requests and make changes for their own specific use case is the same
    as having Dev and Ops in separate rooms. We've worked with plenty of customers
    who have tried approaches like this and ultimately it makes them go slower, rather
    than faster. The teams putting the pipelines in place think they're helping and
    providing a great service, but when things go wrong, they are the bottleneck to
    fixing it. For some teams, the hammer approach might not be applicable for their
    use case and so the pipeline becomes something in the way for them to go faster.
  prefs: []
  type: TYPE_NORMAL
- en: Tekton is another way for us to get greater pipeline reusability and also honor
    more of our GitOps landscape. Let's explore it now for our Java microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Tekton–The Backend
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tekton[8](#footnote-086) is an open source cloud-native CI/CD tool that forms
    the basis for OpenShift Pipelines.[9](#footnote-085)
  prefs: []
  type: TYPE_NORMAL
- en: Tekton Basics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many similarities between what Jenkins does and what Tekton does.
    For example, both can be used to store pipeline definitions as code in a Git repository.
    Tekton is deployed as an operator in our cluster and allows users to define in
    YAML `Pipeline` and `Task` definitions. Tekton Hub[10](#footnote-084) is a repository
    for sharing these YAML resources among the community, giving great reusability
    to standard workflows.
  prefs: []
  type: TYPE_NORMAL
- en: '[8](#footnote-086-backlink) [https://tekton.dev](https://tekton.dev)'
  prefs: []
  type: TYPE_NORMAL
- en: '[9](#footnote-085-backlink) [https://docs.openshift.com/container-platform/4.7/cicd/pipelines/understanding-openshift-pipelines.html](https://docs.openshift.com/container-platform/4.7/cicd/pipelines/understanding-openshift-pipelines.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[10](#footnote-084-backlink) [https://hub.tekton.dev](https://hub.tekton.dev)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.40: Tekton Hub and OpenShift Cluster tasks'
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift also makes these available globally as `ClusterTasks`. To write a
    pipeline you can wire together these task definitions. OpenShift provides a guided
    `Pipeline` builder UI for just this task. You link various tasks together and
    define parameters and outputs as specified in each task definition.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.41: OpenShift Pipeline builder UI'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are numerous task activities in our pipeline definitions that require
    persistent storage. When building our backend PetBattle API and Tournament applications
    using Maven, we pull our Java dependencies via our Nexus repository manager. To
    speed up this process, we can perform the same caching we might do on our laptops
    and store these locally between builds in a `.m2/repository` folder and share
    this between builds. We also use persistent storage for built artifacts so they
    can be shared between different steps in our pipeline. Another use case is to
    mount Kubernetes secrets into our pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE168]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE169]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE170]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE171]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE172]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE174]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE175]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE176]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE177]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE178]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE179]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE180]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE181]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE182]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE183]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE184]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE185]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE186]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE187]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE188]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE189]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE190]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE191]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE192]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE193]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE194]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE195]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE196]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE197]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE198]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE199]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE200]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE201]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE202]'
  prefs: []
  type: TYPE_PRE
- en: In Tekton, we link these Kubernetes objects with the named `workspaces` when
    we create what is called the `PipelineRun`, a piece of code that represents one
    run of a pipeline. Similarly, the execution of a single task is a `TaskRun`. Each
    `workspace` is then made available for the tasks in that `PipelineRun` as shown.
  prefs: []
  type: TYPE_NORMAL
- en: Reusable Pipelines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are some choices to be made before you start writing and designing your
    Tekton pipeline. The first is to choose whether you write a pipeline for each
    application, or whether you write reusable pipelines that can be used for applications
    that are similar.
  prefs: []
  type: TYPE_NORMAL
- en: In PetBattle, we started with one pipeline per application; this is similar
    to having a `Jenkinsfile` in each application Git repository. Both the API and
    Tournament PetBattle applications are built using Java, Quarkus, and Maven, so
    it makes sense to consolidate the pipeline code and write a reusable parameterized
    pipeline for these two applications because they will always have similar tasks.
    We use our `maven-pipeline` in PetBattle to do this.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.42: PetBattle''s Tekton pipelines'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you could keep the reuse to the `Task` level only but we share common
    tasks across the PetBattle UI, API, and Tournament applications. Ultimately, the
    development team has to balance the benefits of maintaining one pipeline over
    application pipeline autonomy. There is no one-size-fits-all answer.
  prefs: []
  type: TYPE_NORMAL
- en: Build, Bake, Deploy with Tekton
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The next step is to start designing what we put into our pipeline. This is a
    very iterative process! In our Big Picture, we talked about the Build, Bake, and
    Deploy process, so it makes sense to add pipeline task steps that follow this
    methodology.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.43: The list of task definitions used by PetBattle''s Tekton Pipelines'
  prefs: []
  type: TYPE_NORMAL
- en: The `maven-pipeline` starts by cloning the application and CI/CD (Ubiquitous
    Journey) repositories into the shared `workspace`. We check the code quality by
    invoking Maven to build and test the application, with quality reports being uploaded
    to our SonarQube image.
  prefs: []
  type: TYPE_NORMAL
- en: We check that the quality gate in SonarQube has passed and then invoke Maven
    to package our application. Tekton offers us useful constructs to retry a task
    step if it fails by specifying the number of `retries` as well as the ordering
    of task steps using the `runAfter` task name list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE203]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE204]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE205]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE206]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE207]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE208]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE209]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE210]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE211]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE212]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE213]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE214]'
  prefs: []
  type: TYPE_PRE
- en: In Java Quarkus, the packaging format could be a fat JAR, an exploded fast JAR,
    or a native GraalVM-based image. There are various trade-offs with each of these
    formats.[11](#footnote-083) However, we are using the exploded fast JAR in PetBattle,
    which allows us to trade off between faster build times or faster startup times.
    This is the end of the Build stage. We have moved the unit testing left in our
    pipeline, so we get fast feedback on any code quality issues before we move on
    to the Bake and Deploy pipeline phases.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_44.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.44: The view of a PipelineRun in OpenShift showing the tasks being
    executed'
  prefs: []
  type: TYPE_NORMAL
- en: '[11](#footnote-083-backlink) [https://quarkus.io/guides/maven-tooling](https://quarkus.io/guides/maven-tooling)'
  prefs: []
  type: TYPE_NORMAL
- en: The Bake stage is next. We use a standard OpenShift BuildConfig object, which
    is loaded using Kustomize, as we do not package that with our Helm chart. We perform
    a binary build using the `oc start build` command on the packaged application.
    We decided not to upload the built application package to Nexus because we want
    to work with container images as our unit of deployment. If we were building libraries
    that needed to support our services, then they should be captured in Nexus at
    this stage. It is worth pointing out that we could also push the image to an external
    registry at this point in time so it can be easily shared between OpenShift clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_45.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.45: Bake part of the pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to lint and package the application Helm chart. The versioned
    chart is then uploaded to Nexus. If we were on an application branch, the next
    pipeline step would be a `helm install` into the `labs-dev` project. We can make
    use of `when` statements in our Tekton pipeline to configure such behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE215]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE216]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE217]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE218]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE219]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE220]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE221]'
  prefs: []
  type: TYPE_PRE
- en: When on trunk/HEAD, the ImageStream is versioned and tagged into the namespaces
    we are going to deploy our application to (`labs-test`, `labs-staging`). Because
    we are practicing GitOps, the applications are deployed using Argo CD and Git.
    The Argo CD app-of-apps values files are updated with the new chart and image
    versions. This is checked into source code by the pipeline and `git commit` is
    executed. Argo CD is configured to automatically sync our applications in `labs-test`
    and `labs-staging`, and the last step of the pipeline is to make sure the sync
    task was successful.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_46.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.46: Deploy part of the pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: There is a lot of pipeline information available to the developer in the OpenShift
    web console and all of the pipeline task logs can be easily seen.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_47.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.47: Tekton pipeline progress and status hover'
  prefs: []
  type: TYPE_NORMAL
- en: Tekton also has a great command-line tool called `tkn`, which can be used to
    perform all of the pipeline actions available in the OpenShift console, such as
    viewing logs, starting pipeline runs, and defining Tekton objects.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE222]'
  prefs: []
  type: TYPE_PRE
- en: NAME STARTED DURATION STATUS
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE223]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE224]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE225]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE226]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE227]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE228]'
  prefs: []
  type: TYPE_PRE
- en: Let's now take a look at how we can trigger a build.
  prefs: []
  type: TYPE_NORMAL
- en: Triggers and Webhooks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On every developer push to Git, we wish to trigger a build. This ensures we
    get the fastest feedback for all of our code changes. In Tekton this is achieved
    by using an `EventListener` pod object. When created, a pod is deployed, exposing
    our defined trigger actions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_48.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.48: Tekton Triggers flow'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tekton Triggers work by having `EventListener` objects receive incoming webhook
    notifications, processing them using an interceptor, and creating Kubernetes resources
    from templates if the interceptor allows it, with the extraction of fields from
    the body of the webhook (there''s an assumption that the body is a JSON file):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE229]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE230]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE231]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE232]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE233]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE234]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE235]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE236]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE237]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE238]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE239]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE240]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE241]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE242]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE243]'
  prefs: []
  type: TYPE_PRE
- en: In OpenShift, we expose the `EventListener` webhook endpoint as a route so that
    it can be wired into Git. Different types of `TriggerBinding` and pass to our
    `TriggerTemplate`. The `TriggerTemplate` then defines the Tekton resources to
    create. In our case, this is a `PipelineRun` or `TaskRun` definition.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE244]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE245]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE246]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE247]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE248]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE249]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE250]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE251]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE252]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE253]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE254]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE255]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE256]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE257]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE258]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE259]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE260]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE261]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE262]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE263]'
  prefs: []
  type: TYPE_PRE
- en: Tekton uses an expression language, known as the **Common Expression Language**
    (**CEL**),[13](#footnote-081) to parse and filter requests based on JSON bodies
    and request headers. This is necessary because of the differing webhook payloads
    and potentially different Git workflows. For example, we are using GitHub and
    treat a pull request differently from changes to our main/HEAD. One customization
    we make that you can see above is to define the Argo CD app-of-apps key in the
    trigger binding based on the Git repository name. This allows us to check the
    synchronization of just the one application that changed and not the whole application
    suite during the Deploy phase of our pipeline. While triggering seems complex,
    the flexibility is required when dealing with all the various Git SCMs and workflows
    that are available to development teams.
  prefs: []
  type: TYPE_NORMAL
- en: '[12](#footnote-082-backlink) [https://docs.github.com/en/developers/webhooks-and-events/webhook-events-and-payloads](https://docs.github.com/en/developers/webhooks-and-events/webhook-events-and-payloads)'
  prefs: []
  type: TYPE_NORMAL
- en: '[13](#footnote-081-backlink) [https://github.com/google/cel-go](https://github.com/google/cel-go)'
  prefs: []
  type: TYPE_NORMAL
- en: There are some convenience templates loaded into the `labs-ci-cd` project by
    Ubiquitous Journey that can be used to manually trigger a `PipelineRun`—this is
    handy if you have not configured the GitHub webhook yet.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE264]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE265]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE266]'
  prefs: []
  type: TYPE_PRE
- en: You can manually add webhooks to your GitHub projects[14](#footnote-080) that
    point to the `EventListener` route exposed in the `labs-ci-cd` project.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE267]'
  prefs: []
  type: TYPE_PRE
- en: Otherwise, check out the PetBattle Ubiquitous Journey documentation for Tekton
    tasks that can be run to automatically add these webhooks to your Git repositories.
  prefs: []
  type: TYPE_NORMAL
- en: GitOps our Pipelines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our pipeline, task, trigger, workspace, and volume definitions are themselves
    applied to our `labs-ci-cd` project using GitOps. The idea here is to minimize
    how hard it is to adapt our pipelines. We may want to add some more security checks
    into our pipeline steps, for example. If there are testing failures, or even service
    failures in production, then we need to adapt our pipelines to cater for further
    quality controls or testing steps. Adding new tools or modifying task steps becomes
    nothing more than pushing the pipeline as code definitions to Git.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE268]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE269]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE270]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE271]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE272]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE273]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE274]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE275]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE276]'
  prefs: []
  type: TYPE_PRE
- en: Within our Tekton source folder, we use Kustomize to apply all of the YAML files
    that define our Tekton objects. These pipeline objects are kept in sync by Argo
    CD.
  prefs: []
  type: TYPE_NORMAL
- en: '[14](#footnote-080-backlink) [https://docs.github.com/en/developers/webhooks-and-events/creating-webhooks](https://docs.github.com/en/developers/webhooks-and-events/creating-webhooks)'
  prefs: []
  type: TYPE_NORMAL
- en: Which One Should I Use?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The CI/CD tooling landscape is massive[15](#footnote-079) and also extremely
    vibrant and healthy. The CNCF landscape for tools in this category has no less
    than 36 products and projects today. In trying to answer the question of which
    one you should use; it is best to consider multiple factors:'
  prefs: []
  type: TYPE_NORMAL
- en: Does your team have previous skills in a certain tooling or language? For example,
    pipelines as code in Jenkins use the Groovy language, so if your team has Groovy
    or JavaScript skills, this could be a good choice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the tool integrate with the platform easily? Most of the tools in CNCF
    have good integration with Kubernetes already and have a cloud-native pedigree.
    That does not mean that all tools are the same in terms of deployment, platform
    integration, or lifecycle management—some may be **Software as a Service** (**SaaS**)-only
    offerings with agents, whereas some can be deployed per team using namespace isolation
    on your cluster. Others, such as Argo CD and Tekton, can be deployed at cluster
    scope using the operator pattern, and have their lifecycle managed via the **Operator
    Lifecycle Manager** (**OLM**). Tekton has great web console integration with OpenShift
    because of the OpenShift Pipelines operator.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tool deployment model: Jenkins and Argo CD both have a client-server model
    for deployment. This can be problematic at larger scales, such as when looking
    after thousands of pipelines or hundreds of applications. It may be necessary
    to use multiple deployments to scale across teams and clusters. Argo CD and Tekton
    extend Kubernetes using CRDs and operator patterns, so deployment is more Kubernetes-native
    in its scaling model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Enterprise support: Most, but not all, the tools have vendor support. This
    is important for enterprise organizations that need a relationship with a vendor
    to cover certification, training, security fixes, and product lifecycles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15](#footnote-079-backlink) [https://landscape.cncf.io/card-mode?category=continuous-integration-delivery&grouping=category](https://landscape.cncf.io/card-mode?category=continuous-integration-delivery&grouping=category)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_14_49.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.49: CNCF CI/CD tooling landscape'
  prefs: []
  type: TYPE_NORMAL
- en: 'Active open-source communities: A vibrant upstream community is important as
    a place to collaborate and share code and knowledge. Rapid development of features
    and plugins often occurs in a community based on real user problems and requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One tool for both CI and CD or use different tools? As we have shown with PetBattle,
    sometimes it makes sense for CI to be a push-type model, and CD to be a pull-type
    model using different tools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Extensibility model: This is important for the ecosystem around the tooling.
    Jenkins has a great plugin model that allows lots of different extensions to the
    core. Tekton has a similar model, but it is different in that users have the ability
    to use any container in a task. It is important to weigh up these extensions as
    they offer a lot of value on top of the core tool itself. A good example is that
    Tekton does not manage test dashboards and results as well as Jenkins and its
    plugins do, so we might lean on Allure to do this. Reporting and dashboarding
    extensions are important to make the feedback loop as short as possible during
    CI/CD.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you have considered a few of these ideals, hopefully you will agree on
    the right set of tools for your product and team. A measure of design and planning
    is required to answer the question of where the various steps in your continuous
    deployment happen and what application packaging approach should be used (templated
    or not templated, for example). By now, we have instilled an experiment-driven
    approach to answering these types of questions, where it is not one or the other
    tool, but about choosing the right tool for the job at hand!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter we introduced how we use Git as the single source of truth.
    We covered taking our source code and packaging it using either
  prefs: []
  type: TYPE_NORMAL
- en: Tekton or Jenkins. In the next chapter, we will focus on testing, introducing
    a new component to our app using Knative, running A/B tests, and capturing user
    metrics using some of the advanced deployment capabilities within OpenShift.
  prefs: []
  type: TYPE_NORMAL
