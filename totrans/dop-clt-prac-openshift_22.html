<html><head></head><body>
		<div>&#13;
			<div id="_idContainer500" class="Content">&#13;
			</div>&#13;
		</div>&#13;
		<div id="_idContainer501" class="Content">&#13;
			<h1 id="_idParaDest-372">15. <a id="_idTextAnchor548"/>Run It</h1>&#13;
		</div>&#13;
		<div id="_idContainer541" class="Content">&#13;
			<p>There is a saying that <em class="italics">your code has no value until it runs in production</em>. The sentiment here is that until your customers use your software, it's of limited value for your business or organization. It is certainly a broad generalization! However, it does speak to the essential nature of software that its utility is directly related to being able to run it for whatever purposes it was ultimately written for. To reach production with the quality of service that our customers expect, all of the code must be put through its paces.</p>&#13;
			<p>In this chapter, we are going to explore how the PetBattle team tests their software so they have greater confidence in its ability to run as expected in production. Testing is multifaceted, as we discussed in <em class="italics">Chapter 7</em>, <em class="italics">Open Technical Practices – The Midpoint</em>, and we are going to cover in some detail the types and scope of testing, from unit tests to end-to-end testing, through to security checks and more.</p>&#13;
			<p>When the hobbyist version of the application went live, the PetBattle founders soon discovered that malicious content was being uploaded to the site. As part of this chapter, we'll look at a modern-day solution to this problem using a trained AI-ML model.</p>&#13;
			<p>In the last section of this chapter, we explore some common cloud deployment patterns and demonstrate A/B testing and experimentation, for gaining insight into how we can safely measure and learn the impact of deploying new features in production.</p>&#13;
			<h2 id="_idParaDest-373"><a id="_idTextAnchor549"/><a id="_idTextAnchor550"/>The Not Safe For Families (NSFF) Component</h2>&#13;
			<p>As we mentioned earlier, one <a id="_idIndexMarker2424"/><a id="_idIndexMarker2425"/>of the major issues that we faced when running the first generation of PetBattle was online trolls uploading inappropriate images to the system. This added to the operational overhead of running the platform because the PetBattle founders would have to <a id="_idIndexMarker2426"/><a id="_idIndexMarker2427"/>search MongoDB for the offending images and remove them by hand—very tedious!</p>&#13;
			<p>Ever innovating, the team decided to try and come up with an automated solution to this problem. One approach we decided to investigate was to <a id="_idIndexMarker2428"/><a id="_idIndexMarker2429"/>use <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) to perform image classification on the uploaded images and incorporate this into the platform.</p>&#13;
			<p>The field of AI in itself is a fascinating area of expertise that we won't even slightly go into here, other than to say that we are using a<a id="_idIndexMarker2430"/><a id="_idIndexMarker2431"/> pre-trained image classification model served by the open <a id="_idIndexMarker2432"/><a id="_idIndexMarker2433"/>source TensorFlow machine learning platform.</p>&#13;
			<p>Great, but how do we go about running this on OpenShift?</p>&#13;
			<p>The plan is to:</p>&#13;
			<ol>&#13;
				<li>Generate or obtain a pre-trained image classification model.</li>&#13;
				<li>Build containers containing the TensorFlow serving component that can serve up the model and make predictions based on our uploaded images.</li>&#13;
				<li>Deploy and run the container on OpenShift in a "scale to zero" deployment model, aka Serverless.</li>&#13;
			</ol>&#13;
			<h3 id="_idParaDest-374"><a id="_idTextAnchor551"/>Why Serverless?</h3>&#13;
			<p>When deploying <a id="_idIndexMarker2434"/><a id="_idIndexMarker2435"/>a container on a <a id="_idIndexMarker2436"/><a id="_idIndexMarker2437"/>Kubernetes-based platform, such as OpenShift, Kubernetes takes on the responsibility of managing the running container and, by default, restarting it if it terminates due to an error. Basically, there's always a container running. This is all good and fine for containers that are constantly receiving and processing traffic, but it's a waste of system resources constantly running a container that receives traffic either occasionally or in bursts.</p>&#13;
			<p>What we'd like to achieve is to deploy a container and have it start up only when needed, that is, during incoming requests. Once active, we want it to process the incoming requests and then, after a period of no traffic, shut down gracefully until further incoming requests are received. We'd also like the container instances to scale up in the event of a surge of incoming requests.</p>&#13;
			<p>It is possible to automate the scaling up and down of the number of container instances running on the platform using the Kubernetes Horizontal Pod Autoscaler; however, this does<a id="_idIndexMarker2438"/><a id="_idIndexMarker2439"/> not scale to zero. We could also use something like the <strong class="inline">oc scale</strong> command, but this requires a fair amount of scripting and component integration. Thankfully, the Kubernetes community thought about this and came up with a<a id="_idIndexMarker2440"/><a id="_idIndexMarker2441"/> solution called Knative.<span id="footnote-149-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-149">1</a></span></p>&#13;
			&#13;
			<p>Knative has two major components, <strong class="bold">Knative Serving</strong> and <strong class="bold">Knative Eventing</strong>. Serving is used to spin <a id="_idIndexMarker2442"/><a id="_idIndexMarker2443"/>up (and down) containers depending on HTTP traffic. Knative Eventing is somewhat equivalent but is <a id="_idIndexMarker2444"/><a id="_idIndexMarker2445"/>focused on spinning up containers based on events and addresses broader use cases. For the purposes <a id="_idIndexMarker2446"/><a id="_idIndexMarker2447"/>of this book, we are going to focus on using Knative Serving. However, we will also give an example of how Knative Eventing could be used.</p>&#13;
			<h3 id="_idParaDest-375"><a id="_idTextAnchor552"/><a id="_idTextAnchor553"/>Generating or Obtaining a Pre-trained Model</h3>&#13;
			<p>We had been experimenting with <a id="_idIndexMarker2448"/><a id="_idIndexMarker2449"/>image classification for a while. We started using some of the components from the Open Data Hub community (<a href="https://opendatahub.io/">https://opendatahub.io/</a>) and trained out <a id="_idIndexMarker2450"/><a id="_idIndexMarker2451"/>models on top of pre-existing open source models that were available. We eventually generated a trained data model that could classify images that we deemed NSFF <a id="_idIndexMarker2452"/><a id="_idIndexMarker2453"/>based on an implementation of Yahoo's Open NSFW Classifier,<span id="footnote-148-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-148">2</a></span> which was <a id="_idIndexMarker2454"/><a id="_idIndexMarker2455"/>rewritten with<a id="_idIndexMarker2456"/><a id="_idIndexMarker2457"/> TensorFlow. While it was not perfect, it was a good enough model to start with.</p>&#13;
			&#13;
			<p>A common pattern in the data science community is to serve up trained data models using tools <a id="_idIndexMarker2458"/><a id="_idIndexMarker2459"/>such as Seldon,<span id="footnote-147-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-147">3</a></span> which are part of Open Data Hub. For our purposes though, a simple object storage tool was all that was required. So, we <a id="_idIndexMarker2460"/><a id="_idIndexMarker2461"/>chose MinIO,<span id="footnote-146-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-146">4</a></span> a Kubernetes <a id="_idIndexMarker2462"/><a id="_idIndexMarker2463"/>native object store. We decided we could scale that out later if needed, using more advanced storage mechanisms, for example, OpenShift Container Storage or AWS S3.</p>&#13;
			<div id="footnote-149" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-149-backlink">1</a>	<a href="https://knative.dev/">https://knative.dev/</a></p>&#13;
			</div>&#13;
			<div id="footnote-148" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-148-backlink">2</a>	<a href="https://github.com/yahoo/open_nsfw">https://github.com/yahoo/open_nsfw</a></p>&#13;
			</div>&#13;
			<div id="footnote-147" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-147-backlink">3</a>	<a href="https://www.seldon.io/">https://www.seldon.io/</a></p>&#13;
				<div id="footnote-146" class="_idFootnote" epub:type="footnote">&#13;
					<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-146-backlink">4</a>	<a href="https://min.io/">https://min.io/</a></p>&#13;
				</div>&#13;
			</div>&#13;
			<p>We loaded the trained data model into <a id="_idIndexMarker2464"/><a id="_idIndexMarker2465"/>MinIO and it looked as follows:</p>&#13;
			<div>&#13;
				<div id="_idContainer502" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_01.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.1: TensorFlow data model saved in MinIO</p>&#13;
			<p>The saved model is something we<a id="_idIndexMarker2466"/><a id="_idIndexMarker2467"/> can serve up <a id="_idIndexMarker2468"/><a id="_idIndexMarker2469"/>using TensorFlow Serving,<span id="footnote-145-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-145">5</a></span> which basically gives us an API endpoint to call our saved <a id="_idIndexMarker2470"/><a id="_idIndexMarker2471"/>model with. There is an open source TensorFlow serving image we can deploy<a id="_idIndexMarker2472"/><a id="_idIndexMarker2473"/> and it's a matter of configuring that to find our saved model in our S3 storage location.</p>&#13;
			&#13;
			<p>We have glossed over the large portion of engineering that goes into making AI, ML, and Ops pipelines<a id="_idIndexMarker2474"/><a id="_idIndexMarker2475"/> not because it is not an interesting subject, but mainly because it would require a whole other book to do it justice! If this subject is close to your heart, then take a look at the Open <a id="_idIndexMarker2476"/><a id="_idIndexMarker2477"/>Data Hub project.<span id="footnote-144-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-144">6</a></span> This is an open source project based on Kubeflow,<span id="footnote-143-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-143">7</a></span> providing tools and techniques for building and running AI and ML workloads on OpenShift.</p>&#13;
			<div id="footnote-145" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-145-backlink">5</a>	<a href="https://www.tensorflow.org/tfx/guide/serving">https://www.tensorflow.org/tfx/guide/serving</a></p>&#13;
			</div>&#13;
			<div id="footnote-144" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-144-backlink">6</a>	<a href="http://opendatahub.io/">http://opendatahub.io/</a></p>&#13;
				<div id="footnote-143" class="_idFootnote" epub:type="footnote">&#13;
					<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-143-backlink">7</a>	<a href="https://www.kubeflow.org/">https://www.kubeflow.org/</a></p>&#13;
				</div>&#13;
			</div>&#13;
			<h3 id="_idParaDest-376">T<a id="_idTextAnchor554"/><a id="_idTextAnchor555"/>he OpenShift Serverless Operator</h3>&#13;
			<p>Before we start deploying<a id="_idIndexMarker2478"/><a id="_idIndexMarker2479"/> our application software for the NSFF service, we need to add the OpenShift Serverless Operator<span id="footnote-142-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-142">8</a></span> to our <a id="_idIndexMarker2480"/><a id="_idIndexMarker2481"/>PetBattle Bootstrap. The operator is installed at the cluster scope so that any project that wants to use the Knative components Knative Serving and Knative Eventing may do so.</p>&#13;
			&#13;
			<p>Let's use GitOps, ArgoCD, and Kustomize to <a id="_idIndexMarker2482"/><a id="_idIndexMarker2483"/>configure and install<a id="_idIndexMarker2484"/><a id="_idIndexMarker2485"/> the serverless operator. First, we can test out the configuration with ArgoCD. Log in using ArgoCD from the command line. Add the Git repository that contains the Knative serverless operator YAML subscription and create the application:</p>&#13;
			<p class="snippet"># Login to ArgoCD</p>&#13;
			<p class="snippet">$ argocd login $(oc get route argocd-server --template='{{ .spec.host }}' \</p>&#13;
			<p class="snippet">-n labs-ci-cd):443 --sso --insecure</p>&#13;
			<p class="snippet"># Add our repository</p>&#13;
			<p class="snippet">$ argocd repo add \</p>&#13;
			<p class="snippet">  https://github.com/rht-labs/refactored-adventure.git</p>&#13;
			<p class="snippet"># Create the Knative Operator - this may have already been created for you but here is how to do it on the command line.</p>&#13;
			<p class="snippet"># Create the Knative Operator</p>&#13;
			<p class="snippet">$ argocd app create knative\</p>&#13;
			<p class="snippet">  --repo https://github.com/rht-labs/refactored-adventure.git \</p>&#13;
			<p class="snippet">  --path knative/base \</p>&#13;
			<p class="snippet">  --dest-server https://kubernetes.default.svc \</p>&#13;
			<p class="snippet">  --dest-namespace openshift-serverless \</p>&#13;
			<p class="snippet">  --revision master \</p>&#13;
			<p class="snippet">  --sync-policy automated</p>&#13;
			<div id="footnote-142" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-142-backlink">8</a>	<a href="https://github.com/openshift-knative/serverless-operator">https://github.com/openshift-knative/serverless-operator</a></p>&#13;
			</div>&#13;
			<p>Once installed, you should be able to see this installed successfully in the <strong class="inline">openshift-serverless</strong> namespace:<a id="_idTextAnchor556"/></p>&#13;
			<div>&#13;
				<div id="_idContainer503" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_02.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Fi<a id="_idTextAnchor557"/>gure 15.2: The OpenShift Serverless Operator (Knative)</p>&#13;
			<p>We can also put this in our <a id="_idIndexMarker2486"/><a id="_idIndexMarker2487"/>PetBattle UJ bootstrap from <em class="italics">Chapter 7</em>, <em class="italics">Open Technical Practices – The Midpoint</em>, so that we don't need to run these commands manually. Add the following to our <strong class="inline">values-tooling.yaml</strong> and check it into Git:</p>&#13;
			<p class="snippet"># Knative stanza in values-tooling.yaml</p>&#13;
			<p class="snippet">- name: knative</p>&#13;
			<p class="snippet">    enabled: true</p>&#13;
			<p class="snippet">    destination: openshift-serverless</p>&#13;
			<p class="snippet">    source: https://github.com/rht-labs/refactored-adventure</p>&#13;
			<p class="snippet">    source_path: knative/base</p>&#13;
			<p class="snippet">    source_ref: master</p>&#13;
			<p class="snippet">    sync_policy: *sync_policy_true</p>&#13;
			<p class="snippet">    no_helm: true</p>&#13;
			<p>The operator is now ready for us to use to deploy our Knative service.</p>&#13;
			<h3 id="_idParaDest-377">De<a id="_idTextAnchor558"/><a id="_idTextAnchor559"/>ploying Knative Serving Services</h3>&#13;
			<p>There are a few ways in which to <a id="_idIndexMarker2488"/><a id="_idIndexMarker2489"/>create Knative Serving services. We can create the Knative service definition and install that into our cluster. We have packaged this up as a Helm chart for easy installation:</p>&#13;
			<p class="snippet">$ helm upgrade --install pet-battle-nsff \</p>&#13;
			<p class="snippet">petbattle/pet-battle-nsff \</p>&#13;
			<p class="snippet">--version=0.0.2 \</p>&#13;
			<p class="snippet">--namespace petbattle</p>&#13;
			<p>It may take a minute or so for the containers to start up and load the model data into MinIO; they may<a id="_idIndexMarker2490"/><a id="_idIndexMarker2491"/> restart a few times while doing this. The output of the <strong class="inline">oc get pods</strong> command should look like this once successful – the MinIO S3 pod and its completed data load and a TensorFlow Knative service pod:</p>&#13;
			<p class="snippet">$ oc get pods --namespace petbattle NAME                                   READY   STATUS   RESTARTS AGE</p>&#13;
			<p class="snippet">Minio-pet-battle-nsff-594fc7759-j7lwv  1/1     Running     0     88s</p>&#13;
			<p class="snippet">minio-pet-battle-nsff-dataload-7x8jg   0/1     Completed   2     88s</p>&#13;
			<p class="snippet">minio-pet-battle-nsff-gfjhz            0/1     Completed   3     88s</p>&#13;
			<p class="snippet">tensorflowserving-pet...-7f79956d9qfp  2/2     Running     2     85s</p>&#13;
			<p>After a couple of minutes, the Knative Serving TensorFlow pod will terminate because it is not yet being called. This is what's called Serverless scale to zero, that is, when there are no calling workloads there is <a id="_idIndexMarker2492"/><a id="_idIndexMarker2493"/>no need to run the service. An <a id="_idIndexMarker2494"/><a id="_idIndexMarker2495"/>equivalent service can also be created using the Knative command-line tool <strong class="bold">kn</strong>, which can be downloaded and installed from the OpenShift<span id="footnote-141-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-141">9</a></span> console. This is useful if you want to create a new service or are developing a service from scratch:</p>&#13;
			&#13;
			<p class="snippet">$ kn service create tensorflowserving-pb-nsff --namespace petbattle \</p>&#13;
			<p class="snippet">  --image=docker.io/tensorflow/serving:latest \</p>&#13;
			<p class="snippet">  --cmd "tensorflow_model_server" \</p>&#13;
			<p class="snippet">  --arg "--model_config_file=s3://models/models.config" \</p>&#13;
			<p class="snippet">  --arg "--monitoring_config_file=s3://models/prometheus_config.config" \</p>&#13;
			<p class="snippet">  --arg "--rest_api_port=8501" \</p>&#13;
			<p class="snippet">  --env S3_LOCATION=minio-pet-battle-nsff:9000 \</p>&#13;
			<p class="snippet">  --env AWS_ACCESS_KEY_ID=minio \</p>&#13;
			<p class="snippet">  --env AWS_SECRET_ACCESS_KEY=minio123 \</p>&#13;
			<p class="snippet">  --env AWS_REGION=us-east-1 \</p>&#13;
			<p class="snippet">  --env S3_REGION=us-east-1 \</p>&#13;
			<p class="snippet">  --env S3_ENDPOINT=minio-pet-battle-nsff:9000 \</p>&#13;
			<p class="snippet">  --env S3_USE_HTTPS="0" \</p>&#13;
			<p class="snippet">  --env S3_VERIFY_SSL="0" \</p>&#13;
			<p class="snippet">  --env AWS_LOG_LEVEL="3" \</p>&#13;
			<p class="snippet">  --port 8501 \</p>&#13;
			<p class="snippet">  --autoscale-window "120s"</p>&#13;
			<div id="footnote-141" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-141-backlink">9</a>	<a href="https://docs.openshift.com/container-platform/4.7/serverless/serverless-getting-started.html">https://docs.openshift.com/container-platform/4.7/serverless/serverless-getting-started.html</a></p>&#13;
			</div>&#13;
			<p>Here, we use command-line arguments and environment variables to tell the TensorFlow serving image <a id="_idIndexMarker2496"/><a id="_idIndexMarker2497"/>how to run. The <strong class="inline">--image</strong> field specifies the container image and version we wish to run – in this case, the latest TensorFlow serving image. The <strong class="inline">--cmd</strong> field specifies the binary in the image we wish to run, for example, the model server command <strong class="inline">tensorflow_model_server</strong>. The <strong class="inline">--arg</strong> and <strong class="inline">––env</strong> variables specify the configuration. The trained model is served from the <strong class="inline">S3 minio</strong> service so we specify how to access the S3 endpoint. There are many configurations available to Knative Serving, such as autoscaling global defaults, metrics, and tracing. The <strong class="inline">--autoscale-window</strong> defines the amount of data that the autoscaler takes into account when scaling, so in this case, if there has been no traffic for two minutes, scale the pod to 0.</p>&#13;
			<p>The Knative website<span id="footnote-140-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-140">10</a></span> goes into<a id="_idIndexMarker2498"/><a id="_idIndexMarker2499"/> a lot more detail about the serving resources that are created when using Knative Serving and the configuration of these. To find the URL for our service, we can use this command:</p>&#13;
			&#13;
			<p class="snippet">$ kn route list</p>&#13;
			<p>This gives us the HTTP URL endpoint to test our service with. It is worth noting that we can have multiple revisions of a service and that within the route mentioned previously, we can load balance traffic across multiple revisions. The following diagram depicts how this works in practice:</p>&#13;
			<div>&#13;
				<div id="_idContainer504" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_03.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.3: Knative routing for multiple application revisions</p>&#13;
			<div id="footnote-140" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-140-backlink">10</a>	<a href="https://knative.dev/docs/serving/">https://knative.dev/docs/serving/</a></p>&#13;
			</div>&#13;
			<p>Kourier<span id="footnote-139-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-139">11</a></span> is a lightweight<a id="_idIndexMarker2500"/><a id="_idIndexMarker2501"/> ingress router based on an Envoy gateway. Using Knative service configuration, a user can specify routing rules that Knative Serving applies. This can be very useful when we're experimenting with different AI models or wanting to do A/B, Blue/Green, or Canary-type deployments, for example.<span id="footnote-138-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-138">12</a></span> </p>&#13;
			&#13;
			<h3 id="_idParaDest-378">Inv<a id="_idTextAnchor560"/><a id="_idTextAnchor561"/>oking the NSFF Component</h3>&#13;
			<p>A simple HTTP GET request on<a id="_idIndexMarker2502"/><a id="_idIndexMarker2503"/> the route is all that's required to invoke the component. The pod spins up and services the request usually within a couple of seconds and then spins down after a period of time (the <strong class="inline">--autoscale-window</strong> specified in the <strong class="inline">kn</strong> command-line argument, that is, 120 seconds). Using the output from the <strong class="inline">kn list route</strong> command, let's check if the AI TensorFlow model is available. The <strong class="inline">state</strong> should read <strong class="inline">AVAILABLE</strong>:</p>&#13;
			<p class="snippet">$ curl &lt;url from kn route list&gt;/v1/models/test_model</p>&#13;
			<p class="snippet"># For example</p>&#13;
			<p class="snippet">$ curl http://tensorflowserving-pet-battle-nsff-labs-dev.apps.hivec.sandbox882.opentlc.com/v1/models/test_model</p>&#13;
			<p class="snippet">{</p>&#13;
			<p class="snippet"> "model_version_status": [</p>&#13;
			<p class="snippet">  {</p>&#13;
			<p class="snippet">   "version": "1",</p>&#13;
			<p class="snippet">   "state": "AVAILABLE",</p>&#13;
			<p class="snippet">   "status": {</p>&#13;
			<p class="snippet">    "error_code": "OK",</p>&#13;
			<p class="snippet">    "error_message": ""</p>&#13;
			<p class="snippet">   }</p>&#13;
			<p class="snippet">  }</p>&#13;
			<p class="snippet"> ]</p>&#13;
			<p class="snippet">}</p>&#13;
			<div id="footnote-139" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-139-backlink">11</a>	<a href="https://developers.redhat.com/blog/2020/06/30/kourier-a-lightweight-knative-serving-ingress/">https://developers.redhat.com/blog/2020/06/30/kourier-a-lightweight-knative-serving-ingress/</a></p>&#13;
				<div id="footnote-138" class="_idFootnote" epub:type="footnote">&#13;
					<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-138-backlink">12</a>	<a href="https://medium.com/@kamesh_sampath/serverless-blue-green-and-canary-with-knative-kn-ad49e8b6aa54">https://medium.com/@kamesh_sampath/serverless-blue-green-and-canary-with-knative-kn-ad49e8b6aa54</a></p>&#13;
				</div>&#13;
			</div>&#13;
			<p>We should also see a pod spinning up to serve the request, using:</p>&#13;
			<p class="snippet">$ oc get pods \</p>&#13;
			<p class="snippet">-l serving.knative.dev/configuration=tensorflowserving-pet-battle-nsff \</p>&#13;
			<p class="snippet">--namespace petbattle</p>&#13;
			<p class="snippet">NAME                          READY   STATUS    RESTARTS   AGE</p>&#13;
			<p class="snippet">tensorflowserving-pet…         2/2     Running   0          21s</p>&#13;
			<p>It then scales down to 0 after two minutes.</p>&#13;
			<p>We want to test <a id="_idIndexMarker2504"/><a id="_idIndexMarker2505"/>that our NSFF service works by sending it some images. We have two test sample images that have been encoded so they can be uploaded to the NSFF service.</p>&#13;
			<div>&#13;
				<div id="_idContainer505" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_04.1.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<div>&#13;
				<div id="_idContainer506" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_04.2.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.4: NSFF test images</p>&#13;
			<p>Let's download these images for testing:</p>&#13;
			<p class="snippet">$ wget https://raw.githubusercontent.com/petbattle/pet-battle-nsff/main/requests/tfserving/nsff-negative.json</p>&#13;
			<p class="snippet">$ wget https://raw.githubusercontent.com/petbattle/pet-battle-nsff/main/requests/tfserving/nsff-positive.json</p>&#13;
			<p>Now submit these to our NSFF service using a simple <strong class="inline">curl</strong> command:</p>&#13;
			<p class="snippet">$ HOST=$(kn service describe tensorflowserving-pb-nsff -o url)/v1/models/test_model:predict # Daisy Cat - Safe for Families</p>&#13;
			<p class="snippet">curl -s -k -H 'Content-Type: application/json \</p>&#13;
			<p class="snippet">  -H 'cache-control: no-cache' \</p>&#13;
			<p class="snippet">  -H 'Accept: application/json' \</p>&#13;
			<p class="snippet">  -X POST --data-binary '@nsff-negative.json' $HOST</p>&#13;
			<p class="snippet">{ "predictions": [[<span class="P-_-Code-Highlight">0.992712617</span>, 0.00728740077]] }</p>&#13;
			<p class="snippet"># Not Safe For Families - Boxing</p>&#13;
			<p class="snippet">curl -s -k -H 'Content-Type: application/json' \</p>&#13;
			<p class="snippet">  -H 'cache-control: no-cache' \</p>&#13;
			<p class="snippet">  -H 'Accept: application/json' \</p>&#13;
			<p class="snippet">  -X POST --data-binary '@nsff-positive.json' $HOST</p>&#13;
			<p class="snippet">{ "predictions": [[<span class="P-_-Code-Highlight">0.30739361</span>, 0.69260639]] }</p>&#13;
			<p>The response from our model is a <a id="_idIndexMarker2506"/><a id="_idIndexMarker2507"/><strong class="inline">predictions</strong> array containing two numbers. The first is a measure of <strong class="bold">Safe for Families</strong>, the second is a measure of <strong class="bold">Not Safe for Families</strong>, and they add up to 1.</p>&#13;
			<p>So, we can see that <a id="_idIndexMarker2508"/><a id="_idIndexMarker2509"/>Daisy Cat has a very high safe for families rating (0.993) compared to our wrestlers (0.014) and we can use this in our PetBattle API to determine whether any given image is safe to display. By arbitrary testing, we have set a limit of &gt;=0.6 for images we think are safe to view in the PetBattle UI.</p>&#13;
			<p>We can redeploy our PetBattle API service to call out to the NSFF service by setting the <strong class="inline">nssf.enabled</strong> feature flag to <strong class="inline">true</strong> and using the hostname from the Knative service from a bash shell using the command line:</p>&#13;
			<p class="snippet">$ HOST=$(kn service describe tensorflowserving-pet-battle-nsff -o url)</p>&#13;
			<p class="snippet">$ helm upgrade --install pet-battle-api petbattle/pet-battle-api \</p>&#13;
			<p class="snippet">--version=1.0.8 \</p>&#13;
			<p class="snippet">--set nsff.enabled=true \</p>&#13;
			<p class="snippet">--set nsff.apiHost=${HOST##http://} \</p>&#13;
			<p class="snippet">--set nsff.apiPort=80 --namespace petbattle</p>&#13;
			<p>If we now upload these test images to PetBattle via the UI and check the API server, we can see that the boxing picture has a <strong class="bold">false</strong> value for the <strong class="inline">ISSFF</strong> (Is Safe for Families) flag and Daisy Cat has a <strong class="bold">true</strong> value:</p>&#13;
			<div>&#13;
				<div id="_idContainer507" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_05.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.5: PetBattle API saved images with the ISSFF flag</p>&#13;
			<p>The API code will not <a id="_idIndexMarker2510"/><a id="_idIndexMarker2511"/>return any pictures to the PetBattle UI that are deemed NSFF. For example, the API code to return all pets in the PetBattle database is filtered by the <strong class="inline">ISSFF</strong> flag being set to <strong class="inline">true</strong>:</p>&#13;
			<p class="snippet">@GET</p>&#13;
			<p class="snippet">@Operation(operationId = "list",</p>&#13;
			<p class="snippet">           summary = "get all cats",</p>&#13;
			<p class="snippet">           description = "This operation retrieves all cats from the</p>&#13;
			<p class="snippet">                          database that are safe for work",</p>&#13;
			<p class="snippet">           deprecated = false, hidden = false)</p>&#13;
			<p class="snippet">public Uni&lt;List&lt;Cat&gt;&gt; list() {</p>&#13;
			<p class="snippet">  return Cat.find(ISSFF, true).list();</p>&#13;
			<p class="snippet">}</p>&#13;
			<p>Now that we have the API up and running it's time to test it and see if it performs as we expect.</p>&#13;
			<h3 id="_idParaDest-379">Let's<a id="_idTextAnchor562"/><a id="_idTextAnchor563"/> Talk about Testing</h3>&#13;
			<p>In our experience<a id="_idIndexMarker2512"/><a id="_idIndexMarker2513"/> with working with many developer teams, nothing can dampen the mood of many developers quite like a discussion on the subject of testing that their software does what it's supposed to do.</p>&#13;
			<p>For ma<a id="_idTextAnchor564"/>ny developers, testing is the equivalent of <a id="_idIndexMarker2514"/><a id="_idIndexMarker2515"/>getting a dental checkup—few like doing it but all of us need to do it a lot more. It's a set of bridges that have to be crossed (often under duress) before our precious, handcrafted, artisan-designed piece of software excellence is accepted into the nirvana that is production. Testers are seen as the other team that ensures that we've dotted our I's and crossed our T's, and they don't appreciate how hard we've suffered for our art. Basically, we write it, <em class="italics">throw it over the fence to test</em>, and they check it.</p>&#13;
			<p>If you're reading the preceding paragraph and mentally going <em class="italics">yep, yep, that's us, that's us, that's how we roll</em>, we've got really bad news for you. <em class="italics">You're basically doing it wrong</em>. It may have made sense when big bang software releases happened<a id="_idIndexMarker2516"/><a id="_idIndexMarker2517"/> every 6-12 months, but in more agile organizations with faster, more frequent releases into production, this approach is considered cumbersome and archaic. There are always exceptions to this, for example, critical control systems, highly regulated environments, and so on, but for the majority of enterprise developers, this isn't the case.</p>&#13;
			<p>The quality of software is the<a id="_idIndexMarker2518"/><a id="_idIndexMarker2519"/> responsibility of the delivery team, from the Product Owner writing user stories to the engineers writing code and the associated tests. As a wise delivery manager once said, "testing is an activity, not a role." In effective software delivery teams, testing is a continuous <a id="_idIndexMarker2520"/><a id="_idIndexMarker2521"/>activity that spans the entire software development life cycle. There are a number of<a id="_idIndexMarker2522"/><a id="_idIndexMarker2523"/> principles that we try to adhere to when it comes to testing:</p>&#13;
			<ol>&#13;
				<li value="1">Automate as much as possible, but not so much that there's no human oversight. There's always value in having users interact with the application under test, in particular when it comes to end-to-end, acceptance, and exploratory testing.</li>&#13;
				<li>Testing code is as important as production code—both need to be kept up to date and removed/deprecated when not adding value.</li>&#13;
				<li>One meaningful test can be worth more than hundreds of scripted test cases.</li>&#13;
			</ol>&#13;
			<p>In <em class="italics">Chapter 7</em>, <em class="italics">Open Technical Practices – The Midpoint</em>, we introduced the idea of the Automation Test Pyramid. For each <a id="_idIndexMarker2524"/><a id="_idIndexMarker2525"/>of the different types of tests defined in the pyramid, there are several testing tools and frameworks we use across our PetBattle application. </p>&#13;
			<p>Generally speaking, we have chosen to use what are considered the default test tools for each of the application technology stacks as these are the simplest to use, are the best supported, have good user <a id="_idIndexMarker2526"/><a id="_idIndexMarker2527"/>documentation, and are generally easy to adopt if people are new to them:</p>&#13;
			<div>&#13;
				<div id="_idContainer1008" class="IMG---Figure">&#13;
					<img src="../Images/B16297_Table_15.1.png" alt="" style="height:150px; width:600px;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Table 15.1: Test Frameworks in use</p>&#13;
			<p>Let's take a look at some of these tests in more detail.</p>&#13;
			<h3 id="_idParaDest-380"><a id="_idTextAnchor565"/><a id="_idTextAnchor566"/>Unit Testing with JUnit</h3>&#13;
			<p>In both the API and<a id="_idIndexMarker2528"/><a id="_idIndexMarker2529"/> Tournament applications, we have different examples of standard unit tests. Quarkus testing<span id="footnote-137-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-137">13</a></span> has great support for the standard unit test framework JUnit.<span id="footnote-136-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-136">14</a></span> The anatomy of all <a id="_idIndexMarker2530"/><a id="_idIndexMarker2531"/>unit tests using this framework is very similar. Let's take a look at the API application <strong class="inline">CatResourceTest.java</strong><span id="footnote-135-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-135">15</a></span> as an example:</p>&#13;
			&#13;
			<p class="snippet">@QuarkusTest</p>&#13;
			<p class="snippet">class CatResourceTest {</p>&#13;
			<p class="snippet">  private static final Logger LOGGER = LoggerFactory</p>&#13;
			<p class="snippet">                                      .getLogger("CatResourceTest");</p>&#13;
			<p class="snippet">  @Test</p>&#13;
			<p class="snippet">  void testCat() {</p>&#13;
			<p class="snippet">    PanacheMock.mock(Cat.class);</p>&#13;
			<p class="snippet">    Mockito.when(Cat.count())</p>&#13;
			<p class="snippet">      .thenReturn(Uni.createFrom().item(23l));</p>&#13;
			<p class="snippet">    Assertions.assertEquals(23, Cat.count().await().indefinitely());</p>&#13;
			<div id="footnote-137" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-137-backlink">13</a>	<a href="https://quarkus.io/guides/getting-started-testing">https://quarkus.io/guides/getting-started-testing</a></p>&#13;
				<div id="footnote-136" class="_idFootnote" epub:type="footnote">&#13;
					<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-136-backlink">14</a>	<a href="https://junit.org/junit5/">https://junit.org/junit5/</a></p>&#13;
					<div id="footnote-135" class="_idFootnote" epub:type="footnote">&#13;
						<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-135-backlink">15</a>	<a href="https://github.com/petbattle/pet-battle-api/blob/master/src/test/java/app/battle/CatResourceTest.java">https://github.com/petbattle/pet-battle-api/blob/master/src/test/java/app/battle/CatResourceTest.java</a></p>&#13;
					</div>&#13;
				</div>&#13;
			</div>&#13;
			<p>In Java, we use <a id="_idIndexMarker2532"/><a id="_idIndexMarker2533"/>annotations to <a id="_idIndexMarker2534"/><a id="_idIndexMarker2535"/>make our Java <a id="_idIndexMarker2536"/><a id="_idIndexMarker2537"/>class objects (POJOs) into tests. We use the <strong class="inline">@QuarkusTest</strong> annotation to bring in the JUnit framework for this class and we can think of the class as a test suite that contains lots of individual tests. Each method is a single test that is annotated with <strong class="inline">@Test</strong>. For this unit test, we don't have a database running, so we use mocks<span id="footnote-134-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-134">16</a></span> for the <strong class="inline">Cat.class</strong>. A mock is a fake object. It does not connect to a real database, and we can use it to test the behavior of the <strong class="inline">Cat</strong> class. In this case, we are asserting in our test that when we call the method <strong class="inline">Cat.count()</strong>, which corresponds to the number of likes of our pet image in PetBattle, we receive back the expected number (<strong class="inline">23</strong>). We use the <strong class="inline">Uni</strong> and <strong class="inline">await()</strong> functions because we are using the reactive programming model in our Quarkus application.<span id="footnote-133-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-133">17</a></span></p>&#13;
			&#13;
			<p>We run these unit tests as <a id="_idIndexMarker2538"/><a id="_idIndexMarker2539"/>part of the automated continuous deployment pipeline and visualize and report on the tests' success and history using our CI/CD tools, including Jenkins, Tekton, and a test report <a id="_idIndexMarker2540"/><a id="_idIndexMarker2541"/>tool such as Allure.<span id="footnote-132-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-132">18</a></span></p>&#13;
			&#13;
			<div>&#13;
				<div id="_idContainer508" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_06.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.6: Visualization of tests using Allure</p>&#13;
			<p>I<a id="_idTextAnchor567"/>n the next section, we'll continue with service and component testing with REST Assured and Jest.</p>&#13;
			<h3 id="_idParaDest-381">S<a id="_idTextAnchor568"/>ervice and Component Testing with REST Assured and Jest</h3>&#13;
			<p>Jest <a id="_idIndexMarker2542"/><a id="_idIndexMarker2543"/>and REST<a id="_idIndexMarker2544"/><a id="_idIndexMarker2545"/> Assured are <strong class="bold">Behavior-Driven Development</strong> (<strong class="bold">BDD</strong>) frameworks for JavaScript <a id="_idIndexMarker2546"/><a id="_idIndexMarker2547"/>and Java. We<a id="_idIndexMarker2548"/><a id="_idIndexMarker2549"/> covered BDD in <em class="italics">Chapter 7</em>, <em class="italics">Open Technical Practices – The Midpoint</em>. These frameworks make it <a id="_idIndexMarker2550"/><a id="_idIndexMarker2551"/>super easy for developers to write tests where the syntax is obvious and easy to follow.</p>&#13;
			<div id="footnote-134" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-134-backlink">16</a>	<a href="https://quarkus.io/guides/mongodb-panache">https://quarkus.io/guides/mongodb-panache</a></p>&#13;
				<div id="footnote-133" class="_idFootnote" epub:type="footnote">&#13;
					<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-133-backlink">17</a>	<a href="https://quarkus.io/guides/getting-started-reactive#mutiny">https://quarkus.io/guides/getting-started-reactive#mutiny</a></p>&#13;
				</div>&#13;
			</div>&#13;
			<div id="footnote-132" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-132-backlink">18</a>	<a href="https://github.com/allure-framework">https://github.com/allure-framework</a></p>&#13;
			</div>&#13;
			<p>We are going to<a id="_idIndexMarker2552"/><a id="_idIndexMarker2553"/> cover the basics of component testing the PetBattle user interface<span id="footnote-131-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-131">19</a></span> using Jest. The user interface is made of several components. The first one you see when landing on the application is the<a id="_idIndexMarker2554"/><a id="_idIndexMarker2555"/> home page. For the home page component, the test class<span id="footnote-130-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-130">20</a></span> is called <strong class="inline">home.component.spec.ts</strong>:</p>&#13;
			&#13;
			<p class="snippet">describe('HomeComponent', () =&gt; {</p>&#13;
			<p class="snippet">  let component: HomeComponent;</p>&#13;
			<p class="snippet">  let fixture: ComponentFixture&lt;HomeComponent&gt;;</p>&#13;
			<p class="snippet">  beforeEach(async () =&gt; {...</p>&#13;
			<p class="snippet">  });</p>&#13;
			<p class="snippet">  beforeEach(() =&gt; {...</p>&#13;
			<p class="snippet">  });</p>&#13;
			<p class="snippet">  it('should create', () =&gt; {</p>&#13;
			<p class="snippet">    expect(component).toBeTruthy();</p>&#13;
			<p class="snippet">  });</p>&#13;
			<p class="snippet">});</p>&#13;
			<p>Each test has a similar anatomy:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li><strong class="inline">describe()</strong>: The name of the test suite and test specification argument</li>&#13;
				<li><strong class="inline">beforeEach()</strong>: Runs the function passed as an argument before running each test</li>&#13;
				<li><strong class="inline">it()</strong>: Defines a single test with a required expectation and a function with logic and assertions</li>&#13;
				<li><strong class="inline">expect()</strong>: Creates an expectation for the test result, normally with a matching function such as <strong class="inline">toEqual()</strong></li>&#13;
			</ul>&#13;
			<p>So in this case, the unit test will expect the <strong class="inline">HomeComponent</strong> to be created correctly when the test is run.</p>&#13;
			<div id="footnote-131" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-131-backlink">19</a>	<a href="https://angular.io/guide/testing">https://angular.io/guide/testing</a></p>&#13;
				<div id="footnote-130" class="_idFootnote" epub:type="footnote">&#13;
					<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-130-backlink">20</a>	<a href="https://github.com/petbattle/pet-battle/blob/master/src/app/home/home.component.spec.ts">https://github.com/petbattle/pet-battle/blob/master/src/app/home/home.component.spec.ts</a></p>&#13;
				</div>&#13;
			</div>&#13;
			<p>Similarly, within the API application, REST <a id="_idIndexMarker2556"/><a id="_idIndexMarker2557"/>Assured is a testing tool that allows us to write tests using the familiar <strong class="inline">Given,</strong> <strong class="inline">When,</strong> <strong class="inline">Then</strong> syntax from <em class="italics">Chapter 7</em>, <em class="italics">Open Technical Practices – The Midpoint</em>. Let's examine <a id="_idIndexMarker2558"/><a id="_idIndexMarker2559"/>one of the service <a id="_idIndexMarker2560"/><a id="_idIndexMarker2561"/>API tests in the test suite <strong class="inline">CatResourceTest.java</strong><span id="footnote-129-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-129">21</a></span>:</p>&#13;
			&#13;
			<p class="snippet">@Test</p>&#13;
			<p class="snippet">@Story("Test pet create")</p>&#13;
			<p class="snippet">void testCatCreate() {</p>&#13;
			<p class="snippet">    CatInstance catInstance = new CatInstance();</p>&#13;
			<p class="snippet">    RestAssured.given()</p>&#13;
			<p class="snippet">            .contentType(ContentType.JSON)</p>&#13;
			<p class="snippet">            .body(catInstance.cat)</p>&#13;
			<p class="snippet">            .log().all()</p>&#13;
			<p class="snippet">            .when().post("/cats")</p>&#13;
			<p class="snippet">            .then()</p>&#13;
			<p class="snippet">            .log().all()</p>&#13;
			<p class="snippet">            .statusCode(201)</p>&#13;
			<p class="snippet">            .body(is(notNullValue()));</p>&#13;
			<p class="snippet">}</p>&#13;
			<p>In this test, we are creating a <strong class="inline">Cat</strong> object. The <strong class="inline">Cat</strong> class is the data object in PetBattle that contains the pet's uploaded image, along with its PetBattle vote count, and is stored in <a id="_idIndexMarker2562"/><a id="_idIndexMarker2563"/>MongoDB. In the test, given the <strong class="inline">Cat</strong> object, we use an <strong class="inline">HTTP POST</strong> to the <strong class="inline">/cats</strong> endpoint and expect a return status code of (<strong class="inline">201</strong>), which is <strong class="inline">CREATED</strong>. We also test the HTTP response body is not empty. It should contain the ID of the newly created <strong class="inline">Cat</strong>:</p>&#13;
			<p class="snippet">@QuarkusTest</p>&#13;
			<p class="snippet">@QuarkusTestResource(MongoTestResource.class)</p>&#13;
			<p class="snippet">@Epic("PetBattle")</p>&#13;
			<p class="snippet">@Feature("PetEndpointTest")</p>&#13;
			<p class="snippet">class CatEndpointTest {</p>&#13;
			<div id="footnote-129" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-129-backlink">21</a>	<a href="https://github.com/petbattle/pet-battle-api/blob/master/src/test/java/app/battle/CatResourceTest.java">https://github.com/petbattle/pet-battle-api/blob/master/src/test/java/app/battle/CatResourceTest.java</a></p>&#13;
			</div>&#13;
			<p>In this service test, we make use <a id="_idIndexMarker2564"/><a id="_idIndexMarker2565"/>of the <strong class="inline">@QuarkusTestResource</strong> annotation to create and start an embedded MongoDB for testing against. So, this test is a bit more sophisticated than the basic unit test <a id="_idIndexMarker2566"/><a id="_idIndexMarker2567"/>that was using mocks only. We <a id="_idIndexMarker2568"/><a id="_idIndexMarker2569"/>also track the execution of these service tests using our test report tool:</p>&#13;
			<div>&#13;
				<div id="_idContainer509" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_07.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.7: Visualization of service tests using Allure</p>&#13;
			<p>Now we have seen what unit tests look like, let's move up the test pyramid to have a look at service-level testing.</p>&#13;
			<h3 id="_idParaDest-382">Se<a id="_idTextAnchor569"/><a id="_idTextAnchor570"/>rvice Testing with Testcontainers</h3>&#13;
			<p>Integration testing is always substantially <a id="_idIndexMarker2570"/><a id="_idIndexMarker2571"/>harder than unit testing as more components have to be either stood up or simulated/mocked. The next level of testing in our test pyramid is integration testing using<a id="_idIndexMarker2572"/><a id="_idIndexMarker2573"/> a Java framework called <strong class="bold">Testcontainers</strong>.<span id="footnote-128-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-128">22</a></span> Testcontainers allows us to easily <a id="_idIndexMarker2574"/><a id="_idIndexMarker2575"/>create and start components such as MongoDB, <strong class="bold">KeyCloak</strong>, and <strong class="bold">Infinispan</strong> and<a id="_idIndexMarker2576"/><a id="_idIndexMarker2577"/> perform tests using those components. The following classes instantiate and manage the containers and inject them into the testing life cycle of the Quarkus framework:</p>&#13;
			&#13;
			<p class="snippet">$ ls src/test/java/com/petbattle/containers/</p>&#13;
			<p class="snippet">InfinispanTestContainer.java KeycloakTestContainer.java   MongoTestContainer.java</p>&#13;
			<p>Within the integration test code at <strong class="inline">ITPetBattleAPITest.java</strong>, we just inject the previously created containers and use them as resources during the test:</p>&#13;
			<p class="snippet">$ head src/test/java/com/petbattle/integration/ITPetBattleAPITest.java</p>&#13;
			<p class="snippet">package com.petbattle.integration;</p>&#13;
			<p class="snippet">...</p>&#13;
			<p class="snippet">@QuarkusTest</p>&#13;
			<p class="snippet">@DisplayName("API Test Cases")</p>&#13;
			<p class="snippet">@QuarkusTestResource(MongoTestContainer.class)</p>&#13;
			<p class="snippet">@QuarkusTestResource(InfinispanTestContainer.class)</p>&#13;
			<p class="snippet">@QuarkusTestResource(KeycloakTestContainer.class)</p>&#13;
			<p class="snippet">public class ITPetBattleAPITest {</p>&#13;
			<div id="footnote-128" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-128-backlink">22</a>	<a href="https://www.testcontainers.org/">https://www.testcontainers.org/</a></p>&#13;
			</div>&#13;
			<p>This is a great example <a id="_idIndexMarker2578"/><a id="_idIndexMarker2579"/>of how containers can be used as part of a<a id="_idIndexMarker2580"/><a id="_idIndexMarker2581"/> testing phase. The containers are spun up, the tests are run, and the containers are removed. The only real prerequisite is that the Docker daemon <a id="_idIndexMarker2582"/><a id="_idIndexMarker2583"/>is run on the machine running the tests. To run the integration tests use the command <strong class="inline">mvn clean verify -Pintegration</strong>.</p>&#13;
			<h3 id="_idParaDest-383">En<a id="_idTextAnchor571"/><a id="_idTextAnchor572"/>d-to-End Testing</h3>&#13;
			<p>Our application is made up <a id="_idIndexMarker2584"/><a id="_idIndexMarker2585"/>of a frontend written in Angular, which makes calls for data to two APIs. One<a id="_idIndexMarker2586"/><a id="_idIndexMarker2587"/> is for tournaments and the other is for cats. We can think of the interplay between these components as the system as a whole. Any time a change is made to either of these individual applications, it should require revalidating the whole system. The end-to-end automated testing is performed primarily in the user interface but exercises the underlying services layer.</p>&#13;
			<p>There are loads of tools to do testing from <a id="_idIndexMarker2588"/><a id="_idIndexMarker2589"/>the user interface level. Some of the more popular ones are things like Selenium <a id="_idIndexMarker2590"/><a id="_idIndexMarker2591"/>and Cypress, which are used to drive a web application and simulate user behavior. There are pros and cons to each – Selenium is just browser automation so you need to bring your own test frameworks, whereas <a id="_idIndexMarker2592"/><a id="_idIndexMarker2593"/>Cypress is an all-in-one testing framework. Selenium Grid, when <a id="_idIndexMarker2594"/><a id="_idIndexMarker2595"/>running on Kubernetes, allows us to test against multiple browsers in parallel by dynamically provisioning the browser on each test execution, meaning we don't have browsers waiting idly for us to use them.</p>&#13;
			<p>For our end-to-end testing, we're using Protractor from the Angular team. We already deployed an instance of Selenium Grid built for Kubernetes by the <a id="_idIndexMarker2596"/><a id="_idIndexMarker2597"/>Zalando team (called Zalenium <a href="https://opensource.zalando.com/zalenium/">https://opensource.zalando.com/zalenium/</a>) when we <a id="_idIndexMarker2598"/><a id="_idIndexMarker2599"/>deployed our tooling. Zalenium is pretty handy as it allows us to play back previous test runs and watch them live. In your cluster, if you get the route for Zalenium (<strong class="inline">oc get routes -n labs-ci-cd</strong>) and append <strong class="inline">/grid/admin/live</strong>, you can follow the tests as they execute or go to <span class="P---Screen-Text">/dashboard</span> to watch the historical test executions.</p>&#13;
			<div>&#13;
				<div id="_idContainer510" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_08.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.8: Zalenium dashboard showing test history and video playback</p>&#13;
			<p>Our <strong class="inline">system-tests</strong> project (<a href="https://github.com/petbattle/system-tests">https://github.com/petbattle/system-tests</a>) has all of the system tests that we should execute after any change is pushed to the frontend or backend services. The tests are <a id="_idIndexMarker2600"/><a id="_idIndexMarker2601"/>written using <a id="_idIndexMarker2602"/><a id="_idIndexMarker2603"/>Cucumber-style BDD. In fact, we should be able to connect the BDD to the acceptance criteria from our PetBattle Sprint items.</p>&#13;
			<div>&#13;
				<div id="_idContainer511" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_09.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.9: Example of BDD written as acceptance criteria on a Sprint board for PetBattle</p>&#13;
			<p>Here's a test for a tournament feature written in the <strong class="inline">Given,</strong> <strong class="inline">When,</strong> <strong class="inline">Then</strong> syntax: </p>&#13;
			<p class="snippet">Feature: Tournament features</p>&#13;
			<p class="snippet">  Scenario: Should only be prompted to login on navigating to the</p>&#13;
			<p class="snippet">            tournament</p>&#13;
			<p class="snippet">    Given I am on the home page</p>&#13;
			<p class="snippet">    When  I move to the tournament page</p>&#13;
			<p class="snippet">    Then  I should be redirected to keycloak</p>&#13;
			<p>The <strong class="inline">system-test</strong> project has its own Jenkinsfile, so it's already connected to Jenkins via our seed job. We won't go through the contents of this Jenkinsfile in detail. Suffice to say, the pipeline has two stages, as per our Big Picture, one to run the tests and the other to promote the app if the tests have passed. Explore the code for this in the accompanying Git repo <a href="https://github.com/petbattle/system-tests">https://github.com/petbattle/system-tests</a>. To extend our Jenkinsfile for <strong class="inline">pet-battle</strong> to trigger our system test job, we just need to add another stage to trigger the job. We could <a id="_idIndexMarker2604"/><a id="_idIndexMarker2605"/>use the Jenkins <strong class="inline">post{}</strong> block, but we only want to trigger the system tests if we're on <strong class="inline">master</strong> or <strong class="inline">main</strong> and <a id="_idIndexMarker2606"/><a id="_idIndexMarker2607"/>producing a release candidate.</p>&#13;
			<div>&#13;
				<div id="_idContainer512" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_10.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.10: The trigger for connecting our pipelines in Jenkins</p>&#13;
			<p>There are a few parameters that are passed between the jobs:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li><strong class="inline">APP_NAME</strong>: Passed to the job so if the tests are successful, the <a id="_idIndexMarker2608"/><a id="_idIndexMarker2609"/>promote stage knows what app to deploy.</li>&#13;
				<li><strong class="inline">CHART_VERSION &amp; VERSION</strong>: Any update to the chart or app needs to be patched in Git so this information is passed by the job that triggers the system tests.</li>&#13;
			</ul>&#13;
			<p>We can run the system tests job manually by supplying this information to the job, but each service with a Jenkinsfile should be able to pass these to the system tests. This job can also be triggered from Tekton too if we were to mix the approach to the pipelines. With the two pipelines wired together, we can trigger one if the webhook is set up by running the following command:</p>&#13;
			<p class="snippet">$ git commit --allow-empty -m "🍌 kickoff jenkins 🦆" &amp;&amp; git push</p>&#13;
			<p>If we now check in<a id="_idIndexMarker2610"/><a id="_idIndexMarker2611"/> the Jenkins Blue Ocean Web UI, we should see the following:</p>&#13;
			<div>&#13;
				<div id="_idContainer513" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_11.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.11: The system tests pipeline</p>&#13;
			<p>On Jenkins, we should see the <a id="_idIndexMarker2612"/><a id="_idIndexMarker2613"/>system tests pipeline running and promoting if successful. The Cucumber reports<a id="_idIndexMarker2614"/><a id="_idIndexMarker2615"/> are also included for the job.</p>&#13;
			<div>&#13;
				<div id="_idContainer514" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_12.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.12: The Cucumber report in Jenkins</p>&#13;
			<p>These pro<a id="_idTextAnchor573"/>vide insight into which cases were executed for what browser and report any failures that may have occurred. Let's switch gear a little now and take a look at non-functional testing.</p>&#13;
			<h2 id="_idParaDest-384">Pipelines<a id="_idTextAnchor574"/><a id="_idTextAnchor575"/> and Quality Gates (Non-functionals)</h2>&#13;
			<p>Quality is often just<a id="_idIndexMarker2616"/><a id="_idIndexMarker2617"/> focused on whether tests pass or not. However there's also the concept of code quality. The code may perform as expected but the manner in which it's been written could be so poor that <a id="_idIndexMarker2618"/><a id="_idIndexMarker2619"/>it could be a future source of problems when changes are added. So now it's time to check the quality of our code.</p>&#13;
			<h3 id="_idParaDest-385">SonarQube<a id="_idTextAnchor576"/><a id="_idTextAnchor577"/></h3>&#13;
			<p>As part of the Ubiquitous Journey, we <a id="_idIndexMarker2620"/><a id="_idIndexMarker2621"/>have automated the Helm chart deployment of SonarQube, which we are using to test and measure code quality. In <strong class="inline">values-tooling.yaml</strong>, the SonarQube stanza references the Helm chart and any extra plugins that are required. Many of the common <a id="_idIndexMarker2622"/><a id="_idIndexMarker2623"/>language profile plugins are already deployed with the base version of SonarQube, for<a id="_idIndexMarker2624"/><a id="_idIndexMarker2625"/> example, Java, JavaScript, and Typescript. We<a id="_idIndexMarker2626"/><a id="_idIndexMarker2627"/> add in extra plugin entries for Checkstyle, our Java <a id="_idIndexMarker2628"/><a id="_idIndexMarker2629"/>formatting check tool, and a dependency checker for detecting publicly disclosed vulnerabilities contained within project dependencies:</p>&#13;
			<p class="snippet">  # Sonarqube</p>&#13;
			<p class="snippet">  - name: sonarqube</p>&#13;
			<p class="snippet">    enabled: true</p>&#13;
			<p class="snippet">    source: https://github.com/redhat-cop/helm-charts.git</p>&#13;
			<p class="snippet">    source_path: "charts/sonarqube"</p>&#13;
			<p class="snippet">    source_ref: "sonarqube-0.0.14"</p>&#13;
			<p class="snippet">    sync_policy: *sync_policy_true</p>&#13;
			<p class="snippet">    destination: *ci_cd_ns</p>&#13;
			<p class="snippet">    values:</p>&#13;
			<p class="snippet">      initContainers: true</p>&#13;
			<p class="snippet">      plugins:</p>&#13;
			<p class="snippet">        install:</p>&#13;
			<p class="snippet">          - https://github.com/checkstyle/sonar-checkstyle/releases/download/8.35/checkstyle-sonar-plugin-8.38.jar</p>&#13;
			<p class="snippet">          - https://github.com/dependency-check/dependency-check-sonar-plugin/releases/download/2.0.7/sonar-dependency-check-plugin-2.0.7.jar</p>&#13;
			<p>With the basic SonarQube pod deployed, there is one more piece of configuration we need to automate – the creation <a id="_idIndexMarker2630"/><a id="_idIndexMarker2631"/>of a code quality gate. The quality gate is the hurdle our code must pass before it is deemed ready to release. This boils down to a set of conditions defined in code that specify particular measurements, for example:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>Do we have new blocking issues with the code that was just added?</li>&#13;
				<li>Is the code test coverage higher than a given percentage?</li>&#13;
				<li>Are there any identifiable code vulnerabilities?</li>&#13;
			</ul>&#13;
			<p>SonarQube lets us define these quality gates<span id="footnote-127-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-127">23</a></span> using its REST API. For PetBattle, we use a Kubernetes job to define our quality<a id="_idIndexMarker2632"/><a id="_idIndexMarker2633"/> gate <strong class="inline">AppDefault</strong> and package it as a Helm chart for deployment. The chart is <a id="_idIndexMarker2634"/><a id="_idIndexMarker2635"/>deployed <a id="_idIndexMarker2636"/><a id="_idIndexMarker2637"/>using Ubiquitous Journey and ArgoCD.</p>&#13;
			&#13;
			<div>&#13;
				<div id="_idContainer515" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_13.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 1<a id="_idTextAnchor578"/><a id="_idTextAnchor579"/>5.13: A SonarQube quality gate definition</p>&#13;
			<p>The SonarQube server can be queried via a REST API, whether a recent report against a particular project has passed or failed this quality gate. We have configured a Tekton step and task in our pipelines to automatically check this each time we run a build.</p>&#13;
			<p>Our PetBattle Java applications are configured <a id="_idIndexMarker2638"/><a id="_idIndexMarker2639"/>using Maven to talk to our SonarQube server pod and generate the SonarQube formatted reports during each build, bake, and deploy. In the reusable <strong class="inline">maven-pipeline.yaml</strong>, we call the<a id="_idIndexMarker2640"/><a id="_idIndexMarker2641"/> following target to generate these reports:</p>&#13;
			<p class="snippet"># code analysis step maven pipeline     - name: code-analysis</p>&#13;
			<p class="snippet">      taskRef:</p>&#13;
			<p class="snippet">        name: maven</p>&#13;
			<p class="snippet">      params:</p>&#13;
			<p class="snippet">        - name: MAVEN_MIRROR_URL</p>&#13;
			<p class="snippet">          value: "$(params.MAVEN_MIRROR_URL)"</p>&#13;
			<p class="snippet">        - name: MAVEN_OPTS</p>&#13;
			<p class="snippet">          value: "$(params.MAVEN_OPTS)"</p>&#13;
			<p class="snippet">        - name: WORK_DIRECTORY</p>&#13;
			<p class="snippet">          value: "$(params.APPLICATION_NAME)/$(params.GIT_BRANCH)"</p>&#13;
			<p class="snippet">        - name: GOALS</p>&#13;
			<p class="snippet">          value:</p>&#13;
			<p class="snippet">            - install</p>&#13;
			<p class="snippet">            - org.owasp:dependency-check-maven:check</p>&#13;
			<p class="snippet">            - sonar:sonar</p>&#13;
			<p class="snippet">        - name: MAVEN_BUILD_OPTS</p>&#13;
			<p class="snippet">          value:</p>&#13;
			<p class="snippet">            - '-Dsonar.host.url=http://sonarqube-sonarqube:9000'</p>&#13;
			<p class="snippet">            - '-Dsonar.userHome=/tmp/sonar'</p>&#13;
			<p class="snippet"># code analysis step nodejs pipeline     - name: code-analysis</p>&#13;
			<p class="snippet">      taskRef:</p>&#13;
			<p class="snippet">        name: nodejs</p>&#13;
			<p class="snippet">      params:</p>&#13;
			<p class="snippet">        - name: NPM_MIRROR_URL</p>&#13;
			<p class="snippet">          value: "$(params.NPM_MIRROR_URL)"</p>&#13;
			<p class="snippet">        - name: GOALS</p>&#13;
			<p class="snippet">          value:</p>&#13;
			<p class="snippet">            - "run"</p>&#13;
			<p class="snippet">            - "sonar"</p>&#13;
			<div id="footnote-127" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-127-backlink">23</a>	<a href="https://docs.sonarqube.org/latest/user-guide/quality-gates/">https://docs.sonarqube.org/latest/user-guide/quality-gates/</a></p>&#13;
			</div>&#13;
			<p>Similarly, for the PetBattle UI using <a id="_idIndexMarker2642"/><a id="_idIndexMarker2643"/><strong class="inline">nodejs</strong>, we can configure the client to call SonarQube as part of its Tekton pipeline. Once these steps have successfully run, we can explore the SonarQube Web UI and drill down into any areas to find out more information.</p>&#13;
			<div>&#13;
				<div id="_idContainer516" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_14.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.14: SonarQube project view</p>&#13;
			<p>In a bit of recent development for A/B testing support in the PetBattle UI, some code bugs seemed to have crept in! Developers can drill down and see exactly what the issues are and remediate them in the code base. SonarQube ranks issues based on severity defined in the Language <a id="_idIndexMarker2644"/><a id="_idIndexMarker2645"/>Quality Profile, which <a id="_idIndexMarker2646"/><a id="_idIndexMarker2647"/>can be altered to suit your development code quality needs.</p>&#13;
			<div>&#13;
				<div id="_idContainer517" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_15.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.15: SonarQube drilling into some bug details</p>&#13;
			<p>SonarQube also reports on the last run's code testing coverage. On the code base side, you generate coverage reports using the <strong class="inline">LCOV</strong><span id="footnote-126-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-126">24</a></span> format, so in Java, this is<a id="_idIndexMarker2648"/><a id="_idIndexMarker2649"/> done by <strong class="inline">JaCoCo</strong><span id="footnote-125-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-125">25</a></span> and in <a id="_idIndexMarker2650"/><a id="_idIndexMarker2651"/>JavaScript, the coverage<a id="_idIndexMarker2652"/><a id="_idIndexMarker2653"/> reports are <a id="_idIndexMarker2654"/><a id="_idIndexMarker2655"/>produced by the <strong class="inline">mocha</strong>/<strong class="inline">jasmine</strong> modules. These reports are uploaded into SonarQube and give the team visibility into which parts of their code base need more testing. A nice way to view this information is using the heatmap, which visualizes the bits of code that have near 100% coverage (green), down to<a id="_idIndexMarker2656"/><a id="_idIndexMarker2657"/> areas that are not covered at all 0% (red). The statistics are also reported – the percentage coverage overall, the number of lines covered, and so on.</p>&#13;
			&#13;
			<div>&#13;
				<div id="_idContainer518" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_16.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.16: SonarQube test coverage heatmap for PetBattle</p>&#13;
			&#13;
			<div id="footnote-126" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-126-backlink">24</a>	<a href="https://github.com/linux-test-project/lcov">https://github.com/linux-test-project/lcov</a></p>&#13;
				<div id="footnote-125" class="_idFootnote" epub:type="footnote">&#13;
					<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-125-backlink">25</a>	<a href="https://www.eclemma.org/jacoco/">https://www.eclemma.org/jacoco/</a></p>&#13;
				</div>&#13;
			</div>&#13;
			<p>The last plugin we use for our Java applications is the OWASP Dependency-Check plugin.<span id="footnote-124-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-124">26</a></span> We move security checking "left" in<a id="_idIndexMarker2658"/><a id="_idIndexMarker2659"/> our pipeline. In other words, we want to discover early in the development process when<a id="_idIndexMarker2660"/><a id="_idIndexMarker2661"/> security<a id="_idIndexMarker2662"/><a id="_idIndexMarker2663"/> vulnerabilities or CVEs are creeping into our applications' dependencies. By identifying which dependencies are vulnerable to a CVE <a id="_idIndexMarker2664"/><a id="_idIndexMarker2665"/>early as part of the build cycle, developers are in a much better position to update them, rather than finding there are issues once our applications are deployed.</p>&#13;
			&#13;
			<div>&#13;
				<div id="_idContainer519" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_17.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.17: Dependency-Check plugin report</p>&#13;
			<p>The plugin sources data from multiple open source resources including the US National Vulnerability Database<span id="footnote-123-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-123">27</a></span> and Sonatype OSS Index.<span id="footnote-122-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-122">28</a></span> In conjunction with security team members, developers can verify known vulnerabilities and suppress any false positives using a configuration file. The report is very detailed and includes links to these sites to assist CVE identification and reporting.</p>&#13;
			<div id="footnote-124" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-124-backlink">26</a>	<a href="https://github.com/dependency-check/dependency-check-sonar-plugin">https://github.com/dependency-check/dependency-check-sonar-plugin</a></p>&#13;
			</div>&#13;
			<div id="footnote-123" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-123-backlink">27</a>	<a href="https://nvd.nist.gov/">https://nvd.nist.gov/</a></p>&#13;
				<div id="footnote-122" class="_idFootnote" epub:type="footnote">&#13;
					<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-122-backlink">28</a>	<a href="https://ossindex.sonatype.org/">https://ossindex.sonatype.org/</a></p>&#13;
				</div>&#13;
			</div>&#13;
			<h3 id="_idParaDest-386">Perf Testing (<a id="_idTextAnchor580"/><a id="_idTextAnchor581"/>Non-Functional)</h3>&#13;
			<p>One of our favorite command-line tools for <a id="_idIndexMarker2666"/><a id="_idIndexMarker2667"/>getting fast feedback on the performance of REST APIs is a tool called <strong class="bold">hey</strong>.<span id="footnote-121-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-121">29</a></span> There are <a id="_idIndexMarker2668"/><a id="_idIndexMarker2669"/>a lot of similar <a id="_idIndexMarker2670"/><a id="_idIndexMarker2671"/>tools available. Apache Bench<span id="footnote-120-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-120">30</a></span> is probably the most venerable.</p>&#13;
			&#13;
			<div>&#13;
				<div id="_idContainer520" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_18.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.18: A simple hey run</p>&#13;
			<div id="footnote-121" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-121-backlink">29</a>	<a href="https://github.com/rakyll/hey">https://github.com/rakyll/hey</a></p>&#13;
				<div id="footnote-120" class="_idFootnote" epub:type="footnote">&#13;
					<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-120-backlink">30</a>	<a href="https://httpd.apache.org/docs/2.4/programs/ab.html">https://httpd.apache.org/docs/2.4/programs/ab.html</a></p>&#13;
				</div>&#13;
			</div>&#13;
			<p>We like <strong class="bold">hey</strong> because it is small, fast, written in Golang, and reports statistics in a format we can easily understand. In the preceding screenshot, we can see a very simple invocation using <strong class="inline">hey</strong> on the command line to call the PetBattle API and list all of the pets. We pass in some parameters that represent:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li><strong class="inline">-c</strong>: Number of workers to run concurrently</li>&#13;
				<li><strong class="inline">-n</strong>: Number of requests to run</li>&#13;
				<li><strong class="inline">-t</strong>: Timeout for each request in seconds</li>&#13;
			</ul>&#13;
			<p>We can see the summary statistics <a id="_idIndexMarker2672"/><a id="_idIndexMarker2673"/>reported, and this is the bit we love – a histogram of latency distribution, HTTP status code distribution, as well as DNS timing details. This is super rich information. Histograms are graphs that display the distribution of the continuous response latency data. A histogram reveals properties about the response times that the summary statistics cannot. In statistics, summary data is used to describe the complete dataset – minimum, maximum, mean, and average, for example. <strong class="bold">Hey</strong> gives us these <a id="_idIndexMarker2674"/><a id="_idIndexMarker2675"/>summary statistics at the top of the output.</p>&#13;
			<p>The graph brings the data to life as we can start to understand the distribution of the latency response over the time the test ran. Over the 4.2 seconds it took to send the 100 requests, we can see that most of the data is clustered around the 0.4-second mark, which is nearly 50% of all traffic. Often, in service performance design, we are interested in what the 95% or 99% percentile number is. That is, for all of the sample data, what the response latency is for 95% (or 99%) of the traffic. In this test run, it is measured at 0.57 seconds – in other words, 95% of the data was at or below this mark.</p>&#13;
			<p>The shape of the histogram is also important. Where are the response latencies grouped? We can easily see if the response times are distributed evenly around the mean (Gaussian) or if they have a longer or shorter tail. This can help us characterize the performance of the service under various loads. There are many types of load profiles you could use, for example, burst loads where we throw a lot of instantaneous traffic at our API, compared to more long-lived soak tests under a lower load. You might even have known loads from similar applications in production already. A great open source tool for designing these types of test loads, which can model threading and ramping really well, is Apache JMeter<span id="footnote-119-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-119">31</a></span> and we highly<a id="_idIndexMarker2676"/><a id="_idIndexMarker2677"/> recommend it as a tool to have in your toolbox. To keep things simple, we won't cover that tool here.</p>&#13;
			<div id="footnote-119" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-119-backlink">31</a>	<a href="https://jmeter.apache.org/">https://jmeter.apache.org/</a></p>&#13;
			</div>&#13;
			<p>The two diagrams shown in <em class="italics">Figure 15.19</em> display simple load tests. The one on the left is a burst type of test – 300 consecutive users calling 900 times to our PetBattle API. We can see the 95% is 15.6 seconds – this is quite a long time for users to wait for their cats! The one on the right is a soak test – 50 consecutive users calling 10,000 times to our PetBattle API. A very different set of statistics: a test duration of 461 seconds, and the 95% is 2.8 sec—much better from an end user's perspective.</p>&#13;
			<p>At this point, it is<a id="_idIndexMarker2678"/><a id="_idIndexMarker2679"/> important to think about what the test is actually doing and how it relates to the PetBattle application suite in general. If we think about it, the test may not be totally indicative of the current user interface behavior. For example, we do not perform a call to return all of the images in <a id="_idIndexMarker2680"/><a id="_idIndexMarker2681"/>our MongoDB at once but rather page the results. And there are of course other API endpoints to test, for example, the <strong class="inline">topcats</strong> API, which returns the top three most popular pets and is called every time you visit the home page. We are returning the test dataset we have loaded into PetBattle, that is, around 15 pet images, so it is not a massive amount of data. It's important to always step back and understand this wider context when we run performance tests so we don't end up testing the wrong thing!</p>&#13;
			<div>&#13;
				<div id="_idContainer521" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_19.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">&#13;
Figure 15.19: Burst and soak tests against the PetBattle API</p>&#13;
			<p>Nonetheless, this is good data to ponder. A good result is that both the soak and burst tests only returned HTTP 200 response statuses – there were no error responses from the API. That gives us confidence that we have not broken anything or reached any internal system limits yet. We can also examine the details to make sure DNS resolution is not causing issues from the client-calling perspective.</p>&#13;
			<p>Now we are familiar<a id="_idIndexMarker2682"/><a id="_idIndexMarker2683"/> with the client or calling side of performance testing, let's switch to the PetBattle API application running on the server side. If we browse to the <span class="P---Screen-Text">Developer</span> view and select the <strong class="inline">pet-battle-api</strong> pod in the <strong class="inline">labs-test</strong> namespace, we can see some important server-side information:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>The PetBattle API is autoscaled to two pods.</li>&#13;
				<li>Monitoring metrics for the pods (check the appendix if you haven't enabled this for CRC).</li>&#13;
			</ul>&#13;
			<p>As a developer, we have configured the PetBattle API application to use the <strong class="bold">Horizontal Pod Autoscaler</strong> (<strong class="bold">HPA</strong>). This specifies how the <a id="_idIndexMarker2684"/><a id="_idIndexMarker2685"/>OpenShift Container Platform can automatically increase or decrease the <a id="_idIndexMarker2686"/><a id="_idIndexMarker2687"/>scale of a replication controller or deployment configuration, the number of running pods, based on the metrics collected from the pods that belong to our application.</p>&#13;
			<div>&#13;
				<div id="_idContainer522" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_20.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">&#13;
Figure 15.20: PetBattle API pod in the labs-test namespace</p>&#13;
			<p>In our PetBattle API Helm chart, we specified the HPA with configurable values for minimum pods, maximum pods, as well as the average CPU and memory targets. Using hey, we can now test <a id="_idIndexMarker2688"/><a id="_idIndexMarker2689"/>out various scenarios to help us tune the PetBattle API application under load:</p>&#13;
			<p class="snippet">apiVersion: autoscaling/v2beta2</p>&#13;
			<p class="snippet">kind: HorizontalPodAutoscaler</p>&#13;
			<p class="snippet">metadata:</p>&#13;
			<p class="snippet">  name: {{ include "pet-battle-api.fullname" . }}</p>&#13;
			<p class="snippet">  labels:</p>&#13;
			<p class="snippet">    {{- include "pet-battle-api.labels" . | nindent 4 }}</p>&#13;
			<p class="snippet">spec:</p>&#13;
			<p class="snippet">  scaleTargetRef:</p>&#13;
			<p class="snippet">    {{- if .Values.deploymentConfig }}</p>&#13;
			<p class="snippet">    apiVersion: v1</p>&#13;
			<p class="snippet">    kind: DeploymentConfig</p>&#13;
			<p class="snippet">    {{- else }}</p>&#13;
			<p class="snippet">    apiVersion: apps/v1</p>&#13;
			<p class="snippet">    kind: Deployment</p>&#13;
			<p class="snippet">    {{- end }}</p>&#13;
			<p class="snippet">    name: {{ include "pet-battle-api.fullname" . }}</p>&#13;
			<p class="snippet">  minReplicas: {{ .Values.replicas.min }}</p>&#13;
			<p class="snippet">  maxReplicas: {{ .Values.replicas.max }}</p>&#13;
			<p class="snippet">  metrics:</p>&#13;
			<p class="snippet">    - type: Resource</p>&#13;
			<p class="snippet">      resource:</p>&#13;
			<p class="snippet">        name: cpu</p>&#13;
			<p class="snippet">        target:</p>&#13;
			<p class="snippet">          type: AverageValue</p>&#13;
			<p class="snippet">          averageValue: {{ .Values.hpa.cpuTarget }}</p>&#13;
			<p class="snippet">    - type: Resource</p>&#13;
			<p class="snippet">      resource:</p>&#13;
			<p class="snippet">        name: memory</p>&#13;
			<p class="snippet">        target:</p>&#13;
			<p class="snippet">          type: AverageValue</p>&#13;
			<p class="snippet">          averageValue: {{ .Values.hpa.memTarget }}</p>&#13;
			<p>We initially took a rough guess at these settings in our HPA, for example, <strong class="inline">min replicas = 2</strong>, <strong class="inline">max replicas =6</strong>, <strong class="inline">CPU = 200m</strong>, <strong class="inline">mem = 300Mi</strong>, and set the resource limits and requests in our Deployment appropriately. We always have a minimum of two pods, for high availability reasons. The HPA is configured to scale based on the average memory and CPU loads. We don't yet understand whether the application is memory- or CPU-intensive, so choose to scale based on both these measurements.</p>&#13;
			<div>&#13;
				<div id="_idContainer523" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_21.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure"> Figure 15.21: PetBattle API HPA in action, scaling pods under load</p>&#13;
			<p>We use hey to start a <a id="_idIndexMarker2690"/><a id="_idIndexMarker2691"/>burst workload, 400 concurrent requests, and watch the behavior of the HPA as it starts more pods to keep to the specified memory and CPU averages. Once the test concludes, the HPA scales our workload back down to the minimum as the application recovers resources, in this case through Java garbage collection. OpenShift supports custom metrics for the HPA as well as other types of pod scalers, for example, the Vertical Pod Autoscaler.<span id="footnote-118-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-118">32</a></span></p>&#13;
			&#13;
			<p>To conclude this section, we want to point out one more Kubernetes object that the developer needs in their toolbelt – the <a id="_idIndexMarker2692"/><a id="_idIndexMarker2693"/><strong class="bold">Pod Disruption Budget</strong> (<strong class="bold">PDB</strong>). Again, using a Helm chart template for <a id="_idIndexMarker2694"/><a id="_idIndexMarker2695"/>the PDB, we can limit the number of concurrent disruptions that the PetBattle API application experiences. By setting up a PDB, we can allow for higher availability while permitting the cluster administrator to manage the life cycle of the cluster nodes.</p>&#13;
			<div id="footnote-118" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-118-backlink">32</a>	<a href="https://docs.openshift.com/container-platform/4.7/nodes/pods/nodes-pods-using.html">https://docs.openshift.com/container-platform/4.7/nodes/pods/nodes-pods-using.html</a></p>&#13;
			</div>&#13;
			<p>If the cluster is being updated and nodes are being restarted, we want a minimum of one <strong class="inline">pet-battle-api</strong> pod available at all times:</p>&#13;
			<p class="snippet">$ oc get pdb </p>&#13;
			<p class="snippet">NAME       MIN AVAILABLE MAX UNAVAILABLE  ALLOWED DISRUPTIONS   AGE</p>&#13;
			<p class="snippet">pet-battle-api   1           N/A                  1             46h</p>&#13;
			<p>This ensures a high level of <a id="_idIndexMarker2696"/><a id="_idIndexMarker2697"/>business service for our PetBattle API. We can see <strong class="inline">ALLOWED_DISRUPTIONS</strong> is set to 1 – this is because, at the time, the HPA had scaled the number of available replicas to 3 and this will change as the number of available pods changes.</p>&#13;
			<p>One of the great things about performance testing applications on OpenShift is that all of the tools are at a developer's fingertips to be able to configure, test, measure, and tune their applications to achieve high availability and performance when under load. Each application service is independently scalable, tunable, and deployable, which makes for a faster and targeted feedback loop when dealing with scale and performance issues.</p>&#13;
			<p>In the next section, we are going to take a look at what makes a good OpenShift Kubernetes citizen, automating Kubernetes resource validation as part of our pipeline.</p>&#13;
			<h3 id="_idParaDest-387"><a id="_idTextAnchor582"/>Resource Validation</h3>&#13;
			<p>O<a id="_idTextAnchor583"/><a id="_idTextAnchor584"/>ne aspect of testing that <a id="_idIndexMarker2698"/><a id="_idIndexMarker2699"/>doesn't yet get much thought is the quality of the Kubernetes resources being deployed on the cluster. For applications to be considered <em class="italics">good citizens</em> on Kubernetes, there are a number of deployment best practices to be followed—including health checks, resource<a id="_idIndexMarker2700"/><a id="_idIndexMarker2701"/> limits, labels, and so on—and we will go through a number of these in in <em class="italics">Chapter 16, Own It</em>. However, we need to validate the resource definitions being applied to the cluster to ensure a high level of compliance to not only industry recommendations but also any other resource recommendations that we see fit to add. This is where <strong class="bold">Open Policy Agent</strong> (<strong class="bold">OPA</strong>)<span id="footnote-117-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-117">33</a></span> and associated tools can come into play. This enables us to validate resource definitions during a CI pipeline and also when applying resources to a cluster. OPA by itself is a policy validator and the policies are written using a language<a id="_idIndexMarker2702"/><a id="_idIndexMarker2703"/> called Rego. Additional OPA tools <a id="_idIndexMarker2704"/><a id="_idIndexMarker2705"/>such as Conftest<span id="footnote-116-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-116">34</a></span> and Gatekeeper<span id="footnote-115-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-115">35</a></span> add a lot of value and governance from a usability and deployment perspective. OPA is also embeddable into <a id="_idIndexMarker2706"/><a id="_idIndexMarker2707"/>other third-party tools such as KubeLinter.<span id="footnote-114-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-114">36</a></span></p>&#13;
			<div id="footnote-117" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-117-backlink">33</a>	<a href="https://www.openpolicyagent.org/">https://www.openpolicyagent.org/</a></p>&#13;
				<div id="footnote-116" class="_idFootnote" epub:type="footnote">&#13;
					<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-116-backlink">34</a>	<a href="https://github.com/open-policy-agent/conftest">https://github.com/open-policy-agent/conftest</a></p>&#13;
					<div id="footnote-115" class="_idFootnote" epub:type="footnote">&#13;
						<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-115-backlink">35</a>	<a href="https://github.com/open-policy-agent/gatekeeper">https://github.com/open-policy-agent/gatekeeper</a></p>&#13;
						<div id="footnote-114" class="_idFootnote" epub:type="footnote">&#13;
							<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-114-backlink">36</a>	<a href="https://github.com/stackrox/kube-linter">https://github.com/stackrox/kube-linter</a></p>&#13;
						</div>&#13;
					</div>&#13;
				</div>&#13;
			</div>&#13;
			<p>We haven't used OPA's server-side validation component, Gatekeeper,<span id="footnote-113-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-113">37</a></span> as part of PetBattle but there are example Rego policies in the <a id="_idIndexMarker2708"/><a id="_idIndexMarker2709"/>Red Hat Community of Practice GitHub repo<span id="footnote-112-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-112">38</a></span> that are definitely worth exploring. If this is something of interest to you, definitely check out the blog on OpenShift.com that details setting up all of these components.<span id="footnote-111-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-111">39</a></span></p>&#13;
			&#13;
			<p>However, to show how <a id="_idIndexMarker2710"/><a id="_idIndexMarker2711"/>easy it is to use client-side resource validation and why you should include at least some resource validation in a pipeline, a simple<a id="_idIndexMarker2712"/><a id="_idIndexMarker2713"/> Rego example has been created. Rego policies are easy enough to write<a id="_idIndexMarker2714"/><a id="_idIndexMarker2715"/> and the Rego playground<span id="footnote-110-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-110">40</a></span> is a great place to write and verify policies, so check it out.</p>&#13;
			&#13;
			<p>Let's get into an example. In our Non-Functional Requirements Map, we said we wanted to be consistent with our labeling. It makes sense that we should adopt the Kubernetes best practice that suggests the <strong class="inline">app.kubernetes.io/instance</strong> label should be on all resources, so let's see how we can write a test to this effect and add it to our pipeline in Jenkins.</p>&#13;
			<p>The makeup of a policy that denies the creation of a resource is simple enough. A message is formed and passed back to the interpreter if all of the statements are true in the rule. For example, we have written a policy that checks that all resources conform to Kubernetes best practice for naming conventions. The policy here is checking whether <strong class="inline">app.kubernetes.io/instance</strong> exists on the resource supplied to it (<strong class="inline">input</strong>). If each statement is true, then a message is returned as the error, guiding someone to fix the issue:</p>&#13;
			<p class="snippet">deny[msg] {</p>&#13;
			<p class="snippet">  label := "app.kubernetes.io/instance"</p>&#13;
			<p class="snippet">  not input.metadata.labels[label]</p>&#13;
			<p class="snippet">  msg := sprintf("\n%s: does not contain all the expected k8s labels</p>&#13;
			<p class="snippet">                 in 'metadata.labels'.\n Missing '%s'. \nSee: https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels", [input.kind, label])</p>&#13;
			<p class="snippet">}</p>&#13;
			<div id="footnote-113" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-113-backlink">37</a>	<a href="https://github.com/open-policy-agent/gatekeeper">https://github.com/open-policy-agent/gatekeeper</a></p>&#13;
				<div id="footnote-112" class="_idFootnote" epub:type="footnote">&#13;
					<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-112-backlink">38</a>	<a href="https://github.com/redhat-cop/rego-policies">https://github.com/redhat-cop/rego-policies</a></p>&#13;
					<div id="footnote-111" class="_idFootnote" epub:type="footnote">&#13;
						<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-111-backlink">39</a>	<a href="https://www.openshift.com/blog/automate-your-security-practices-and-policies-on-openshift-with-open-policy-agent">https://www.openshift.com/blog/automate-your-security-practices-and-policies-on-openshift-with-open-policy-agent</a></p>&#13;
					</div>&#13;
				</div>&#13;
			</div>&#13;
			<div id="footnote-110" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-110-backlink">40</a>	<a href="https://play.openpolicyagent.org/">https://play.openpolicyagent.org/</a></p>&#13;
			</div>&#13;
			<p>We can combine this rule <a id="_idIndexMarker2716"/><a id="_idIndexMarker2717"/>with Conftest and a Helm template to create a way to statically validate our resources. In the PetBattle frontend code, there is a policy folder that has a few more policies to check whether all the standard Kubernetes labels<span id="footnote-109-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-109">41</a></span> are set on our generated resources after we run the <strong class="inline">helm template</strong> command. By running a few commands, we can verify these are in place. First, we template our chart to produce the <a id="_idIndexMarker2718"/><a id="_idIndexMarker2719"/>Kubernetes resources we will apply in deploying our software, and secondly, we tell Conftest to check each file <a id="_idIndexMarker2720"/><a id="_idIndexMarker2721"/>generated against the rule:</p>&#13;
			&#13;
			<p class="snippet"># from the pet battle front end repository (https://github.com/petbattle/pet-battle.git)</p>&#13;
			<p class="snippet">$ for file in $(ls policy/helm-output/pet-battle/templates/); do conftest test policy/helm-output/pet-battle/templates/$file; done</p>&#13;
			<p class="snippet">6 tests, 6 passed, 0 warnings, 0 failures, 0 exceptions</p>&#13;
			<p class="snippet">FAIL - policy/helm-output/pet-battle/templates/deploymentconfig.yaml - </p>&#13;
			<p class="snippet">DeploymentConfig: does not contain all the expected k8s labels in 'metadata.labels'.</p>&#13;
			<p class="snippet"> Missing 'app.kubernetes.io/name'. </p>&#13;
			<p class="snippet">See: https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels</p>&#13;
			<p class="snippet">6 tests, 5 passed, 0 warnings, 1 failure,  0 exceptions</p>&#13;
			<p class="snippet">6 tests, 6 passed, 0 warnings, 0 failures, 0 exceptions</p>&#13;
			<p class="snippet">6 tests, 6 passed, 0 warnings, 0 failures, 0 exceptions</p>&#13;
			<p>When executing the rules from the command line, we get a good insight into what's missing from our chart. Of course, we could just assume that we'd always make our charts adhere to the best practices, but the <strong class="inline">jenkins-agent-helm</strong> has also got the Conftest binary so we can execute the preceding statements in our Jenkins pipeline too. This example might seem simple but, hopefully, it gives you some idea of the things that can be automated and tested that might seem less obvious.</p>&#13;
			<div id="footnote-109" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-109-backlink">41</a>	<a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/#labels">https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/#labels</a></p>&#13;
			</div>&#13;
			<h3 id="_idParaDest-388"><a id="_idTextAnchor585"/>Image Scanning</h3>&#13;
			<p>Red Ha<a id="_idTextAnchor586"/><a id="_idTextAnchor587"/><a id="_idTextAnchor588"/>t <a id="_idIndexMarker2722"/><a id="_idIndexMarker2723"/>provides <a id="_idIndexMarker2724"/><a id="_idIndexMarker2725"/>the Quay Container Security Operator in OpenShift to bring Quay and Clair image scanning and vulnerability information into our OpenShift cluster. Any container image that is <a id="_idIndexMarker2726"/><a id="_idIndexMarker2727"/>hosted on <a href="http://Quay.io">Quay.io</a> is scanned by Clair.</p>&#13;
			<div>&#13;
				<div id="_idContainer524" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_22.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.22: Quay Container Security Operator</p>&#13;
			<p>Any image vulnerability data is exposed back in the OpenShift Web UI so that users and administrators <a id="_idIndexMarker2728"/><a id="_idIndexMarker2729"/>can easily view which images are considered vulnerable and which namespace they are deployed to.</p>&#13;
			<div>&#13;
				<div id="_idContainer525" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_23.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.23: Vulnerable container images</p>&#13;
			<p>With this operator deployed, the OpenShift overview status displays image vulnerability data, which an operator can drill into to find out the status of container images running on the platform. For PetBattle, we don't have any enforcement for image vulnerabilities discovered in our cluster. If <a id="_idIndexMarker2730"/><a id="_idIndexMarker2731"/>we wanted to move the security scanner "left" in our deployment pipeline, there are some great open source scanning tools available on the OpenSCAP<a id="_idIndexMarker2732"/><a id="_idIndexMarker2733"/> website.<span id="footnote-108-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-108">42</a></span></p>&#13;
			&#13;
			<h3>Other Non-functional Test<a id="_idTextAnchor589"/><a id="_idTextAnchor590"/>ing</h3>&#13;
			<p>There are lots of <a id="_idIndexMarker2734"/><a id="_idIndexMarker2735"/>other types of testing we can do to validate our application. In this section are some of the things we feel are important to include in a pipeline, but the reality is there is much more than just this list and books could be written on this topic in and of itself!</p>&#13;
			<h3 id="_idParaDest-389"><a id="_idTextAnchor591"/>Linting</h3>&#13;
			<p>A linter is a sta<a id="_idTextAnchor592"/><a id="_idTextAnchor593"/>tic code <a id="_idIndexMarker2736"/><a id="_idIndexMarker2737"/>analysis tool that can check a code base for common pitfalls in design or stylistic errors. This does not check the compiled application, but the structure of the application. This is super important for languages that are not compiled, such as JavaScript. Browsers can interpret JavaScript in different ways so consistency is super critical.</p>&#13;
			<p>If you think about a large enterprise application, there could be hundreds of developers working on the one code base. These developers could even be globally distributed with different teams looking after different parts of the application's life cycle. Having consistency in the approach to writing the software can dramatically improve maintenance costs. JavaScript is very flexible in how you can write it, whether this is from a functional programming standpoint or object-oriented, so it is important to get this consistency right.</p>&#13;
			<p>The PetBattle frontend uses TSLint/ESLint<span id="footnote-107-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-107">43</a></span> to check the<a id="_idIndexMarker2738"/><a id="_idIndexMarker2739"/> style of the code adheres to a standard set of rules. These rules can be manipulated by the team, but the rules are checked into Git so if someone was to disable them or manipulate them, it would be noticed. Our Jenkins pipeline is configured to automatically check the code base using the <strong class="inline">npm lint</strong> command and our build will fail if a developer does not adhere to the standard.</p>&#13;
			<div id="footnote-108" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-108-backlink">42</a>	<a href="https://www.open-scap.org">https://www.open-scap.org</a></p>&#13;
			</div>&#13;
			<div id="footnote-107" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-107-backlink">43</a>	<a href="https://eslint.org/">https://eslint.org/</a></p>&#13;
			</div>&#13;
			<div>&#13;
				<div id="_idContainer526" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_24.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.24: Linting PetBattle's frontend locally scans both the JavaScript and HTML</p>&#13;
			<p>For Java Quarkus <a id="_idIndexMarker2740"/><a id="_idIndexMarker2741"/>apps, Checkstyle<span id="footnote-106-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-106">44</a></span> is used to <a id="_idIndexMarker2742"/><a id="_idIndexMarker2743"/>analyze the code base.</p>&#13;
			&#13;
			<p>For Kubernetes resources, the aforementioned Open Policy Agent can assist, and Helm also has the <strong class="inline">helm lint</strong><span id="footnote-105-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-105">45</a></span> command to validate your charts.</p>&#13;
			&#13;
			<h3 id="_idParaDest-390"><a id="_idTextAnchor594"/>Code Coverage</h3>&#13;
			<p>So, you've w<a id="_idTextAnchor595"/><a id="_idTextAnchor596"/>ritten a load of<a id="_idIndexMarker2744"/><a id="_idIndexMarker2745"/> tests and you think things are going great – but how do you know your tests are any good and covering all parts of the code base? Allow me to introduce code coverage metrics! A code<a id="_idIndexMarker2746"/><a id="_idIndexMarker2747"/> coverage reporter is a piece of software that runs alongside your unit<a id="_idIndexMarker2748"/><a id="_idIndexMarker2749"/> test suites to see what lines of code are executed by the tests and how many times. Coverage reports can also highlight when if/else control flows within an application are not being tested. This insight can provide valuable feedback as to areas of a system that remain untested and ultimately reduce the number of bugs. </p>&#13;
			<div id="footnote-106" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-106-backlink">44</a>	<a href="https://checkstyle.sourceforge.io/">https://checkstyle.sourceforge.io/</a></p>&#13;
			</div>&#13;
			<div id="footnote-105" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-105-backlink">45</a>	<a href="https://helm.sh/docs/helm/helm_lint/">https://helm.sh/docs/helm/helm_lint/</a></p>&#13;
			</div>&#13;
			<p>Our PetBattle frontend is configured to run a coverage report when our Jest tests execute. Jest makes generating the report very simple as it has a flag that can be passed to the test runner to collect the coverage for us. The coverage report is run on every execution of the build and so should be reported through Jenkins.</p>&#13;
			<div>&#13;
				<div id="_idContainer527" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_25.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.25: Code coverage report from the frontend unit tests locally</p>&#13;
			<p>When executing <a id="_idIndexMarker2750"/><a id="_idIndexMarker2751"/>our tests in the Jenkins pipeline, we have configured Jest to produce an HTML report that can be reported by Jenkins on the <span class="P---Screen-Text">jobs</span> page. For any build execution, the report is added to the <span class="P---Screen-Text">jobs</span> home page. The report will allow us to discover what lines are being missed by our tests. Being able to drill into a report like this can give a good insight into where our testing is lacking.</p>&#13;
			<div>&#13;
				<div id="_idContainer528" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_26.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.26: Code coverage report in Jenkins gives us detailed insight</p>&#13;
			<p>So, what should I <a id="_idIndexMarker2752"/><a id="_idIndexMarker2753"/>do with these results? Historically, we have worked where coverage is low. It can serve as a great talking point to bring up in a Retrospective. Printing out the reports and discussing them as a team is a great way to assess why the team is struggling to write enough tests. Sometimes teams are drowning by being overwhelmed with pressure to churn out features and so testing can slip to the wayside. Having a coverage reporter in your build can help keep a team honest. You could even set thresholds so that if testing coverage falls below a certain percentage (some teams aim for 80% and above), the build will fail, thus blocking the pipeline until the quality is increased.</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-391" class="Author-Heading"><a id="_idTextAnchor597"/>Untested Software Watermark</h2>&#13;
			<div>&#13;
				<div id="_idContainer529" class="IMG---Figure" style="float: right; margin: 6px; hight:6cm; width:6cm;">&#13;
					<img src="../Images/Author_26.jpg" alt="" width="250" height="250"/>&#13;
				</div>&#13;
			</div>&#13;
			<p>I worked on a project a long time ago that was poorly structured. I was a member of the DevOps team, which I know now is an antipattern in most implementations! This project had many issues, the team had planned three Sprints in advance but hadn’t allocated enough time for testing. It was always the thing that got squeezed.</p>&#13;
			<p>Through Retrospectives with the teams, we discovered that there was simply not enough time for tests. This may sound hard to hear, but the root cause for this was not laziness by the team or a lack of skills; it really was time. The project was running in 8-week blocks that were pre-planned from the beginning with a fixed output at the end. The team thought they were doing Scrum, but in actual fact, they had milestones of functionality to accomplish each sprint and there was no feedback loop. Of course, none of the Scrum team members were involved in the sizing or planning ceremonies either. This meant the teams were constantly under pressure to deliver.</p>&#13;
			<div>&#13;
				<div id="_idContainer530" class="IMG---Figure" style="float: left; margin: 6px; hight:6cm; width:6cm;">&#13;
					<img src="../Images/Draft.jpg" alt="" width="220" height="220"/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Through a Retrospective, we decided to try to radiate some of this pressure the teams were under as we were not happy that quality was being sacrificed for some arbitrary deadlines. Knowing that failing the pipeline simply would not work for these customers, we had to get creative in showing the software quality. We decided to inject a watermark into any application that had low test coverage. This watermark resembled a DRAFT logo you would find on any document, but ours was a little different. </p>&#13;
			<p>A large banner reading UNTESTED SOFTWARE was placed across the applications that failed the tests. This watermark did not affect the user behavior of the app; it was just an overlay but it was an amazing way to get people talking. Seeing a giant banner saying UNTESTED is a surefire way to have people question why things have gotten this way.</p>&#13;
			</div>&#13;
			<p>Let's look at some other ways we can visualize risks during continuous delivery.</p>&#13;
			<h3 id="_idParaDest-392"><a id="_idTextAnchor598"/><a id="_idTextAnchor599"/>The OWASP Zed Attack Proxy (ZAP)</h3>&#13;
			<p>Security scanning is always a <a id="_idIndexMarker2754"/><a id="_idIndexMarker2755"/>hot topic. From image scanning, which we discussed earlier, to dependency checking for our application that happens in our pipelines, there are limitless numbers of things to automate from a <a id="_idIndexMarker2756"/><a id="_idIndexMarker2757"/>security perspective. Let's take another example of something that can be useful to include in a pipeline – the OWASP Zed Attack Proxy.<span id="footnote-104-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-104">46</a></span></p>&#13;
			&#13;
			<p>From their website: <em class="italics">The OWASP Zed Attack Proxy (ZAP) is one of the world's most popular free security tools which lets you automatically find security vulnerabilities in your applications. This allows the developers to automate </em><em class="italics"><a id="_idIndexMarker2758"/><a id="_idIndexMarker2759"/></em><em class="italics">penetration testing and security regression testing of the application in the CI/CD pipeline.</em></p>&#13;
			<p>Adding the ZAP <a id="_idIndexMarker2760"/><a id="_idIndexMarker2761"/>security scanning tool to our pipelines is simple. Just add the following <strong class="inline">stage</strong> and add the URL you <a id="_idIndexMarker2762"/><a id="_idIndexMarker2763"/>want to test. The source code for this image is available, like our other Jenkins images from the Red Hat CoP.<span id="footnote-103-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-103">47</a></span> The ZAP scan in Jenkins will produce a report showing some potential vulnerabilities in our application.</p>&#13;
			&#13;
			<p class="snippet">stage('🤖 OWASP Scan') {</p>&#13;
			<p class="snippet">  agent { label "jenkins-agent-zap" }</p>&#13;
			<p class="snippet">  steps {</p>&#13;
			<p class="snippet">    sh '''</p>&#13;
			<p class="snippet">      /zap/zap-baseline.py -r index.html -t http://&lt;some website url&gt; || return_code=$?</p>&#13;
			<p class="snippet">      echo "exit value was  - " $return_code</p>&#13;
			<p class="snippet">    '''</p>&#13;
			<p class="snippet">  }</p>&#13;
			<p class="snippet">  post {</p>&#13;
			<p class="snippet">    always {</p>&#13;
			<p class="snippet">      // publish html</p>&#13;
			<p class="snippet">      publishHTML target: [</p>&#13;
			<p class="snippet">          allowMissing: false,</p>&#13;
			<p class="snippet">          alwaysLinkToLastBuild: false,</p>&#13;
			<p class="snippet">          keepAll: true,</p>&#13;
			<p class="snippet">          reportDir: '/zap/wrk',</p>&#13;
			<p class="snippet">          reportFiles: 'index.html',</p>&#13;
			<p class="snippet">          reportName: 'OWASP Zed Attack Proxy'</p>&#13;
			<p class="snippet">        ]</p>&#13;
			<p class="snippet">    }</p>&#13;
			<p class="snippet">  }</p>&#13;
			<p class="snippet">}</p>&#13;
			<div id="footnote-104" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-104-backlink">46</a>	<a href="https://www.zaproxy.org/">https://www.zaproxy.org/</a></p>&#13;
			</div>&#13;
			<div id="footnote-103" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-103-backlink">47</a>	<a href="https://github.com/redhat-cop/containers-quickstarts/tree/master/jenkins-agents">https://github.com/redhat-cop/containers-quickstarts/tree/master/jenkins-agents</a></p>&#13;
			</div>&#13;
			<p>In doing so, the web report that's created can be viewed in Jenkins, which gives great details on the cause of the security vulnerability as well as any action that should be taken to remedy it.</p>&#13;
			<div>&#13;
				<div id="_idContainer531" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_27.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.27: Example Zap report for PetBattle </p>&#13;
			<p>In the final non-functional <a id="_idIndexMarker2764"/><a id="_idIndexMarker2765"/>testing section, let's have a look at deliberately breaking our code using a technique called chaos engineering. </p>&#13;
			<h3 id="_idParaDest-393">Ch<a id="_idTextAnchor600"/><a id="_idTextAnchor601"/>aos Engineering</h3>&#13;
			<p>Chaos engineering is the <a id="_idIndexMarker2766"/><a id="_idIndexMarker2767"/>process of deliberately breaking, hobbling, or impacting a system to see how it performs and whether it recovers in the<a id="_idIndexMarker2768"/><a id="_idIndexMarker2769"/> ensuing "chaos." While most testing is seen <a id="_idIndexMarker2770"/><a id="_idIndexMarker2771"/>as an endeavor to understand how a system performs in a known, stable state, chaos engineering is the computing equivalent of setting a bull free in a fine-china shop—you know it's going to end badly but you just don't know exactly the magnitude of how bad it's going to be.</p>&#13;
			<p>The purpose of chaos engineering is to build confidence<a id="_idIndexMarker2772"/><a id="_idIndexMarker2773"/> in the resiliency of <a id="_idIndexMarker2774"/><a id="_idIndexMarker2775"/>the system. It also allows you to better understand where <a id="_idIndexMarker2776"/><a id="_idIndexMarker2777"/>breakage points occur and the blast radius of any failures. There are many resilience features built into the Kubernetes API specification. Pod replicas are probably the simplest mechanism, having more than one of your applications running at any given time. It is also desirable to use application-specific mechanisms such as circuit breakers, which prevent failures from spreading <a id="_idIndexMarker2778"/><a id="_idIndexMarker2779"/>throughout your system. Chaos engineering takes these ideas one step further and tests a system when one or more components fully or partially fail, such as when CPU or memory resources are low.</p>&#13;
			<p>The basic premise is that the system under test is observed in a stable working state, then a fault is injected. The system is then observed to see if it recovers successfully from the fault or not. Outcomes from such testing are a potential list of areas to tune/fix as well as an understanding <a id="_idIndexMarker2780"/><a id="_idIndexMarker2781"/>of the <strong class="bold">Mean Time to Recovery</strong> (<strong class="bold">MTTR</strong>) of a system. It's important to note that chaos engineering is<a id="_idIndexMarker2782"/><a id="_idIndexMarker2783"/> focused on the system as a whole—both application and infrastructure performance need to be considered and tested.</p>&#13;
			<p>One of the key mantras <a id="_idIndexMarker2784"/><a id="_idIndexMarker2785"/>behind chaos engineering is contained in its defining principles<span id="footnote-102-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-102">48</a></span> – <em class="italics">The need to identify weaknesses before they manifest in system-wide, aberrant behaviors</em>.</p>&#13;
			&#13;
			<p>This is one of the most important aspects to be considered when adopting this approach. You don't want to be learning about weaknesses during a production-impacting incident. It's similar to the rationale behind regularly testing disaster recovery plans. To paraphrase, a colleague of ours here at Red Hat said, "<em class="italics">When the excrement hits the fan, the first thing to do is turn off the fan!</em>" Not much time for learning there.</p>&#13;
			<p>There are a number of tools and frameworks that can help with setting up a chaos engineering <a id="_idIndexMarker2786"/><a id="_idIndexMarker2787"/>practice. Here's<a id="_idIndexMarker2788"/><a id="_idIndexMarker2789"/> some to <a id="_idIndexMarker2790"/><a id="_idIndexMarker2791"/>get started with (though there are others):</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>Litmus Chaos<span id="footnote-101-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-101">49</a></span></li>&#13;
				<li>Kraken<span id="footnote-100-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-100">50</a></span></li>&#13;
				<li>Chaos Mesh<span id="footnote-099-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-099">51</a></span></li>&#13;
			</ul>&#13;
			<div id="footnote-102" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-102-backlink">48</a>	<a href="https://principlesofchaos.org/">https://principlesofchaos.org/</a></p>&#13;
			</div>&#13;
			<div id="footnote-101" class="_idFootnote" epub:type="footnote"><p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-101-backlink">49</a>	<a href="https://litmuschaos.io/">https://litmuschaos.io/</a></p>&#13;
</div>&#13;
			<div id="footnote-100" class="_idFootnote" epub:type="footnote"><p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-100-backlink">50</a>	<a href="https://github.com/cloud-bulldozer/kraken">https://github.com/cloud-bulldozer/kraken</a></p>&#13;
</div>&#13;
			<div id="footnote-099" class="_idFootnote" epub:type="footnote"><p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-099-backlink">51</a>	<a href="https://chaos-mesh.org/">https://chaos-mesh.org/</a></p>&#13;
</div>&#13;
			<p>In a world where practices such as everything-as-code and GitOps are our only way to build software and the systems that support them, a great way to validate the ability to respond to missing items is to redeploy everything, including your infrastructure, from scratch every week or every night! This might seem extreme, but it's a great way to validate that there is no hidden magic that someone has forgotten to write down or codify.</p>&#13;
			&#13;
<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-394" class="Author-Heading"><a id="_idTextAnchor602"/>Accidental Chaos Testing</h2>&#13;
			<div>&#13;
				<div id="_idContainer532" class="IMG---Figure" style="float: right; margin: 6px; hight:6cm; width:6cm;">&#13;
					<img src="../Images/Donal.jpg" alt="" width="250" height="250"/>&#13;
				</div>&#13;
			</div>&#13;
			<p>This is a story that I used to be reluctant to share, but over the years (and having done it twice), I realized it was actually a good thing to have done.</p>&#13;
			<p>While working for an airline, I accidentally deleted the <strong class="inline">labs-ci-cd</strong> project along with a few other namespaces where our apps were deployed, including the authentication provider for our cluster. At the time, we were several weeks into our development. We were used to re-deploying applications and it was not a big deal for us to delete CI tools such as Nexus or Jenkins, knowing that our automation would kick back in swiftly to redeploy them.</p>&#13;
			<p>However, on this engagement, we were also using GitLab and, unfortunately for me, GitLab was in the same project as these other tools!</p>&#13;
			<p>I checked with the team first and asked whether it was OK to rebuild everything in our tooling namespace. I got a resounding "yes" from my teammates, so proceeded to delete some of the things I thought needed to be cleared out and accidentally removed a few extra projects. About 30 seconds later, someone on the team perked up and asked, <em class="italics">Is Git down for anyone else?</em> This was promptly followed by another person saying, <em class="italics">Is anyone else not able to log in to the cluster?</em> My face lit up red as I immediately realized what I'd just done. Git, as we keep saying in the book, is our single source of truth. <em class="italics">If it's not in Git, it's not real</em> is our mantra! We even had it written on the walls! But I had just deleted it.</p>&#13;
			<p>So, what happens when some silly person accidentally deletes it? After the initial shock and panic, the team pulled the Andon Cord. We quickly stormed together to see what exactly had happened in order to plan how we could recover not just Git but all the things we'd added to the cluster. Luckily for us, everything we had done was stored in Git so we were able to redeploy our tools and push our local, distributed copies of the software and infrastructure back into the shared Git repository.</p>&#13;
			<p>The team was cross-functional and had all the tools and access we needed to be able to respond to this. Within 1 hour, we had fully restored all our applications and tools with all of our automation running smoothly again.</p>&#13;
			<p>I think the real power in this example is how, given the right equipment and the right ownership, an empowered team can have it all. We acted as one unit fixing things at lightning speed. We were not stuck waiting in a queue or having to raise a ticket on another team to restore our infrastructure. We could do it for ourselves within minutes – not days or weeks later.</p>&#13;
			<p>Another thing I learned was not to keep Git in the same project as the other tools in case another person like me comes along. I also learned to be mindful of the permissions we have within a cluster. As an administrator, I was able to remove things that perhaps I should not have been playing with.</p>&#13;
			</div>&#13;
			<p>So we've <a id="_idIndexMarker2792"/><a id="_idIndexMarker2793"/>written the code, tested, quality-checked it and even scanned it for vulnerabilities. Now it's time to deploy it onto the cluster. Let's explore one of the key areas of benefit of using Kubernetes - the different ways you can deploy applications depending on your needs and perform user-driven experiments to determine what features your users prefer.</p>&#13;
			<h2 id="_idParaDest-395"><a id="_idTextAnchor603"/>Advanced Deployments</h2>&#13;
			<p>The time between software being written and tested till it is deployed in production should be as short as possible. That way your organization is able to realize value from the software changes as quickly as possible. The modern approach to this problem is, of course, through automation. There are simply too many details and configuration items that need to be changed when deploying to production that even for a small application suite like PetBattle, manual deployment becomes error-prone and tedious. This drive to reduce manual toil is at the heart of many of the DevOps practices we have been discovering in this book.</p>&#13;
			<p>We can minimize the downtime (ideally to zero!) during software deployment changes by adopting the right application architecture and combining that with the many platform capabilities that <a id="_idIndexMarker2794"/><a id="_idIndexMarker2795"/>OpenShift offers. Let's look at some common deployment strategies that OpenShift supports:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>Rolling deployment:<ul style="list-style-type:disc;"><li>Spin up a pod<a id="_idIndexMarker2796"/><a id="_idIndexMarker2797"/> of the new version and then spin down a pod of the existing old version automatically. Very useful for a zero-downtime approach.</li>&#13;
</ul></li>&#13;
				<li>Canary deployment:<ul style="list-style-type:disc;"><li>Spin up a single<a id="_idIndexMarker2798"/><a id="_idIndexMarker2799"/> pod of the new <a id="_idIndexMarker2800"/><a id="_idIndexMarker2801"/>version, perform testing to ensure that everything is working correctly, and then replace all the old pods with new ones. </li>&#13;
</ul></li>&#13;
				<li>Blue/Green deployment:<ul style="list-style-type:disc;"><li>Create a parallel <a id="_idIndexMarker2802"/><a id="_idIndexMarker2803"/>deployment and verify that everything is working correctly before switching traffic over.</li>&#13;
<li>Service Mesh traffic mirroring functionality can be useful with this approach to validate that the new version is working as expected.</li>&#13;
</ul></li>&#13;
				<li>Recreate deployment:<ul style="list-style-type:disc;"><li>Basically, scale the <a id="_idIndexMarker2804"/><a id="_idIndexMarker2805"/>existing pods down to zero and then spin up the new version.</li>&#13;
<li>Use where <a id="_idIndexMarker2806"/><a id="_idIndexMarker2807"/>an application must be restarted, for example, to migrate database schema or tables.</li>&#13;
<li>Think of this<a id="_idIndexMarker2808"/><a id="_idIndexMarker2809"/> as a Ripley deployment: "take off and nuke the entire site from orbit. It's the only way to be sure."<span id="footnote-098-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-098">52</a></span></li>&#13;
</ul></li>&#13;
			</ul>&#13;
			<p>We can roll back to previous deployment versions using the Helm chart life cycle or the out-of-the-box <strong class="inline">oc rollback</strong> support. Images and configuration are versioned and cached in OpenShift to easily support rolling back to previous versions.</p>&#13;
			<div id="footnote-098" class="_idFootnote" epub:type="footnote"><p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-098-backlink">52</a>	<a href="https://en.wikiquote.org/wiki/Aliens_(film)">https://en.wikiquote.org/wiki/Aliens_(film)</a></p>&#13;
</div>&#13;
			<h3 id="_idParaDest-396"><a id="_idTextAnchor604"/><a id="_idTextAnchor605"/>A/B Testing</h3>&#13;
			<p>A/B testing an application is an <a id="_idIndexMarker2810"/><a id="_idIndexMarker2811"/>amazing way to test or validate a new feature in production. The process is pretty simple: you deploy two (or more) different versions of your application to production, measure some aspect, and see which version performs <em class="italics">better</em>. Given that A/B testing is primarily a mechanism of gauging user experience, <em class="italics">better</em> depends on what aspect/feature you're experimenting with. For example, you could make a subtle change to a web page layout and measure how long it takes for the user to navigate to some button or how long the user continues to interact with specific items on the page.</p>&#13;
			<p>It's a brilliant way to de-risk a new release or validate some new business or UI features with a smaller audience before releasing to a wider group. User behavior can be captured and experiments can be run to make informed decisions about what direction a product should take.</p>&#13;
			<h3 id="_idParaDest-397"><a id="_idTextAnchor606"/><a id="_idTextAnchor607"/>The Experiment</h3>&#13;
			<p>Let's cast our minds back<a id="_idIndexMarker2812"/><a id="_idIndexMarker2813"/> to the earlier chapters where we spoke about generating options. There we spoke about the importance of experiments and our Value Slicing board included an item for which we could do an A/B test. One experiment that came up was to assess how users would vote for cats in the competition. Should they just be able to upvote (with a 👍) or should they be able to downvote (👎) too? We can build and deploy two versions of our application: one with the ability to both upvote and downvote, and one with just the ability to upvote. Our experiment is simple: to track how often people actually use the downvote button, so we can decide whether it's a feature we need or whether we should focus on building different functionality.</p>&#13;
			<div>&#13;
				<div id="_idContainer533" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_28.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.28: Experiment defined on a Value Slicing board</p>&#13;
			<p>Let<a id="_idTextAnchor608"/><a id="_idTextAnchor609"/>'s now look at how we could set up a simple experiment to deploy both variants of the application and route traffic between each deployed instance to generate some data to help inform our decision-making.</p>&#13;
			<h3 id="_idParaDest-398"><a id="_idTextAnchor610"/>Matomo – Open Source Analytics</h3>&#13;
			<p>OpenShift <a id="_idIndexMarker2814"/><a id="_idIndexMarker2815"/>provides us <a id="_idIndexMarker2816"/><a id="_idIndexMarker2817"/>with a mechanism to push traffic to different versions of an application. This in itself is useful, but it provides no information that we can base a decision on. For this, we need to measure how the users interact with the platform. To do this, we're going to introduce user analytics, which records metrics on the users' interactions with the website. We're<a id="_idIndexMarker2818"/><a id="_idIndexMarker2819"/> going to use the open source Matomo<span id="footnote-097-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-097">53</a></span> platform. There are others we could have used but, at the time of writing, this was our choice as it was open source and quite feature-complete. Let's add Matomo to our Big Picture for consistency.</p>&#13;
			&#13;
			<div>&#13;
				<div id="_idContainer534" class="IMG---Figure">&#13;
					<img src="../Images/figure-15-29.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.29: Big Picture with added tools including Matomo</p>&#13;
			<div id="footnote-097" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-097-backlink">53</a>	<a href="https://matomo.org/">https://matomo.org/</a></p>&#13;
			</div>&#13;
			<p>So how do we<a id="_idIndexMarker2820"/><a id="_idIndexMarker2821"/> install the Matomo platform? Here comes Helm to the rescue again. We automated this installation as part of the PetBattle platform by just enabling it in our Ubiquitous Journey project. It's <a id="_idIndexMarker2822"/><a id="_idIndexMarker2823"/>deployed by default into our <strong class="inline">labs-ci-cd</strong> namespace from this configuration in <strong class="inline">ubiquitous-journey/values-tooling.yaml</strong>:</p>&#13;
			<p class="snippet"># Matamo</p>&#13;
			<p class="snippet">  - name: matomo</p>&#13;
			<p class="snippet">    enabled: true</p>&#13;
			<p class="snippet">    source: https://github.com/petbattle/pet-battle-analytics.git</p>&#13;
			<p class="snippet">    source_path: charts/matomo</p>&#13;
			<p class="snippet">    sync_policy: *sync_policy_true</p>&#13;
			<p class="snippet">    destination: labs-ci-cd</p>&#13;
			<p class="snippet">    source_ref: main</p>&#13;
			<p class="snippet">    ignore_differences:</p>&#13;
			<p class="snippet">    - group: apps</p>&#13;
			<p class="snippet">      kind: Deployment</p>&#13;
			<p class="snippet">      jsonPointers:</p>&#13;
			<p class="snippet">        - /spec/replicas</p>&#13;
			<p class="snippet">        - /spec/template/spec/containers/0/image</p>&#13;
			<p>However, if you want<a id="_idIndexMarker2824"/><a id="_idIndexMarker2825"/> to just install the tool without involving ArgoCD, you can just clone the repository and install it manually. This chart has been forked from an existing chart<span id="footnote-096-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-096">54</a></span> to tweak it for easier installation on OpenShift. Specifically, the security contexts in the MariaDB and Redis dependencies have been disabled so that the deployment will automatically use the target namespace default service account and associated <strong class="bold">Security Context Constraint</strong> (<strong class="bold">SCC</strong>) in <a id="_idIndexMarker2826"/><a id="_idIndexMarker2827"/>OpenShift. For COTS software where repackaging or running as a random UID is not always possible, there are other more permissive, less secure SCCs such as <strong class="inline">anyuid</strong>. Also, an OpenShift route has been added to the chart to allow ingress traffic to the application:</p>&#13;
			&#13;
			<p class="snippet">$ oc login ...</p>&#13;
			<p class="snippet">$ git clone https://github.com/petbattle/pet-battle-analytics.git \</p>&#13;
			<p class="snippet">&amp;&amp; cd pet-battle-analytics</p>&#13;
			<p class="snippet">$ helm install pba charts/matomo</p>&#13;
			<p>With the Matomo analytics deployed, we just need to configure the frontend to connect to it. To do this just update the config map's <strong class="inline">matomoUrl</strong> in the <strong class="inline">chart/values.yaml</strong> in the frontend to have the tracking code automatically track the site. This will provide basic site tracking such as the time spent on a page or the number of pages visited.</p>&#13;
			<div id="footnote-096" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-096-backlink">54</a>	<a href="https://gitlab.com/ideaplexus/helm/matomo">https://gitlab.com/ideaplexus/helm/matomo</a></p>&#13;
			</div>&#13;
			<div>&#13;
				<div id="_idContainer535" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_30.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.30: Configuring the config_map for matomoUrl</p>&#13;
			<p>For a more meaningful test, we might want to capture specific user behavior. The application has been instrumented to report certain events back to the Matomo server, such as mouse clicks. Whenever a user clicks the button to vote for a cat, it will capture it and report it in Matomo <a id="_idIndexMarker2828"/><a id="_idIndexMarker2829"/>for us. It's very simple to do this – we just add a one-liner to the event we want to track:</p>&#13;
			<p class="snippet">this.matomoTracker.trackEvent('A/B Tests', 'Voting', voting)</p>&#13;
			<h3 id="_idParaDest-399">Deplo<a id="_idTextAnchor611"/><a id="_idTextAnchor612"/>ying the A/B Test </h3>&#13;
			<p>In PetBattle land, let's <a id="_idIndexMarker2830"/><a id="_idIndexMarker2831"/>see how we could configure the deployments of the frontend to run this simple A/B test. Luckily for us, OpenShift makes this super easy by having a way to expose a <strong class="inline">route</strong> and connect it to more than one service, using the <strong class="inline">alternateBackends</strong> array to configure additional services to send traffic to. We can then apply <strong class="inline">weights</strong> to each service defined here in order to set the percentage of the traffic to either service that's deployed, A or B. The weights can be set between 0 and 256, and if a service is reduced to 0 then it carries on serving existing connections but no new ones. In fact, OpenShift allows us to do more than just an A or B test – also C and D, as <strong class="inline">alternateBackends</strong> supports up to three services!</p>&#13;
			<p>Let's deploy our A/B experiment for <strong class="inline">pet-battle</strong>. We could integrate these steps with ArgoCD but to keep things nice and easy for illustrative purposes, let's just stick with using our trusty friend Helm to deploy things. We've prebuilt an image that has no ability to downvote on the home page <strong class="inline">quay.io/petbattle/pet-battle:no-down-vote</strong>. Let's deploy this image to our cluster by running a simple Helm command (make sure to set the config map to the correct endpoints for your cluster): </p>&#13;
			<p class="snippet">$ git clone git@github.com:petbattle/pet-battle.git &amp;&amp; cd pet-battle </p>&#13;
			<p class="snippet">$ helm install nodownvote --set image_version=no-down-vote \</p>&#13;
			<p class="snippet">--set route=false chart --namespace petbattle</p>&#13;
			<p>With this command, we're deploying a new instance of the <strong class="inline">pet-battle</strong> frontend by setting the image to the prebuilt one and disabling the OpenShift route for this as it's not needed. We'll configure our route to production by updating our <strong class="inline">prod</strong> app.</p>&#13;
			<p>Running <strong class="inline">oc get pods</strong> should show the app started and if you check for routes, you should see none exposed:</p>&#13;
			<p class="snippet">$ oc get pods</p>&#13;
			<p class="snippet">&#13;
NAME                            READY   STATUS      RESTARTS   AGE</p>&#13;
			<p class="snippet">nodownvote-pet-battle-1-deploy  0/1     Completed   0          2m47s</p>&#13;
			<p class="snippet">nodownvote-pet-battle-1-jxzhf   1/1     Running     0          2m43s</p>&#13;
			<p>Let's deploy our <strong class="inline">prod</strong> version <a id="_idIndexMarker2832"/><a id="_idIndexMarker2833"/>of the <strong class="inline">pet-battle</strong> application and add the <strong class="inline">no-down-vote</strong> app as one of the services we'll connect to. Our Helm chart is configured to accept the name of the service and the weight we want to apply to the experiment feature via <strong class="inline">a_b_deploy.svc_name</strong> and <strong class="inline">a_b_deploy.weight</strong>. It's defaulted to be a 50/50 round-robin split. Let's deploy it with this setup:</p>&#13;
			<p class="snippet"># install prod version</p>&#13;
			<p class="snippet">$ helm install prod --set image_version=latest chart \</p>&#13;
			<p class="snippet">--set a_b_deploy.svc_name=no-down-vote-pet-battle --namespace petbattle</p>&#13;
			<p class="snippet"># list pods</p>&#13;
			<p class="snippet">$ oc get pods</p>&#13;
			<p class="snippet">NAME                            READY  STATUS              RESTARTS  AGE</p>&#13;
			<p class="snippet">nodownvote-pet-battle-1-deploy  0/1    Completed           0         4m53s</p>&#13;
			<p class="snippet">nodownvote-pet-battle-1-jxzhf   1/1    Running             0         4m49s</p>&#13;
			<p class="snippet">prod-pet-battle-1-6bbv8         0/1    ContainerCreating   0         12s</p>&#13;
			<p class="snippet">prod-pet-battle-1-deploy        1/1    Running             0         16s</p>&#13;
			<p>Navigate to the <strong class="inline">pet-battle</strong> UI and you should see on refreshing that there is a 50/50 chance that you will get the upvote-only version. If you open up incognito mode or a different browser and try to hit the frontend, you should get the alternative one. A different browser session is required, as the OpenShift router will by default return you to the same pod, so you'll always land on the same site version. </p>&#13;
			<div>&#13;
				<div id="_idContainer536" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_31.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.31: The no-downvote PetBattle frontend</p>&#13;
			<p>Running <strong class="inline">oc get routes</strong> <a id="_idIndexMarker2834"/><a id="_idIndexMarker2835"/>should show one route and more than one service connected to it with a 50/50 <strong class="inline">split prod-pet-battle(50%),no-down-vote-pet-battle(50%)</strong>. You can view the weights set as 100 each by running <strong class="inline">oc get route prod-pet-battle -o yaml</strong>:</p>&#13;
			<p class="snippet"># display the routes</p>&#13;
			<p class="snippet">$ oc get routes</p>&#13;
			<p class="snippet">NAME       HOST/PORT     PATH   SERVICES  PORT  TERMINATION  WILDCARD</p>&#13;
			<p class="snippet">prod-pet-battle      prod-pet-battle-labs-dev.apps…</p>&#13;
			<p class="snippet">      prod-pet-battle(50%),no-down-vote-pet-battle(50%)</p>&#13;
			<p class="snippet">             8080-tcp   edge/Redirect   None</p>&#13;
			<p>The weights for the traffic routed to each application can be updated quite easily using Helm:</p>&#13;
			<p class="snippet"># update route weights</p>&#13;
			<p class="snippet">$ helm upgrade prod --set image_version=latest chart \ </p>&#13;
			<p class="snippet">    --set a_b_deploy.svc_name=no-down-vote-pet-battle \</p>&#13;
			<p class="snippet">    --set a_b_deploy.weight=10 --namespace petbattle</p>&#13;
			<h3 id="_idParaDest-400">Unders<a id="_idTextAnchor613"/><a id="_idTextAnchor614"/>tanding the results</h3>&#13;
			<p>If we play around with the two versions that are deployed, we can see how the results of clicking the buttons are captured. If you <a id="_idIndexMarker2836"/><a id="_idIndexMarker2837"/>open the Matomo app and log in, you will see some statistics there. The default password for Matomo, as set in the chart, is <strong class="inline">My$uper$ecretPassword123#</strong>. This might not be exactly secure out of the box but it can easily be changed via the Helm chart's values.</p>&#13;
			<div>&#13;
				<div id="_idContainer537" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_32.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.32: Matomo showing the number of clicks for UP_VOTE versus DOWN_VOTE</p>&#13;
			<p>It might take a few minutes for Matomo to render the pie chart. Our simple experiment<a id="_idIndexMarker2838"/><a id="_idIndexMarker2839"/> shows that more people use the <span class="P---Screen-Text">UP_VOTE</span> feature than the <span class="P---Screen-Text">DOWN_VOTE</span> feature. By connecting the A/B test to the data captured in Matomo, we can now make more informed decisions about the next actions that need to be taken for our product.</p>&#13;
			<p>This experiment proves how easy it is to set up an A/B test. We can use the OpenShift platform to dynamically route users to multiple application versions concurrently deployed while we collect data about what is working well and what is not. There is some thinking that needs to be put into how we instrument the application to collect specific data, but the open source tooling available to us makes this easy too!</p>&#13;
			<h3 id="_idParaDest-401">Blue/G<a id="_idTextAnchor615"/>r<a id="_idTextAnchor616"/><a id="_idTextAnchor617"/>een deployments</h3>&#13;
			<p>The Blue/Green deployment strategy is one of the fundamental deployment strategies that every team deploying <a id="_idIndexMarker2840"/><a id="_idIndexMarker2841"/>applications into production should know about. Using this strategy minimizes the time it takes to perform a deployment cutover by ensuring you have two versions of the application available during deployment. It is also advantageous in that you can quickly roll back to the original version of the application without having to roll back any changes.</p>&#13;
			<p> </p>&#13;
			<div>&#13;
				<div id="_idContainer538" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_33.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.33: The canonical Blue/Green deployment</p>&#13;
			<p>The trade-off here is that you need to have enough resources to be able to run two versions of the application stack you are deploying. If your application has persistent state, for example, a database or non-shared disk, then the application architecture and constraints must be able to accommodate the two concurrent versions. This is normally not an issue for smaller microservices and is one of the benefits of choosing that style of deployment.</p>&#13;
			<p>Let's run through Blue/Green deployment using the PetBattle API as the example application stack. In this case, we are going to deploy two full stacks, that is, both the application <a id="_idIndexMarker2842"/><a id="_idIndexMarker2843"/>and MongoDB. Let's deploy the blue version of our application:</p>&#13;
			<p class="snippet"># install the blue app stack $ helm upgrade --install pet-battle-api-blue \</p>&#13;
			<p class="snippet">    petbattle/pet-battle-api --version=1.0.15 \</p>&#13;
			<p class="snippet">    --namespace petbattle --create-namespace</p>&#13;
			<p>Now deploy the green application stack. Note that we have a different tagged image version for this:</p>&#13;
			<p class="snippet"># install the green app stack</p>&#13;
			<p class="snippet">$ helm upgrade --install pet-battle-api-green \</p>&#13;
			<p class="snippet">  petbattle/pet-battle-api --version=1.0.15 \</p>&#13;
			<p class="snippet">    --set image_version=green \</p>&#13;
			<p class="snippet">    --namespace petbattle</p>&#13;
			<p>Next, we expose our production URL endpoint as a route that points to the blue service:</p>&#13;
			<p class="snippet"># create the production route</p>&#13;
			<p class="snippet">$ oc expose service pet-battle-api-blue --name=bluegreen \</p>&#13;
			<p class="snippet">--namespace petbattle</p>&#13;
			<p>Finally, we can switch between the two using the <strong class="inline">oc patch</strong> command:</p>&#13;
			<p class="snippet"># switch service to green</p>&#13;
			<p class="snippet">$ oc patch route/bluegreen --namespace petbattle -p \</p>&#13;
			<p class="snippet">    '{"spec":{"to":{"name":"pet-battle-api-green"}}}'</p>&#13;
			<p class="snippet"># switch back to blue again</p>&#13;
			<p class="snippet">$ oc patch route/bluegreen --namespace petbattle -p \</p>&#13;
			<p class="snippet">    '{"spec":{"to":{"name":"pet-battle-api-blue"}}}'</p>&#13;
			<p>If you browse to the <a id="_idIndexMarker2844"/><a id="_idIndexMarker2845"/><strong class="inline">bluegreen</strong> route endpoint, you should be able to easily determine the application stack:</p>&#13;
			<div>&#13;
				<div id="_idContainer539" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_34.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.34: Blue/Green deployment for the PetBattle API</p>&#13;
			<p>Even though this is somewhat of a contrived example, you can see the power of developers being allowed to manipulate the OpenShift routing tier in a self-service manner. A similar approach could be used to deploy the NSFF feature as an example – use the Helm chart parameters <strong class="inline">--set nsff.enabled=true</strong> to deploy an NSFF-enabled version. You can also point both applications to the same database if you want to with similar manipulation of the Helm chart values.</p>&#13;
			<p>If you have more<a id="_idIndexMarker2846"/><a id="_idIndexMarker2847"/> complex use cases where you need to worry about long-running transactions in the original blue stack, that is, you need to drain them, or you have data stores that need migrating alongside the green rollout, there are several other more advanced ways of performing Blue/Green deployments. Check out the ArgoCD rollout capability, which has a ton of advanced features,<span id="footnote-095-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-095">55</a></span> the Knative Blue/Green rollout capability, or indeed Istio<span id="footnote-094-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-094">56</a></span> for more ideas.</p>&#13;
			&#13;
			<h3 id="_idParaDest-402">Deployment<a id="_idTextAnchor618"/><a id="_idTextAnchor619"/> previews</h3>&#13;
			<p>We should think of<a id="_idIndexMarker2848"/><a id="_idIndexMarker2849"/> OpenShift as something of a playground that we can use to deploy our applications for production all the way down to a developer preview. Gone are the days when a development team needed to raise a ticket to provision a server and manually configure it to show off their applications. Building applications in containers allows us to make shippable applications that can be repeatedly deployed in many environments. Our automation for PetBattle in Jenkins is configured to run on every commit. For Jenkins, we're using the multi-branch plugin so anytime a developer pushes a new feature to a branch, it will automatically scaffold out a new pipeline and deploy the latest changes for that feature. </p>&#13;
			<p>When this was discussed in the previous chapter, about sandbox builds, you<a id="_idIndexMarker2850"/><a id="_idIndexMarker2851"/> may have thought this was overkill and a bit of a waste. Why not just build on a pull request? It's a valid question to ask and depending on the objective you're trying to achieve, building on a pull request is probably sufficient. We have used the sandbox builds as another way to introduce feedback loops.</p>&#13;
			<div id="footnote-095" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-095-backlink">55</a>	<a href="https://argoproj.github.io/argo-rollouts">https://argoproj.github.io/argo-rollouts</a></p>&#13;
				<div id="footnote-094" class="_idFootnote" epub:type="footnote">&#13;
					<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-094-backlink">56</a>	<a href="https://github.com/hub-kubernetes/istio-blue-green-deployment">https://github.com/hub-kubernetes/istio-blue-green-deployment</a></p>&#13;
				</div>&#13;
			</div>&#13;
			<p>Developers do not exist in isolation; they are surrounded by other members of the team, including Product Owners and Designers. Our ability to dynamically spin up a new deployment of a feature from our pipeline means we can connect the coding efforts to the design team really easily. Developers can get very fast feedback by sharing a link to the latest changes or the implementation of a new feature with the design team. This feedback loop can quickly allow subtle changes and revisions to be made before the engineer loses the context of the piece of work. Creating deployment previews from every commit also allows a developer to very quickly share two versions of what an app might look like with a Product Owner while they make their decision about which to choose.</p>&#13;
			<p>From our Jenkins pipeline, there<a id="_idIndexMarker2852"/><a id="_idIndexMarker2853"/> is a branch called <strong class="inline">cool-new-cat</strong>. When this is built, it will push a new version of the app to the <strong class="inline">dev</strong> environment. The change in the app is subtle for illustrative purposes, but we can see the banner has been changed. With this new version of the app in the <strong class="inline">dev</strong> environment, we can get some feedback prior to merging it to master and <a id="_idIndexMarker2854"/><a id="_idIndexMarker2855"/>generating a release candidate.</p>&#13;
			<div>&#13;
				<div id="_idContainer540" class="IMG---Figure">&#13;
					<img src="../Images/B16297_15_35.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 15.35: New feature deployed to the sandbox generating a deploy preview to collect feedback</p>&#13;
			<p><em class="italics">Figure 15.35</em> shows the sandbox version of the being deployed along with it's associated route, service and configmap.</p>&#13;
			<h2 id="_idParaDest-403">Conclusion<a id="_idTextAnchor620"/></h2>&#13;
			<p><a id="_idTextAnchor621"/>Congratulations! You've just finished the most technologically focused chapter of this book so far. Please don't go off and think that you have to use each and every technology and technique that has been mentioned—that's not the point. Investigate, evaluate, and choose which of these technologies applies to your own use cases and environment.</p>&#13;
			<p>Several of the testing practices are part of our technical foundation. Unit testing, non-functional testing, and measuring code coverage are all critical practices for helping build quality into our applications and products from the start. We covered many small but invaluable techniques, such as resource validation, code linting, and formatting, that help make our code base less of a burden to maintain.</p>&#13;
			<p>We covered a number of different approaches for deployments, including A/B, Canary, Blue/Green, and Serverless. These core techniques allow us to deliver applications more reliably into different environments. We even briefly covered artificial intelligence for reducing unwanted images uploaded into our PetBattle product. By focusing our efforts on what happens when things go wrong, we can more easily embrace and prepare for failures—big and small.</p>&#13;
		</div>&#13;
</body></html>