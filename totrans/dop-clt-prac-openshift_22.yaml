- en: 15\. Run It
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a saying that *your code has no value until it runs in production*.
    The sentiment here is that until your customers use your software, it's of limited
    value for your business or organization. It is certainly a broad generalization!
    However, it does speak to the essential nature of software that its utility is
    directly related to being able to run it for whatever purposes it was ultimately
    written for. To reach production with the quality of service that our customers
    expect, all of the code must be put through its paces.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to explore how the PetBattle team tests their
    software so they have greater confidence in its ability to run as expected in
    production. Testing is multifaceted, as we discussed in *Chapter 7*, *Open Technical
    Practices – The Midpoint*, and we are going to cover in some detail the types
    and scope of testing, from unit tests to end-to-end testing, through to security
    checks and more.
  prefs: []
  type: TYPE_NORMAL
- en: When the hobbyist version of the application went live, the PetBattle founders
    soon discovered that malicious content was being uploaded to the site. As part
    of this chapter, we'll look at a modern-day solution to this problem using a trained
    AI-ML model.
  prefs: []
  type: TYPE_NORMAL
- en: In the last section of this chapter, we explore some common cloud deployment
    patterns and demonstrate A/B testing and experimentation, for gaining insight
    into how we can safely measure and learn the impact of deploying new features
    in production.
  prefs: []
  type: TYPE_NORMAL
- en: The Not Safe For Families (NSFF) Component
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned earlier, one of the major issues that we faced when running
    the first generation of PetBattle was online trolls uploading inappropriate images
    to the system. This added to the operational overhead of running the platform
    because the PetBattle founders would have to search MongoDB for the offending
    images and remove them by hand—very tedious!
  prefs: []
  type: TYPE_NORMAL
- en: Ever innovating, the team decided to try and come up with an automated solution
    to this problem. One approach we decided to investigate was to use **artificial
    intelligence** (**AI**) to perform image classification on the uploaded images
    and incorporate this into the platform.
  prefs: []
  type: TYPE_NORMAL
- en: The field of AI in itself is a fascinating area of expertise that we won't even
    slightly go into here, other than to say that we are using a pre-trained image
    classification model served by the open source TensorFlow machine learning platform.
  prefs: []
  type: TYPE_NORMAL
- en: Great, but how do we go about running this on OpenShift?
  prefs: []
  type: TYPE_NORMAL
- en: 'The plan is to:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate or obtain a pre-trained image classification model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build containers containing the TensorFlow serving component that can serve
    up the model and make predictions based on our uploaded images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy and run the container on OpenShift in a "scale to zero" deployment model,
    aka Serverless.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why Serverless?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When deploying a container on a Kubernetes-based platform, such as OpenShift,
    Kubernetes takes on the responsibility of managing the running container and,
    by default, restarting it if it terminates due to an error. Basically, there's
    always a container running. This is all good and fine for containers that are
    constantly receiving and processing traffic, but it's a waste of system resources
    constantly running a container that receives traffic either occasionally or in
    bursts.
  prefs: []
  type: TYPE_NORMAL
- en: What we'd like to achieve is to deploy a container and have it start up only
    when needed, that is, during incoming requests. Once active, we want it to process
    the incoming requests and then, after a period of no traffic, shut down gracefully
    until further incoming requests are received. We'd also like the container instances
    to scale up in the event of a surge of incoming requests.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to automate the scaling up and down of the number of container
    instances running on the platform using the Kubernetes Horizontal Pod Autoscaler;
    however, this does not scale to zero. We could also use something like the `oc
    scale` command, but this requires a fair amount of scripting and component integration.
    Thankfully, the Kubernetes community thought about this and came up with a solution
    called Knative.[1](#footnote-149)
  prefs: []
  type: TYPE_NORMAL
- en: Knative has two major components, **Knative Serving** and **Knative Eventing**.
    Serving is used to spin up (and down) containers depending on HTTP traffic. Knative
    Eventing is somewhat equivalent but is focused on spinning up containers based
    on events and addresses broader use cases. For the purposes of this book, we are
    going to focus on using Knative Serving. However, we will also give an example
    of how Knative Eventing could be used.
  prefs: []
  type: TYPE_NORMAL
- en: Generating or Obtaining a Pre-trained Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We had been experimenting with image classification for a while. We started
    using some of the components from the Open Data Hub community ([https://opendatahub.io/](https://opendatahub.io/))
    and trained out models on top of pre-existing open source models that were available.
    We eventually generated a trained data model that could classify images that we
    deemed NSFF based on an implementation of Yahoo's Open NSFW Classifier,[2](#footnote-148)
    which was rewritten with TensorFlow. While it was not perfect, it was a good enough
    model to start with.
  prefs: []
  type: TYPE_NORMAL
- en: A common pattern in the data science community is to serve up trained data models
    using tools such as Seldon,[3](#footnote-147) which are part of Open Data Hub.
    For our purposes though, a simple object storage tool was all that was required.
    So, we chose MinIO,[4](#footnote-146) a Kubernetes native object store. We decided
    we could scale that out later if needed, using more advanced storage mechanisms,
    for example, OpenShift Container Storage or AWS S3.
  prefs: []
  type: TYPE_NORMAL
- en: '[1](#footnote-149-backlink) [https://knative.dev/](https://knative.dev/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2](#footnote-148-backlink) [https://github.com/yahoo/open_nsfw](https://github.com/yahoo/open_nsfw)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3](#footnote-147-backlink) [https://www.seldon.io/](https://www.seldon.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4](#footnote-146-backlink) [https://min.io/](https://min.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We loaded the trained data model into MinIO and it looked as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.1: TensorFlow data model saved in MinIO'
  prefs: []
  type: TYPE_NORMAL
- en: The saved model is something we can serve up using TensorFlow Serving,[5](#footnote-145)
    which basically gives us an API endpoint to call our saved model with. There is
    an open source TensorFlow serving image we can deploy and it's a matter of configuring
    that to find our saved model in our S3 storage location.
  prefs: []
  type: TYPE_NORMAL
- en: We have glossed over the large portion of engineering that goes into making
    AI, ML, and Ops pipelines not because it is not an interesting subject, but mainly
    because it would require a whole other book to do it justice! If this subject
    is close to your heart, then take a look at the Open Data Hub project.[6](#footnote-144)
    This is an open source project based on Kubeflow,[7](#footnote-143) providing
    tools and techniques for building and running AI and ML workloads on OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: '[5](#footnote-145-backlink) [https://www.tensorflow.org/tfx/guide/serving](https://www.tensorflow.org/tfx/guide/serving)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6](#footnote-144-backlink) [http://opendatahub.io/](http://opendatahub.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[7](#footnote-143-backlink) [https://www.kubeflow.org/](https://www.kubeflow.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: The OpenShift Serverless Operator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we start deploying our application software for the NSFF service, we
    need to add the OpenShift Serverless Operator[8](#footnote-142) to our PetBattle
    Bootstrap. The operator is installed at the cluster scope so that any project
    that wants to use the Knative components Knative Serving and Knative Eventing
    may do so.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use GitOps, ArgoCD, and Kustomize to configure and install the serverless
    operator. First, we can test out the configuration with ArgoCD. Log in using ArgoCD
    from the command line. Add the Git repository that contains the Knative serverless
    operator YAML subscription and create the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[8](#footnote-142-backlink) [https://github.com/openshift-knative/serverless-operator](https://github.com/openshift-knative/serverless-operator)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once installed, you should be able to see this installed successfully in the
    `openshift-serverless` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.2: The OpenShift Serverless Operator (Knative)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also put this in our PetBattle UJ bootstrap from *Chapter 7*, *Open
    Technical Practices – The Midpoint*, so that we don''t need to run these commands
    manually. Add the following to our `values-tooling.yaml` and check it into Git:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The operator is now ready for us to use to deploy our Knative service.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Knative Serving Services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are a few ways in which to create Knative Serving services. We can create
    the Knative service definition and install that into our cluster. We have packaged
    this up as a Helm chart for easy installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'It may take a minute or so for the containers to start up and load the model
    data into MinIO; they may restart a few times while doing this. The output of
    the `oc get pods` command should look like this once successful – the MinIO S3
    pod and its completed data load and a TensorFlow Knative service pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'After a couple of minutes, the Knative Serving TensorFlow pod will terminate
    because it is not yet being called. This is what''s called Serverless scale to
    zero, that is, when there are no calling workloads there is no need to run the
    service. An equivalent service can also be created using the Knative command-line
    tool **kn**, which can be downloaded and installed from the OpenShift[9](#footnote-141)
    console. This is useful if you want to create a new service or are developing
    a service from scratch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[9](#footnote-141-backlink) [https://docs.openshift.com/container-platform/4.7/serverless/serverless-getting-started.html](https://docs.openshift.com/container-platform/4.7/serverless/serverless-getting-started.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we use command-line arguments and environment variables to tell the TensorFlow
    serving image how to run. The `--image` field specifies the container image and
    version we wish to run – in this case, the latest TensorFlow serving image. The
    `--cmd` field specifies the binary in the image we wish to run, for example, the
    model server command `tensorflow_model_server`. The `--arg` and `––env` variables
    specify the configuration. The trained model is served from the `S3 minio` service
    so we specify how to access the S3 endpoint. There are many configurations available
    to Knative Serving, such as autoscaling global defaults, metrics, and tracing.
    The `--autoscale-window` defines the amount of data that the autoscaler takes
    into account when scaling, so in this case, if there has been no traffic for two
    minutes, scale the pod to 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Knative website[10](#footnote-140) goes into a lot more detail about the
    serving resources that are created when using Knative Serving and the configuration
    of these. To find the URL for our service, we can use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the HTTP URL endpoint to test our service with. It is worth noting
    that we can have multiple revisions of a service and that within the route mentioned
    previously, we can load balance traffic across multiple revisions. The following
    diagram depicts how this works in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.3: Knative routing for multiple application revisions'
  prefs: []
  type: TYPE_NORMAL
- en: '[10](#footnote-140-backlink) [https://knative.dev/docs/serving/](https://knative.dev/docs/serving/)'
  prefs: []
  type: TYPE_NORMAL
- en: Kourier[11](#footnote-139) is a lightweight ingress router based on an Envoy
    gateway. Using Knative service configuration, a user can specify routing rules
    that Knative Serving applies. This can be very useful when we're experimenting
    with different AI models or wanting to do A/B, Blue/Green, or Canary-type deployments,
    for example.[12](#footnote-138)
  prefs: []
  type: TYPE_NORMAL
- en: Invoking the NSFF Component
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A simple HTTP GET request on the route is all that''s required to invoke the
    component. The pod spins up and services the request usually within a couple of
    seconds and then spins down after a period of time (the `--autoscale-window` specified
    in the `kn` command-line argument, that is, 120 seconds). Using the output from
    the `kn list route` command, let''s check if the AI TensorFlow model is available.
    The `state` should read `AVAILABLE`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[11](#footnote-139-backlink) [https://developers.redhat.com/blog/2020/06/30/kourier-a-lightweight-knative-serving-ingress/](https://developers.redhat.com/blog/2020/06/30/kourier-a-lightweight-knative-serving-ingress/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[12](#footnote-138-backlink) [https://medium.com/@kamesh_sampath/serverless-blue-green-and-canary-with-knative-kn-ad49e8b6aa54](https://medium.com/@kamesh_sampath/serverless-blue-green-and-canary-with-knative-kn-ad49e8b6aa54)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We should also see a pod spinning up to serve the request, using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: It then scales down to 0 after two minutes.
  prefs: []
  type: TYPE_NORMAL
- en: We want to test that our NSFF service works by sending it some images. We have
    two test sample images that have been encoded so they can be uploaded to the NSFF
    service.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_04.1.jpg)![](img/B16297_15_04.2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.4: NSFF test images'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s download these images for testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Now submit these to our NSFF service using a simple `curl` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: The response from our model is a `predictions` array containing two numbers.
    The first is a measure of **Safe for Families**, the second is a measure of **Not
    Safe for Families**, and they add up to 1.
  prefs: []
  type: TYPE_NORMAL
- en: So, we can see that Daisy Cat has a very high safe for families rating (0.993)
    compared to our wrestlers (0.014) and we can use this in our PetBattle API to
    determine whether any given image is safe to display. By arbitrary testing, we
    have set a limit of >=0.6 for images we think are safe to view in the PetBattle
    UI.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can redeploy our PetBattle API service to call out to the NSFF service by
    setting the `nssf.enabled` feature flag to `true` and using the hostname from
    the Knative service from a bash shell using the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'If we now upload these test images to PetBattle via the UI and check the API
    server, we can see that the boxing picture has a `ISSFF` (Is Safe for Families)
    flag and Daisy Cat has a **true** value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.5: PetBattle API saved images with the ISSFF flag'
  prefs: []
  type: TYPE_NORMAL
- en: 'The API code will not return any pictures to the PetBattle UI that are deemed
    NSFF. For example, the API code to return all pets in the PetBattle database is
    filtered by the `ISSFF` flag being set to `true`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the API up and running it's time to test it and see if it performs
    as we expect.
  prefs: []
  type: TYPE_NORMAL
- en: Let's Talk about Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our experience with working with many developer teams, nothing can dampen
    the mood of many developers quite like a discussion on the subject of testing
    that their software does what it's supposed to do.
  prefs: []
  type: TYPE_NORMAL
- en: For many developers, testing is the equivalent of getting a dental checkup—few
    like doing it but all of us need to do it a lot more. It's a set of bridges that
    have to be crossed (often under duress) before our precious, handcrafted, artisan-designed
    piece of software excellence is accepted into the nirvana that is production.
    Testers are seen as the other team that ensures that we've dotted our I's and
    crossed our T's, and they don't appreciate how hard we've suffered for our art.
    Basically, we write it, *throw it over the fence to test*, and they check it.
  prefs: []
  type: TYPE_NORMAL
- en: If you're reading the preceding paragraph and mentally going *yep, yep, that's
    us, that's us, that's how we roll*, we've got really bad news for you. *You're
    basically doing it wrong*. It may have made sense when big bang software releases
    happened every 6-12 months, but in more agile organizations with faster, more
    frequent releases into production, this approach is considered cumbersome and
    archaic. There are always exceptions to this, for example, critical control systems,
    highly regulated environments, and so on, but for the majority of enterprise developers,
    this isn't the case.
  prefs: []
  type: TYPE_NORMAL
- en: 'The quality of software is the responsibility of the delivery team, from the
    Product Owner writing user stories to the engineers writing code and the associated
    tests. As a wise delivery manager once said, "testing is an activity, not a role."
    In effective software delivery teams, testing is a continuous activity that spans
    the entire software development life cycle. There are a number of principles that
    we try to adhere to when it comes to testing:'
  prefs: []
  type: TYPE_NORMAL
- en: Automate as much as possible, but not so much that there's no human oversight.
    There's always value in having users interact with the application under test,
    in particular when it comes to end-to-end, acceptance, and exploratory testing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Testing code is as important as production code—both need to be kept up to date
    and removed/deprecated when not adding value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One meaningful test can be worth more than hundreds of scripted test cases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In *Chapter 7*, *Open Technical Practices – The Midpoint*, we introduced the
    idea of the Automation Test Pyramid. For each of the different types of tests
    defined in the pyramid, there are several testing tools and frameworks we use
    across our PetBattle application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally speaking, we have chosen to use what are considered the default test
    tools for each of the application technology stacks as these are the simplest
    to use, are the best supported, have good user documentation, and are generally
    easy to adopt if people are new to them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_Table_15.1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 15.1: Test Frameworks in use'
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at some of these tests in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Unit Testing with JUnit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In both the API and Tournament applications, we have different examples of
    standard unit tests. Quarkus testing[13](#footnote-137) has great support for
    the standard unit test framework JUnit.[14](#footnote-136) The anatomy of all
    unit tests using this framework is very similar. Let''s take a look at the API
    application `CatResourceTest.java`[15](#footnote-135) as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: '[13](#footnote-137-backlink) [https://quarkus.io/guides/getting-started-testing](https://quarkus.io/guides/getting-started-testing)'
  prefs: []
  type: TYPE_NORMAL
- en: '[14](#footnote-136-backlink) [https://junit.org/junit5/](https://junit.org/junit5/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[15](#footnote-135-backlink) [https://github.com/petbattle/pet-battle-api/blob/master/src/test/java/app/battle/CatResourceTest.java](https://github.com/petbattle/pet-battle-api/blob/master/src/test/java/app/battle/CatResourceTest.java)'
  prefs: []
  type: TYPE_NORMAL
- en: In Java, we use annotations to make our Java class objects (POJOs) into tests.
    We use the `@QuarkusTest` annotation to bring in the JUnit framework for this
    class and we can think of the class as a test suite that contains lots of individual
    tests. Each method is a single test that is annotated with `@Test`. For this unit
    test, we don't have a database running, so we use mocks[16](#footnote-134) for
    the `Cat.class`. A mock is a fake object. It does not connect to a real database,
    and we can use it to test the behavior of the `Cat` class. In this case, we are
    asserting in our test that when we call the method `Cat.count()`, which corresponds
    to the number of likes of our pet image in PetBattle, we receive back the expected
    number (`23`). We use the `Uni` and `await()` functions because we are using the
    reactive programming model in our Quarkus application.[17](#footnote-133)
  prefs: []
  type: TYPE_NORMAL
- en: We run these unit tests as part of the automated continuous deployment pipeline
    and visualize and report on the tests' success and history using our CI/CD tools,
    including Jenkins, Tekton, and a test report tool such as Allure.[18](#footnote-132)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.6: Visualization of tests using Allure'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll continue with service and component testing with
    REST Assured and Jest.
  prefs: []
  type: TYPE_NORMAL
- en: Service and Component Testing with REST Assured and Jest
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Jest and REST Assured are **Behavior-Driven Development** (**BDD**) frameworks
    for JavaScript and Java. We covered BDD in *Chapter 7*, *Open Technical Practices
    – The Midpoint*. These frameworks make it super easy for developers to write tests
    where the syntax is obvious and easy to follow.
  prefs: []
  type: TYPE_NORMAL
- en: '[16](#footnote-134-backlink) [https://quarkus.io/guides/mongodb-panache](https://quarkus.io/guides/mongodb-panache)'
  prefs: []
  type: TYPE_NORMAL
- en: '[17](#footnote-133-backlink) [https://quarkus.io/guides/getting-started-reactive#mutiny](https://quarkus.io/guides/getting-started-reactive#mutiny)'
  prefs: []
  type: TYPE_NORMAL
- en: '[18](#footnote-132-backlink) [https://github.com/allure-framework](https://github.com/allure-framework)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to cover the basics of component testing the PetBattle user interface[19](#footnote-131)
    using Jest. The user interface is made of several components. The first one you
    see when landing on the application is the home page. For the home page component,
    the test class[20](#footnote-130) is called `home.component.spec.ts`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: 'Each test has a similar anatomy:'
  prefs: []
  type: TYPE_NORMAL
- en: '`describe()`: The name of the test suite and test specification argument'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beforeEach()`: Runs the function passed as an argument before running each
    test'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`it()`: Defines a single test with a required expectation and a function with
    logic and assertions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`expect()`: Creates an expectation for the test result, normally with a matching
    function such as `toEqual()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So in this case, the unit test will expect the `HomeComponent` to be created
    correctly when the test is run.
  prefs: []
  type: TYPE_NORMAL
- en: '[19](#footnote-131-backlink) [https://angular.io/guide/testing](https://angular.io/guide/testing)'
  prefs: []
  type: TYPE_NORMAL
- en: '[20](#footnote-130-backlink) [https://github.com/petbattle/pet-battle/blob/master/src/app/home/home.component.spec.ts](https://github.com/petbattle/pet-battle/blob/master/src/app/home/home.component.spec.ts)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, within the API application, REST Assured is a testing tool that
    allows us to write tests using the familiar `Given,` `When,` `Then` syntax from
    *Chapter 7*, *Open Technical Practices – The Midpoint*. Let''s examine one of
    the service API tests in the test suite `CatResourceTest.java`[21](#footnote-129):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: 'In this test, we are creating a `Cat` object. The `Cat` class is the data object
    in PetBattle that contains the pet''s uploaded image, along with its PetBattle
    vote count, and is stored in MongoDB. In the test, given the `Cat` object, we
    use an `HTTP POST` to the `/cats` endpoint and expect a return status code of
    (`201`), which is `CREATED`. We also test the HTTP response body is not empty.
    It should contain the ID of the newly created `Cat`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: '[21](#footnote-129-backlink) [https://github.com/petbattle/pet-battle-api/blob/master/src/test/java/app/battle/CatResourceTest.java](https://github.com/petbattle/pet-battle-api/blob/master/src/test/java/app/battle/CatResourceTest.java)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this service test, we make use of the `@QuarkusTestResource` annotation
    to create and start an embedded MongoDB for testing against. So, this test is
    a bit more sophisticated than the basic unit test that was using mocks only. We
    also track the execution of these service tests using our test report tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.7: Visualization of service tests using Allure'
  prefs: []
  type: TYPE_NORMAL
- en: Now we have seen what unit tests look like, let's move up the test pyramid to
    have a look at service-level testing.
  prefs: []
  type: TYPE_NORMAL
- en: Service Testing with Testcontainers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Integration testing is always substantially harder than unit testing as more
    components have to be either stood up or simulated/mocked. The next level of testing
    in our test pyramid is integration testing using a Java framework called **Testcontainers**.[22](#footnote-128)
    Testcontainers allows us to easily create and start components such as MongoDB,
    **KeyCloak**, and **Infinispan** and perform tests using those components. The
    following classes instantiate and manage the containers and inject them into the
    testing life cycle of the Quarkus framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: 'Within the integration test code at `ITPetBattleAPITest.java`, we just inject
    the previously created containers and use them as resources during the test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: '[22](#footnote-128-backlink) [https://www.testcontainers.org/](https://www.testcontainers.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: This is a great example of how containers can be used as part of a testing phase.
    The containers are spun up, the tests are run, and the containers are removed.
    The only real prerequisite is that the Docker daemon is run on the machine running
    the tests. To run the integration tests use the command `mvn clean verify -Pintegration`.
  prefs: []
  type: TYPE_NORMAL
- en: End-to-End Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our application is made up of a frontend written in Angular, which makes calls
    for data to two APIs. One is for tournaments and the other is for cats. We can
    think of the interplay between these components as the system as a whole. Any
    time a change is made to either of these individual applications, it should require
    revalidating the whole system. The end-to-end automated testing is performed primarily
    in the user interface but exercises the underlying services layer.
  prefs: []
  type: TYPE_NORMAL
- en: There are loads of tools to do testing from the user interface level. Some of
    the more popular ones are things like Selenium and Cypress, which are used to
    drive a web application and simulate user behavior. There are pros and cons to
    each – Selenium is just browser automation so you need to bring your own test
    frameworks, whereas Cypress is an all-in-one testing framework. Selenium Grid,
    when running on Kubernetes, allows us to test against multiple browsers in parallel
    by dynamically provisioning the browser on each test execution, meaning we don't
    have browsers waiting idly for us to use them.
  prefs: []
  type: TYPE_NORMAL
- en: For our end-to-end testing, we're using Protractor from the Angular team. We
    already deployed an instance of Selenium Grid built for Kubernetes by the Zalando
    team (called Zalenium [https://opensource.zalando.com/zalenium/](https://opensource.zalando.com/zalenium/))
    when we deployed our tooling. Zalenium is pretty handy as it allows us to play
    back previous test runs and watch them live. In your cluster, if you get the route
    for Zalenium (`oc get routes -n labs-ci-cd`) and append `/grid/admin/live`, you
    can follow the tests as they execute or go to /dashboard to watch the historical
    test executions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.8: Zalenium dashboard showing test history and video playback'
  prefs: []
  type: TYPE_NORMAL
- en: Our `system-tests` project ([https://github.com/petbattle/system-tests](https://github.com/petbattle/system-tests))
    has all of the system tests that we should execute after any change is pushed
    to the frontend or backend services. The tests are written using Cucumber-style
    BDD. In fact, we should be able to connect the BDD to the acceptance criteria
    from our PetBattle Sprint items.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.9: Example of BDD written as acceptance criteria on a Sprint board
    for PetBattle'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a test for a tournament feature written in the `Given,` `When,` `Then`
    syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: The `system-test` project has its own Jenkinsfile, so it's already connected
    to Jenkins via our seed job. We won't go through the contents of this Jenkinsfile
    in detail. Suffice to say, the pipeline has two stages, as per our Big Picture,
    one to run the tests and the other to promote the app if the tests have passed.
    Explore the code for this in the accompanying Git repo [https://github.com/petbattle/system-tests](https://github.com/petbattle/system-tests).
    To extend our Jenkinsfile for `pet-battle` to trigger our system test job, we
    just need to add another stage to trigger the job. We could use the Jenkins `post{}`
    block, but we only want to trigger the system tests if we're on `master` or `main`
    and producing a release candidate.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.10: The trigger for connecting our pipelines in Jenkins'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few parameters that are passed between the jobs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`APP_NAME`: Passed to the job so if the tests are successful, the promote stage
    knows what app to deploy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CHART_VERSION & VERSION`: Any update to the chart or app needs to be patched
    in Git so this information is passed by the job that triggers the system tests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can run the system tests job manually by supplying this information to the
    job, but each service with a Jenkinsfile should be able to pass these to the system
    tests. This job can also be triggered from Tekton too if we were to mix the approach
    to the pipelines. With the two pipelines wired together, we can trigger one if
    the webhook is set up by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: 'If we now check in the Jenkins Blue Ocean Web UI, we should see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.11: The system tests pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: On Jenkins, we should see the system tests pipeline running and promoting if
    successful. The Cucumber reports are also included for the job.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.12: The Cucumber report in Jenkins'
  prefs: []
  type: TYPE_NORMAL
- en: These provide insight into which cases were executed for what browser and report
    any failures that may have occurred. Let's switch gear a little now and take a
    look at non-functional testing.
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines and Quality Gates (Non-functionals)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Quality is often just focused on whether tests pass or not. However there's
    also the concept of code quality. The code may perform as expected but the manner
    in which it's been written could be so poor that it could be a future source of
    problems when changes are added. So now it's time to check the quality of our
    code.
  prefs: []
  type: TYPE_NORMAL
- en: SonarQube
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As part of the Ubiquitous Journey, we have automated the Helm chart deployment
    of SonarQube, which we are using to test and measure code quality. In `values-tooling.yaml`,
    the SonarQube stanza references the Helm chart and any extra plugins that are
    required. Many of the common language profile plugins are already deployed with
    the base version of SonarQube, for example, Java, JavaScript, and Typescript.
    We add in extra plugin entries for Checkstyle, our Java formatting check tool,
    and a dependency checker for detecting publicly disclosed vulnerabilities contained
    within project dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE159]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE162]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE163]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE164]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE165]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE167]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE168]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE169]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE170]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE171]'
  prefs: []
  type: TYPE_PRE
- en: 'With the basic SonarQube pod deployed, there is one more piece of configuration
    we need to automate – the creation of a code quality gate. The quality gate is
    the hurdle our code must pass before it is deemed ready to release. This boils
    down to a set of conditions defined in code that specify particular measurements,
    for example:'
  prefs: []
  type: TYPE_NORMAL
- en: Do we have new blocking issues with the code that was just added?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the code test coverage higher than a given percentage?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there any identifiable code vulnerabilities?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SonarQube lets us define these quality gates[23](#footnote-127) using its REST
    API. For PetBattle, we use a Kubernetes job to define our quality gate `AppDefault`
    and package it as a Helm chart for deployment. The chart is deployed using Ubiquitous
    Journey and ArgoCD.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.13: A SonarQube quality gate definition'
  prefs: []
  type: TYPE_NORMAL
- en: The SonarQube server can be queried via a REST API, whether a recent report
    against a particular project has passed or failed this quality gate. We have configured
    a Tekton step and task in our pipelines to automatically check this each time
    we run a build.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our PetBattle Java applications are configured using Maven to talk to our SonarQube
    server pod and generate the SonarQube formatted reports during each build, bake,
    and deploy. In the reusable `maven-pipeline.yaml`, we call the following target
    to generate these reports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE172]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE174]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE175]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE176]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE177]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE178]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE179]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE180]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE181]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE182]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE183]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE184]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE185]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE186]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE187]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE188]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE189]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE190]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE191]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE192]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE193]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE194]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE195]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE196]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE197]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE198]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE199]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE200]'
  prefs: []
  type: TYPE_PRE
- en: '[23](#footnote-127-backlink) [https://docs.sonarqube.org/latest/user-guide/quality-gates/](https://docs.sonarqube.org/latest/user-guide/quality-gates/)'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, for the PetBattle UI using `nodejs`, we can configure the client
    to call SonarQube as part of its Tekton pipeline. Once these steps have successfully
    run, we can explore the SonarQube Web UI and drill down into any areas to find
    out more information.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.14: SonarQube project view'
  prefs: []
  type: TYPE_NORMAL
- en: In a bit of recent development for A/B testing support in the PetBattle UI,
    some code bugs seemed to have crept in! Developers can drill down and see exactly
    what the issues are and remediate them in the code base. SonarQube ranks issues
    based on severity defined in the Language Quality Profile, which can be altered
    to suit your development code quality needs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.15: SonarQube drilling into some bug details'
  prefs: []
  type: TYPE_NORMAL
- en: SonarQube also reports on the last run's code testing coverage. On the code
    base side, you generate coverage reports using the `LCOV`[24](#footnote-126) format,
    so in Java, this is done by `JaCoCo`[25](#footnote-125) and in JavaScript, the
    coverage reports are produced by the `mocha`/`jasmine` modules. These reports
    are uploaded into SonarQube and give the team visibility into which parts of their
    code base need more testing. A nice way to view this information is using the
    heatmap, which visualizes the bits of code that have near 100% coverage (green),
    down to areas that are not covered at all 0% (red). The statistics are also reported
    – the percentage coverage overall, the number of lines covered, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.16: SonarQube test coverage heatmap for PetBattle'
  prefs: []
  type: TYPE_NORMAL
- en: '[24](#footnote-126-backlink) [https://github.com/linux-test-project/lcov](https://github.com/linux-test-project/lcov)'
  prefs: []
  type: TYPE_NORMAL
- en: '[25](#footnote-125-backlink) [https://www.eclemma.org/jacoco/](https://www.eclemma.org/jacoco/)'
  prefs: []
  type: TYPE_NORMAL
- en: The last plugin we use for our Java applications is the OWASP Dependency-Check
    plugin.[26](#footnote-124) We move security checking "left" in our pipeline. In
    other words, we want to discover early in the development process when security
    vulnerabilities or CVEs are creeping into our applications' dependencies. By identifying
    which dependencies are vulnerable to a CVE early as part of the build cycle, developers
    are in a much better position to update them, rather than finding there are issues
    once our applications are deployed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.17: Dependency-Check plugin report'
  prefs: []
  type: TYPE_NORMAL
- en: The plugin sources data from multiple open source resources including the US
    National Vulnerability Database[27](#footnote-123) and Sonatype OSS Index.[28](#footnote-122)
    In conjunction with security team members, developers can verify known vulnerabilities
    and suppress any false positives using a configuration file. The report is very
    detailed and includes links to these sites to assist CVE identification and reporting.
  prefs: []
  type: TYPE_NORMAL
- en: '[26](#footnote-124-backlink) [https://github.com/dependency-check/dependency-check-sonar-plugin](https://github.com/dependency-check/dependency-check-sonar-plugin)'
  prefs: []
  type: TYPE_NORMAL
- en: '[27](#footnote-123-backlink) [https://nvd.nist.gov/](https://nvd.nist.gov/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[28](#footnote-122-backlink) [https://ossindex.sonatype.org/](https://ossindex.sonatype.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: Perf Testing (Non-Functional)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of our favorite command-line tools for getting fast feedback on the performance
    of REST APIs is a tool called **hey**.[29](#footnote-121) There are a lot of similar
    tools available. Apache Bench[30](#footnote-120) is probably the most venerable.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.18: A simple hey run'
  prefs: []
  type: TYPE_NORMAL
- en: '[29](#footnote-121-backlink) [https://github.com/rakyll/hey](https://github.com/rakyll/hey)'
  prefs: []
  type: TYPE_NORMAL
- en: '[30](#footnote-120-backlink) [https://httpd.apache.org/docs/2.4/programs/ab.html](https://httpd.apache.org/docs/2.4/programs/ab.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We like `hey` on the command line to call the PetBattle API and list all of
    the pets. We pass in some parameters that represent:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-c`: Number of workers to run concurrently'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-n`: Number of requests to run'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-t`: Timeout for each request in seconds'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can see the summary statistics reported, and this is the bit we love – a
    histogram of latency distribution, HTTP status code distribution, as well as DNS
    timing details. This is super rich information. Histograms are graphs that display
    the distribution of the continuous response latency data. A histogram reveals
    properties about the response times that the summary statistics cannot. In statistics,
    summary data is used to describe the complete dataset – minimum, maximum, mean,
    and average, for example. **Hey** gives us these summary statistics at the top
    of the output.
  prefs: []
  type: TYPE_NORMAL
- en: The graph brings the data to life as we can start to understand the distribution
    of the latency response over the time the test ran. Over the 4.2 seconds it took
    to send the 100 requests, we can see that most of the data is clustered around
    the 0.4-second mark, which is nearly 50% of all traffic. Often, in service performance
    design, we are interested in what the 95% or 99% percentile number is. That is,
    for all of the sample data, what the response latency is for 95% (or 99%) of the
    traffic. In this test run, it is measured at 0.57 seconds – in other words, 95%
    of the data was at or below this mark.
  prefs: []
  type: TYPE_NORMAL
- en: The shape of the histogram is also important. Where are the response latencies
    grouped? We can easily see if the response times are distributed evenly around
    the mean (Gaussian) or if they have a longer or shorter tail. This can help us
    characterize the performance of the service under various loads. There are many
    types of load profiles you could use, for example, burst loads where we throw
    a lot of instantaneous traffic at our API, compared to more long-lived soak tests
    under a lower load. You might even have known loads from similar applications
    in production already. A great open source tool for designing these types of test
    loads, which can model threading and ramping really well, is Apache JMeter[31](#footnote-119)
    and we highly recommend it as a tool to have in your toolbox. To keep things simple,
    we won't cover that tool here.
  prefs: []
  type: TYPE_NORMAL
- en: '[31](#footnote-119-backlink) [https://jmeter.apache.org/](https://jmeter.apache.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The two diagrams shown in *Figure 15.19* display simple load tests. The one
    on the left is a burst type of test – 300 consecutive users calling 900 times
    to our PetBattle API. We can see the 95% is 15.6 seconds – this is quite a long
    time for users to wait for their cats! The one on the right is a soak test – 50
    consecutive users calling 10,000 times to our PetBattle API. A very different
    set of statistics: a test duration of 461 seconds, and the 95% is 2.8 sec—much
    better from an end user''s perspective.'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, it is important to think about what the test is actually doing
    and how it relates to the PetBattle application suite in general. If we think
    about it, the test may not be totally indicative of the current user interface
    behavior. For example, we do not perform a call to return all of the images in
    our MongoDB at once but rather page the results. And there are of course other
    API endpoints to test, for example, the `topcats` API, which returns the top three
    most popular pets and is called every time you visit the home page. We are returning
    the test dataset we have loaded into PetBattle, that is, around 15 pet images,
    so it is not a massive amount of data. It's important to always step back and
    understand this wider context when we run performance tests so we don't end up
    testing the wrong thing!
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.19: Burst and soak tests against the PetBattle API'
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, this is good data to ponder. A good result is that both the soak
    and burst tests only returned HTTP 200 response statuses – there were no error
    responses from the API. That gives us confidence that we have not broken anything
    or reached any internal system limits yet. We can also examine the details to
    make sure DNS resolution is not causing issues from the client-calling perspective.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are familiar with the client or calling side of performance testing,
    let''s switch to the PetBattle API application running on the server side. If
    we browse to the Developer view and select the `pet-battle-api` pod in the `labs-test`
    namespace, we can see some important server-side information:'
  prefs: []
  type: TYPE_NORMAL
- en: The PetBattle API is autoscaled to two pods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring metrics for the pods (check the appendix if you haven't enabled this
    for CRC).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a developer, we have configured the PetBattle API application to use the
    **Horizontal Pod Autoscaler** (**HPA**). This specifies how the OpenShift Container
    Platform can automatically increase or decrease the scale of a replication controller
    or deployment configuration, the number of running pods, based on the metrics
    collected from the pods that belong to our application.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.20: PetBattle API pod in the labs-test namespace'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our PetBattle API Helm chart, we specified the HPA with configurable values
    for minimum pods, maximum pods, as well as the average CPU and memory targets.
    Using hey, we can now test out various scenarios to help us tune the PetBattle
    API application under load:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE201]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE202]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE203]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE204]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE205]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE206]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE207]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE208]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE209]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE210]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE211]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE212]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE213]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE214]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE215]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE216]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE217]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE218]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE219]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE220]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE221]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE222]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE223]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE224]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE225]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE226]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE227]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE228]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE229]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE230]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE231]'
  prefs: []
  type: TYPE_PRE
- en: We initially took a rough guess at these settings in our HPA, for example, `min
    replicas = 2`, `max replicas =6`, `CPU = 200m`, `mem = 300Mi`, and set the resource
    limits and requests in our Deployment appropriately. We always have a minimum
    of two pods, for high availability reasons. The HPA is configured to scale based
    on the average memory and CPU loads. We don't yet understand whether the application
    is memory- or CPU-intensive, so choose to scale based on both these measurements.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.21: PetBattle API HPA in action, scaling pods under load'
  prefs: []
  type: TYPE_NORMAL
- en: We use hey to start a burst workload, 400 concurrent requests, and watch the
    behavior of the HPA as it starts more pods to keep to the specified memory and
    CPU averages. Once the test concludes, the HPA scales our workload back down to
    the minimum as the application recovers resources, in this case through Java garbage
    collection. OpenShift supports custom metrics for the HPA as well as other types
    of pod scalers, for example, the Vertical Pod Autoscaler.[32](#footnote-118)
  prefs: []
  type: TYPE_NORMAL
- en: To conclude this section, we want to point out one more Kubernetes object that
    the developer needs in their toolbelt – the **Pod Disruption Budget** (**PDB**).
    Again, using a Helm chart template for the PDB, we can limit the number of concurrent
    disruptions that the PetBattle API application experiences. By setting up a PDB,
    we can allow for higher availability while permitting the cluster administrator
    to manage the life cycle of the cluster nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '[32](#footnote-118-backlink) [https://docs.openshift.com/container-platform/4.7/nodes/pods/nodes-pods-using.html](https://docs.openshift.com/container-platform/4.7/nodes/pods/nodes-pods-using.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the cluster is being updated and nodes are being restarted, we want a minimum
    of one `pet-battle-api` pod available at all times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE232]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE233]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE234]'
  prefs: []
  type: TYPE_PRE
- en: This ensures a high level of business service for our PetBattle API. We can
    see `ALLOWED_DISRUPTIONS` is set to 1 – this is because, at the time, the HPA
    had scaled the number of available replicas to 3 and this will change as the number
    of available pods changes.
  prefs: []
  type: TYPE_NORMAL
- en: One of the great things about performance testing applications on OpenShift
    is that all of the tools are at a developer's fingertips to be able to configure,
    test, measure, and tune their applications to achieve high availability and performance
    when under load. Each application service is independently scalable, tunable,
    and deployable, which makes for a faster and targeted feedback loop when dealing
    with scale and performance issues.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to take a look at what makes a good OpenShift
    Kubernetes citizen, automating Kubernetes resource validation as part of our pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Resource Validation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One aspect of testing that doesn't yet get much thought is the quality of the
    Kubernetes resources being deployed on the cluster. For applications to be considered
    *good citizens* on Kubernetes, there are a number of deployment best practices
    to be followed—including health checks, resource limits, labels, and so on—and
    we will go through a number of these in in *Chapter 16, Own It*. However, we need
    to validate the resource definitions being applied to the cluster to ensure a
    high level of compliance to not only industry recommendations but also any other
    resource recommendations that we see fit to add. This is where **Open Policy Agent**
    (**OPA**)[33](#footnote-117) and associated tools can come into play. This enables
    us to validate resource definitions during a CI pipeline and also when applying
    resources to a cluster. OPA by itself is a policy validator and the policies are
    written using a language called Rego. Additional OPA tools such as Conftest[34](#footnote-116)
    and Gatekeeper[35](#footnote-115) add a lot of value and governance from a usability
    and deployment perspective. OPA is also embeddable into other third-party tools
    such as KubeLinter.[36](#footnote-114)
  prefs: []
  type: TYPE_NORMAL
- en: '[33](#footnote-117-backlink) [https://www.openpolicyagent.org/](https://www.openpolicyagent.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[34](#footnote-116-backlink) [https://github.com/open-policy-agent/conftest](https://github.com/open-policy-agent/conftest)'
  prefs: []
  type: TYPE_NORMAL
- en: '[35](#footnote-115-backlink) [https://github.com/open-policy-agent/gatekeeper](https://github.com/open-policy-agent/gatekeeper)'
  prefs: []
  type: TYPE_NORMAL
- en: '[36](#footnote-114-backlink) [https://github.com/stackrox/kube-linter](https://github.com/stackrox/kube-linter)'
  prefs: []
  type: TYPE_NORMAL
- en: We haven't used OPA's server-side validation component, Gatekeeper,[37](#footnote-113)
    as part of PetBattle but there are example Rego policies in the Red Hat Community
    of Practice GitHub repo[38](#footnote-112) that are definitely worth exploring.
    If this is something of interest to you, definitely check out the blog on OpenShift.com
    that details setting up all of these components.[39](#footnote-111)
  prefs: []
  type: TYPE_NORMAL
- en: However, to show how easy it is to use client-side resource validation and why
    you should include at least some resource validation in a pipeline, a simple Rego
    example has been created. Rego policies are easy enough to write and the Rego
    playground[40](#footnote-110) is a great place to write and verify policies, so
    check it out.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get into an example. In our Non-Functional Requirements Map, we said we
    wanted to be consistent with our labeling. It makes sense that we should adopt
    the Kubernetes best practice that suggests the `app.kubernetes.io/instance` label
    should be on all resources, so let's see how we can write a test to this effect
    and add it to our pipeline in Jenkins.
  prefs: []
  type: TYPE_NORMAL
- en: 'The makeup of a policy that denies the creation of a resource is simple enough.
    A message is formed and passed back to the interpreter if all of the statements
    are true in the rule. For example, we have written a policy that checks that all
    resources conform to Kubernetes best practice for naming conventions. The policy
    here is checking whether `app.kubernetes.io/instance` exists on the resource supplied
    to it (`input`). If each statement is true, then a message is returned as the
    error, guiding someone to fix the issue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE235]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE236]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE237]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE238]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE239]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE240]'
  prefs: []
  type: TYPE_PRE
- en: '[37](#footnote-113-backlink) [https://github.com/open-policy-agent/gatekeeper](https://github.com/open-policy-agent/gatekeeper)'
  prefs: []
  type: TYPE_NORMAL
- en: '[38](#footnote-112-backlink) [https://github.com/redhat-cop/rego-policies](https://github.com/redhat-cop/rego-policies)'
  prefs: []
  type: TYPE_NORMAL
- en: '[39](#footnote-111-backlink) [https://www.openshift.com/blog/automate-your-security-practices-and-policies-on-openshift-with-open-policy-agent](https://www.openshift.com/blog/automate-your-security-practices-and-policies-on-openshift-with-open-policy-agent)'
  prefs: []
  type: TYPE_NORMAL
- en: '[40](#footnote-110-backlink) [https://play.openpolicyagent.org/](https://play.openpolicyagent.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can combine this rule with Conftest and a Helm template to create a way
    to statically validate our resources. In the PetBattle frontend code, there is
    a policy folder that has a few more policies to check whether all the standard
    Kubernetes labels[41](#footnote-109) are set on our generated resources after
    we run the `helm template` command. By running a few commands, we can verify these
    are in place. First, we template our chart to produce the Kubernetes resources
    we will apply in deploying our software, and secondly, we tell Conftest to check
    each file generated against the rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE241]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE242]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE243]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE244]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE245]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE246]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE247]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE248]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE249]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE250]'
  prefs: []
  type: TYPE_PRE
- en: When executing the rules from the command line, we get a good insight into what's
    missing from our chart. Of course, we could just assume that we'd always make
    our charts adhere to the best practices, but the `jenkins-agent-helm` has also
    got the Conftest binary so we can execute the preceding statements in our Jenkins
    pipeline too. This example might seem simple but, hopefully, it gives you some
    idea of the things that can be automated and tested that might seem less obvious.
  prefs: []
  type: TYPE_NORMAL
- en: '[41](#footnote-109-backlink) [https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/#labels](https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/#labels)'
  prefs: []
  type: TYPE_NORMAL
- en: Image Scanning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Red Hat provides the Quay Container Security Operator in OpenShift to bring
    Quay and Clair image scanning and vulnerability information into our OpenShift
    cluster. Any container image that is hosted on [Quay.io](http://Quay.io) is scanned
    by Clair.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.22: Quay Container Security Operator'
  prefs: []
  type: TYPE_NORMAL
- en: Any image vulnerability data is exposed back in the OpenShift Web UI so that
    users and administrators can easily view which images are considered vulnerable
    and which namespace they are deployed to.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.23: Vulnerable container images'
  prefs: []
  type: TYPE_NORMAL
- en: With this operator deployed, the OpenShift overview status displays image vulnerability
    data, which an operator can drill into to find out the status of container images
    running on the platform. For PetBattle, we don't have any enforcement for image
    vulnerabilities discovered in our cluster. If we wanted to move the security scanner
    "left" in our deployment pipeline, there are some great open source scanning tools
    available on the OpenSCAP website.[42](#footnote-108)
  prefs: []
  type: TYPE_NORMAL
- en: Other Non-functional Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are lots of other types of testing we can do to validate our application.
    In this section are some of the things we feel are important to include in a pipeline,
    but the reality is there is much more than just this list and books could be written
    on this topic in and of itself!
  prefs: []
  type: TYPE_NORMAL
- en: Linting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A linter is a static code analysis tool that can check a code base for common
    pitfalls in design or stylistic errors. This does not check the compiled application,
    but the structure of the application. This is super important for languages that
    are not compiled, such as JavaScript. Browsers can interpret JavaScript in different
    ways so consistency is super critical.
  prefs: []
  type: TYPE_NORMAL
- en: If you think about a large enterprise application, there could be hundreds of
    developers working on the one code base. These developers could even be globally
    distributed with different teams looking after different parts of the application's
    life cycle. Having consistency in the approach to writing the software can dramatically
    improve maintenance costs. JavaScript is very flexible in how you can write it,
    whether this is from a functional programming standpoint or object-oriented, so
    it is important to get this consistency right.
  prefs: []
  type: TYPE_NORMAL
- en: The PetBattle frontend uses TSLint/ESLint[43](#footnote-107) to check the style
    of the code adheres to a standard set of rules. These rules can be manipulated
    by the team, but the rules are checked into Git so if someone was to disable them
    or manipulate them, it would be noticed. Our Jenkins pipeline is configured to
    automatically check the code base using the `npm lint` command and our build will
    fail if a developer does not adhere to the standard.
  prefs: []
  type: TYPE_NORMAL
- en: '[42](#footnote-108-backlink) [https://www.open-scap.org](https://www.open-scap.org)'
  prefs: []
  type: TYPE_NORMAL
- en: '[43](#footnote-107-backlink) [https://eslint.org/](https://eslint.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.24: Linting PetBattle''s frontend locally scans both the JavaScript
    and HTML'
  prefs: []
  type: TYPE_NORMAL
- en: For Java Quarkus apps, Checkstyle[44](#footnote-106) is used to analyze the
    code base.
  prefs: []
  type: TYPE_NORMAL
- en: For Kubernetes resources, the aforementioned Open Policy Agent can assist, and
    Helm also has the `helm lint`[45](#footnote-105) command to validate your charts.
  prefs: []
  type: TYPE_NORMAL
- en: Code Coverage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So, you've written a load of tests and you think things are going great – but
    how do you know your tests are any good and covering all parts of the code base?
    Allow me to introduce code coverage metrics! A code coverage reporter is a piece
    of software that runs alongside your unit test suites to see what lines of code
    are executed by the tests and how many times. Coverage reports can also highlight
    when if/else control flows within an application are not being tested. This insight
    can provide valuable feedback as to areas of a system that remain untested and
    ultimately reduce the number of bugs.
  prefs: []
  type: TYPE_NORMAL
- en: '[44](#footnote-106-backlink) [https://checkstyle.sourceforge.io/](https://checkstyle.sourceforge.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[45](#footnote-105-backlink) [https://helm.sh/docs/helm/helm_lint/](https://helm.sh/docs/helm/helm_lint/)'
  prefs: []
  type: TYPE_NORMAL
- en: Our PetBattle frontend is configured to run a coverage report when our Jest
    tests execute. Jest makes generating the report very simple as it has a flag that
    can be passed to the test runner to collect the coverage for us. The coverage
    report is run on every execution of the build and so should be reported through
    Jenkins.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.25: Code coverage report from the frontend unit tests locally'
  prefs: []
  type: TYPE_NORMAL
- en: When executing our tests in the Jenkins pipeline, we have configured Jest to
    produce an HTML report that can be reported by Jenkins on the jobs page. For any
    build execution, the report is added to the jobs home page. The report will allow
    us to discover what lines are being missed by our tests. Being able to drill into
    a report like this can give a good insight into where our testing is lacking.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.26: Code coverage report in Jenkins gives us detailed insight'
  prefs: []
  type: TYPE_NORMAL
- en: So, what should I do with these results? Historically, we have worked where
    coverage is low. It can serve as a great talking point to bring up in a Retrospective.
    Printing out the reports and discussing them as a team is a great way to assess
    why the team is struggling to write enough tests. Sometimes teams are drowning
    by being overwhelmed with pressure to churn out features and so testing can slip
    to the wayside. Having a coverage reporter in your build can help keep a team
    honest. You could even set thresholds so that if testing coverage falls below
    a certain percentage (some teams aim for 80% and above), the build will fail,
    thus blocking the pipeline until the quality is increased.
  prefs: []
  type: TYPE_NORMAL
- en: Untested Software Watermark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](img/Author_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: I worked on a project a long time ago that was poorly structured. I was a member
    of the DevOps team, which I know now is an antipattern in most implementations!
    This project had many issues, the team had planned three Sprints in advance but
    hadn’t allocated enough time for testing. It was always the thing that got squeezed.
  prefs: []
  type: TYPE_NORMAL
- en: Through Retrospectives with the teams, we discovered that there was simply not
    enough time for tests. This may sound hard to hear, but the root cause for this
    was not laziness by the team or a lack of skills; it really was time. The project
    was running in 8-week blocks that were pre-planned from the beginning with a fixed
    output at the end. The team thought they were doing Scrum, but in actual fact,
    they had milestones of functionality to accomplish each sprint and there was no
    feedback loop. Of course, none of the Scrum team members were involved in the
    sizing or planning ceremonies either. This meant the teams were constantly under
    pressure to deliver.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Draft.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Through a Retrospective, we decided to try to radiate some of this pressure
    the teams were under as we were not happy that quality was being sacrificed for
    some arbitrary deadlines. Knowing that failing the pipeline simply would not work
    for these customers, we had to get creative in showing the software quality. We
    decided to inject a watermark into any application that had low test coverage.
    This watermark resembled a DRAFT logo you would find on any document, but ours
    was a little different.
  prefs: []
  type: TYPE_NORMAL
- en: A large banner reading UNTESTED SOFTWARE was placed across the applications
    that failed the tests. This watermark did not affect the user behavior of the
    app; it was just an overlay but it was an amazing way to get people talking. Seeing
    a giant banner saying UNTESTED is a surefire way to have people question why things
    have gotten this way.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at some other ways we can visualize risks during continuous delivery.
  prefs: []
  type: TYPE_NORMAL
- en: The OWASP Zed Attack Proxy (ZAP)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Security scanning is always a hot topic. From image scanning, which we discussed
    earlier, to dependency checking for our application that happens in our pipelines,
    there are limitless numbers of things to automate from a security perspective.
    Let's take another example of something that can be useful to include in a pipeline
    – the OWASP Zed Attack Proxy.[46](#footnote-104)
  prefs: []
  type: TYPE_NORMAL
- en: 'From their website: *The OWASP Zed Attack Proxy (ZAP) is one of the world''s
    most popular free security tools which lets you automatically find security vulnerabilities
    in your applications. This allows the developers to automate* *penetration testing
    and security regression testing of the application in the CI/CD pipeline.*'
  prefs: []
  type: TYPE_NORMAL
- en: Adding the ZAP security scanning tool to our pipelines is simple. Just add the
    following `stage` and add the URL you want to test. The source code for this image
    is available, like our other Jenkins images from the Red Hat CoP.[47](#footnote-103)
    The ZAP scan in Jenkins will produce a report showing some potential vulnerabilities
    in our application.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE251]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE252]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE253]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE254]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE255]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE256]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE257]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE258]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE259]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE260]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE261]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE262]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE263]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE264]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE265]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE266]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE267]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE268]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE269]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE270]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE271]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE272]'
  prefs: []
  type: TYPE_PRE
- en: '[46](#footnote-104-backlink) [https://www.zaproxy.org/](https://www.zaproxy.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[47](#footnote-103-backlink) [https://github.com/redhat-cop/containers-quickstarts/tree/master/jenkins-agents](https://github.com/redhat-cop/containers-quickstarts/tree/master/jenkins-agents)'
  prefs: []
  type: TYPE_NORMAL
- en: In doing so, the web report that's created can be viewed in Jenkins, which gives
    great details on the cause of the security vulnerability as well as any action
    that should be taken to remedy it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.27: Example Zap report for PetBattle'
  prefs: []
  type: TYPE_NORMAL
- en: In the final non-functional testing section, let's have a look at deliberately
    breaking our code using a technique called chaos engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Chaos Engineering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Chaos engineering is the process of deliberately breaking, hobbling, or impacting
    a system to see how it performs and whether it recovers in the ensuing "chaos."
    While most testing is seen as an endeavor to understand how a system performs
    in a known, stable state, chaos engineering is the computing equivalent of setting
    a bull free in a fine-china shop—you know it's going to end badly but you just
    don't know exactly the magnitude of how bad it's going to be.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of chaos engineering is to build confidence in the resiliency of
    the system. It also allows you to better understand where breakage points occur
    and the blast radius of any failures. There are many resilience features built
    into the Kubernetes API specification. Pod replicas are probably the simplest
    mechanism, having more than one of your applications running at any given time.
    It is also desirable to use application-specific mechanisms such as circuit breakers,
    which prevent failures from spreading throughout your system. Chaos engineering
    takes these ideas one step further and tests a system when one or more components
    fully or partially fail, such as when CPU or memory resources are low.
  prefs: []
  type: TYPE_NORMAL
- en: The basic premise is that the system under test is observed in a stable working
    state, then a fault is injected. The system is then observed to see if it recovers
    successfully from the fault or not. Outcomes from such testing are a potential
    list of areas to tune/fix as well as an understanding of the **Mean Time to Recovery**
    (**MTTR**) of a system. It's important to note that chaos engineering is focused
    on the system as a whole—both application and infrastructure performance need
    to be considered and tested.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key mantras behind chaos engineering is contained in its defining
    principles[48](#footnote-102) – *The need to identify weaknesses before they manifest
    in system-wide, aberrant behaviors*.
  prefs: []
  type: TYPE_NORMAL
- en: This is one of the most important aspects to be considered when adopting this
    approach. You don't want to be learning about weaknesses during a production-impacting
    incident. It's similar to the rationale behind regularly testing disaster recovery
    plans. To paraphrase, a colleague of ours here at Red Hat said, "*When the excrement
    hits the fan, the first thing to do is turn off the fan!*" Not much time for learning
    there.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a number of tools and frameworks that can help with setting up a
    chaos engineering practice. Here''s some to get started with (though there are
    others):'
  prefs: []
  type: TYPE_NORMAL
- en: Litmus Chaos[49](#footnote-101)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kraken[50](#footnote-100)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaos Mesh[51](#footnote-099)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48](#footnote-102-backlink) [https://principlesofchaos.org/](https://principlesofchaos.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[49](#footnote-101-backlink) [https://litmuschaos.io/](https://litmuschaos.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[50](#footnote-100-backlink) [https://github.com/cloud-bulldozer/kraken](https://github.com/cloud-bulldozer/kraken)'
  prefs: []
  type: TYPE_NORMAL
- en: '[51](#footnote-099-backlink) [https://chaos-mesh.org/](https://chaos-mesh.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: In a world where practices such as everything-as-code and GitOps are our only
    way to build software and the systems that support them, a great way to validate
    the ability to respond to missing items is to redeploy everything, including your
    infrastructure, from scratch every week or every night! This might seem extreme,
    but it's a great way to validate that there is no hidden magic that someone has
    forgotten to write down or codify.
  prefs: []
  type: TYPE_NORMAL
- en: Accidental Chaos Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](img/Donal.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is a story that I used to be reluctant to share, but over the years (and
    having done it twice), I realized it was actually a good thing to have done.
  prefs: []
  type: TYPE_NORMAL
- en: While working for an airline, I accidentally deleted the `labs-ci-cd` project
    along with a few other namespaces where our apps were deployed, including the
    authentication provider for our cluster. At the time, we were several weeks into
    our development. We were used to re-deploying applications and it was not a big
    deal for us to delete CI tools such as Nexus or Jenkins, knowing that our automation
    would kick back in swiftly to redeploy them.
  prefs: []
  type: TYPE_NORMAL
- en: However, on this engagement, we were also using GitLab and, unfortunately for
    me, GitLab was in the same project as these other tools!
  prefs: []
  type: TYPE_NORMAL
- en: I checked with the team first and asked whether it was OK to rebuild everything
    in our tooling namespace. I got a resounding "yes" from my teammates, so proceeded
    to delete some of the things I thought needed to be cleared out and accidentally
    removed a few extra projects. About 30 seconds later, someone on the team perked
    up and asked, *Is Git down for anyone else?* This was promptly followed by another
    person saying, *Is anyone else not able to log in to the cluster?* My face lit
    up red as I immediately realized what I'd just done. Git, as we keep saying in
    the book, is our single source of truth. *If it's not in Git, it's not real* is
    our mantra! We even had it written on the walls! But I had just deleted it.
  prefs: []
  type: TYPE_NORMAL
- en: So, what happens when some silly person accidentally deletes it? After the initial
    shock and panic, the team pulled the Andon Cord. We quickly stormed together to
    see what exactly had happened in order to plan how we could recover not just Git
    but all the things we'd added to the cluster. Luckily for us, everything we had
    done was stored in Git so we were able to redeploy our tools and push our local,
    distributed copies of the software and infrastructure back into the shared Git
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: The team was cross-functional and had all the tools and access we needed to
    be able to respond to this. Within 1 hour, we had fully restored all our applications
    and tools with all of our automation running smoothly again.
  prefs: []
  type: TYPE_NORMAL
- en: I think the real power in this example is how, given the right equipment and
    the right ownership, an empowered team can have it all. We acted as one unit fixing
    things at lightning speed. We were not stuck waiting in a queue or having to raise
    a ticket on another team to restore our infrastructure. We could do it for ourselves
    within minutes – not days or weeks later.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing I learned was not to keep Git in the same project as the other
    tools in case another person like me comes along. I also learned to be mindful
    of the permissions we have within a cluster. As an administrator, I was able to
    remove things that perhaps I should not have been playing with.
  prefs: []
  type: TYPE_NORMAL
- en: So we've written the code, tested, quality-checked it and even scanned it for
    vulnerabilities. Now it's time to deploy it onto the cluster. Let's explore one
    of the key areas of benefit of using Kubernetes - the different ways you can deploy
    applications depending on your needs and perform user-driven experiments to determine
    what features your users prefer.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced Deployments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The time between software being written and tested till it is deployed in production
    should be as short as possible. That way your organization is able to realize
    value from the software changes as quickly as possible. The modern approach to
    this problem is, of course, through automation. There are simply too many details
    and configuration items that need to be changed when deploying to production that
    even for a small application suite like PetBattle, manual deployment becomes error-prone
    and tedious. This drive to reduce manual toil is at the heart of many of the DevOps
    practices we have been discovering in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can minimize the downtime (ideally to zero!) during software deployment
    changes by adopting the right application architecture and combining that with
    the many platform capabilities that OpenShift offers. Let''s look at some common
    deployment strategies that OpenShift supports:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Rolling deployment:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spin up a pod of the new version and then spin down a pod of the existing old
    version automatically. Very useful for a zero-downtime approach.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Canary deployment:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spin up a single pod of the new version, perform testing to ensure that everything
    is working correctly, and then replace all the old pods with new ones.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Blue/Green deployment:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a parallel deployment and verify that everything is working correctly
    before switching traffic over.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Service Mesh traffic mirroring functionality can be useful with this approach
    to validate that the new version is working as expected.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Recreate deployment:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basically, scale the existing pods down to zero and then spin up the new version.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use where an application must be restarted, for example, to migrate database
    schema or tables.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Think of this as a Ripley deployment: "take off and nuke the entire site from
    orbit. It''s the only way to be sure."[52](#footnote-098)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We can roll back to previous deployment versions using the Helm chart life cycle
    or the out-of-the-box `oc rollback` support. Images and configuration are versioned
    and cached in OpenShift to easily support rolling back to previous versions.
  prefs: []
  type: TYPE_NORMAL
- en: '[52](#footnote-098-backlink) [https://en.wikiquote.org/wiki/Aliens_(film)](https://en.wikiquote.org/wiki/Aliens_(film))'
  prefs: []
  type: TYPE_NORMAL
- en: A/B Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A/B testing an application is an amazing way to test or validate a new feature
    in production. The process is pretty simple: you deploy two (or more) different
    versions of your application to production, measure some aspect, and see which
    version performs *better*. Given that A/B testing is primarily a mechanism of
    gauging user experience, *better* depends on what aspect/feature you''re experimenting
    with. For example, you could make a subtle change to a web page layout and measure
    how long it takes for the user to navigate to some button or how long the user
    continues to interact with specific items on the page.'
  prefs: []
  type: TYPE_NORMAL
- en: It's a brilliant way to de-risk a new release or validate some new business
    or UI features with a smaller audience before releasing to a wider group. User
    behavior can be captured and experiments can be run to make informed decisions
    about what direction a product should take.
  prefs: []
  type: TYPE_NORMAL
- en: The Experiment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s cast our minds back to the earlier chapters where we spoke about generating
    options. There we spoke about the importance of experiments and our Value Slicing
    board included an item for which we could do an A/B test. One experiment that
    came up was to assess how users would vote for cats in the competition. Should
    they just be able to upvote (with a 👍) or should they be able to downvote (👎)
    too? We can build and deploy two versions of our application: one with the ability
    to both upvote and downvote, and one with just the ability to upvote. Our experiment
    is simple: to track how often people actually use the downvote button, so we can
    decide whether it''s a feature we need or whether we should focus on building
    different functionality.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.28: Experiment defined on a Value Slicing board'
  prefs: []
  type: TYPE_NORMAL
- en: Let's now look at how we could set up a simple experiment to deploy both variants
    of the application and route traffic between each deployed instance to generate
    some data to help inform our decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: Matomo – Open Source Analytics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenShift provides us with a mechanism to push traffic to different versions
    of an application. This in itself is useful, but it provides no information that
    we can base a decision on. For this, we need to measure how the users interact
    with the platform. To do this, we're going to introduce user analytics, which
    records metrics on the users' interactions with the website. We're going to use
    the open source Matomo[53](#footnote-097) platform. There are others we could
    have used but, at the time of writing, this was our choice as it was open source
    and quite feature-complete. Let's add Matomo to our Big Picture for consistency.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/figure-15-29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.29: Big Picture with added tools including Matomo'
  prefs: []
  type: TYPE_NORMAL
- en: '[53](#footnote-097-backlink) [https://matomo.org/](https://matomo.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'So how do we install the Matomo platform? Here comes Helm to the rescue again.
    We automated this installation as part of the PetBattle platform by just enabling
    it in our Ubiquitous Journey project. It''s deployed by default into our `labs-ci-cd`
    namespace from this configuration in `ubiquitous-journey/values-tooling.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE273]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE274]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE275]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE276]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE277]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE278]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE279]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE280]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE281]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE282]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE283]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE284]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE285]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE286]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if you want to just install the tool without involving ArgoCD, you
    can just clone the repository and install it manually. This chart has been forked
    from an existing chart[54](#footnote-096) to tweak it for easier installation
    on OpenShift. Specifically, the security contexts in the MariaDB and Redis dependencies
    have been disabled so that the deployment will automatically use the target namespace
    default service account and associated `anyuid`. Also, an OpenShift route has
    been added to the chart to allow ingress traffic to the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE287]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE288]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE289]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE290]'
  prefs: []
  type: TYPE_PRE
- en: With the Matomo analytics deployed, we just need to configure the frontend to
    connect to it. To do this just update the config map's `matomoUrl` in the `chart/values.yaml`
    in the frontend to have the tracking code automatically track the site. This will
    provide basic site tracking such as the time spent on a page or the number of
    pages visited.
  prefs: []
  type: TYPE_NORMAL
- en: '[54](#footnote-096-backlink) [https://gitlab.com/ideaplexus/helm/matomo](https://gitlab.com/ideaplexus/helm/matomo)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.30: Configuring the config_map for matomoUrl'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a more meaningful test, we might want to capture specific user behavior.
    The application has been instrumented to report certain events back to the Matomo
    server, such as mouse clicks. Whenever a user clicks the button to vote for a
    cat, it will capture it and report it in Matomo for us. It''s very simple to do
    this – we just add a one-liner to the event we want to track:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE291]'
  prefs: []
  type: TYPE_PRE
- en: Deploying the A/B Test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In PetBattle land, let's see how we could configure the deployments of the frontend
    to run this simple A/B test. Luckily for us, OpenShift makes this super easy by
    having a way to expose a `route` and connect it to more than one service, using
    the `alternateBackends` array to configure additional services to send traffic
    to. We can then apply `weights` to each service defined here in order to set the
    percentage of the traffic to either service that's deployed, A or B. The weights
    can be set between 0 and 256, and if a service is reduced to 0 then it carries
    on serving existing connections but no new ones. In fact, OpenShift allows us
    to do more than just an A or B test – also C and D, as `alternateBackends` supports
    up to three services!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s deploy our A/B experiment for `pet-battle`. We could integrate these
    steps with ArgoCD but to keep things nice and easy for illustrative purposes,
    let''s just stick with using our trusty friend Helm to deploy things. We''ve prebuilt
    an image that has no ability to downvote on the home page `quay.io/petbattle/pet-battle:no-down-vote`.
    Let''s deploy this image to our cluster by running a simple Helm command (make
    sure to set the config map to the correct endpoints for your cluster):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE292]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE293]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE294]'
  prefs: []
  type: TYPE_PRE
- en: With this command, we're deploying a new instance of the `pet-battle` frontend
    by setting the image to the prebuilt one and disabling the OpenShift route for
    this as it's not needed. We'll configure our route to production by updating our
    `prod` app.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running `oc get pods` should show the app started and if you check for routes,
    you should see none exposed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE295]'
  prefs: []
  type: TYPE_PRE
- en: NAME READY STATUS RESTARTS AGE
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE296]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE297]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s deploy our `prod` version of the `pet-battle` application and add the
    `no-down-vote` app as one of the services we''ll connect to. Our Helm chart is
    configured to accept the name of the service and the weight we want to apply to
    the experiment feature via `a_b_deploy.svc_name` and `a_b_deploy.weight`. It''s
    defaulted to be a 50/50 round-robin split. Let''s deploy it with this setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE298]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE299]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE300]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE301]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE302]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE303]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE304]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE305]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE306]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE307]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `pet-battle` UI and you should see on refreshing that there
    is a 50/50 chance that you will get the upvote-only version. If you open up incognito
    mode or a different browser and try to hit the frontend, you should get the alternative
    one. A different browser session is required, as the OpenShift router will by
    default return you to the same pod, so you'll always land on the same site version.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.31: The no-downvote PetBattle frontend'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running `oc get routes` should show one route and more than one service connected
    to it with a 50/50 `split prod-pet-battle(50%),no-down-vote-pet-battle(50%)`.
    You can view the weights set as 100 each by running `oc get route prod-pet-battle
    -o yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE308]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE309]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE310]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE311]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE312]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE313]'
  prefs: []
  type: TYPE_PRE
- en: 'The weights for the traffic routed to each application can be updated quite
    easily using Helm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE314]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE315]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE316]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE317]'
  prefs: []
  type: TYPE_PRE
- en: Understanding the results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we play around with the two versions that are deployed, we can see how the
    results of clicking the buttons are captured. If you open the Matomo app and log
    in, you will see some statistics there. The default password for Matomo, as set
    in the chart, is `My$uper$ecretPassword123#`. This might not be exactly secure
    out of the box but it can easily be changed via the Helm chart's values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.32: Matomo showing the number of clicks for UP_VOTE versus DOWN_VOTE'
  prefs: []
  type: TYPE_NORMAL
- en: It might take a few minutes for Matomo to render the pie chart. Our simple experiment
    shows that more people use the UP_VOTE feature than the DOWN_VOTE feature. By
    connecting the A/B test to the data captured in Matomo, we can now make more informed
    decisions about the next actions that need to be taken for our product.
  prefs: []
  type: TYPE_NORMAL
- en: This experiment proves how easy it is to set up an A/B test. We can use the
    OpenShift platform to dynamically route users to multiple application versions
    concurrently deployed while we collect data about what is working well and what
    is not. There is some thinking that needs to be put into how we instrument the
    application to collect specific data, but the open source tooling available to
    us makes this easy too!
  prefs: []
  type: TYPE_NORMAL
- en: Blue/Green deployments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Blue/Green deployment strategy is one of the fundamental deployment strategies
    that every team deploying applications into production should know about. Using
    this strategy minimizes the time it takes to perform a deployment cutover by ensuring
    you have two versions of the application available during deployment. It is also
    advantageous in that you can quickly roll back to the original version of the
    application without having to roll back any changes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.33: The canonical Blue/Green deployment'
  prefs: []
  type: TYPE_NORMAL
- en: The trade-off here is that you need to have enough resources to be able to run
    two versions of the application stack you are deploying. If your application has
    persistent state, for example, a database or non-shared disk, then the application
    architecture and constraints must be able to accommodate the two concurrent versions.
    This is normally not an issue for smaller microservices and is one of the benefits
    of choosing that style of deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run through Blue/Green deployment using the PetBattle API as the example
    application stack. In this case, we are going to deploy two full stacks, that
    is, both the application and MongoDB. Let''s deploy the blue version of our application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE318]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE319]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE320]'
  prefs: []
  type: TYPE_PRE
- en: 'Now deploy the green application stack. Note that we have a different tagged
    image version for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE321]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE322]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE323]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE324]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE325]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we expose our production URL endpoint as a route that points to the blue
    service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE326]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE327]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE328]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can switch between the two using the `oc patch` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE329]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE330]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE331]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE332]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE333]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE334]'
  prefs: []
  type: TYPE_PRE
- en: 'If you browse to the `bluegreen` route endpoint, you should be able to easily
    determine the application stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.34: Blue/Green deployment for the PetBattle API'
  prefs: []
  type: TYPE_NORMAL
- en: Even though this is somewhat of a contrived example, you can see the power of
    developers being allowed to manipulate the OpenShift routing tier in a self-service
    manner. A similar approach could be used to deploy the NSFF feature as an example
    – use the Helm chart parameters `--set nsff.enabled=true` to deploy an NSFF-enabled
    version. You can also point both applications to the same database if you want
    to with similar manipulation of the Helm chart values.
  prefs: []
  type: TYPE_NORMAL
- en: If you have more complex use cases where you need to worry about long-running
    transactions in the original blue stack, that is, you need to drain them, or you
    have data stores that need migrating alongside the green rollout, there are several
    other more advanced ways of performing Blue/Green deployments. Check out the ArgoCD
    rollout capability, which has a ton of advanced features,[55](#footnote-095) the
    Knative Blue/Green rollout capability, or indeed Istio[56](#footnote-094) for
    more ideas.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment previews
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We should think of OpenShift as something of a playground that we can use to
    deploy our applications for production all the way down to a developer preview.
    Gone are the days when a development team needed to raise a ticket to provision
    a server and manually configure it to show off their applications. Building applications
    in containers allows us to make shippable applications that can be repeatedly
    deployed in many environments. Our automation for PetBattle in Jenkins is configured
    to run on every commit. For Jenkins, we're using the multi-branch plugin so anytime
    a developer pushes a new feature to a branch, it will automatically scaffold out
    a new pipeline and deploy the latest changes for that feature.
  prefs: []
  type: TYPE_NORMAL
- en: When this was discussed in the previous chapter, about sandbox builds, you may
    have thought this was overkill and a bit of a waste. Why not just build on a pull
    request? It's a valid question to ask and depending on the objective you're trying
    to achieve, building on a pull request is probably sufficient. We have used the
    sandbox builds as another way to introduce feedback loops.
  prefs: []
  type: TYPE_NORMAL
- en: '[55](#footnote-095-backlink) [https://argoproj.github.io/argo-rollouts](https://argoproj.github.io/argo-rollouts)'
  prefs: []
  type: TYPE_NORMAL
- en: '[56](#footnote-094-backlink) [https://github.com/hub-kubernetes/istio-blue-green-deployment](https://github.com/hub-kubernetes/istio-blue-green-deployment)'
  prefs: []
  type: TYPE_NORMAL
- en: Developers do not exist in isolation; they are surrounded by other members of
    the team, including Product Owners and Designers. Our ability to dynamically spin
    up a new deployment of a feature from our pipeline means we can connect the coding
    efforts to the design team really easily. Developers can get very fast feedback
    by sharing a link to the latest changes or the implementation of a new feature
    with the design team. This feedback loop can quickly allow subtle changes and
    revisions to be made before the engineer loses the context of the piece of work.
    Creating deployment previews from every commit also allows a developer to very
    quickly share two versions of what an app might look like with a Product Owner
    while they make their decision about which to choose.
  prefs: []
  type: TYPE_NORMAL
- en: From our Jenkins pipeline, there is a branch called `cool-new-cat`. When this
    is built, it will push a new version of the app to the `dev` environment. The
    change in the app is subtle for illustrative purposes, but we can see the banner
    has been changed. With this new version of the app in the `dev` environment, we
    can get some feedback prior to merging it to master and generating a release candidate.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_15_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.35: New feature deployed to the sandbox generating a deploy preview
    to collect feedback'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 15.35* shows the sandbox version of the being deployed along with it''s
    associated route, service and configmap.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Congratulations! You've just finished the most technologically focused chapter
    of this book so far. Please don't go off and think that you have to use each and
    every technology and technique that has been mentioned—that's not the point. Investigate,
    evaluate, and choose which of these technologies applies to your own use cases
    and environment.
  prefs: []
  type: TYPE_NORMAL
- en: Several of the testing practices are part of our technical foundation. Unit
    testing, non-functional testing, and measuring code coverage are all critical
    practices for helping build quality into our applications and products from the
    start. We covered many small but invaluable techniques, such as resource validation,
    code linting, and formatting, that help make our code base less of a burden to
    maintain.
  prefs: []
  type: TYPE_NORMAL
- en: We covered a number of different approaches for deployments, including A/B,
    Canary, Blue/Green, and Serverless. These core techniques allow us to deliver
    applications more reliably into different environments. We even briefly covered
    artificial intelligence for reducing unwanted images uploaded into our PetBattle
    product. By focusing our efforts on what happens when things go wrong, we can
    more easily embrace and prepare for failures—big and small.
  prefs: []
  type: TYPE_NORMAL
