<html><head></head><body>
		<div>&#13;
			<div id="_idContainer542" class="Content">&#13;
			</div>&#13;
		</div>&#13;
		<div id="_idContainer543" class="Content">&#13;
			<h1 id="_idParaDest-404">16. <a id="_idTextAnchor622"/>Own It</h1>&#13;
		</div>&#13;
		<div id="_idContainer555" class="Content">&#13;
			<p>"<em class="italics">Annnddd we're live! PetBattle is finally in production, we can crack open the Champagne and toast our success</em>." But now what? How do we know that the site is doing what we expect it to do and, more importantly, how will we know when it isn't performing as we intended? Do we just sit around waiting for customers to complain that the site is down or that errors are happening? Not exactly a good user experience model—or a good use of our time.</p>&#13;
			<p>In this chapter, we will discuss the tools and techniques that can be utilized to monitor the site and notify us when things start to go wrong so we can react before the entire site goes down. We will also discuss advanced techniques, such<a id="_idIndexMarker2856"/><a id="_idIndexMarker2857"/> as Operators, that can help you automate a lot of the day-to-day operations.</p>&#13;
			<h2 id="_idParaDest-405"><a id="_idTextAnchor623"/><a id="_idTextAnchor624"/><a id="_idTextAnchor625"/>Observability</h2>&#13;
			<p>Observability<span id="footnote-181-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-181">1</a></span> is the process <a id="_idIndexMarker2858"/><a id="_idIndexMarker2859"/>of instrumenting software components to assist with extracting data. This data can then be used to determine how well a system is functioning and subsequently be used to notify administrators in the event of issues.</p>&#13;
			<div id="footnote-181" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-181-backlink">1</a>	<a href="https://en.wikipedia.org/wiki/Observability">https://en.wikipedia.org/wiki/Observability</a></p>&#13;
			</div>&#13;
			<p>When it comes to observing the <a id="_idIndexMarker2860"/><a id="_idIndexMarker2861"/>state of our PetBattle applications, there are a number of aspects to consider:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>How do we know if an application instance is initialized and ready to process traffic?</li>&#13;
				<li>How do we know if the application has failed without crashing, such as by becoming deadlocked or blocked?</li>&#13;
				<li>How do we access the application logs?</li>&#13;
				<li>How do we access the application metrics?</li>&#13;
				<li>How do we know what version of the application is running?</li>&#13;
			</ul>&#13;
			<p>Let's start by exploring some application health checks.</p>&#13;
			<h3 id="_idParaDest-406"><a id="_idTextAnchor626"/><a id="_idTextAnchor627"/>Probes</h3>&#13;
			<p><em class="italics">Hello, hello?... Is this thing on?</em> In Kubernetes, the health of an application is determined by a set of<a id="_idIndexMarker2862"/><a id="_idIndexMarker2863"/> software <em class="italics">probes</em> that are <a id="_idIndexMarker2864"/>periodically invoked by the kubelet. A <a id="_idIndexMarker2865"/><a id="_idIndexMarker2866"/>probe is basically an action invoked by the platform on each Pod that either returns a success value or a failure value.</p>&#13;
			<p>Probes can be configured to <a id="_idIndexMarker2867"/>perform one of the following types of actions:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>Connect to a specific TCP port that the container is listening on. If the port is open, the probe is considered successful.</li>&#13;
				<li>Invoke an HTTP endpoint, if the HTTP response code is 200 or greater but less than 400.</li>&#13;
				<li>Shell into a container and execute a command—this may involve checking for a specific file in the directory. This enables probes to be placed on applications that don't natively provide health checks out of the box. If the command exits with a status code of 0, then the probe is successful.</li>&#13;
			</ul>&#13;
			<p>If a probe fails a configured number of times, the kubelet managing the Pod will take a pre-determined action, for example, by removing the Pod from the service or restarting the Pod.</p>&#13;
			<p>Kubernetes currently supports three different kinds of probes:</p>&#13;
			<ol>&#13;
				<li><strong class="bold">Readiness</strong>: This decides whether the <a id="_idIndexMarker2868"/><a id="_idIndexMarker2869"/>Pod is ready to process incoming requests. If the application needs some time to start up, this probe ensures that no traffic is sent to the Pod until this probe passes. Also, if the probe fails while it's running, the platform stops sending any traffic to the Pod until the probe once again succeeds. Readiness probes are key to ensuring a zero-downtime experience for the user when scaling up or upgrading Pods.</li>&#13;
				<li><strong class="bold">Liveness</strong>: This checks to <a id="_idIndexMarker2870"/><a id="_idIndexMarker2871"/>see whether a Pod has a process deadlock or it's crashed without exiting; if so, the platform will kill the Pod.</li>&#13;
				<li><strong class="bold">Startup</strong>: This is used to prevent the platform from killing a Pod that is initializing but is slow in starting up. When the startup probe is configured, the readiness and liveness probes are disabled <a id="_idIndexMarker2872"/><a id="_idIndexMarker2873"/>until the startup probe passes. If the startup probe never passes, the Pod is eventually killed and restarted.</li>&#13;
			</ol>&#13;
			<p>Most of the time, you will probably only utilize the readiness and liveness probes, unless you have a container that's very slow in starting up.</p>&#13;
			<p>In the PetBattle Tournament Service component, the liveness and<a id="_idIndexMarker2874"/><a id="_idIndexMarker2875"/> readiness probes are configured as follows.</p>&#13;
			<p>In <strong class="inline">DeploymentConfig</strong> (or the <a id="_idIndexMarker2876"/>Deployment), the <strong class="inline">/health/live</strong> and <strong class="inline">/health/ready</strong> URLs are automatically created by the Quarkus framework:</p>&#13;
			<p class="snippet">        ...</p>&#13;
			<p class="snippet"><span class="P-_-Code-Highlight">        livenessProbe</span>:</p>&#13;
			<p class="snippet">          failureThreshold: 3</p>&#13;
			<p class="snippet">          httpGet:</p>&#13;
			<p class="snippet">            path: <span class="P-_-Code-Highlight">/health/live</span></p>&#13;
			<p class="snippet">            port: 8080</p>&#13;
			<p class="snippet">            scheme: HTTP</p>&#13;
			<p class="snippet">          initialDelaySeconds: 0</p>&#13;
			<p class="snippet">          periodSeconds: 30</p>&#13;
			<p class="snippet">          successThreshold: 1</p>&#13;
			<p class="snippet">          timeoutSeconds: 10</p>&#13;
			<p class="snippet">        <span class="P-_-Code-Highlight">readinessProbe</span>:</p>&#13;
			<p class="snippet">          failureThreshold: 3</p>&#13;
			<p class="snippet">          httpGet:</p>&#13;
			<p class="snippet">            path: <span class="P-_-Code-Highlight">/health/ready</span></p>&#13;
			<p class="snippet">            port: 8080</p>&#13;
			<p class="snippet">            scheme: HTTP</p>&#13;
			<p class="snippet">          initialDelaySeconds: 0</p>&#13;
			<p class="snippet">          periodSeconds: 30</p>&#13;
			<p class="snippet">          successThreshold: 1</p>&#13;
			<p class="snippet">          timeoutSeconds: 10</p>&#13;
			<p>Different probes can invoke the same action, but we consider this bad practice. The semantics of a readiness probe are different from those <a id="_idIndexMarker2877"/><a id="_idIndexMarker2878"/>of a liveness probe. It's recommended that liveness and readiness probes invoke different endpoints or actions on the container.</p>&#13;
			<p>For example, a readiness probe can invoke an action that verifies whether an application can accept requests. If during the Pod's lifetime the readiness probe fails, Kubernetes will stop sending requests to the Pod until the probe is successful again.</p>&#13;
			<p>A liveness probe is <a id="_idIndexMarker2879"/><a id="_idIndexMarker2880"/>one that verifies whether an application can process a request successfully; for example, if an application were blocked or accepting a request but waiting a long time<a id="_idIndexMarker2881"/><a id="_idIndexMarker2882"/> for a database connection to become available, the probe would fail and Kubernetes would restart the Pod. Think of liveness probes as the Kubernetes equivalent of the IT Crowd<span id="footnote-180-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-180">2</a></span> way of working.</p>&#13;
			<h3 id="_idParaDest-407"><a id="_idTextAnchor628"/><a id="_idTextAnchor629"/>Domino Effect</h3>&#13;
			<p>One question that we get asked a lot is, should a health check reflect the state of the application's downstream <a id="_idIndexMarker2883"/><a id="_idIndexMarker2884"/>dependencies as well as the application itself? The absolute, definitive answer is <em class="italics">it depends</em>. Most of the time, a health check should only focus on the application, but there are always scenarios where this isn't the case.</p>&#13;
			<p>If your health check functionality does a deep check of downstream systems, this can be expensive and result in cascading failures, where a downstream system has an issue and an upstream Pod is restarted due to this downstream issue. Some legacy downstream systems may not have health checks, and a more appropriate approach in this scenario is to add resilience and fault tolerance to your application and architecture.</p>&#13;
			<h3 id="_idParaDest-408"><a id="_idTextAnchor630"/><a id="_idTextAnchor631"/>Fault Tolerance</h3>&#13;
			<p>A key aspect of this is to utilize a <strong class="bold">circuit breaker</strong> pattern when invoking dependencies. Circuit breakers can <a id="_idIndexMarker2885"/><a id="_idIndexMarker2886"/><em class="italics">short circuit</em> the invocation of <a id="_idIndexMarker2887"/><a id="_idIndexMarker2888"/>downstream systems when they detect that previous calls have failed. This can give the downstream system time to recover or restart without having to process incoming traffic.</p>&#13;
			<p>The basic premise of a<a id="_idIndexMarker2889"/><a id="_idIndexMarker2890"/> circuit breaker is that in the case of the failure of a downstream system, the upstream system should just assume that the next request will fail and not send it. It potentially also takes appropriate actions for recovery by, say, returning a default value.</p>&#13;
			<p>After a given period of time, known as the backoff period, the upstream system should try sending a request to the downstream system, and if that succeeds, it reverts to normal processing. The rationale behind the backoff period is to avoid the situation where the upstream systems overwhelm the downstream systems with requests as soon as it starts up.</p>&#13;
			<div id="footnote-180" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-180-backlink">2</a>	<a href="https://www.quotes.net/mquote/901983">https://www.quotes.net/mquote/901983</a></p>&#13;
			</div>&#13;
			<p>Circuit breaker functionality <a id="_idIndexMarker2891"/><a id="_idIndexMarker2892"/>can be performed at an individual level within an application's code: multiple frameworks such as Quarkus, Netflix Hystrix, and Apache Camel support circuit breakers and other fault-tolerant components. Check out the Quarkus fault tolerance plugin for more details.<span id="footnote-179-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-179">3</a></span></p>&#13;
			<p>Platform-level circuit <a id="_idIndexMarker2893"/><a id="_idIndexMarker2894"/>breaker functionality is provided by the <strong class="bold">service mesh</strong> component within OpenShift. This has various substantial advantages over application-level circuit breakers:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>It can be used on any container communicating via HTTP/HTTPS. A sidecar proxy is used to inject the circuit breaker functionality without<a id="_idIndexMarker2895"/><a id="_idIndexMarker2896"/> having to modify the code.</li>&#13;
				<li>It provides dynamic configuration of the circuit breaker functionality.</li>&#13;
				<li>It provides metrics and visibility of the state of circuit breakers throughout the platform.</li>&#13;
				<li>Service mesh also provides other fault-tolerance functionalities, such as timeout and retries.</li>&#13;
			</ul>&#13;
			<h3 id="_idParaDest-409"><a id="_idTextAnchor632"/><a id="_idTextAnchor633"/>Logging</h3>&#13;
			<p><em class="italics">Ahh, logging!</em> No <em class="italics">true</em> developer<span id="footnote-178-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-178">4</a></span> has earned their <a id="_idIndexMarker2897"/><a id="_idIndexMarker2898"/>stripes until they've spent countless hours of their existence trawling through production logs trying to figure out exactly what went wrong when a user clicked "Confirm". If you have managed to do this across multiple log<a id="_idIndexMarker2899"/><a id="_idIndexMarker2900"/> files, all hosted on separate systems via multiple terminal windows, then you are truly righteous in the eyes of the IDE-bound masses.</p>&#13;
			<p>The good news is that application logging on Kubernetes is a first-class citizen on the platform—just configure your application to write its logs to STDOUT and the platform will pick it up and you can view/trawl through them. OpenShift goes one level deeper by shipping an aggregated logging stack with EFK (Elasticsearch, Fluentd, and Kibana) out of the box. This allows developers to search and view logs across multiple containers running on multiple nodes across the cluster. If you want to give this a try, follow the documentation at <a href="https://docs.openshift.com/container-platform/4.7/logging/cluster-logging-deploying.html">https://docs.openshift.com/container-platform/4.7/logging/cluster-logging-deploying.html</a>. </p>&#13;
			<div id="footnote-179" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-179-backlink">3</a>	<a href="https://quarkus.io/guides/smallrye-fault-tolerance">https://quarkus.io/guides/smallrye-fault-tolerance</a></p>&#13;
			</div>&#13;
			<div id="footnote-178" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-178-backlink">4</a>	<a href="https://en.wikipedia.org/wiki/No_true_Scotsman">https://en.wikipedia.org/wiki/No_true_Scotsman</a></p>&#13;
			</div>&#13;
			<h3 id="_idParaDest-410"><a id="_idTextAnchor634"/><a id="_idTextAnchor635"/>Tracing</h3>&#13;
			<p>So, first things first: no, tracing is not application logging running at the trace log level. When it comes to OpenShift, tracing is the <a id="_idIndexMarker2901"/><a id="_idIndexMarker2902"/>functionality added to the Kubernetes platform that enables developers to trace a request across a distributed set of application components running in different containers on different nodes of a cluster. Tracing is an exceptionally useful tool used to determine and visualize inter-service/component dependencies and performance/latency blackholes throughout a distributed system.</p>&#13;
			<p>Tracing is provided as part of the OpenShift service mesh component. The underlying tracing functionality is <a id="_idIndexMarker2903"/><a id="_idIndexMarker2904"/>provided by the Jaeger<span id="footnote-177-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-177">5</a></span> distributed tracing platform. To support tracing, applications must include a client library that sends instrumented request metadata to a Jaeger collector, which in turn processes it and stores the data. This data can then be queried to help visualize the end-to-end request workflow. The Jaeger client libraries are language-specific <a id="_idIndexMarker2905"/><a id="_idIndexMarker2906"/>and utilize the vendor-neutral OpenTracing specification.</p>&#13;
			<p>If you're thinking, "<em class="italics">Woah!</em> <em class="italics">Collecting metadata for every request would be very expensive to store and process</em>," you'd be right. Jaeger <em class="italics">can</em> do <a id="_idIndexMarker2907"/><a id="_idIndexMarker2908"/>this, but for scale purposes it's better to record and process a <em class="italics">sample</em> of requests, rather than each and every one of them.</p>&#13;
			<h3 id="_idParaDest-411"><a id="_idTextAnchor636"/><a id="_idTextAnchor637"/>Metrics</h3>&#13;
			<p>Probes are useful for<a id="_idIndexMarker2909"/><a id="_idIndexMarker2910"/> telling when an application is ready to accept traffic, or whether it is stuck. Tracing is great at providing a measure of latency throughout a distributed system, while logging is a <a id="_idIndexMarker2911"/><a id="_idIndexMarker2912"/>great tool to retrospectively understand exactly what occurred and when it occurred. </p>&#13;
			<p>However, to comprehend the deep state (no, not <em class="italics">that</em> deep state!) of a system and potentially <a id="_idIndexMarker2913"/><a id="_idIndexMarker2914"/>predict its future state after a period of time, you need to measure some of the key quantitative characteristics of the system and visualize/compare them over a period of time. </p>&#13;
			<p>The good news is that metrics are relatively easy to obtain; you can get them from infrastructure components and software components such as JVM, and you can also add domain-specific/custom metrics to your application.</p>&#13;
			<div id="footnote-177" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-177-backlink">5</a>	<a href="https://www.jaegertracing.io/">https://www.jaegertracing.io/</a></p>&#13;
			</div>&#13;
			<p>Given the multitude of metrics available, the hard bit is figuring out which metrics are valuable to your role and need to be retained; for example, for application operator connection pool counts, JVM garbage collection pause times are invaluable. For a Kubernetes platform operator, JVM garbage collection pause times are less critical, but metrics from platform components, such as etcd-related metrics, are crucial.</p>&#13;
			<p>The good news is that <a id="_idIndexMarker2915"/><a id="_idIndexMarker2916"/>OpenShift provides metrics for both the cluster and the applications running on it. In this section, we're going to focus on the application-level perspective. In the Kubernetes community, the <em class="italics">de facto</em> approach is to use Prometheus<span id="footnote-176-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-176">6</a></span> for gathering and Grafana<span id="footnote-175-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-175">7</a></span> for the<a id="_idIndexMarker2917"/><a id="_idIndexMarker2918"/> visualization of metrics. This doesn't mean that you can't use other metrics solutions, and there are some very good ones out there with additional features.</p>&#13;
			<p>OpenShift ships with both Prometheus and Grafana as the default metrics stack. Additionally, it also ships with the Prometheus Alertmanager. The Alertmanager facilitates the sending of notifications to operators when metric values indicate that something is going or has gone wrong <a id="_idIndexMarker2919"/><a id="_idIndexMarker2920"/>and <em class="italics">la merde</em> has or is about to hit the fan. Examples of this include a high number of threads or large JVM garbage collection pause times.</p>&#13;
			<p>Great, so how do we enable this for PetBattle? It is relatively straightforward:</p>&#13;
			<ol>&#13;
				<li value="1">Use a metrics framework in your application that records metrics and exposes the metrics to Prometheus.</li>&#13;
				<li>Configure Prometheus to retrieve the metrics from the application.</li>&#13;
				<li>Visualize the metrics in OpenShift.</li>&#13;
			</ol>&#13;
			<p>Once the metrics are being retrieved, the final step is to configure an alert using the Prometheus Alertmanager.</p>&#13;
			<h3><a id="_idTextAnchor638"/><a id="_idTextAnchor639"/>Gather Metrics in the Application</h3>&#13;
			<p>Taking the PetBattle Tournament <a id="_idIndexMarker2921"/><a id="_idIndexMarker2922"/>service component as an example, this is developed using the Quarkus Java framework. Out of the box, Quarkus supports/recommends the use of the open-source <a id="_idIndexMarker2923"/><a id="_idIndexMarker2924"/>Micrometer metrics framework.<span id="footnote-174-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-174">8</a></span></p>&#13;
			<div id="footnote-176" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-176-backlink">6</a>	<a href="https://prometheus.io/">https://prometheus.io/</a></p>&#13;
				<div id="footnote-175" class="_idFootnote" epub:type="footnote">&#13;
					<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-175-backlink">7</a>	<a href="https://grafana.com/oss/">https://grafana.com/oss/</a></p>&#13;
				</div>&#13;
			</div>&#13;
			<div id="footnote-174" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-174-backlink">8</a>	<a href="https://micrometer.io/">https://micrometer.io/</a></p>&#13;
			</div>&#13;
			<p>To add this to the Tournament service, we simply need to add the dependency to the Maven POM along with the Prometheus dependency. For example:</p>&#13;
			<p class="snippet">&lt;dependency&gt;</p>&#13;
			<p class="snippet">   &lt;groupId&gt;io.quarkus&lt;/groupId&gt;</p>&#13;
			<p class="snippet">   &lt;artifactId&gt;quarkus-micrometer&lt;/artifactId&gt;</p>&#13;
			<p class="snippet">&lt;/dependency&gt;</p>&#13;
			<p class="snippet">&lt;dependency&gt;</p>&#13;
			<p class="snippet">   &lt;groupId&gt;io.micrometer&lt;/groupId&gt;</p>&#13;
			<p class="snippet">   &lt;artifactId&gt;micrometer-registry-prometheus&lt;/artifactId&gt;</p>&#13;
			<p class="snippet">&lt;/dependency&gt;</p>&#13;
			<p>Then, we configure a Prometheus<a id="_idIndexMarker2925"/><a id="_idIndexMarker2926"/> registry, which is used to store the metrics locally in the application before being retrieved by the Prometheus collector. This is done in the <strong class="inline">src/main/resources/application.properties</strong> file.</p>&#13;
			<p class="snippet"># Metrics</p>&#13;
			<p class="snippet">quarkus.micrometer.enabled=true</p>&#13;
			<p class="snippet">quarkus.micrometer.registry-enabled-default=true</p>&#13;
			<p class="snippet">quarkus.micrometer.binder-enabled-default=true</p>&#13;
			<p class="snippet">quarkus.micrometer.binder.jvm=true</p>&#13;
			<p class="snippet">quarkus.micrometer.binder.system=true</p>&#13;
			<p class="snippet">quarkus.micrometer.export.prometheus.path=/metrics</p>&#13;
			<p>With this configuration, the Prometheus endpoint is exposed by the application Pod. Let's go ahead and test it:</p>&#13;
			<p class="snippet"># grab the pod name for the running tournament service</p>&#13;
			<p class="snippet">$ oc get pod -n petbattle | grep tournament </p>&#13;
			<p class="snippet">$ oc exec YOUR_TOURNAMENT_PODNAME -- curl localhost:8080/metrics</p>&#13;
			<p class="snippet">...</p>&#13;
			<p class="snippet"># HELP mongodb_driver_pool_size the current size of the connection pool, including idle and and in-use members</p>&#13;
			<p class="snippet"># TYPE mongodb_driver_pool_size gauge</p>&#13;
			<p class="snippet">mongodb_driver_pool_size{cluster_id="5fce8815a685d63c216022d5",server_address="my-mongodb:27017",} 0.0</p>&#13;
			<p class="snippet"># TYPE http_server_requests_seconds summary</p>&#13;
			<p class="snippet">http_server_requests_seconds_count{method="GET",outcome="SUCCESS",status="200",uri="/openapi",} 1.0</p>&#13;
			<p class="snippet">http_server_requests_seconds_sum{method="GET",outcome="SUCCESS",status="200",uri="/openapi",} 0.176731581</p>&#13;
			<p class="snippet">http_server_requests_seconds_count{method="GET",outcome="CLIENT_ERROR",status="404",uri="NOT_FOUND",} 3.0</p>&#13;
			<p class="snippet">http_server_requests_seconds_sum{method="GET",outcome="CLIENT_ERROR",status="404",uri="NOT_FOUND",} 0.089066563</p>&#13;
			<p class="snippet">http_server_requests_seconds_count{method="GET",outcome="SUCCESS",status="200",uri="/metrics",} 100.0</p>&#13;
			<p class="snippet"># HELP http_server_requests_seconds_max</p>&#13;
			<p class="snippet"># TYPE http_server_requests_seconds_max gauge</p>&#13;
			<p class="snippet">http_server_requests_seconds_max{method="GET",outcome="SUCCESS",status="200",uri="/openapi",} 0.176731581</p>&#13;
			<p class="snippet">http_server_requests_seconds_max{method="GET",outcome="CLIENT_ERROR",status="404",uri="NOT_FOUND",} 0.0 ...</p>&#13;
			<p>If successful, you should get an output similar to the above. Notice that you're not just getting the application-level metrics—the MongoDB connection pool metrics are also there. These are automatically added<a id="_idIndexMarker2927"/><a id="_idIndexMarker2928"/> by the Quarkus framework once configured in the <strong class="inline">application.properties</strong> file.</p>&#13;
			<h3 id="_idParaDest-412"><a id="_idTextAnchor640"/><a id="_idTextAnchor641"/>Configuring Prometheus To Retrieve Metrics From the Application</h3>&#13;
			<p>Prometheus is somewhat unusual in its <a id="_idIndexMarker2929"/>mode of operation. Rather than having some sort of agent pushing metrics to a central collector, it uses a pull model where the collector retrieves/scrapes metrics from a known HTTP/HTTPS endpoint exposed by the applications. In our case, as seen above, we're exposing metrics using the <strong class="inline">/metrics</strong> endpoint.</p>&#13;
			<p>So how does the Prometheus collector know when, where, and how to gather these metrics? OpenShift uses a Prometheus operator<span id="footnote-173-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-173">9</a></span> that simplifies configuring Prometheus to gather metrics. We just need to<a id="_idIndexMarker2930"/><a id="_idIndexMarker2931"/> deploy a <strong class="inline">ServiceMonitor </strong>object to instruct Prometheus on how to gather <a id="_idIndexMarker2932"/><a id="_idIndexMarker2933"/>our application metrics.</p>&#13;
			<p class="snippet">apiVersion: monitoring.coreos.com/v1</p>&#13;
			<p class="snippet">kind: ServiceMonitor</p>&#13;
			<p class="snippet">metadata:</p>&#13;
			<p class="snippet"> labels:</p>&#13;
			<p class="snippet">   app.kubernetes.io/component: pet-battle-tournament</p>&#13;
			<p class="snippet">   k8s-app: pet-battle-tournament</p>&#13;
			<p class="snippet"> name: pet-battle-tournament-monitor</p>&#13;
			<p class="snippet">spec:</p>&#13;
			<p class="snippet"> endpoints:</p>&#13;
			<p class="snippet"> - interval: 30s</p>&#13;
			<p class="snippet">   port:<span class="P-_-Code-Highlight"> tcp-8080</span></p>&#13;
			<p class="snippet">   scheme: http</p>&#13;
			<p class="snippet"> selector:</p>&#13;
			<p class="snippet">   matchLabels:</p>&#13;
			<p class="snippet">     app.kubernetes.io/component: pet-battle-tournament</p>&#13;
			<div id="footnote-173" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-173-backlink">9</a>	<a href="https://github.com/prometheus-operator/prometheus-operator">https://github.com/prometheus-operator/prometheus-operator</a></p>&#13;
			</div>&#13;
			<p>There are a few things to note that might save you some time when trying to understand this configuration: basically, this configuration will scrape associated components every 30 seconds using the default HTTP path <strong class="inline">/metrics</strong>. Now, <strong class="inline">port: tcp-8080</strong> is mapped to the port <a id="_idIndexMarker2934"/><a id="_idIndexMarker2935"/>name in the service—see below, highlighted in bold. If the service had a port name of <strong class="inline">web</strong>, then the configuration would be <strong class="inline">port: web</strong>.</p>&#13;
			<p class="snippet">$ oc describe svc my-pet-battle-tournament</p>&#13;
			<p class="snippet">Name:              my-pet-battle-tournament</p>&#13;
			<p class="snippet">Namespace:         pet-battle-tournament</p>&#13;
			<p class="snippet">Labels:            app.kubernetes.io/component=pet-battle-tournament</p>&#13;
			<p class="snippet">                   app.kubernetes.io/instance=my</p>&#13;
			<p class="snippet">                   app.kubernetes.io/managed-by=Helm</p>&#13;
			<p class="snippet">                   app.kubernetes.io/name=pet-battle-tournament</p>&#13;
			<p class="snippet">                   app.kubernetes.io/version=1.0.0</p>&#13;
			<p class="snippet">                   deploymentconfig=my-pet-battle-tournament</p>&#13;
			<p class="snippet">                   helm.sh/chart=pet-battle-tournament-1.0.0</p>&#13;
			<p class="snippet">Annotations:       Selector:  app.kubernetes.io/component=pet-battle-tournament,app.kubernetes.io/instance=my,app.kubernetes.io/name=pet-battle-tournament,deploymentconfig=my-pet-battle-tournament</p>&#13;
			<p class="snippet">Type:              ClusterIP</p>&#13;
			<p class="snippet">IP:                172.30.228.67</p>&#13;
			<p class="snippet">Port:<span class="P-_-Code-Highlight">              tcp-8080</span>  8080/TCP</p>&#13;
			<p class="snippet">TargetPort:        8080/TCP</p>&#13;
			<p class="snippet">Endpoints:         10.131.0.28:8080</p>&#13;
			<p class="snippet">Port:              tcp-8443  8443/TCP</p>&#13;
			<p class="snippet">TargetPort:        8443/TCP</p>&#13;
			<p class="snippet">Endpoints:         10.131.0.28:8443</p>&#13;
			<p class="snippet">Session Affinity:  None</p>&#13;
			<p class="snippet">Events:            &lt;none&gt;</p>&#13;
			<p>User workload monitoring needs to be enabled at the cluster level before ServiceMonitoring will work.<span id="footnote-172-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-172">10</a></span> This is also a classic demonstration of two of the major, powerful, misunderstood, and unused features of <a id="_idIndexMarker2936"/><a id="_idIndexMarker2937"/>Kubernetes, <em class="italics">labels</em> and<em class="italics"> label selectors</em>.</p>&#13;
			<div id="footnote-172" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-172-backlink">10</a>	<a href="https://docs.openshift.com/container-platform/4.7/monitoring/enabling-monitoring-for-user-defined-projects.html">https://docs.openshift.com/container-platform/4.7/monitoring/enabling-monitoring-for-user-defined-projects.html</a></p>&#13;
			</div>&#13;
			<p>The following line means that Prometheus will attempt to retrieve metrics from all components that have the label <strong class="inline">app.kubernetes.io/component: pet-battle-tournament</strong>. We don't need to list each component independently; we just need to ensure that the component has the correct <em class="italics">label</em>, and that the <em class="italics">selector</em> is used to match that label. If we add a new component to the architecture, then all we have to do is ensure that it has the correct label. Of course, all of this assumes that the method of scraping the metrics is consistent across all of the selected components; that they are all using the <strong class="inline">tcp-8080 port</strong>, for example.</p>&#13;
			<p class="snippet">selector:</p>&#13;
			<p class="snippet">   matchLabels:</p>&#13;
			<p class="snippet">     app.kubernetes.io/component: pet-battle-tournament</p>&#13;
			<p>We're big fans of labels and their associated selectors. They're very powerful as a method of grouping components: Pods, Services, and so on. It's one of those hidden gems that you wish you knew of earlier.</p>&#13;
			<h3 id="_idParaDest-413"><a id="_idTextAnchor642"/><a id="_idTextAnchor643"/>Visualizing the Metrics in OpenShift</h3>&#13;
			<p>Once we have our <a id="_idIndexMarker2938"/>metrics retrieved, we need to interpret the information that they're conveying about the system.</p>&#13;
			<h3 id="_idParaDest-414"><a id="_idTextAnchor644"/><a id="_idTextAnchor645"/>Querying using <a id="_idIndexMarker2939"/>Prometheus</h3>&#13;
			<p>To visualize the metrics, go into the developer console and click on <strong class="bold">Monitoring (1)</strong>, as shown in <em class="italics">Figure 16.1</em>. Then click on <strong class="bold">Custom Query (2)</strong> in the dropdown, and enter a query using the <strong class="bold">Prometheus query language (PromQL) (3)</strong>. In the <a id="_idIndexMarker2940"/><a id="_idIndexMarker2941"/>following example, we've used the <em class="italics">http_server_requests_seconds_count</em> metric, but there are others as well.</p>&#13;
			<div>&#13;
				<div id="_idContainer544" class="IMG---Figure">&#13;
					<img src="../Images/B16297_16_01.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 16.1: PetBattle tournament metrics</p>&#13;
			<p>Let's explore some of the built-in dashboards OpenShift provides for monitoring.</p>&#13;
			<h3 id="_idParaDest-415"><a id="_idTextAnchor646"/>Visualizing Metrics Using Grafana</h3>&#13;
			<p>OpenShift comes with <a id="_idIndexMarker2942"/><a id="_idIndexMarker2943"/>dedicated Grafana dashboards for cluster monitoring. It is not possible to modify these dashboards and add custom application metrics, but it is possible to deploy an application-specific Grafana instance and customize that as we see fit. To do this, we first need to ensure that the Grafana Operator is installed in the namespace that we're using. </p>&#13;
			<p>We will then deploy a custom Grafana setup by deploying the following custom resources:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>A <em class="italics">Grafana resource</em> used to create a custom <em class="italics">grafana</em> instance in the namespace</li>&#13;
				<li>A<em class="italics"> GrafanaDataSource resource</em> to pull metrics from the cluster-wide Prometheus instance</li>&#13;
				<li>A <em class="italics">GrafanaDashboard</em> resource for creating the dashboard</li>&#13;
			</ul>&#13;
			<p>The good news is that all of this is done via Helm charts, so you just have to do the following:</p>&#13;
			<p class="snippet">$ oc get routes</p>&#13;
			<p class="snippet">...                                                                </p>&#13;
			<p class="snippet">grafana-route     grafana-route-pb-noc.apps.someinstance.com     </p>&#13;
			<p>Open <strong class="inline">grafana-route</strong> in a browser, log in, <em class="italics">et voila</em>! It should look something like that shown in <em class="italics">Figure 16.2. </em>If there is an error with no data, check the BEARER_TOKEN is in place. This can be fixed manually by running the commands at <a href="https://github.com/petbattle/pet-battle-infra/blob/main/templates/insert-bearer-token-hook.yaml#L80">https://github.com/petbattle/pet-battle-infra/blob/main/templates/insert-bearer-token-hook.yaml#L80</a></p>&#13;
			<div>&#13;
				<div id="_idContainer545" class="IMG---Figure">&#13;
					<img src="../Images/B16297_16_02.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 16.2: PetBattle metrics in Grafana</p>&#13;
			<p>We will now take a look at some of the tools that can help us further with observability.</p>&#13;
			<h2 id="_idParaDest-416">Me<a id="_idTextAnchor647"/><a id="_idTextAnchor648"/>tadata and Traceability</h2>&#13;
			<p>With the adoption of independently deployable services-based architectures, the complexity of managing these components and their inter-relationships is becoming problematic. In the following sections, we will outline a number of techniques that can assist you with this.</p>&#13;
			<h3 id="_idParaDest-417">La<a id="_idTextAnchor649"/><a id="_idTextAnchor650"/>bels</h3>&#13;
			<p>As mentioned earlier, labels <a id="_idIndexMarker2944"/><a id="_idIndexMarker2945"/>and label selectors are <a id="_idIndexMarker2946"/><a id="_idIndexMarker2947"/>among the more powerful <em class="italics">metadata management</em> features of Kubernetes. At its core, labels are a collection of text-based key/value pairs that can be attached to one or more objects: Pods, services, Deployments, and so on. Labels are intended to add information/semantics to objects that are relevant to the user and not the core Kubernetes system. A label selector is a method by which a user can group items together that have the same labels.</p>&#13;
			<p>One of the most common uses of labels and label selectors in Kubernetes is the way that services use label selectors to group related Pods as endpoints for the service.</p>&#13;
			<p>It's probably better shown by way of an example.</p>&#13;
			<p>So, let's start with our three Infinispan Pods. Given<a id="_idIndexMarker2948"/><a id="_idIndexMarker2949"/> that the Infinispan operator deploys its Pods via StatefulSets, the Pod names are pretty straightforward: <strong class="inline">infinispan-0</strong>, <strong class="inline">infinispan-1</strong>, <strong class="inline">infinispan-2</strong>. Take note of the labels attached to the Pods (highlighted in bold).</p>&#13;
			<p class="snippet">$ oc get pods --show-labels=true</p>&#13;
			<p class="snippet">NAME                                   READY   STATUS      RESTARTS   AGE     LABELS</p>&#13;
			<p class="snippet">infinispan-0                           1/1     Running     0          2m25s   <span class="P-_-Code-Highlight">app=infinispan-pod,clusterName=infinispan,controller-revision-hash=infinispan-66785c8f,infinispan_cr=infinispan,statefulset.kubernetes.io/pod-name=infinispan-0</span></p>&#13;
			<p class="snippet">infinispan-1                           1/1     Running     0          5m51s   <span class="P-_-Code-Highlight">app=infinispan-pod,clusterName=infinispan,controller-revision-hash=infinispan-66785c8f,infinispan_cr=infinispan,statefulset.kubernetes.io/pod-name=infinispan-1</span></p>&#13;
			<p class="snippet">infinispan-2                           1/1     Running     0          4m12s   <span class="P-_-Code-Highlight">app=infinispan-pod,clusterName=infinispan,controller-revision-hash=infinispan-66785c8f,infinispan_cr=infinispan,statefulset.kubernetes.io/pod-name=infinispan-2</span></p>&#13;
			<p>When the Tournament service wants to connect to one of these Infinispan pods, it uses the Infinispan service that is also created and managed by the operator.</p>&#13;
			<p class="snippet"> $ oc get svc</p>&#13;
			<p class="snippet">NAME         TYPE        CLUSTER-IP  EXTERNAL-IP   PORT(S)     AGE</p>&#13;
			<p class="snippet">infinispan   ClusterIP   172.30.154.122   &lt;none&gt;   11222/TCP   5d20h</p>&#13;
			<p>If we go into the definition of the service, we'll see the selector (highlighted in bold):</p>&#13;
			<p class="snippet">$ oc describe service infinispan</p>&#13;
			<p class="snippet">Name: infinispan</p>&#13;
			<p class="snippet">Namespace: pet-battle-tournament</p>&#13;
			<p class="snippet">Labels: app=infinispan-service</p>&#13;
			<p class="snippet">        clusterName=infinispan</p>&#13;
			<p class="snippet">        infinispan_cr=infinispan</p>&#13;
			<p class="snippet">Annotations: service.alpha.openshift.io/serving-cert-signed-by:               openshift-service-serving-signer@1607294893</p>&#13;
			<p class="snippet">             service.beta.openshift.io/serving-cert-secret-name:</p>&#13;
			<p class="snippet">               infinispan-cert-secret</p>&#13;
			<p class="snippet">             service.beta.openshift.io/serving-cert-signed-by:</p>&#13;
			<p class="snippet">               openshift-service-serving-signer@1607294893</p>&#13;
			<p class="snippet"><span class="P-_-Code-Highlight">Selector:          app=infinispan-pod,clusterName=infinispan</span></p>&#13;
			<p class="snippet">Type:              ClusterIP</p>&#13;
			<p class="snippet">IP:                172.30.154.122</p>&#13;
			<p class="snippet">Port:              infinispan  11222/TCP</p>&#13;
			<p class="snippet">TargetPort:        11222/TCP</p>&#13;
			<p class="snippet">Endpoints: 10.128.2.158:11222,10.129.3.145:11222,10.131.0.25:11222</p>&#13;
			<p class="snippet">Session Affinity:  None</p>&#13;
			<p class="snippet">Events:            &lt;none&gt;</p>&#13;
			<p>This adds the Pods with the<a id="_idIndexMarker2950"/><a id="_idIndexMarker2951"/> labels <strong class="inline">app=infinispan-pod,clusterName=infinispan</strong> into the service as endpoints. Two things to note here: the selector didn't use all the labels assigned to the Pod; and if we scaled up the number of Infinispan Pods, the selector would be continuously assessed and the new Pods automatically added to the service. The preceding example is a pretty basic example of a selector; in fact, selectors are far more powerful, with equality- and set-based operations also available. Check out the examples in the Kubernetes documentation for more information.<span id="footnote-171-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-171">11</a></span></p>&#13;
			<div id="footnote-171" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-171-backlink">11</a>	<a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/</a></p>&#13;
			</div>&#13;
			<p>Great, so now what? What information could you use to label a resource? It depends on what your needs are. As demonstrated previously in the monitoring section, labels and selectors can be useful in configuring Prometheus. Labels can also be useful in grouping components together, as in<a id="_idIndexMarker2952"/><a id="_idIndexMarker2953"/> the components that comprise a distributed application. </p>&#13;
			<p>Kubernetes has a <a id="_idIndexMarker2954"/><a id="_idIndexMarker2955"/>set of recommended labels<span id="footnote-170-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-170">12</a></span> that we've used when building and deploying the PetBattle application:</p>&#13;
			<div>&#13;
				<div id="_idContainer1009" class="IMG---Figure">&#13;
					<img src="../Images/B16297_Table_16.1.png" alt="" style="height:300px; width:600px;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Table 16.1: Kubernetes-recommended labels</p>&#13;
			<div id="footnote-170" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-170-backlink">12</a>	<a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/">https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/</a></p>&#13;
			</div>&#13;
			<p>With these labels in place, it is possible to retrieve and view the components of the application using selectors, such as to show the component parts of the PetBattle application without the <a id="_idIndexMarker2956"/><a id="_idIndexMarker2957"/>supporting application infrastructure, that is, Infinispan or Keycloak. The following command demonstrates this:</p>&#13;
			<p class="snippet">$ oc get all -l app.kubernetes.io/part-of=petbattleworld \</p>&#13;
			<p class="snippet">    --server-print=false</p>&#13;
			<p class="snippet">NAME                                                            AGE</p>&#13;
			<p class="snippet">replicationcontroller/dabook-mongodb-1                          2d18h</p>&#13;
			<p class="snippet">replicationcontroller/dabook-pet-battle-tournament-1            26m</p>&#13;
			<p class="snippet">NAME                                                            AGE</p>&#13;
			<p class="snippet">service/dabook-mongodb                                          2d18h</p>&#13;
			<p class="snippet">service/dabook-pet-battle-tournament                            2d18h</p>&#13;
			<p class="snippet">NAME                                                            AGE</p>&#13;
			<p class="snippet">deploymentconfig.apps.openshift.io/dabook-mongodb                2d18h</p>&#13;
			<p class="snippet">deploymentconfig.apps.openshift.io/dabook-pet-battle-tournament  26m</p>&#13;
			<p class="snippet">&#13;
NAME                                                            AGE</p>&#13;
			<p class="snippet">imagestream.image.openshift.io/dabook-pet-battle-tournament     26m</p>&#13;
			<p class="snippet">&#13;
NAME                                                            AGE</p>&#13;
			<p class="snippet">route.route.openshift.io/dabook-pet-battle-tournament           2d18h</p>&#13;
			<p>Let's look at other mechanisms we can use to enhance traceability.</p>&#13;
			<h3 id="_idParaDest-418"><a id="_idTextAnchor651"/><a id="_idTextAnchor652"/>Software Traceability</h3>&#13;
			<p>One of the issues that we've <a id="_idIndexMarker2958"/><a id="_idIndexMarker2959"/>observed from customers over the years is the reliance that people have on the name of the software artifact that they're putting into production, such as <strong class="inline">super-important-app-1.2.99.0.bin or critical-service-1.2.jar</strong>. While this works 99.9% of the time, occasionally we've noticed issues where an incorrect version has been deployed with interesting outcomes.</p>&#13;
			<p>In the land of containers, your deployment is a versioned artifact that contains a version of your software, and this in turn may be deployed using a versioned Helm chart via a GitOps approach. A good build and deployment pipeline will ensure that these levels of artifact versioning will always be consistent and provide traceability. As a backup, we also add additional traceability to the deployed artifacts as <a id="_idIndexMarker2960"/><a id="_idIndexMarker2961"/>annotations on the resources and build info logging in the application binary.</p>&#13;
			<h3 id="_idParaDest-419"><a id="_idTextAnchor653"/><a id="_idTextAnchor654"/>Annotations</h3>&#13;
			<p>Annotations are<a id="_idIndexMarker2962"/><a id="_idIndexMarker2963"/> similar to <a id="_idIndexMarker2964"/><a id="_idIndexMarker2965"/>Kubernetes labels—that is, string-based key/value pairs—except that they're not used to group or identify objects via selectors. Annotations can be used to <a id="_idIndexMarker2966"/><a id="_idIndexMarker2967"/>store different types of information; in our case, we're going to use annotations to store Git information to help with software traceability.</p>&#13;
			<p class="snippet">apiVersion: v1</p>&#13;
			<p class="snippet">kind: Service</p>&#13;
			<p class="snippet">metadata:</p>&#13;
			<p class="snippet">  annotations:</p>&#13;
			<p class="snippet">    app.openshift.io/vcs-url:      https://github.com/petbattle/tournamentservice.git</p>&#13;
			<p class="snippet">    app.quarkus.io/commit-id:a01a310aadd46911bc4c66b3a063ddb090a3feba</p>&#13;
			<p class="snippet">    app.quarkus.io/vcs-url:      https://github.com/petbattle/tournamentservice.git</p>&#13;
			<p class="snippet">    app.quarkus.io/build-timestamp: 2020-12-23 - 16:43:07 +0000</p>&#13;
			<p class="snippet"><span class="P-_-Code-Highlight">    prometheus.io/scrape: "true"</span></p>&#13;
			<p class="snippet">    prometheus.io/path: /metrics</p>&#13;
			<p class="snippet">    prometheus.io/port: "8080"</p>&#13;
			<p>The annotations are<a id="_idIndexMarker2968"/><a id="_idIndexMarker2969"/> automatically added as part of the Maven build process using the Quarkus Maven plugin. Also notice the annotations are used to provide scrape information for Prometheus, as can be seen highlighted in the preceding code.</p>&#13;
			<h3 id="_idParaDest-420"><a id="_idTextAnchor655"/><a id="_idTextAnchor656"/>Build Information</h3>&#13;
			<p>An approach that has nothing to <a id="_idIndexMarker2970"/><a id="_idIndexMarker2971"/>do with Kubernetes per se, but we strongly recommend to be used in general, is to output source control and build information as part of the application startup. An example of this is embedded into the Tournament service.</p>&#13;
			<p class="snippet">$ java -jar tournament-1.0.0-SNAPSHOT-runner.jar</p>&#13;
			<p class="snippet">GITINFO -&gt; git.tags:</p>&#13;
			<p class="snippet">GITINFO -&gt; git.build.version:1.0.0-SNAPSHOT</p>&#13;
			<p class="snippet">GITINFO -&gt; git.commit.id.full:b5d6bfabeea6251b9c17ea52f0e87e2c8e967efd</p>&#13;
			<p class="snippet">GITINFO -&gt; git.commit.id.abbrev:b5d6bfa</p>&#13;
			<p class="snippet">GITINFO -&gt; git.branch:noc-git-info</p>&#13;
			<p class="snippet">GITINFO -&gt; git.build.time:2020-12-24T13:26:25+0000</p>&#13;
			<p class="snippet">GITINFO -&gt; git.commit.message.full:Moved gitinfo output to Main class</p>&#13;
			<p class="snippet">GITINFO -&gt; git.remote.origin.url:git@github.com:petbattle/tournamentservice.git</p>&#13;
			<p>We use the Maven plugin <strong class="inline">git-commit-id-plugin</strong> to generate a file containing the Git information and package that file as part<a id="_idIndexMarker2972"/><a id="_idIndexMarker2973"/> of the <strong class="bold">Java archive</strong> (<strong class="bold">jar</strong>). On startup, we simply read this file and output its <a id="_idIndexMarker2974"/>contents to the console. Very simple stuff, but very effective and a lifesaver when <a id="_idIndexMarker2975"/><a id="_idIndexMarker2976"/>needed. When running on OpenShift, this information will be picked up by the OpenShift logging components.</p>&#13;
			<h2 id="_idParaDest-421"><a id="_idTextAnchor657"/><a id="_idTextAnchor658"/>Alerting</h2>&#13;
			<p>So we have all the metrics to provide us <a id="_idIndexMarker2977"/><a id="_idIndexMarker2978"/>with some insight into how the system is performing. We've got spectacular graphs and gauges in Grafana but we're hardly going to sit watching them all day to see if something happens. It's time to add alerting to the solution.</p>&#13;
			<h3 id="_idParaDest-422"><a id="_idTextAnchor659"/><a id="_idTextAnchor660"/>What Is an Alert?</h3>&#13;
			<p>An alert is an event that is<a id="_idIndexMarker2979"/><a id="_idIndexMarker2980"/> generated when some measurement threshold (observed or calculated) is about to be or has been breached. The <a id="_idIndexMarker2981"/><a id="_idIndexMarker2982"/>following are some examples of alerts:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>The average system response time in the last five minutes goes above 100 milliseconds.</li>&#13;
				<li>The number of currently active users on the site falls below a certain threshold. </li>&#13;
				<li>Application memory usage is approaching its maximum limit.</li>&#13;
			</ul>&#13;
			<p>Alerts usually result in notifications being sent to human operators, whether that is through an email or instant message, say. Notifications can also be sent to trigger automation scripts/processes to deal with the alert. Service owners can analyze their existing alerts to help improve the reliability of their services and systems and reduce the manual work associated with remediating problems.</p>&#13;
			<h3 id="_idParaDest-423"><a id="_idTextAnchor661"/><a id="_idTextAnchor662"/>Why Alert?</h3>&#13;
			<p>Alerts call for human<a id="_idIndexMarker2983"/><a id="_idIndexMarker2984"/> action when a situation has arisen within the system that cannot be automatically handled. This may include scenarios where automatic resolution of the problem is deemed too risky and human intervention is required to help triage, mitigate, and resolve the issue. Alerting can also be an issue by causing concern for site reliability engineers who manage and operate the system, particularly when alerts are numerous, misleading, or don't really help in problem cause analysis. They may generate benign alerts that don't prompt any action.</p>&#13;
			<p>There are certain qualities that make up a <em class="italics">good alert</em>. Alerts should be actionable by the human beings who respond to them. To be actionable, the alert must also have arrived in time for something to be done about it and it should be delivered to the correct team or location for triaging. Alerts <a id="_idIndexMarker2985"/><a id="_idIndexMarker2986"/>can also include helpful metadata such as documentation links to assist in making triage faster.</p>&#13;
			<h3 id="_idParaDest-424"><a id="_idTextAnchor663"/><a id="_idTextAnchor664"/>Alert Types</h3>&#13;
			<p>We can think of alerts as falling into three broad categories.<span id="footnote-169-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-169">13</a></span> The first are <strong class="bold">proactive </strong>alerts, meaning that your business service or system is not in danger yet but may be in trouble after some period of time. A good example of this is where your system response time is degrading but it is not at a stage where external users would be aware of the issue yet. Another example may be where your disk quota is filling up but is not 100% full yet, but it may<a id="_idIndexMarker2987"/><a id="_idIndexMarker2988"/> do in a few days' time.</p>&#13;
			<p>A <strong class="bold">reactive </strong>alert means your business service or system is in immediate danger. You are about to breach a service level <a id="_idIndexMarker2989"/><a id="_idIndexMarker2990"/>and immediate action is needed to prevent the breach.</p>&#13;
			<p>An <strong class="bold">investigative</strong> alert is one where <a id="_idIndexMarker2991"/><a id="_idIndexMarker2992"/>your business service or system is in an unknown state. For example, it may be suffering a form of partial failure or there may be unusual errors being generated. Another example may be where an application is restarting too many times, which is indicative of an unusual crash situation.</p>&#13;
			<p>Each of these alerts may also be directed to different teams, depending on their severity. Not all alerts need to be managed with the same level of urgency. For example, some alerts must be handled by an on-call human resource immediately, while for others it may be fine to handle them during business hours by an application business support team the following day. Let's explore how we can easily configure and add alerting to our applications using the OpenShift platform features to help us out.</p>&#13;
			<div id="footnote-169" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-169-backlink">13</a>	<a href="https://www.oreilly.com/content/reduce-toil-through-better-alerting/">https://www.oreilly.com/content/reduce-toil-through-better-alerting/</a></p>&#13;
			</div>&#13;
			<h3 id="_idParaDest-425"><a id="_idTextAnchor665"/><a id="_idTextAnchor666"/>Managing Alerts</h3>&#13;
			<p>OpenShift has platform <a id="_idIndexMarker2993"/><a id="_idIndexMarker2994"/>monitoring and alerting that supports both built-in platform components and user workloads. The product documentation is the best place to start when looking to configure these.<span id="footnote-168-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-168">14</a></span> As we outlined earlier, monitoring and alerting make use of the Prometheus monitoring stack. This is combined with an open-source tool called Thanos<span id="footnote-167-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-167">15</a></span> that aggregates and provides access to multiple instances of Prometheus in our cluster. </p>&#13;
			<p>A basic configuration for the PetBattle application suite consists of creating two ConfigMaps for user <a id="_idIndexMarker2995"/><a id="_idIndexMarker2996"/>workload monitoring and alerting. We use ArgoCD and a simple kustomize YAML configuration to apply these ConfigMaps using GitOps. If we open up the ubiquitous journey <strong class="inline">values-day2ops.yaml</strong> file, we can create an entry for user workload monitoring.</p>&#13;
			<p class="snippet">  # User Workload Monitoring</p>&#13;
			<p class="snippet">  - name: user-workload-monitoring</p>&#13;
			<p class="snippet">    enabled: true</p>&#13;
			<p class="snippet">    destination: openshift-monitoring</p>&#13;
			<p class="snippet">    source: https://github.com/rht-labs/refactored-adventure.git</p>&#13;
			<p class="snippet">    source_path: user-workload-monitoring/base</p>&#13;
			<p class="snippet">    source_ref: master</p>&#13;
			<p class="snippet">    sync_policy: *sync_policy_true</p>&#13;
			<p class="snippet">    no_helm: true</p>&#13;
			<p>The next step is to make <a id="_idIndexMarker2997"/><a id="_idIndexMarker2998"/>use of application metrics and<a id="_idIndexMarker2999"/> a ServiceMonitor and configure specific Prometheus alerts for our PetBattle suite.</p>&#13;
			<h3 id="_idParaDest-426"><a id="_idTextAnchor667"/><a id="_idTextAnchor668"/>User-Defined Alerts</h3>&#13;
			<p>In the <em class="italics">Metrics </em>section, we created <a id="_idIndexMarker3000"/><a id="_idIndexMarker3001"/>ServiceMonitors for our API and Tournament applications that allow us to collect the micrometer metrics <a id="_idIndexMarker3002"/><a id="_idIndexMarker3003"/>from our Quarkus applications. We <a id="_idIndexMarker3004"/><a id="_idIndexMarker3005"/>want to use these metrics to configure our alerts. The simplest approach is to browse to the Thanos query endpoint that aggregates all of our Prometheus metrics. You can find this in the <strong class="inline">openshift-monitoring</strong> project.</p>&#13;
			<p class="snippet">$ oc get route thanos-querier -n openshift-monitoring</p>&#13;
			<div id="footnote-168" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-168-backlink">14</a>	<a href="https://docs.openshift.com/container-platform/4.7/monitoring/configuring-the-monitoring-stack.html#configuring-the-monitoring-stack">https://docs.openshift.com/container-platform/4.7/monitoring/configuring-the-monitoring-stack.html#configuring-the-monitoring-stack</a></p>&#13;
				<div id="footnote-167" class="_idFootnote" epub:type="footnote">&#13;
					<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-167-backlink">15</a>	<a href="https://github.com/thanos-io/thanos">https://github.com/thanos-io/thanos</a></p>&#13;
				</div>&#13;
			</div>&#13;
			<p>We want to create a simple reactive alert based on whether the PetBattle API, Tournament, and UI Pods are running in a certain project. We can make use of Kubernetes Pod labels <a id="_idIndexMarker3006"/><a id="_idIndexMarker3007"/>and the Prometheus query language to test whether our Pods are running.</p>&#13;
			<div>&#13;
				<div id="_idContainer547" class="IMG---Figure">&#13;
					<img src="../Images/B16297_16_03.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 16.3: Thanos query interface</p>&#13;
			<p>For this use case, we combine the <strong class="inline">kube_pod_status_ready and kube_pod_labels</strong> query for each Pod and namespace combination and create a PrometheusRule to alert us when a condition is not met. We wrapped the generation of the alerts in a Helm chart so we can easily template the project and alert severity values<span id="footnote-166-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-166">16</a></span> and connect the deployment with our GitOps automation.</p>&#13;
			<div id="footnote-166" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-166-backlink">16</a>	<a href="https://github.com/petbattle/ubiquitous-journey/blob/main/applications/alerting/chart/templates/application-alerts.yaml">https://github.com/petbattle/ubiquitous-journey/blob/main/applications/alerting/chart/templates/application-alerts.yaml</a></p>&#13;
			</div>&#13;
			<p class="snippet">spec:</p>&#13;
			<p class="snippet">  groups:</p>&#13;
			<p class="snippet">  - name: petbattle.rules</p>&#13;
			<p class="snippet">    rules:</p>&#13;
			<p class="snippet">    - alert: PetBattleApiNotAvailable</p>&#13;
			<p class="snippet">      annotations:</p>&#13;
			<p class="snippet">        message: 'Pet Battle Api in namespace {{ .Release.Namespace }} is not available for the last 1 minutes.'</p>&#13;
			<p class="snippet">      expr: (1 - absent(kube_pod_status_ready{condition="true" ... for: 1m</p>&#13;
			<p class="snippet">      labels:</p>&#13;
			<p class="snippet">        severity: {{ .Values.petbattle.rules.severity }}</p>&#13;
			<p>The firing alerts can be <a id="_idIndexMarker3008"/><a id="_idIndexMarker3009"/>seen in the OpenShift web console as seen in <em class="italics">Figure 16.4</em>. In this example, we have configured the <strong class="inline">labs-dev</strong> alerts to only have a severity of <em class="italics">info</em> because they are not deemed as crucial deployments in that environment. The severity may be set as <em class="italics">info, warning</em>, or<em class="italics"> critical</em>, and we use <em class="italics">warning </em>for our <strong class="inline">labs-test</strong> <em class="italics">and</em> <strong class="inline">labs-staging</strong> environments, for example. These are arbitrary but standard severity levels, and we can use them for routing alerts, which we will cover in a moment.</p>&#13;
			<div>&#13;
				<div id="_idContainer548" class="IMG---Figure">&#13;
					<img src="../Images/B16297_16_04.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 16.4: PetBattle alerts firing in OpenShift</p>&#13;
			<p>We can use the same method to create an investigative or proactive alert. This time we wish to measure the HTTP request time for our API application. During testing, we found that if API calls took longer than ~1.5 sec, the user experience in the PetBattle frontend was deemed too slow by end users and there was a chance they would disengage from using the web application altogether.</p>&#13;
			<div>&#13;
				<div id="_idContainer549" class="IMG---Figure">&#13;
					<img src="../Images/B16297_16_05.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 16.5: Maximum request time alert rule</p>&#13;
			<p>In this alert, we use the Prometheus query language and the <strong class="inline">http_server_requests_seconds_max</strong> metric for the PetBattle API application to test whether the maximum request time over the <a id="_idIndexMarker3010"/><a id="_idIndexMarker3011"/>last five-minute period exceeded our 1.5 sec threshold. If this alert starts to fire, possible remediation actions might include manually scaling up the number of API Pods or perhaps increasing the database resources if that is seen to be slow for some reason. In future iterations, we may even try to automate the application scale-up by using a Horizontal Pod Autoscaler, a Kubernetes construct that can scale our applications automatically based on metrics.</p>&#13;
			<p>In this way, we can continue to build on our set of alerting rules for our PetBattle application suite, modifying them as we run the applications in different environments, and learn what conditions to look out for while automating as much of the remediation as we can.</p>&#13;
			<h3 id="_idParaDest-427">Ope<a id="_idTextAnchor669"/><a id="_idTextAnchor670"/>nShift Alertmanager</h3>&#13;
			<p>As we have seen, OpenShift <a id="_idIndexMarker3012"/><a id="_idIndexMarker3013"/>supports three severity levels of alerting: <em class="italics">info</em>,<em class="italics"> warning</em>, and<em class="italics"> critical</em>. We can group and route alerts based on their severity as well as on custom labels—that is, project or application labels. In the OpenShift administrator console,<span id="footnote-165-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-165">17</a></span> you can configure the <a id="_idIndexMarker3014"/><a id="_idIndexMarker3015"/>Alertmanager under <strong class="bold">Cluster Settings</strong>.</p>&#13;
			<div>&#13;
				<div id="_idContainer550" class="IMG---Figure">&#13;
					<img src="../Images/B16297_16_06.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 16.6: Alertmanager routing configuration</p>&#13;
			<p>Aler<a id="_idTextAnchor671"/>ts may be grouped <a id="_idIndexMarker3016"/><a id="_idIndexMarker3017"/>and filtered using labels and then routed to specific receivers, such as PagerDuty, Webhook, Email, or Slack. We can fine-tune the routing rules so that<a id="_idIndexMarker3018"/><a id="_idIndexMarker3019"/> the correct teams receive the alerts in the correct channel, based on their urgency. For example, all <em class="italics">info</em> and <em class="italics">warning</em> severity alerts for the PetBattle UI application may be routed to the <em class="italics">frontend developers</em> Slack channel, whereas all <em class="italics">critical</em> alerts are routed to the on-call PagerDuty endpoint as well as the Slack channel.</p>&#13;
			<div id="footnote-165" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-165-backlink">17</a>	<a href="https://docs.openshift.com/container-platform/4.7/monitoring/managing-alerts.html">https://docs.openshift.com/container-platform/4.7/monitoring/managing-alerts.html</a></p>&#13;
			</div>&#13;
			<p>Aler<a id="_idTextAnchor672"/>ting is a critical component to successfully manage the operational aspects of a system but you should be careful and ensure that the operations team isn't overwhelmed with alerts. Too many alerts or many minor or false-positive alerts can lead to <em class="italics">alert fatigue</em>, where it becomes an established <a id="_idIndexMarker3020"/><a id="_idIndexMarker3021"/>practice within a team to ignore alerts, thus robbing them of their importance to the successful management of the system.</p>&#13;
			<h2 id="_idParaDest-428">Serv<a id="_idTextAnchor673"/><a id="_idTextAnchor674"/><a id="_idTextAnchor675"/>ice Mesh</h2>&#13;
			<p>Service mesh functionality has been one of the largest additions/extensions to Kubernetes in its short history. There's a lot of debate around the additional complexity of using a service mesh and whether all the features are even required.</p>&#13;
			<p>For the purposes of this book, we're going to focus on the service mesh provided out of the box within OpenShift, which is based on the open-source Istio project. There are other implementations, such <a id="_idIndexMarker3022"/><a id="_idIndexMarker3023"/>as Linkerd, SuperGloo, and <a id="_idIndexMarker3024"/><a id="_idIndexMarker3025"/>Traefik, out there <a id="_idIndexMarker3026"/><a id="_idIndexMarker3027"/>that are excellent and offer similar functionality to Istio.</p>&#13;
			<p>The OpenShift service mesh provides the following features out of the box:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li><strong class="bold">Security</strong>: Authentication and authorization, mutual TLS (encryption), policies</li>&#13;
				<li><strong class="bold">Traffic management</strong>: Resiliency <a id="_idIndexMarker3028"/><a id="_idIndexMarker3029"/>features, virtual <a id="_idIndexMarker3030"/><a id="_idIndexMarker3031"/>services, policies, fault injection</li>&#13;
				<li><strong class="bold">Observability</strong>: Service metrics, call tracing, access logs</li>&#13;
			</ul>&#13;
			<h3 id="_idParaDest-429">Why <a id="_idTextAnchor676"/><a id="_idTextAnchor677"/>Service Mesh?</h3>&#13;
			<p>We previously talked <a id="_idIndexMarker3032"/><a id="_idIndexMarker3033"/>about resiliency and how <a id="_idIndexMarker3034"/><a id="_idIndexMarker3035"/>patterns like<a id="_idIndexMarker3036"/><a id="_idIndexMarker3037"/> circuit breakers can help systems recover from downstream failures. A circuit breaker can be added in the scope of application code through frameworks <a id="_idIndexMarker3038"/><a id="_idIndexMarker3039"/>such as <strong class="bold">SmallRye Fault Tolerance</strong> or <strong class="bold">Spring Cloud Circuit Breaker</strong> for Java<a id="_idIndexMarker3040"/><a id="_idIndexMarker3041"/> projects; similar frameworks <a id="_idIndexMarker3042"/><a id="_idIndexMarker3043"/>such as <strong class="bold">Polly</strong><span id="footnote-164-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-164">18</a></span> exist for .NET, <strong class="bold">PyBreaker</strong><span id="footnote-163-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-163">19</a></span> for Python, and <strong class="bold">Opossum</strong><span id="footnote-162-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-162">20</a></span> for Node.js. A key requirement for all of these <a id="_idIndexMarker3045"/><a id="_idIndexMarker3046"/>frameworks is that they have to<a id="_idIndexMarker3047"/><a id="_idIndexMarker3048"/> be added to the existing source code of the application, and the application needs to be rebuilt. When using a service mesh, a circuit breaker is external to the application code and no changes are required at the application level to take advantage of this feature.</p>&#13;
			<div id="footnote-164" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-164-backlink">18</a>	<a href="https://github.com/App-vNext/Polly">https://github.com/App-vNext/Polly</a></p>&#13;
				<div id="footnote-163" class="_idFootnote" epub:type="footnote">&#13;
					<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-163-backlink">19</a>	<a href="https://pypi.org/project/pybreaker/">https://pypi.org/project/<span id="_idIndexMarker3044"/>pybreaker/</a></p>&#13;
					<div id="footnote-162" class="_idFootnote" epub:type="footnote">&#13;
						<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-162-backlink">20</a>	<a href="https://nodeshift.dev/opossum/">https://nodeshift.dev/opossum/</a></p>&#13;
					</div>&#13;
				</div>&#13;
			</div>&#13;
			<p>The same is true with <strong class="bold">Mutual TLS</strong> (<strong class="bold">mTLS</strong>), which is used for encrypting traffic between services. Operators such <a id="_idIndexMarker3049"/><a id="_idIndexMarker3050"/>as CertManager or CertUtil can assist with managing and distributing certificates, but <a id="_idIndexMarker3051"/><a id="_idIndexMarker3052"/>modification of the application code is still required to use the feature. Service meshes simplify this as the inter-component traffic is sent via a <em class="italics">sidecar proxy</em> and functionality such as mTLS is <em class="italics">automagically</em> added to this—once again, without having to change the application code.</p>&#13;
			<p>The Istio component of a <a id="_idIndexMarker3053"/><a id="_idIndexMarker3054"/>service mesh also manages TLS certificate generation and distribution so that it helps <a id="_idIndexMarker3055"/><a id="_idIndexMarker3056"/>reduce the management overhead when using mTLS.</p>&#13;
			<p>So how does a service mesh perform all of this magical functionality? Basically, the service mesh operator adds a service proxy container (based on the Envoy project) to the application Pod and configures the application traffic to be routed through this proxy. The proxy registers with the Istio control plane and configuration settings, certificates, and routing rules are retrieved and the proxy configured. The Istio documentation goes into much more detail.<span id="footnote-161-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-161">21</a></span></p>&#13;
			<h3 id="_idParaDest-430">Asid<a id="_idTextAnchor678"/><a id="_idTextAnchor679"/>e – Sidecar Containers</h3>&#13;
			<p>A common object that <a id="_idIndexMarker3057"/><a id="_idIndexMarker3058"/>people visualize when they hear the word <em class="italics">sidecar</em> is that of a motorbike with a single-wheel passenger car—a <em class="italics">pod</em>—attached to it. Being attached to the bike, the pod goes wherever the bike goes—except in comedy sketches where the bike and sidecar separate and rejoin, but that's another subject entirely.</p>&#13;
			<p>In Kubernetes, a sidecar is a <a id="_idIndexMarker3059"/><a id="_idIndexMarker3060"/>container that runs in the same Kubernetes Pod as the main application container. The containers share the same network and ICP namespace and can also share storage. In OpenShift, when using the service mesh functionality, a Pod annotated with the correct annotation <strong class="inline">sidecar.istio.io/inject: "true"</strong> will have an Istio proxy automatically injected as a sidecar alongside the application container. All subsequent communications between the application and external resources will flow through this sidecar proxy and hence enable the usage of features such as circuit breakers, tracing, and TLS, as and when they are needed. As the great Freddie Mercury once said, "<em class="italics">It's a kind of magic</em>."</p>&#13;
			<p class="snippet"># Let’s patch the deployment for our pet battle apps </p>&#13;
			<p class="snippet"># running in petbattle ns and istioify it</p>&#13;
			<p class="snippet">$ helm upgrade \</p>&#13;
			<p class="snippet">--install pet-battle-tournament \</p>&#13;
			<p class="snippet">--version=1.0.39 \</p>&#13;
			<p class="snippet">--set pet-battle-infra.install_cert_util=true \</p>&#13;
			<p class="snippet">--set istio.enabled=true \</p>&#13;
			<p class="snippet">--timeout=10m \</p>&#13;
			<p class="snippet">--namespace petbattle</p>&#13;
			<p class="snippet">petbattle/pet-battle-tournament</p>&#13;
			<p class="snippet">$ oc get deployment pet-battle-tournament -o yaml \</p>&#13;
			<p class="snippet">--namespace petbattle</p>&#13;
			<p class="snippet">...  </p>&#13;
			<p class="snippet">template:</p>&#13;
			<p class="snippet">    metadata:</p>&#13;
			<p class="snippet">      annotations:</p>&#13;
			<p class="snippet">...</p>&#13;
			<p class="snippet"><span class="P-_-Code-Highlight">        sidecar.istio.io/inject: "true"</span></p>&#13;
			<p class="snippet">      labels:</p>&#13;
			<p class="snippet">        app.kubernetes.io/component: pet-battle-tournament</p>&#13;
			<p class="snippet">        app.kubernetes.io/instance: pet-battle-tournament</p>&#13;
			<p class="snippet">        app.kubernetes.io/name: pet-battle-tournament</p>&#13;
			<div id="footnote-161" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-161-backlink">21</a>	<a href="https://istio.io/latest/docs/">https://istio.io/latest/docs/</a></p>&#13;
			</div>&#13;
			<p>It is possible to have more than one sidecar container if required. Each container can bring different features to the<a id="_idIndexMarker3061"/><a id="_idIndexMarker3062"/> application Pod: for example, one for Istio, another for log forwarding, another<a id="_idIndexMarker3063"/><a id="_idIndexMarker3064"/> for the retrieval of security credentials, and so on. It's easy to know when a Pod is running more than a single container; for example, the <strong class="inline">READY</strong> column indicates how many containers are available per Pod and how many are ready—that is, its readiness probe has passed.</p>&#13;
			<p class="snippet">$ oc get pods </p>&#13;
			<p class="snippet">NAME                            READY   STATUS      RESTARTS   AGE</p>&#13;
			<p class="snippet">infinispan-0                    1/1     Running     0          4h13m</p>&#13;
			<p class="snippet">Pet-battle-3-68fm5              <span class="P-_-Code-Highlight">2/2</span>     Running     0          167m</p>&#13;
			<p class="snippet">pet-battle-api-574f77ddc5-l5qx8 <span class="P-_-Code-Highlight">2/2</span>     Running     0          163m</p>&#13;
			<p class="snippet">pet-battle-api-mongodb-1-7pgfd  1/1     Running     0          3h50m</p>&#13;
			<p class="snippet">pet-battle-tournament-3-wjr6r   <span class="P-_-Code-Highlight">2/2</span>     Running     0          167m</p>&#13;
			<p class="snippet">pet-battle-tou.-mongodb-1-x7t9h 1/1     Running     0          4h9m</p>&#13;
			<p>Be aware, though, that there is a temptation to try and utilize all the service mesh features at once, known as the <em class="italics">ooh… shiny</em> problem.</p>&#13;
			<h3 id="_idParaDest-431">Here<a id="_idTextAnchor680"/><a id="_idTextAnchor681"/> Be Dragons!</h3>&#13;
			<p>The adoption of a service mesh isn't a trivial exercise when it comes to complex solutions with multiple components and development teams. One thing to understand about a service mesh is that it crosses a lot of team boundaries and responsibilities. It includes features that are focused on the developer, operations, and security teams; all of these teams/personnel need to work together to understand and get the best out of using the features provided by the mesh. If you're just starting out, our advice is to start small and figure out what features are necessary in production and iterate from there.</p>&#13;
			<p>In the case of PetBattle, we decided that we were going to primarily focus on using some of the features in the areas of traffic management and <a id="_idIndexMarker3065"/><a id="_idIndexMarker3066"/>observability. The rationale behind this was that Keycloak already addressed many of the security requirements, and we also wanted to finish the book before the end of the decade.</p>&#13;
			<h3 id="_idParaDest-432">Serv<a id="_idTextAnchor682"/><a id="_idTextAnchor683"/>ice Mesh Components</h3>&#13;
			<p>The functionality of the<a id="_idIndexMarker3067"/><a id="_idIndexMarker3068"/> service mesh is made up of a number of independent components:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>Jaeger and Elasticsearch provide the call tracing functionality and logging functionality.</li>&#13;
				<li>Kiali provides <a id="_idIndexMarker3069"/><a id="_idIndexMarker3070"/>the mesh visualization functionality.</li>&#13;
				<li>OpenShift Service Mesh provides the core Istio functionality.</li>&#13;
			</ul>&#13;
			<p>The good news is that all of these components are installed and managed by Operators, so installation is reasonably straightforward. These components are installed via Helm, and if you want to know <a id="_idIndexMarker3071"/><a id="_idIndexMarker3072"/>more about how they are installed, then the Red Hat OpenShift documentation will have the relevant details.</p>&#13;
			<p>One key thing to note is that at the time of writing this book, OpenShift Service Mesh ships with a downstream version of Istio <a id="_idIndexMarker3073"/><a id="_idIndexMarker3074"/>called Maistra. This is primarily due to the out-of-the-box multi-tenancy nature of OpenShift, as well as limiting the scope of Istio cluster-scoped resources. OpenShift <a id="_idIndexMarker3075"/><a id="_idIndexMarker3076"/>Service Mesh also ships with an <strong class="bold">Istio</strong> <strong class="bold">OpenShift Routing</strong> (<strong class="bold">IOR</strong>) component that <a id="_idIndexMarker3077"/><a id="_idIndexMarker3078"/>maps the Istio gateway definitions onto OpenShift routes. Note that Istio is still the upstream <a id="_idIndexMarker3079"/><a id="_idIndexMarker3080"/>project and bugs/feature requests are fixed/implemented, as necessary.</p>&#13;
			<p>For traffic management, Istio has the following <a id="_idIndexMarker3081"/><a id="_idIndexMarker3082"/>core set of resources:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li><strong class="bold">Gateways</strong>: Controls how <a id="_idIndexMarker3083"/><a id="_idIndexMarker3084"/>traffic gets into the mesh from the outside, akin to OpenShift routes.</li>&#13;
				<li><strong class="bold">Virtual service</strong>: Controls how traffic is routed within the service mesh to a destination service. This is where functionality such as timeouts, context-based routing, retries, mirroring, and so on, are configured.</li>&#13;
				<li><strong class="bold">Destination rule</strong>: Service location where traffic is routed to once traffic rules have been applied. Destination rules can be configured to control traffic aspects such as load balancing strategies, connection pools, TLS setting, and outlier detection (circuit breakers).</li>&#13;
			</ul>&#13;
			<p>There are other resources such as service entry, filters, and workloads, but we're not going to cover them here.</p>&#13;
			<h3 id="_idParaDest-433">PetB<a id="_idTextAnchor684"/><a id="_idTextAnchor685"/>attle Service Mesh Resources</h3>&#13;
			<p>We'll briefly introduce<a id="_idIndexMarker3085"/><a id="_idIndexMarker3086"/> some of the resources that we use in PetBattle and explain how we use them.</p>&#13;
			<h3>Gate<a id="_idTextAnchor686"/><a id="_idTextAnchor687"/>ways</h3>&#13;
			<p>The gateway resource, as stated earlier, is used to create an ingress route for traffic coming into the service mesh.</p>&#13;
			<p class="snippet">apiVersion: networking.istio.io/v1alpha3</p>&#13;
			<p class="snippet">kind: <span class="P-_-Code-Highlight">Gateway</span></p>&#13;
			<p class="snippet">metadata:</p>&#13;
			<p class="snippet">  name: <span class="P-_-Code-Highlight">petbattle-gateway-tls</span></p>&#13;
			<p class="snippet">spec:</p>&#13;
			<p class="snippet">  selector:</p>&#13;
			<p class="snippet">    istio: ingressgateway </p>&#13;
			<p class="snippet">  servers:</p>&#13;
			<p class="snippet">  - port:</p>&#13;
			<p class="snippet">      number: 443</p>&#13;
			<p class="snippet">      name: https</p>&#13;
			<p class="snippet">      protocol: HTTPS</p>&#13;
			<p class="snippet">    tls:</p>&#13;
			<p class="snippet">      mode: SIMPLE</p>&#13;
			<p class="snippet">      credentialName: <span class="P-_-Code-Highlight">"pb-ingressgateway-certs"</span></p>&#13;
			<p class="snippet">    hosts:     </p>&#13;
			<p class="snippet">    - "*"</p>&#13;
			<p>A few things to note with this definition is that it will create an OpenShift route in the <em class="italics">istio-system</em> namespace and not the local namespace. Secondly, the route itself will use SSL, but it won't be able to utilize the OpenShift router certificates by default. Service mesh routes have to provide their own certificates. As part of writing this book, we took the pragmatic approach and copied the OpenShift router certificates into the <em class="italics">istio-system</em> namespace and provided them to the gateway via the <em class="italics">pb-ingressgateway-certs</em> secret. Note that this is for demonstration purposes only—<em class="italics">do not try this in production</em>. The correct approach for production is to generate and manage the PKI using as-a-service certificates.</p>&#13;
			<h3>Virt<a id="_idTextAnchor688"/><a id="_idTextAnchor689"/>ual services</h3>&#13;
			<p>PetBattle contains a <a id="_idIndexMarker3087"/><a id="_idIndexMarker3088"/>number of VirtualServices, such as <em class="italics">pet-battle-cats-tls, pet-battle-main-tls, </em>and<em class="italics"> pet-battle-tournament-tls.</em></p>&#13;
			<p class="snippet">apiVersion: networking.istio.io/v1alpha3</p>&#13;
			<p class="snippet">kind: <span class="P-_-Code-Highlight">VirtualService</span></p>&#13;
			<p class="snippet">metadata:</p>&#13;
			<p class="snippet">  name: <span class="P-_-Code-Highlight">pet-battle-cats-tls</span></p>&#13;
			<p class="snippet">spec:</p>&#13;
			<p class="snippet">  hosts:</p>&#13;
			<p class="snippet">  - "*"</p>&#13;
			<p class="snippet">  gateways:</p>&#13;
			<p class="snippet">  - <span class="P-_-Code-Highlight">petbattle-gateway-tls</span></p>&#13;
			<p class="snippet">  http:</p>&#13;
			<p class="snippet">  - match:</p>&#13;
			<p class="snippet">    - uri:</p>&#13;
			<p class="snippet">        <span class="P-_-Code-Highlight">prefix: /cats</span></p>&#13;
			<p class="snippet">      ignoreUriCase: true</p>&#13;
			<p class="snippet">    route:</p>&#13;
			<p class="snippet">    - destination:</p>&#13;
			<p class="snippet">        host: pet-battle-api</p>&#13;
			<p class="snippet">        port:</p>&#13;
			<p class="snippet">          number: 8080</p>&#13;
			<p class="snippet">    <span class="P-_-Code-Highlight">retries</span>:</p>&#13;
			<p class="snippet">      attempts: 3</p>&#13;
			<p class="snippet">      perTryTimeout: 2s</p>&#13;
			<p class="snippet">      retryOn: gateway-error,connect-failure,refused-stream</p>&#13;
			<p>The VirtualServices are <a id="_idIndexMarker3089"/><a id="_idIndexMarker3090"/>all similar in function in that they are all configured to:</p>&#13;
			<ol>&#13;
				<li value="1">Match a specific URI; in the example above, <strong class="inline">/cats</strong>.</li>&#13;
				<li>Once matched, route the traffic to a specific destination.</li>&#13;
				<li>Handle specific errors by performing a fixed number of request retries.</li>&#13;
			</ol>&#13;
			<h3>Dest<a id="_idTextAnchor690"/><a id="_idTextAnchor691"/>ination Rule</h3>&#13;
			<p>Finally, the traffic is sent to <a id="_idIndexMarker3091"/><a id="_idIndexMarker3092"/>a destination or even distributed to a set of destinations depending on the configuration. This is where DestinationRules come into play.</p>&#13;
			<p class="snippet">apiVersion: networking.istio.io/v1alpha3</p>&#13;
			<p class="snippet">kind: <span class="P-_-Code-Highlight">DestinationRule</span></p>&#13;
			<p class="snippet">metadata:</p>&#13;
			<p class="snippet">  name: <span class="P-_-Code-Highlight">pet-battle-api-port</span></p>&#13;
			<p class="snippet">spec:</p>&#13;
			<p class="snippet">  host: pet-battle-api.prod.svc.cluster.local</p>&#13;
			<p class="snippet">  trafficPolicy: # Apply to all ports</p>&#13;
			<p class="snippet">    portLevelSettings:</p>&#13;
			<p class="snippet">    - port:</p>&#13;
			<p class="snippet">        number: 8080</p>&#13;
			<p class="snippet">      loadBalancer:</p>&#13;
			<p class="snippet">        simple: <span class="P-_-Code-Highlight">LEAST_CONN</span></p>&#13;
			<p>In our example, the traffic sent to a<a id="_idIndexMarker3093"/><a id="_idIndexMarker3094"/> specific port is load balanced based on a simple strategy that selects the Pod with the least number of active requests. There are many load balancing strategies that can be used here, depending on the needs of the application—everything from simple round robin to advanced consistent hashing load balancing strategies, which can be used for session affinity. As ever, the documentation goes into far greater detail.<span id="footnote-160-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-160">22</a></span> </p>&#13;
			<p>We can visualize the flow of traffic from the above example, as seen in <em class="italics">Figure 16.7</em>:</p>&#13;
			<div>&#13;
				<div id="_idContainer551" class="IMG---Figure">&#13;
					<img src="../Images/B16297_16_07.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 16.7: PetBattle traffic flow</p>&#13;
			<div id="footnote-160" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-160-backlink">22</a>	<a href="https://istio.io/latest/docs/">https://istio.io/latest/docs/</a></p>&#13;
			</div>&#13;
			<p>Note that <em class="italics">Figure 16.7</em> shows an example of how destination rules can be used to send traffic to an alternative version of the service. This can be useful for advanced deployment strategies such as Canary, Blue/Green, and so on. We haven't discussed how to do this with OpenShift Service Mesh in this book, but the reader is encouraged to explore this area in more detail. A good place to start is the aforementioned Istio documentation.</p>&#13;
			<p>Managing all of these<a id="_idIndexMarker3095"/><a id="_idIndexMarker3096"/> resources is reasonably simple when it's just a few services, and PetBattle utilizes service mesh functionality in a very basic manner. However, when there are many services and features, such as multiple destinations used in advanced deployment models, the amount of settings and YAML to interpret can be overwhelming. This is where mesh visualization functionality can be useful to visualize how all of this works together. For this, we use <a id="_idIndexMarker3097"/><a id="_idIndexMarker3098"/>the Kiali functionality, which is part of OpenShift Service Mesh. <em class="italics">Figure 16.8</em> shows how PetBattle is visualized using Kiali.</p>&#13;
			<div>&#13;
				<div id="_idContainer552" class="IMG---Figure">&#13;
					<img src="../Images/B16297_16_08.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p> </p>&#13;
			<p class="figure">Figure 16.8: Kiali service graph for PetBattle</p>&#13;
			<p>Kiali can be very useful for diagnosing the current state of the mesh, as it can dynamically show where traffic is being sent as well as the state of any circuit breakers being used. It also integrates with Jaeger for tracing requests across multiple systems. Kiali can also help prevent configuration issues by semantically validating the deployed service mesh resources.</p>&#13;
			<p>Next we're going to explore one of the most powerful features of OpenShift 4 - Operators.</p>&#13;
			<h2 id="_idParaDest-434">Operat<a id="_idTextAnchor692"/><a id="_idTextAnchor693"/><a id="_idTextAnchor694"/>ors Everywhere</h2>&#13;
			<p>Fundamental to the OpenShift 4 platform is the concept of <em class="italics">Operators</em>. So far, we have used them without talking about why we need them and what they actually represent on a Kubernetes <a id="_idIndexMarker3099"/><a id="_idIndexMarker3100"/>platform such as OpenShift. Let's cover this briefly without totally rewriting the book on the subject.<span id="footnote-159-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-159">23</a></span></p>&#13;
			<p>At its heart, the Operator is a software pattern that codifies knowledge about the running and operation of a particular software application. That application could be a distributed key value store, such as etcd. It might be a web application such as the OpenShift web console. Fundamentally, the operator can represent <em class="italics">any</em> application domain that could be codified. A good analogy for an operator is the <em class="italics">expert system</em>, a rules-based bit of software that represents knowledge about a certain thing that is put to work in a meaningful way. If we take a database as an example, the Operator might codify what a real human database administrator does on a day-to-day basis, such as the deployment, running, scaling, backup, patching, and upgrading of that database.</p>&#13;
			<p>The physical runtime for an operator is nothing more than a Kubernetes Pod, that is, a collection of containers that run on a Kubernetes platform such as OpenShift. Operators work by extending or adding new APIs to the existing Kubernetes and OpenShift platform APIs. This new<a id="_idIndexMarker3101"/><a id="_idIndexMarker3102"/> endpoint is called a <a id="_idIndexMarker3103"/><strong class="bold">Custom Resource</strong> (<strong class="bold">CR</strong>). CRs are one of the many extension mechanisms in Kubernetes.</p>&#13;
			<div>&#13;
				<div id="_idContainer553" class="IMG---Figure">&#13;
					<img src="../Images/B16297_16_09.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 16.9: The Operator pattern</p>&#13;
			<div id="footnote-159" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-159-backlink">23</a>	<a href="https://www.redhat.com/en/resources/oreilly-kubernetes-operators-automation-ebook">https://www.redhat.com/en/resources/oreilly-kubernetes-operators-automation-ebook</a></p>&#13;
			</div>&#13;
			<p>A <a id="_idIndexMarker3104"/><strong class="bold">Custom Resource Definition</strong> (<strong class="bold">CRD</strong>) defines what the CR is. Think of it as the definition or schema for the CR. The Operator Pod <em class="italics">watches</em> for events on the platform that are related to their custom resources and takes <em class="italics">reconciliation</em> actions to achieve the desired state of the system. When an Operator Pod stops or is deleted from the cluster, the application(s) that it manages should continue to function. Removing a CRD from your cluster does affect the application(s) that it manages. In fact, deleting a CRD will in turn delete its CR instances. This is <a id="_idIndexMarker3105"/><a id="_idIndexMarker3106"/>the Operator pattern.</p>&#13;
			<p>With Operators, all of the <a id="_idIndexMarker3107"/><a id="_idIndexMarker3108"/>operational experience required to run/manage a piece of software can be packaged up and delivered as a set of containers and associated resources. In fact, the whole of the OpenShift 4 platform exists as a collection of operators! So, as the platform owner, you are receiving the most advanced administrator knowledge bundled up through Operators. Even better, Operators can become more advanced over time as new features and capabilities are added to them. A good understanding of how to configure Operators is required for OpenShift platform administrators. This usually involves setting properties in the OpenShift cluster global configuration web console, setting CR property values, using <a id="_idIndexMarker3109"/><a id="_idIndexMarker3110"/>ConfigMaps, or similar approaches. The product documentation<span id="footnote-158-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-158">24</a></span> is usually the best place to find out what these settings are for each Operator.</p>&#13;
			<div>&#13;
				<div id="_idContainer554" class="IMG---Figure">&#13;
					<img src="../Images/B16297_16_10.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 16.10: OpenShift Cluster Settings—configuring platform Operators</p>&#13;
			<div id="footnote-158" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-158-backlink">24</a>	<a href="https://docs.openshift.com/container-platform/4.7">https://docs.openshift.com/container-platform/4.7</a></p>&#13;
			</div>&#13;
			<p>In OpenShift, the lifecycle management (upgrading, patching, managing) of Operators themselves is automated through the <a id="_idIndexMarker3111"/><strong class="bold">Operator Lifecycle Manager</strong> (<strong class="bold">OLM</strong>). These components make upgrading the OpenShift <a id="_idIndexMarker3112"/><a id="_idIndexMarker3113"/>platform itself a lot more reliable and easier to manage from a user's perspective; it massively reduces the operational burden. Because Operators themselves are delivered as versioned images, we get the same benefits from immutable container images that we do for our own applications, i.e., the same image version can be run consistently in multiple cloud environments increasing quality and eliminating snowflakes (unique applications for unique environments).</p>&#13;
			<p>And it is not just the <a id="_idIndexMarker3114"/><a id="_idIndexMarker3115"/>OpenShift platform itself that can take advantage of Operators. The sharing and distribution of software using <a id="_idIndexMarker3116"/><a id="_idIndexMarker3117"/>Operators via the Operator Hub<span id="footnote-157-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-157">25</a></span> is open to software developers and vendors from all over the world. We use OLM and Operator <em class="italics">subscriptions</em> to deploy these into our cluster. The tooling required to build and develop Operators (the SDK) is open source and available to all.<span id="footnote-156-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-156">26</a></span></p>&#13;
			<p>So, should <em class="italics">all</em> applications be delivered as Operators? The short answer is no. The effort required to code, package, test, and maintain an Operator may be seen as overkill for many applications. For example, if your applications are not being distributed and shared with others, and you only need to build, package, deploy, and configure your application in a few clusters, there are many simpler ways this can be achieved, such as using container images, Kubernetes, and OpenShift<a id="_idIndexMarker3118"/><a id="_idIndexMarker3119"/> native constructs (BuildConfigs, Deployments, ReplicaSets, ConfigMaps, Secrets, and more) with tools such as Helm to achieve your goals.</p>&#13;
			<h3 id="_idParaDest-435">Operator<a id="_idTextAnchor695"/><a id="_idTextAnchor696"/>s Under the Hood</h3>&#13;
			<p>To fully<a id="_idTextAnchor697"/><a id="_idTextAnchor698"/> understand how operators work, you need to understand how the Kubernetes control loop works.</p>&#13;
			<h3 id="_idParaDest-436">Control <a id="_idTextAnchor699"/><a id="_idTextAnchor700"/>Loops</h3>&#13;
			<p>In very basic terms, core <a id="_idIndexMarker3120"/><a id="_idIndexMarker3121"/>Kubernetes is <a id="_idIndexMarker3122"/><a id="_idIndexMarker3123"/>just a <strong class="bold">Key-Value</strong> (<strong class="bold">KV</strong>) store—an etcd datastore with an API. Processes use this API to perform <strong class="bold">Create</strong>, <strong class="bold">Read</strong>, <strong class="bold">Update</strong>, and <strong class="bold">Delete</strong> (<strong class="bold">CRUD</strong>) actions on keys within the KV store. Processes can also register with the KV store to be notified when there are value changes to keys or sets of keys that they're interested in. </p>&#13;
			<div id="footnote-157" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-157-backlink">25</a>	<a href="https://operatorhub.io">https://operatorhub.io</a></p>&#13;
				<div id="footnote-156" class="_idFootnote" epub:type="footnote">&#13;
					<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-156-backlink">26</a>	<a href="https://github.com/operator-framework/operator-sdk">https://github.com/operator-framework/operator-sdk</a></p>&#13;
				</div>&#13;
			</div>&#13;
			<p>When these processes get a change notification, they react to that notification by performing some activity, such as configuring iptables rules, provisioning storage, and so on. These processes understand the current state of the system and the desired state and work toward achieving that desired state. In other words, these processes are performing the role of a <em class="italics">control loop</em>, meaning they attempt to bring the state of the system to a desired state from where it currently resides.</p>&#13;
			<p>In this example, the <em class="italics">process</em> is a controller that observes the <a id="_idIndexMarker3124"/><a id="_idIndexMarker3125"/>state of a resource or set of resources and then makes changes to move the resource state closer to the desired state. As consumers of Kubernetes, we constantly use controllers. For example, when we instruct Kubernetes to deploy a Pod, the Pod controller works to make that a reality. Control loops are key to<a id="_idIndexMarker3126"/><a id="_idIndexMarker3127"/> the operation on Kubernetes and it's a declarative and, eventually, consistent approach. For much more information, take a look at the Kubernetes Controller docs<span id="footnote-155-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-155">27</a></span> and the OpenShift blog site for recommendations on how to build your own Operator.<span id="footnote-154-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-154">28</a></span></p>&#13;
			<h3 id="_idParaDest-437">Operator<a id="_idTextAnchor701"/><a id="_idTextAnchor702"/> Scopes</h3>&#13;
			<p>Operators <a id="_idIndexMarker3128"/><a id="_idIndexMarker3129"/>can either <a id="_idIndexMarker3130"/><a id="_idIndexMarker3131"/>be cluster-scoped or<a id="_idIndexMarker3132"/><a id="_idIndexMarker3133"/> namespace-scoped. A cluster-scoped operator is installed once in a namespace and can create and manage resources in other<a id="_idIndexMarker3134"/><a id="_idIndexMarker3135"/> namespaces; that is, cluster-wide. The OpenShift service mesh operator and its related operators <a id="_idIndexMarker3136"/><a id="_idIndexMarker3137"/>such as <a id="_idIndexMarker3138"/><a id="_idIndexMarker3139"/>Kiali and Jaeger <a id="_idIndexMarker3140"/><a id="_idIndexMarker3141"/>are cluster-scoped. They are installed by default into the <strong class="inline">openshift-operators</strong> or <strong class="inline">openshift-operators-redhat</strong> namespace and create and manage resources when a related CRD is deployed in another namespace, such as PetBattle.</p>&#13;
			<p>A namespace-scoped operator is one that is deployed in a namespace and only manages resources in that namespace. We<a id="_idIndexMarker3142"/><a id="_idIndexMarker3143"/> use a number of these in PetBattle, such as Cert-Utils and Keycloak.</p>&#13;
			<p>All Operators are installed via a CRD called a <strong class="bold">Subscription</strong>. Without going into too much detail (see the official documentation for more), a Subscription describes how to retrieve and install an instance of an operator. The following is an example of a Subscription that we use to install the Grafana operator.</p>&#13;
			<div id="footnote-155" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-155-backlink">27</a>	<a href="https://kubernetes.io/docs/concepts/architecture/controller/">https://kubernetes.io/docs/concepts/architecture/controller/</a></p>&#13;
				<div id="footnote-154" class="_idFootnote" epub:type="footnote">&#13;
					<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-154-backlink">28</a>	<a href="https://www.openshift.com/blog/kubernetes-operators-best-practices">https://www.openshift.com/blog/kubernetes-operators-best-practices</a></p>&#13;
				</div>&#13;
			</div>&#13;
			<p class="snippet">apiVersion: operators.coreos.com/v1alpha1</p>&#13;
			<p class="snippet">kind: Subscription</p>&#13;
			<p class="snippet">metadata:</p>&#13;
			<p class="snippet"> name: grafana-operator</p>&#13;
			<p class="snippet">spec:</p>&#13;
			<p class="snippet"> channel: alpha</p>&#13;
			<p class="snippet"> installPlanApproval: Automatic</p>&#13;
			<p class="snippet"> name: grafana-operator</p>&#13;
			<p class="snippet"> source: community-operators</p>&#13;
			<p class="snippet"> sourceNamespace: openshift-marketplace</p>&#13;
			<p class="snippet"> startingCSV: grafana-operator.v3.7.0</p>&#13;
			<p>To see some <a id="_idIndexMarker3144"/><a id="_idIndexMarker3145"/>of the namespace-scoped operators that PetBattle needs, run the following command.</p>&#13;
			<p class="snippet">$ oc get subscriptions</p>&#13;
			<p class="snippet">NAME                PACKAGE             SOURCE                CHANNEL</p>&#13;
			<p class="snippet">cert-utils-operator cert-utils-operator community-operators   alpha</p>&#13;
			<p class="snippet">grafana-operator    grafana-operator    community-operators   alpha</p>&#13;
			<p class="snippet">infinispan          infinispan          community-operators   2.1.x</p>&#13;
			<p class="snippet">keycloak-operator   keycloak-operator   community-operators   alpha</p>&#13;
			<p>Let us now take a look at how <a id="_idIndexMarker3146"/><a id="_idIndexMarker3147"/>operators can be used by our PetBattle team.</p>&#13;
			<h3 id="_idParaDest-438"><a id="_idTextAnchor703"/>Operators in PetBattle</h3>&#13;
			<p>We use operators to create and <a id="_idIndexMarker3148"/><a id="_idIndexMarker3149"/>manage resources such as Infinispan cache and Keycloak SSO instances. We simply install the Infinispan operator and deploy a relevant custom resource to tell it to create and manage a replicated cache. We don't have to know about spinning up Infinispan Pods or creating SSL certificates or provisioning storage space. The operator will do all of this for us, and if something fails or is accidentally deleted, the operator will look after the recreation of the resource. In the Infinispan example, if we delete the Infinispan K8s service, the operator will be notified about its deletion and recreate the service automatically. As developers, we don't have to worry about managing it.</p>&#13;
			<p>It is simpler to think of Operators as <em class="italics">looking after stuff so you don't have to</em>. It is also possible to use multiple operators in combination to<a id="_idIndexMarker3150"/><a id="_idIndexMarker3151"/> automate complex workflows. For example, we use Keycloak for its SSO gateway and user management functionality. The Keycloak instance is deployed and managed via a Keycloak Operator. We just need to build and send a custom resource to the API and the operator will do the rest. One of the resources managed by the Operator is a Kubernetes Secret that contains the TLS certificates and keys, which clients interacting with the Keycloak instance will need to use. Given that Keycloak is the security gateway to our application, it is prudent to ensure that all communications are encrypted. However, this causes issues for Java-based applications; to use SSL, the JVM requires that it be provided with a Java TrustStore containing the SSL/TLS certificates and keys so that the JVM can trust them. </p>&#13;
			<p>So, how do we take the Secret with the TLS certificates and keys and convert that into a TrustStore that the Java applications can use? We could do a whole heap of scripting with Bash, the Java Keytool, and potentially other tools to extract the certs/keys, creating the TrustStore, converting, and finally injecting the certs/keys into said TrustStore. This is manual, complex, and error-prone work. We will also have to recreate these TrustStores for each environment and handle lifecycle events such as certificate expiry.</p>&#13;
			<p>Alternatively, we <a id="_idIndexMarker3152"/><a id="_idIndexMarker3153"/>could use an operator, in<a id="_idIndexMarker3154"/><a id="_idIndexMarker3155"/> this case, the <em class="italics">Cert-Utils</em> operator. We first install the <em class="italics">Cert-Utils</em> operator in the PetBattle namespace. This Operator was developed by the Red Hat Consulting PAAS Community of Practice<span id="footnote-153-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-153">29</a></span> to help manage certificates and JVM Keystores, along with TrustStores.</p>&#13;
			<p>To use this Operator, we first create a ConfigMap containing a set of specific annotations. The Cert-Utils operator will detect these annotations and create a TrustStore containing the relevant certificates and keys; it will also add the TrustStore to the ConfigMap. Finally, we can mount the ConfigMap into a Deployment and instruct the JVM to use that TrustStore. The following resource definition will create the TrustStore with the relevant certificates and keys.</p>&#13;
			<p class="snippet">apiVersion: v1</p>&#13;
			<p class="snippet">kind: ConfigMap</p>&#13;
			<p class="snippet">metadata:</p>&#13;
			<p class="snippet">  annotations:</p>&#13;
			<p class="snippet">    <span class="P-_-Code-Highlight">service.beta.openshift.io/inject-cabundle : "true"</span></p>&#13;
			<p class="snippet"><span class="P-_-Code-Highlight">    cert-utils-operator.redhat-cop.io/generate-java-truststore: "true"</span></p>&#13;
			<p class="snippet"><span class="P-_-Code-Highlight">    cert-utils-operator.redhat-cop.io/source-ca-key: "service-ca.crt"</span></p>&#13;
			<p class="snippet"><span class="P-_-Code-Highlight">    cert-utils-operator.redhat-cop.io/java-keystore-password: "jkspassword"</span></p>&#13;
			<p class="snippet">  name: java-truststore</p>&#13;
			<div id="footnote-153" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-153-backlink">29</a>	<a href="https://github.com/redhat-cop/cert-utils-operator">https://github.com/redhat-cop/cert-utils-operator</a></p>&#13;
			</div>&#13;
			<p>This does the following:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>The <strong class="inline">service.beta.openshift.io/inject-cabundle</strong> annotation will inject the service signing certificate bundle into the ConfigMap as a <strong class="inline">service-sa.crt</strong> field. </li>&#13;
				<li>The <strong class="inline">cert-utils-operator.redhat-cop.io</strong> annotation will create the Java TrustStore in the ConfigMap under the name <strong class="inline">truststore.jks</strong> with the <strong class="inline">jkpassword</strong> password.</li>&#13;
			</ul>&#13;
			<p>In the Tournament <a id="_idIndexMarker3156"/><a id="_idIndexMarker3157"/>service, the following Quarkus configuration will mount the <strong class="inline">java-truststore</strong> ConfigMap and configure the JVM accordingly.</p>&#13;
			<p class="snippet"># Mount the configmap into the application pod in the /tmp/config/ directory</p>&#13;
			<p class="snippet">quarkus.kubernetes-config.enabled=true</p>&#13;
			<p class="snippet">quarkus.openshift.config-map-volumes.javatruststore.config-map-name=java-truststore</p>&#13;
			<p class="snippet">quarkus.openshift.mounts.javatruststore.path=/tmp/config/</p>&#13;
			<p class="snippet"># Instruct the JVM to use the Truststore</p>&#13;
			<p class="snippet">quarkus.openshift.env-vars.JAVA_OPTS.value=-Djavax.net.ssl.trustStore=/tmp/config/truststore.jks -Djavax.net.ssl.trustStorePassword=jkspassword</p>&#13;
			<p class="snippet"># Tell Infinispan client to use the Truststore when connecting</p>&#13;
			<p class="snippet">quarkus.infinispan-client.trust-store=/tmp/config/truststore.jks</p>&#13;
			<p class="snippet">quarkus.infinispan-client.trust-store-password=jkspassword</p>&#13;
			<p>We've only just scratched the surface of operators. OpenShift ships with a number of supported operators and there are many community operators available as well. We used many community-based operators in this book, such as the Infinispan operator and Keycloak operator; there are productized versions of these operators available as well. There are many more operators from multiple vendors available from OperatorHub.<span id="footnote-152-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-152">30</a></span></p>&#13;
			<p>It is also possible to write your own operators if required. The <a id="_idIndexMarker3158"/><a id="_idIndexMarker3159"/>OperatorFramework<span id="footnote-151-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-151">31</a></span> is an open-source SDK with which<a id="_idIndexMarker3160"/><a id="_idIndexMarker3161"/> you can write your<a id="_idIndexMarker3162"/><a id="_idIndexMarker3163"/> own operators using either Go, Ansible, or Helm. </p>&#13;
			<div id="footnote-152" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-152-backlink">30</a>	<a href="https://operatorhub.io/">https://operatorhub.io/</a></p>&#13;
			</div>&#13;
			<div id="footnote-151" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-151-backlink">31</a>	<a href="https://operatorframework.io/">https://operatorframework.io/</a></p>&#13;
			</div>&#13;
			<h3 id="_idParaDest-439">Service <a id="_idTextAnchor704"/><a id="_idTextAnchor705"/>Serving Certificate Secrets</h3>&#13;
			<p>Keycloak<a id="_idTextAnchor706"/> uses an <a id="_idIndexMarker3164"/><a id="_idIndexMarker3165"/>OpenShift feature called <em class="italics">service serving certificate secrets</em>.<span id="footnote-150-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-150">32</a></span> This is used for traffic encryption. Using this feature, OpenShift automatically generates certificates signed by the OpenShift<a id="_idIndexMarker3166"/><a id="_idIndexMarker3167"/> certificate authority and stores them in a secret. The application, in this case Keycloak, can then mount this secret and use these certificates to encrypt traffic. Any application interacting with a Keycloak instance then just has to trust these certificates. OpenShift also manages the lifecycle of these certificates and automatically generates new certificates when the existing certificates are about to expire.</p>&#13;
			<p>To turn on this feature, simply add the following annotation to a service:</p>&#13;
			<p class="snippet">service.beta.openshift.io/serving-cert-secret-name=&lt;NameOfMysecret&gt;</p>&#13;
			<p>In the case of Keycloak, the operator does this as part of its processing:</p>&#13;
			<p class="snippet">$ oc get svc keycloak -o yaml</p>&#13;
			<p class="snippet">apiVersion: v1</p>&#13;
			<p class="snippet">kind: Service</p>&#13;
			<p class="snippet">metadata:</p>&#13;
			<p class="snippet">  annotations:</p>&#13;
			<p class="snippet">    description: The web server's https port.</p>&#13;
			<p class="snippet"><span class="P-_-Code-Highlight">    service.alpha.openshift.io/serving-cert-secret-name: sso-x509-https-secret</span></p>&#13;
			<p class="snippet">    service.alpha.openshift.io/serving-cert-signed-by: openshift-service-serving-signer@1615684126</p>&#13;
			<p class="snippet">    service.beta.openshift.io/serving-cert-signed-by: openshift-service-serving-signer@1615684126</p>&#13;
			<p>The secret contains the actual certificate and associated key:</p>&#13;
			<p class="snippet">$ oc get secret sso-x509-https-secret -o yaml</p>&#13;
			<p class="snippet">apiVersion: v1</p>&#13;
			<p class="snippet">data:</p>&#13;
			<p class="snippet"><span class="P-_-Code-Highlight">  tls.crt: …...</span></p>&#13;
			<p class="snippet"><span class="P-_-Code-Highlight">  tls.key: …..</span></p>&#13;
			<p class="snippet">kind: Secret</p>&#13;
			<p class="snippet">metadata:</p>&#13;
			<p class="snippet">  annotations:</p>&#13;
			<div id="footnote-150" class="_idFootnote" epub:type="footnote">&#13;
				<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-150-backlink">32</a>	<a href="https://docs.openshift.com/container-platform/4.7/security/certificates/service-serving-certificate.html">https://docs.openshift.com/container-platform/4.7/security/certificates/service-serving-certificate.html</a></p>&#13;
			</div>&#13;
			<p>And it contains the certificate details as well:</p>&#13;
			<p class="snippet">$ oc get<a id="_idTextAnchor707"/><a id="_idTextAnchor708"/> secret sso-x509-https-secret -o json \</p>&#13;
			<p class="snippet">    | jq -r '.data."tls.crt"' | base64 --decode \</p>&#13;
			<p class="snippet">    |openssl x509  -text -noout</p>&#13;
			<p class="snippet">Certificate:</p>&#13;
			<p class="snippet">    Data:</p>&#13;
			<p class="snippet">        Version: 3 (0x2)</p>&#13;
			<p class="snippet">        Serial Number: 1283774295358672234 (0x11d0e1eb7ea18d6a)</p>&#13;
			<p class="snippet">        Signature Algorithm: sha256WithRSAEncryption</p>&#13;
			<p class="snippet">        Issuer: CN = openshift-service-serving-signer@1615684126</p>&#13;
			<p class="snippet">        Validity</p>&#13;
			<p class="snippet">            Not Before: Mar 15 08:34:54 2021 GMT</p>&#13;
			<p class="snippet">            Not After : Mar 15 08:34:55 2023 GMT</p>&#13;
			<p class="snippet">        Subject: CN = keycloak.labs-staging.svc</p>&#13;
			<p class="snippet">        Subject Public Key Info:</p>&#13;
			<p class="snippet">            Public Key Algorithm: rsaEncryption</p>&#13;
			<p>Such Operator<a id="_idIndexMarker3168"/><a id="_idIndexMarker3169"/> patterns simplify the burden of running complex middleware infrastructure applications on the OpenShift platform.</p>&#13;
			<h2 id="_idParaDest-440"><a id="_idTextAnchor709"/>Conclusion</h2>&#13;
			<p>To be able to successfully run your software at scale in production, a good understanding of the instrumentation that surrounds the software stack is required. OpenShift is a modern platform that provides all of the capabilities required to observe and, in a lot of cases, automatically heal your applications while they're running.</p>&#13;
			<p>In this chapter, we have discussed many common technical patterns that allow application developers to make use of these common platform capabilities. For example, one of the simplest patterns is to always log to STDOUT so the platform logging mechanisms can be leveraged. With containers, it becomes an antipattern to log to specific files mounted in a temporary filesystem within your container, because they are not clearly visible.</p>&#13;
			<p>More complex patterns are also important to keep your business service applications running, even during disruption and change. Correctly configuring liveness, readiness, and startup probes so that your application <a id="_idIndexMarker3170"/><a id="_idIndexMarker3171"/>can deploy without loss of service, configuring Pod disruption budgets for when nodes are restarted. Using application features to expose Prometheus metric endpoints for alerting and monitoring on the platform is a great way to alert teams when human interaction is required.</p>&#13;
			<p>The service mesh is an advanced extension to OpenShift, extrapolating many features that would have traditionally been packaged into your applications so they can be more efficiently managed at the platform level. This is a common theme: taking application and development cross-cutting features and leveraging them to the benefit of all platform services.</p>&#13;
			<p>The Operator pattern eases the operational burden of running complex middleware infrastructure applications on the OpenShift platform, packaging all the years of expert knowledge as software. It is no secret that OpenShift itself uses this fantastic pattern for all of its core capabilities. The real power comes in being able to lifecycle manage this complexity in an automated manner. Human toil is massively reduced because the system can self-heal and auto-upgrade without interference. Doing more with less is still the name of the game.</p>&#13;
			<p>As a cross-functional product team, once you have learned and mastered these capabilities, it really does become possible to <em class="italics">give the developers the pagers</em>. The quality of any business <a id="_idIndexMarker3172"/><a id="_idIndexMarker3173"/>service delivery starts with business discovery, which then transitions to application software, expands through to platform capabilities, and finally on and out into the world of networking and end user devices connected via the internet. Once developers and cross-functional product teams are empowered to build, run, and own their software—in every environment that it is required to run in—only then will they fully equate and connect happy customers with the software supply chain that they code, automate, and continuously deliver.</p>&#13;
		</div>&#13;
</body></html>