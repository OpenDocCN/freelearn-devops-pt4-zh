- en: 16\. Own It
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16. 担当责任
- en: '"*Annnddd we''re live! PetBattle is finally in production, we can crack open
    the Champagne and toast our success*." But now what? How do we know that the site
    is doing what we expect it to do and, more importantly, how will we know when
    it isn''t performing as we intended? Do we just sit around waiting for customers
    to complain that the site is down or that errors are happening? Not exactly a
    good user experience model—or a good use of our time.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '"*好啦，我们上线了！PetBattle终于投入生产了，我们可以打开香槟，庆祝我们的成功*。"但是现在怎么办？我们怎么知道网站是否按预期运行？更重要的是，我们怎么知道它什么时候没有按照我们的意图运行？我们是不是只能等着客户抱怨网站崩溃或发生错误？这显然不是一个好的用户体验模型——也不是我们时间的好用法。'
- en: In this chapter, we will discuss the tools and techniques that can be utilized
    to monitor the site and notify us when things start to go wrong so we can react
    before the entire site goes down. We will also discuss advanced techniques, such
    as Operators, that can help you automate a lot of the day-to-day operations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论可以用来监控网站并在问题开始出现时通知我们的工具和技术，以便我们在整个网站崩溃之前作出反应。我们还将讨论一些高级技术，例如Operators，帮助你自动化许多日常操作。
- en: Observability
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可观察性
- en: Observability[1](#footnote-181) is the process of instrumenting software components
    to assist with extracting data. This data can then be used to determine how well
    a system is functioning and subsequently be used to notify administrators in the
    event of issues.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 可观察性[1](#footnote-181)是对软件组件进行监控，以帮助提取数据的过程。这些数据可以用来确定系统的运行状况，并在发生问题时通知管理员。
- en: '[1](#footnote-181-backlink) [https://en.wikipedia.org/wiki/Observability](https://en.wikipedia.org/wiki/Observability)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[1](#footnote-181-backlink) [https://en.wikipedia.org/wiki/Observability](https://en.wikipedia.org/wiki/Observability)'
- en: 'When it comes to observing the state of our PetBattle applications, there are
    a number of aspects to consider:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们观察PetBattle应用程序的状态时，有许多方面需要考虑：
- en: How do we know if an application instance is initialized and ready to process
    traffic?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们怎么知道应用程序实例是否已经初始化并准备好处理流量？
- en: How do we know if the application has failed without crashing, such as by becoming
    deadlocked or blocked?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们怎么知道应用程序是否失败而没有崩溃，比如死锁或被阻塞？
- en: How do we access the application logs?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们怎么访问应用程序日志？
- en: How do we access the application metrics?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们怎么访问应用程序的度量指标？
- en: How do we know what version of the application is running?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们怎么知道当前运行的应用程序版本？
- en: Let's start by exploring some application health checks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从探索一些应用程序健康检查开始。
- en: Probes
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探针
- en: '*Hello, hello?... Is this thing on?* In Kubernetes, the health of an application
    is determined by a set of software *probes* that are periodically invoked by the
    kubelet. A probe is basically an action invoked by the platform on each Pod that
    either returns a success value or a failure value.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*哈喽，哈喽？…这个东西有声音吗？* 在Kubernetes中，应用程序的健康状况由一组由kubelet定期调用的软件*探针*来决定。探针基本上是平台在每个Pod上调用的一个操作，它要么返回成功的值，要么返回失败的值。'
- en: 'Probes can be configured to perform one of the following types of actions:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 探针可以配置为执行以下几种类型的操作：
- en: Connect to a specific TCP port that the container is listening on. If the port
    is open, the probe is considered successful.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接到容器监听的特定TCP端口。如果端口是开放的，则探针被认为是成功的。
- en: Invoke an HTTP endpoint, if the HTTP response code is 200 or greater but less
    than 400.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用HTTP端点，如果HTTP响应代码大于或等于200但小于400。
- en: Shell into a container and execute a command—this may involve checking for a
    specific file in the directory. This enables probes to be placed on applications
    that don't natively provide health checks out of the box. If the command exits
    with a status code of 0, then the probe is successful.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进入容器并执行命令——这可能涉及检查目录中的特定文件。这使得我们可以在那些没有原生提供健康检查的应用程序上放置探针。如果命令以状态码0退出，则探针成功。
- en: If a probe fails a configured number of times, the kubelet managing the Pod
    will take a pre-determined action, for example, by removing the Pod from the service
    or restarting the Pod.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果探针失败超过配置的次数，管理Pod的kubelet将采取预定的行动，例如从服务中移除Pod或重新启动Pod。
- en: 'Kubernetes currently supports three different kinds of probes:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes目前支持三种不同类型的探针：
- en: '**Readiness**: This decides whether the Pod is ready to process incoming requests.
    If the application needs some time to start up, this probe ensures that no traffic
    is sent to the Pod until this probe passes. Also, if the probe fails while it''s
    running, the platform stops sending any traffic to the Pod until the probe once
    again succeeds. Readiness probes are key to ensuring a zero-downtime experience
    for the user when scaling up or upgrading Pods.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**就绪**：决定 Pod 是否准备好处理传入的请求。如果应用程序需要一些时间来启动，该探针确保在该探针通过之前，不会向 Pod 发送任何流量。此外，如果探针在运行时失败，平台将停止向该
    Pod 发送任何流量，直到探针再次成功。就绪探针对于确保在扩展或升级 Pod 时为用户提供零停机时间的体验至关重要。'
- en: '**Liveness**: This checks to see whether a Pod has a process deadlock or it''s
    crashed without exiting; if so, the platform will kill the Pod.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**存活**：检查 Pod 是否存在进程死锁或崩溃而没有退出；如果是，平台将终止该 Pod。'
- en: '**Startup**: This is used to prevent the platform from killing a Pod that is
    initializing but is slow in starting up. When the startup probe is configured,
    the readiness and liveness probes are disabled until the startup probe passes.
    If the startup probe never passes, the Pod is eventually killed and restarted.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**启动**：用于防止平台终止一个正在初始化但启动很慢的 Pod。当启动探针被配置时，就绪探针和存活探针会被禁用，直到启动探针通过。如果启动探针一直不通过，Pod
    最终会被终止并重新启动。'
- en: Most of the time, you will probably only utilize the readiness and liveness
    probes, unless you have a container that's very slow in starting up.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，你可能只会使用就绪探针和存活探针，除非你有一个启动非常慢的容器。
- en: In the PetBattle Tournament Service component, the liveness and readiness probes
    are configured as follows.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PetBattle Tournament Service 组件中，存活探针和就绪探针的配置如下：
- en: 'In `DeploymentConfig` (or the Deployment), the `/health/live` and `/health/ready`
    URLs are automatically created by the Quarkus framework:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `DeploymentConfig`（或 Deployment）中，`/health/live` 和 `/health/ready` URLs 是由
    Quarkus 框架自动创建的：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Different probes can invoke the same action, but we consider this bad practice.
    The semantics of a readiness probe are different from those of a liveness probe.
    It's recommended that liveness and readiness probes invoke different endpoints
    or actions on the container.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的探针可以调用相同的操作，但我们认为这是不好的做法。就绪探针的语义与存活探针的语义不同。建议存活探针和就绪探针调用容器上的不同端点或操作。
- en: For example, a readiness probe can invoke an action that verifies whether an
    application can accept requests. If during the Pod's lifetime the readiness probe
    fails, Kubernetes will stop sending requests to the Pod until the probe is successful
    again.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，就绪探针可以调用一个操作来验证应用程序是否能够接受请求。如果在 Pod 的生命周期内，就绪探针失败，Kubernetes 将停止向该 Pod 发送请求，直到该探针再次成功。
- en: A liveness probe is one that verifies whether an application can process a request
    successfully; for example, if an application were blocked or accepting a request
    but waiting a long time for a database connection to become available, the probe
    would fail and Kubernetes would restart the Pod. Think of liveness probes as the
    Kubernetes equivalent of the IT Crowd[2](#footnote-180) way of working.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 存活探针是验证应用程序是否能成功处理请求的探针；例如，如果应用程序被阻塞，或者正在接受请求但长时间等待数据库连接变得可用，那么探针会失败，Kubernetes
    将重新启动 Pod。可以把存活探针看作是 Kubernetes 版本的 IT Crowd[2](#footnote-180) 工作方式。
- en: Domino Effect
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多米诺效应
- en: One question that we get asked a lot is, should a health check reflect the state
    of the application's downstream dependencies as well as the application itself?
    The absolute, definitive answer is *it depends*. Most of the time, a health check
    should only focus on the application, but there are always scenarios where this
    isn't the case.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常被问到的一个问题是，健康检查是否应该反映应用程序下游依赖项的状态以及应用程序本身的状态？绝对的、明确的答案是 *这取决于情况*。大多数情况下，健康检查应该只关注应用程序，但总是存在一些例外情况。
- en: If your health check functionality does a deep check of downstream systems,
    this can be expensive and result in cascading failures, where a downstream system
    has an issue and an upstream Pod is restarted due to this downstream issue. Some
    legacy downstream systems may not have health checks, and a more appropriate approach
    in this scenario is to add resilience and fault tolerance to your application
    and architecture.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的健康检查功能对下游系统进行了深度检查，这可能会非常昂贵，并且可能导致级联故障，即下游系统出现问题时，上游 Pod 会因下游问题而被重启。一些遗留的下游系统可能没有健康检查，在这种情况下，更合适的方法是为你的应用程序和架构添加弹性和容错能力。
- en: Fault Tolerance
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 故障容错
- en: A key aspect of this is to utilize a **circuit breaker** pattern when invoking
    dependencies. Circuit breakers can *short circuit* the invocation of downstream
    systems when they detect that previous calls have failed. This can give the downstream
    system time to recover or restart without having to process incoming traffic.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点的关键是，在调用依赖时利用**熔断器**模式。当熔断器检测到之前的调用失败时，它可以*短路*下游系统的调用。这可以给下游系统留出恢复或重启的时间，而不必处理进入的流量。
- en: The basic premise of a circuit breaker is that in the case of the failure of
    a downstream system, the upstream system should just assume that the next request
    will fail and not send it. It potentially also takes appropriate actions for recovery
    by, say, returning a default value.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 熔断器的基本前提是，当下游系统出现故障时，上游系统应该假设下一个请求也会失败，并且不发送该请求。它还可能采取适当的恢复措施，例如返回默认值。
- en: After a given period of time, known as the backoff period, the upstream system
    should try sending a request to the downstream system, and if that succeeds, it
    reverts to normal processing. The rationale behind the backoff period is to avoid
    the situation where the upstream systems overwhelm the downstream systems with
    requests as soon as it starts up.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个给定的时间段（称为回退期）后，上游系统应该尝试向下游系统发送请求，如果成功，则恢复正常处理。回退期的目的是避免上游系统在下游系统启动时立即向其发送过多请求，导致下游系统被压垮。
- en: '[2](#footnote-180-backlink) [https://www.quotes.net/mquote/901983](https://www.quotes.net/mquote/901983)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[2](#footnote-180-backlink) [https://www.quotes.net/mquote/901983](https://www.quotes.net/mquote/901983)'
- en: 'Circuit breaker functionality can be performed at an individual level within
    an application''s code: multiple frameworks such as Quarkus, Netflix Hystrix,
    and Apache Camel support circuit breakers and other fault-tolerant components.
    Check out the Quarkus fault tolerance plugin for more details.[3](#footnote-179)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 熔断器功能也可以在应用代码的单独级别上执行：多个框架，如Quarkus、Netflix Hystrix和Apache Camel，都支持熔断器和其他故障容错组件。有关更多详细信息，请查看Quarkus故障容错插件。[3](#footnote-179)
- en: 'Platform-level circuit breaker functionality is provided by the **service mesh**
    component within OpenShift. This has various substantial advantages over application-level
    circuit breakers:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 平台级熔断器功能由OpenShift中的**服务网格**组件提供。与应用级熔断器相比，它具有多个显著的优势：
- en: It can be used on any container communicating via HTTP/HTTPS. A sidecar proxy
    is used to inject the circuit breaker functionality without having to modify the
    code.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以用于任何通过HTTP/HTTPS通信的容器。侧车代理被用来注入熔断器功能，而无需修改代码。
- en: It provides dynamic configuration of the circuit breaker functionality.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供熔断器功能的动态配置。
- en: It provides metrics and visibility of the state of circuit breakers throughout
    the platform.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供关于熔断器状态的指标和可视化，贯穿整个平台。
- en: Service mesh also provides other fault-tolerance functionalities, such as timeout
    and retries.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务网格还提供其他故障容错功能，如超时和重试。
- en: Logging
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 日志记录
- en: '*Ahh, logging!* No *true* developer[4](#footnote-178) has earned their stripes
    until they''ve spent countless hours of their existence trawling through production
    logs trying to figure out exactly what went wrong when a user clicked "Confirm".
    If you have managed to do this across multiple log files, all hosted on separate
    systems via multiple terminal windows, then you are truly righteous in the eyes
    of the IDE-bound masses.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*啊，日志记录！* 没有*真正*的开发者[4](#footnote-178)能在他们的职业生涯中获得应有的认可，直到他们在生产日志中花费无数小时，试图找出用户点击“确认”按钮时究竟出了什么问题。如果你曾经在多个日志文件中完成过这项工作，这些文件都托管在不同的系统上，并通过多个终端窗口进行查看，那么你在IDE依赖的开发者眼中就是个真正的英雄。'
- en: The good news is that application logging on Kubernetes is a first-class citizen
    on the platform—just configure your application to write its logs to STDOUT and
    the platform will pick it up and you can view/trawl through them. OpenShift goes
    one level deeper by shipping an aggregated logging stack with EFK (Elasticsearch,
    Fluentd, and Kibana) out of the box. This allows developers to search and view
    logs across multiple containers running on multiple nodes across the cluster.
    If you want to give this a try, follow the documentation at [https://docs.openshift.com/container-platform/4.7/logging/cluster-logging-deploying.html](https://docs.openshift.com/container-platform/4.7/logging/cluster-logging-deploying.html).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，Kubernetes 上的应用日志记录是该平台的一级公民——只需配置应用程序将日志写入 STDOUT，平台会自动捕捉并让你查看/筛选这些日志。OpenShift
    更进一步，默认提供一个聚合的日志栈，包含 EFK（Elasticsearch、Fluentd 和 Kibana）。这使得开发人员可以跨多个节点上的多个容器搜索和查看日志。如果你想试试看，可以参考
    [https://docs.openshift.com/container-platform/4.7/logging/cluster-logging-deploying.html](https://docs.openshift.com/container-platform/4.7/logging/cluster-logging-deploying.html)
    中的文档。
- en: '[3](#footnote-179-backlink) [https://quarkus.io/guides/smallrye-fault-tolerance](https://quarkus.io/guides/smallrye-fault-tolerance)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[3](#footnote-179-backlink) [https://quarkus.io/guides/smallrye-fault-tolerance](https://quarkus.io/guides/smallrye-fault-tolerance)'
- en: '[4](#footnote-178-backlink) [https://en.wikipedia.org/wiki/No_true_Scotsman](https://en.wikipedia.org/wiki/No_true_Scotsman)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[4](#footnote-178-backlink) [https://en.wikipedia.org/wiki/No_true_Scotsman](https://en.wikipedia.org/wiki/No_true_Scotsman)'
- en: Tracing
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跟踪
- en: 'So, first things first: no, tracing is not application logging running at the
    trace log level. When it comes to OpenShift, tracing is the functionality added
    to the Kubernetes platform that enables developers to trace a request across a
    distributed set of application components running in different containers on different
    nodes of a cluster. Tracing is an exceptionally useful tool used to determine
    and visualize inter-service/component dependencies and performance/latency blackholes
    throughout a distributed system.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，首先：不，跟踪并不是在跟踪日志级别下运行的应用日志记录。对于 OpenShift，跟踪是添加到 Kubernetes 平台的功能，它使开发人员能够跨分布式的应用组件集（这些组件运行在集群中不同节点的不同容器中）跟踪请求。跟踪是一个非常有用的工具，用于确定和可视化分布式系统中各服务/组件之间的依赖关系以及性能/延迟瓶颈。
- en: Tracing is provided as part of the OpenShift service mesh component. The underlying
    tracing functionality is provided by the Jaeger[5](#footnote-177) distributed
    tracing platform. To support tracing, applications must include a client library
    that sends instrumented request metadata to a Jaeger collector, which in turn
    processes it and stores the data. This data can then be queried to help visualize
    the end-to-end request workflow. The Jaeger client libraries are language-specific
    and utilize the vendor-neutral OpenTracing specification.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪功能是作为 OpenShift 服务网格组件的一部分提供的。底层的跟踪功能由 Jaeger[5](#footnote-177) 分布式跟踪平台提供。为了支持跟踪，应用程序必须包含一个客户端库，该库将仪表化的请求元数据发送到
    Jaeger 收集器，收集器处理并存储数据。然后，可以查询这些数据，以帮助可视化端到端请求工作流。Jaeger 客户端库是特定于语言的，并采用供应商中立的
    OpenTracing 规范。
- en: If you're thinking, "*Woah!* *Collecting metadata for every request would be
    very expensive to store and process*," you'd be right. Jaeger *can* do this, but
    for scale purposes it's better to record and process a *sample* of requests, rather
    than each and every one of them.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在想，"*哇！* *为每个请求收集元数据将非常昂贵，需要存储和处理*"，你是对的。Jaeger *可以* 这么做，但出于规模考虑，最好记录和处理一部分请求，而不是每一个请求。
- en: Metrics
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指标
- en: Probes are useful for telling when an application is ready to accept traffic,
    or whether it is stuck. Tracing is great at providing a measure of latency throughout
    a distributed system, while logging is a great tool to retrospectively understand
    exactly what occurred and when it occurred.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 探针对于判断一个应用是否准备好接受流量，或者它是否卡住了，十分有用。跟踪对于提供分布式系统中的延迟度量非常有效，而日志记录则是一个非常有用的工具，能让你回顾并准确理解发生了什么以及何时发生。
- en: However, to comprehend the deep state (no, not *that* deep state!) of a system
    and potentially predict its future state after a period of time, you need to measure
    some of the key quantitative characteristics of the system and visualize/compare
    them over a period of time.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，要理解一个系统的深层状态（不，不是*那个*深层状态！）并可能预测其在一段时间后的未来状态，你需要衡量系统的一些关键定量特征，并在一段时间内对其进行可视化/比较。
- en: The good news is that metrics are relatively easy to obtain; you can get them
    from infrastructure components and software components such as JVM, and you can
    also add domain-specific/custom metrics to your application.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，指标相对容易获取；你可以从基础设施组件和软件组件（如 JVM）中获取指标，也可以向应用程序中添加特定领域/自定义的指标。
- en: '[5](#footnote-177-backlink) [https://www.jaegertracing.io/](https://www.jaegertracing.io/)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[5](#footnote-177-backlink) [https://www.jaegertracing.io/](https://www.jaegertracing.io/)'
- en: Given the multitude of metrics available, the hard bit is figuring out which
    metrics are valuable to your role and need to be retained; for example, for application
    operator connection pool counts, JVM garbage collection pause times are invaluable.
    For a Kubernetes platform operator, JVM garbage collection pause times are less
    critical, but metrics from platform components, such as etcd-related metrics,
    are crucial.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于可用的指标众多，困难的部分是弄清楚哪些指标对你的角色有价值并需要保留；例如，对于应用程序操作员来说，连接池计数和 JVM 垃圾回收暂停时间是极为宝贵的。对于
    Kubernetes 平台操作员来说，JVM 垃圾回收暂停时间不那么关键，但平台组件的指标，如与 etcd 相关的指标，则至关重要。
- en: The good news is that OpenShift provides metrics for both the cluster and the
    applications running on it. In this section, we're going to focus on the application-level
    perspective. In the Kubernetes community, the *de facto* approach is to use Prometheus[6](#footnote-176)
    for gathering and Grafana[7](#footnote-175) for the visualization of metrics.
    This doesn't mean that you can't use other metrics solutions, and there are some
    very good ones out there with additional features.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，OpenShift 提供了集群以及运行在其上的应用程序的指标。在本节中，我们将重点关注应用程序级别的视角。在 Kubernetes 社区中，*事实上的*做法是使用
    Prometheus[6](#footnote-176) 来收集指标，使用 Grafana[7](#footnote-175) 来可视化指标。这并不意味着你不能使用其他的指标解决方案，实际上市场上有一些非常优秀的方案，提供了额外的功能。
- en: OpenShift ships with both Prometheus and Grafana as the default metrics stack.
    Additionally, it also ships with the Prometheus Alertmanager. The Alertmanager
    facilitates the sending of notifications to operators when metric values indicate
    that something is going or has gone wrong and *la merde* has or is about to hit
    the fan. Examples of this include a high number of threads or large JVM garbage
    collection pause times.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 默认提供 Prometheus 和 Grafana 作为指标栈。此外，它还附带了 Prometheus Alertmanager。Alertmanager
    可以在指标值显示某些东西出现问题时（例如线程数量过高或 JVM 垃圾回收暂停时间过长），向操作员发送通知，表明“*麻烦来了*”。例如，线程数量过多或 JVM
    垃圾回收暂停时间过长等情况。
- en: 'Great, so how do we enable this for PetBattle? It is relatively straightforward:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，那么我们如何为 PetBattle 启用这个功能呢？其实非常简单：
- en: Use a metrics framework in your application that records metrics and exposes
    the metrics to Prometheus.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的应用程序中使用一个记录指标并将指标暴露给 Prometheus 的指标框架。
- en: Configure Prometheus to retrieve the metrics from the application.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置 Prometheus 从应用程序中获取指标。
- en: Visualize the metrics in OpenShift.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 OpenShift 中可视化指标。
- en: Once the metrics are being retrieved, the final step is to configure an alert
    using the Prometheus Alertmanager.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦指标被获取，最后一步是使用 Prometheus Alertmanager 配置一个警报。
- en: Gather Metrics in the Application
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在应用程序中收集指标
- en: Taking the PetBattle Tournament service component as an example, this is developed
    using the Quarkus Java framework. Out of the box, Quarkus supports/recommends
    the use of the open-source Micrometer metrics framework.[8](#footnote-174)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 以 PetBattle Tournament 服务组件为例，它是使用 Quarkus Java 框架开发的。开箱即用，Quarkus 支持并推荐使用开源的
    Micrometer 指标框架。[8](#footnote-174)
- en: '[6](#footnote-176-backlink) [https://prometheus.io/](https://prometheus.io/)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[6](#footnote-176-backlink) [https://prometheus.io/](https://prometheus.io/)'
- en: '[7](#footnote-175-backlink) [https://grafana.com/oss/](https://grafana.com/oss/)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[7](#footnote-175-backlink) [https://grafana.com/oss/](https://grafana.com/oss/)'
- en: '[8](#footnote-174-backlink) [https://micrometer.io/](https://micrometer.io/)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[8](#footnote-174-backlink) [https://micrometer.io/](https://micrometer.io/)'
- en: 'To add this to the Tournament service, we simply need to add the dependency
    to the Maven POM along with the Prometheus dependency. For example:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 要将此功能添加到 Tournament 服务中，我们只需要将依赖项添加到 Maven POM 文件中，并且添加 Prometheus 依赖。例如：
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Then, we configure a Prometheus registry, which is used to store the metrics
    locally in the application before being retrieved by the Prometheus collector.
    This is done in the `src/main/resources/application.properties` file.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们配置一个 Prometheus 注册表，用于在 Prometheus 收集器获取之前将指标存储在应用程序本地。这是在 `src/main/resources/application.properties`
    文件中完成的。
- en: '[PRE29]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'With this configuration, the Prometheus endpoint is exposed by the application
    Pod. Let''s go ahead and test it:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此配置，Prometheus端点由应用程序Pod暴露。我们继续进行测试：
- en: '[PRE36]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: If successful, you should get an output similar to the above. Notice that you're
    not just getting the application-level metrics—the MongoDB connection pool metrics
    are also there. These are automatically added by the Quarkus framework once configured
    in the `application.properties` file.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果成功，你应该得到类似于上述的输出。注意，你不仅获得了应用程序级别的度量数据——MongoDB连接池的度量数据也在其中。这些数据在`application.properties`文件配置后，由Quarkus框架自动添加。
- en: Configuring Prometheus To Retrieve Metrics From the Application
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置Prometheus从应用程序中检索度量数据
- en: Prometheus is somewhat unusual in its mode of operation. Rather than having
    some sort of agent pushing metrics to a central collector, it uses a pull model
    where the collector retrieves/scrapes metrics from a known HTTP/HTTPS endpoint
    exposed by the applications. In our case, as seen above, we're exposing metrics
    using the `/metrics` endpoint.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus在操作模式上有些不同寻常。它不像某些代理那样将度量数据推送到中央收集器，而是采用拉取模式，收集器从应用程序暴露的已知HTTP/HTTPS端点检索/抓取度量数据。在我们的例子中，正如上面所见，我们通过`/metrics`端点暴露度量数据。
- en: So how does the Prometheus collector know when, where, and how to gather these
    metrics? OpenShift uses a Prometheus operator[9](#footnote-173) that simplifies
    configuring Prometheus to gather metrics. We just need to deploy a `ServiceMonitor`
    object to instruct Prometheus on how to gather our application metrics.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，Prometheus收集器如何知道何时、何地以及如何收集这些度量数据呢？OpenShift使用一个Prometheus操作符[9](#footnote-173)，简化了配置Prometheus收集度量数据的过程。我们只需部署一个`ServiceMonitor`对象，来指导Prometheus如何收集我们的应用程序度量数据。
- en: '[PRE53]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[9](#footnote-173-backlink) [https://github.com/prometheus-operator/prometheus-operator](https://github.com/prometheus-operator/prometheus-operator)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[9](#footnote-173-backlink) [https://github.com/prometheus-operator/prometheus-operator](https://github.com/prometheus-operator/prometheus-operator)'
- en: 'There are a few things to note that might save you some time when trying to
    understand this configuration: basically, this configuration will scrape associated
    components every 30 seconds using the default HTTP path `/metrics`. Now, `port:
    tcp-8080` is mapped to the port name in the service—see below, highlighted in
    bold. If the service had a port name of `web`, then the configuration would be
    `port: web`.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '在尝试理解此配置时，有几点需要注意，这可能为你节省一些时间：基本上，此配置将在每30秒抓取一次相关组件的数据，使用默认的HTTP路径`/metrics`。现在，`port:
    tcp-8080`映射到服务中的端口名称——见下文，以粗体突出显示。如果服务的端口名称是`web`，那么配置将是`port: web`。'
- en: '[PRE68]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: User workload monitoring needs to be enabled at the cluster level before ServiceMonitoring
    will work.[10](#footnote-172) This is also a classic demonstration of two of the
    major, powerful, misunderstood, and unused features of Kubernetes, *labels* and
    *label selectors*.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 用户工作负载监控需要在集群级别启用，才能使ServiceMonitoring生效。[10](#footnote-172) 这也是Kubernetes两个强大但常被误解和未充分使用的主要特性之一的经典示例，*标签*和*标签选择器*。
- en: '[10](#footnote-172-backlink) [https://docs.openshift.com/container-platform/4.7/monitoring/enabling-monitoring-for-user-defined-projects.html](https://docs.openshift.com/container-platform/4.7/monitoring/enabling-monitoring-for-user-defined-projects.html)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[10](#footnote-172-backlink) [https://docs.openshift.com/container-platform/4.7/monitoring/enabling-monitoring-for-user-defined-projects.html](https://docs.openshift.com/container-platform/4.7/monitoring/enabling-monitoring-for-user-defined-projects.html)'
- en: 'The following line means that Prometheus will attempt to retrieve metrics from
    all components that have the label `app.kubernetes.io/component: pet-battle-tournament`.
    We don''t need to list each component independently; we just need to ensure that
    the component has the correct *label*, and that the *selector* is used to match
    that label. If we add a new component to the architecture, then all we have to
    do is ensure that it has the correct label. Of course, all of this assumes that
    the method of scraping the metrics is consistent across all of the selected components;
    that they are all using the `tcp-8080 port`, for example.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '以下行表示Prometheus将尝试从所有具有`app.kubernetes.io/component: pet-battle-tournament`标签的组件中检索度量数据。我们无需单独列出每个组件；只需要确保该组件具有正确的*标签*，并且使用*选择器*来匹配该标签。如果我们在架构中添加了新组件，那么只需确保它具有正确的标签。当然，所有这些都假设抓取度量数据的方法在所有选定组件之间是一致的；例如，它们都使用`tcp-8080端口`。'
- en: '[PRE89]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'We''re big fans of labels and their associated selectors. They''re very powerful
    as a method of grouping components: Pods, Services, and so on. It''s one of those
    hidden gems that you wish you knew of earlier.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们非常喜欢标签及其关联的选择器。它们作为一种分组组件的方法非常强大：Pods、Services等。它是一个隐藏的宝藏，你希望早些知道。
- en: Visualizing the Metrics in OpenShift
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在OpenShift中可视化度量数据
- en: Once we have our metrics retrieved, we need to interpret the information that
    they're conveying about the system.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们检索到度量数据，我们需要解释它们所传递的系统信息。
- en: Querying using Prometheus
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Prometheus进行查询
- en: To visualize the metrics, go into the developer console and click on **Monitoring
    (1)**, as shown in *Figure 16.1*. Then click on **Custom Query (2)** in the dropdown,
    and enter a query using the **Prometheus query language (PromQL) (3)**. In the
    following example, we've used the *http_server_requests_seconds_count* metric,
    but there are others as well.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化度量数据，请进入开发者控制台并点击**监控 (1)**，如*图16.1*所示。然后点击下拉菜单中的**自定义查询 (2)**，并使用**Prometheus
    查询语言 (PromQL) (3)** 输入查询。在下面的示例中，我们使用了*http_server_requests_seconds_count*度量指标，但也有其他的度量指标。
- en: '![](img/B16297_16_01.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16297_16_01.jpg)'
- en: 'Figure 16.1: PetBattle tournament metrics'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1：PetBattle比赛度量数据
- en: Let's explore some of the built-in dashboards OpenShift provides for monitoring.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索OpenShift提供的一些内置仪表盘，用于监控。
- en: Visualizing Metrics Using Grafana
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Grafana可视化度量数据
- en: OpenShift comes with dedicated Grafana dashboards for cluster monitoring. It
    is not possible to modify these dashboards and add custom application metrics,
    but it is possible to deploy an application-specific Grafana instance and customize
    that as we see fit. To do this, we first need to ensure that the Grafana Operator
    is installed in the namespace that we're using.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift自带了用于集群监控的专用Grafana仪表盘。无法修改这些仪表盘并添加自定义的应用程序度量数据，但可以部署一个特定于应用程序的Grafana实例并根据需要进行定制。为此，我们首先需要确保Grafana
    Operator已安装在我们使用的命名空间中。
- en: 'We will then deploy a custom Grafana setup by deploying the following custom
    resources:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将通过部署以下自定义资源来部署一个自定义的Grafana设置：
- en: A *Grafana resource* used to create a custom *grafana* instance in the namespace
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*Grafana资源*，用于在命名空间中创建自定义的*grafana*实例
- en: A *GrafanaDataSource resource* to pull metrics from the cluster-wide Prometheus
    instance
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*GrafanaDataSource资源*，用于从集群范围内的Prometheus实例中拉取度量数据
- en: A *GrafanaDashboard* resource for creating the dashboard
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*GrafanaDashboard*资源，用于创建仪表盘
- en: 'The good news is that all of this is done via Helm charts, so you just have
    to do the following:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，所有这些操作都是通过Helm图表完成的，所以你只需要做以下操作：
- en: '[PRE92]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: Open `grafana-route` in a browser, log in, *et voila*! It should look something
    like that shown in *Figure 16.2\.* If there is an error with no data, check the
    BEARER_TOKEN is in place. This can be fixed manually by running the commands at
    [https://github.com/petbattle/pet-battle-infra/blob/main/templates/insert-bearer-token-hook.yaml#L80](https://github.com/petbattle/pet-battle-infra/blob/main/templates/insert-bearer-token-hook.yaml#L80)
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中打开`grafana-route`，登录，*看！* 它应该类似于*图16.2*所示。如果出现没有数据的错误，请检查BEARER_TOKEN是否已经设置。可以通过运行以下命令手动修复：[https://github.com/petbattle/pet-battle-infra/blob/main/templates/insert-bearer-token-hook.yaml#L80](https://github.com/petbattle/pet-battle-infra/blob/main/templates/insert-bearer-token-hook.yaml#L80)
- en: '![](img/B16297_16_02.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16297_16_02.jpg)'
- en: 'Figure 16.2: PetBattle metrics in Grafana'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2：Grafana中的PetBattle度量数据
- en: We will now take a look at some of the tools that can help us further with observability.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看看一些可以帮助我们进一步进行可观察性的工具。
- en: Metadata and Traceability
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 元数据和可追溯性
- en: With the adoption of independently deployable services-based architectures,
    the complexity of managing these components and their inter-relationships is becoming
    problematic. In the following sections, we will outline a number of techniques
    that can assist you with this.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 随着独立可部署的基于服务的架构的采用，管理这些组件及其相互关系的复杂性变得越来越有问题。在接下来的章节中，我们将概述一些可以帮助你解决这一问题的技术。
- en: Labels
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标签
- en: 'As mentioned earlier, labels and label selectors are among the more powerful
    *metadata management* features of Kubernetes. At its core, labels are a collection
    of text-based key/value pairs that can be attached to one or more objects: Pods,
    services, Deployments, and so on. Labels are intended to add information/semantics
    to objects that are relevant to the user and not the core Kubernetes system. A
    label selector is a method by which a user can group items together that have
    the same labels.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，标签和标签选择器是 Kubernetes 中更强大的*元数据管理*功能之一。从核心来看，标签是基于文本的键/值对集合，可以附加到一个或多个对象上：Pods、服务、部署等等。标签旨在为用户相关的对象添加信息/语义，而不是核心
    Kubernetes 系统。标签选择器是一种方法，用户可以通过它将具有相同标签的项分组在一起。
- en: One of the most common uses of labels and label selectors in Kubernetes is the
    way that services use label selectors to group related Pods as endpoints for the
    service.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，标签和标签选择器最常见的用途之一是服务使用标签选择器将相关的 Pods 分组为服务的端点。
- en: It's probably better shown by way of an example.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个示例展示可能更容易理解。
- en: 'So, let''s start with our three Infinispan Pods. Given that the Infinispan
    operator deploys its Pods via StatefulSets, the Pod names are pretty straightforward:
    `infinispan-0`, `infinispan-1`, `infinispan-2`. Take note of the labels attached
    to the Pods (highlighted in bold).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们从三个 Infinispan Pods 开始。鉴于 Infinispan 操作符通过 StatefulSets 部署 Pods，Pod 名称非常直接：`infinispan-0`、`infinispan-1`、`infinispan-2`。注意附加到
    Pods 上的标签（以粗体显示）。
- en: '[PRE95]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: When the Tournament service wants to connect to one of these Infinispan pods,
    it uses the Infinispan service that is also created and managed by the operator.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Tournament 服务想要连接到这些 Infinispan Pods 之一时，它使用由操作符创建和管理的 Infinispan 服务。
- en: '[PRE100]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'If we go into the definition of the service, we''ll see the selector (highlighted
    in bold):'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们进入服务的定义，就会看到选择器（以粗体显示）：
- en: '[PRE103]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '[PRE118]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'This adds the Pods with the labels `app=infinispan-pod,clusterName=infinispan`
    into the service as endpoints. Two things to note here: the selector didn''t use
    all the labels assigned to the Pod; and if we scaled up the number of Infinispan
    Pods, the selector would be continuously assessed and the new Pods automatically
    added to the service. The preceding example is a pretty basic example of a selector;
    in fact, selectors are far more powerful, with equality- and set-based operations
    also available. Check out the examples in the Kubernetes documentation for more
    information.[11](#footnote-171)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这将带有标签 `app=infinispan-pod,clusterName=infinispan` 的 Pods 添加到服务中作为端点。这里有两点需要注意：选择器并没有使用分配给
    Pod 的所有标签；如果我们扩展了 Infinispan Pods 的数量，选择器会持续进行评估，并自动将新的 Pods 添加到服务中。上述示例是一个非常基础的选择器示例；事实上，选择器要强大得多，支持等式操作和基于集合的操作。请查看
    Kubernetes 文档中的示例了解更多信息。[11](#footnote-171)
- en: '[11](#footnote-171-backlink) [https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[11](#footnote-171-backlink) [https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)'
- en: Great, so now what? What information could you use to label a resource? It depends
    on what your needs are. As demonstrated previously in the monitoring section,
    labels and selectors can be useful in configuring Prometheus. Labels can also
    be useful in grouping components together, as in the components that comprise
    a distributed application.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，那么接下来呢？你可以使用什么信息来标记一个资源？这取决于你的需求。如前面在监控部分所示，标签和选择器在配置 Prometheus 时非常有用。标签还可以用于将组件分组在一起，例如组成分布式应用程序的组件。
- en: 'Kubernetes has a set of recommended labels[12](#footnote-170) that we''ve used
    when building and deploying the PetBattle application:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了一组推荐的标签[12](#footnote-170)，我们在构建和部署 PetBattle 应用程序时使用了这些标签：
- en: '![](img/B16297_Table_16.1.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16297_Table_16.1.png)'
- en: 'Table 16.1: Kubernetes-recommended labels'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 表 16.1：Kubernetes 推荐的标签
- en: '[12](#footnote-170-backlink) [https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/](https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[12](#footnote-170-backlink) [https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/](https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/)'
- en: 'With these labels in place, it is possible to retrieve and view the components
    of the application using selectors, such as to show the component parts of the
    PetBattle application without the supporting application infrastructure, that
    is, Infinispan or Keycloak. The following command demonstrates this:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些标签，就可以使用选择器检索并查看应用程序的组件，例如显示 PetBattle 应用的组件部分，而不包括支持应用程序基础设施，即 Infinispan
    或 Keycloak。以下命令演示了这一点：
- en: '[PRE122]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '[PRE125]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '[PRE126]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '[PRE129]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[PRE130]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[PRE131]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: NAME AGE
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 名称 年龄
- en: '[PRE133]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: NAME AGE
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 名称 年龄
- en: '[PRE134]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: Let's look at other mechanisms we can use to enhance traceability.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看其他可以用来增强可追溯性的方法。
- en: Software Traceability
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 软件可追溯性
- en: One of the issues that we've observed from customers over the years is the reliance
    that people have on the name of the software artifact that they're putting into
    production, such as `super-important-app-1.2.99.0.bin or critical-service-1.2.jar`.
    While this works 99.9% of the time, occasionally we've noticed issues where an
    incorrect version has been deployed with interesting outcomes.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，我们从客户那里观察到的一个问题是，许多人依赖于他们投入生产的软件工件的名称，例如 `super-important-app-1.2.99.0.bin`
    或 `critical-service-1.2.jar`。虽然这种方式在99.9%的情况下有效，但偶尔我们会发现有错误的版本被部署，产生了有趣的结果。
- en: In the land of containers, your deployment is a versioned artifact that contains
    a version of your software, and this in turn may be deployed using a versioned
    Helm chart via a GitOps approach. A good build and deployment pipeline will ensure
    that these levels of artifact versioning will always be consistent and provide
    traceability. As a backup, we also add additional traceability to the deployed
    artifacts as annotations on the resources and build info logging in the application
    binary.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在容器领域，你的部署是一个版本化的工件，包含了你的软件版本，而这又可以通过 GitOps 方法使用版本化的 Helm chart 进行部署。一个良好的构建和部署流水线将确保这些工件版本的各个层级始终保持一致，并提供可追溯性。作为备份，我们还通过在资源上添加注解和在应用二进制文件中记录构建信息来增强已部署工件的可追溯性。
- en: Annotations
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注解
- en: Annotations are similar to Kubernetes labels—that is, string-based key/value
    pairs—except that they're not used to group or identify objects via selectors.
    Annotations can be used to store different types of information; in our case,
    we're going to use annotations to store Git information to help with software
    traceability.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 注解类似于 Kubernetes 标签——即基于字符串的键/值对——不同之处在于它们并不用于通过选择器来分组或识别对象。注解可以用来存储不同类型的信息；在我们的案例中，我们将使用注解来存储
    Git 信息，以帮助软件的可追溯性。
- en: '[PRE135]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: '[PRE136]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: '[PRE137]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '[PRE138]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: '[PRE139]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: '[PRE140]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: '[PRE141]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: '[PRE143]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: '[PRE144]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: '[PRE145]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: The annotations are automatically added as part of the Maven build process using
    the Quarkus Maven plugin. Also notice the annotations are used to provide scrape
    information for Prometheus, as can be seen highlighted in the preceding code.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 注解是通过使用 Quarkus Maven 插件，作为 Maven 构建过程的一部分自动添加的。同时请注意，注解也用于为 Prometheus 提供抓取信息，正如前面代码中所突出显示的那样。
- en: Build Information
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建信息
- en: An approach that has nothing to do with Kubernetes per se, but we strongly recommend
    to be used in general, is to output source control and build information as part
    of the application startup. An example of this is embedded into the Tournament
    service.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种与 Kubernetes 本身无关的方法，但我们强烈推荐一般使用，它是在应用启动时输出源代码控制和构建信息。一个例子嵌入在 Tournament
    服务中。
- en: '[PRE146]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: '[PRE147]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: '[PRE148]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: '[PRE149]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: '[PRE151]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: '[PRE152]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: '[PRE153]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: We use the Maven plugin `git-commit-id-plugin` to generate a file containing
    the Git information and package that file as part of the **Java archive** (**jar**).
    On startup, we simply read this file and output its contents to the console. Very
    simple stuff, but very effective and a lifesaver when needed. When running on
    OpenShift, this information will be picked up by the OpenShift logging components.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Maven 插件 `git-commit-id-plugin` 来生成包含 Git 信息的文件，并将该文件打包为 **Java 压缩包**（**jar**）的一部分。在启动时，我们只需读取该文件并将其内容输出到控制台。非常简单的操作，但非常有效，并且在需要时可以救命。当在
    OpenShift 上运行时，这些信息将被 OpenShift 日志组件收集。
- en: Alerting
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 警报
- en: So we have all the metrics to provide us with some insight into how the system
    is performing. We've got spectacular graphs and gauges in Grafana but we're hardly
    going to sit watching them all day to see if something happens. It's time to add
    alerting to the solution.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们拥有所有的度量指标，可以让我们对系统的表现有一些洞察。我们在 Grafana 中有壮观的图表和仪表，但我们几乎不可能整天盯着它们看，以查看是否发生了什么事情。现在是时候为解决方案添加警报功能了。
- en: What Is an Alert?
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是警报？
- en: 'An alert is an event that is generated when some measurement threshold (observed
    or calculated) is about to be or has been breached. The following are some examples
    of alerts:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 警报是当某个测量阈值（观察到或计算出的）即将被突破或已经突破时生成的事件。以下是一些警报的例子：
- en: The average system response time in the last five minutes goes above 100 milliseconds.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统在过去五分钟内的平均响应时间超过了100毫秒。
- en: The number of currently active users on the site falls below a certain threshold.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前活跃用户数低于某个阈值。
- en: Application memory usage is approaching its maximum limit.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序的内存使用量接近其最大限制。
- en: Alerts usually result in notifications being sent to human operators, whether
    that is through an email or instant message, say. Notifications can also be sent
    to trigger automation scripts/processes to deal with the alert. Service owners
    can analyze their existing alerts to help improve the reliability of their services
    and systems and reduce the manual work associated with remediating problems.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 警报通常会导致通知发送给人工操作员，无论是通过电子邮件还是即时消息等方式。通知还可以发送触发自动化脚本/流程来处理警报。服务所有者可以分析他们现有的警报，帮助提高服务和系统的可靠性，并减少修复问题时的手动工作量。
- en: Why Alert?
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么需要警报？
- en: Alerts call for human action when a situation has arisen within the system that
    cannot be automatically handled. This may include scenarios where automatic resolution
    of the problem is deemed too risky and human intervention is required to help
    triage, mitigate, and resolve the issue. Alerting can also be an issue by causing
    concern for site reliability engineers who manage and operate the system, particularly
    when alerts are numerous, misleading, or don't really help in problem cause analysis.
    They may generate benign alerts that don't prompt any action.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 当系统内发生无法自动处理的情况时，警报就需要人工干预。这可能包括自动解决问题被认为过于危险，或需要人工干预来帮助初步处理、缓解和解决问题的场景。警报也可能成为网站可靠性工程师的一个问题，特别是当警报数量众多、误导性强或对问题原因分析没有帮助时。它们可能会产生无害的警报，但不会引起任何行动。
- en: There are certain qualities that make up a *good alert*. Alerts should be actionable
    by the human beings who respond to them. To be actionable, the alert must also
    have arrived in time for something to be done about it and it should be delivered
    to the correct team or location for triaging. Alerts can also include helpful
    metadata such as documentation links to assist in making triage faster.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些特点构成了*良好的警报*。警报应该是可操作的，意思是响应警报的人能采取行动。为了具有可操作性，警报必须及时到达，使得能够采取行动，并且应送达正确的团队或位置进行初步处理。警报还可以包含有助于加快初步处理的元数据，例如文档链接。
- en: Alert Types
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 警报类型
- en: We can think of alerts as falling into three broad categories.[13](#footnote-169)
    The first are **proactive** alerts, meaning that your business service or system
    is not in danger yet but may be in trouble after some period of time. A good example
    of this is where your system response time is degrading but it is not at a stage
    where external users would be aware of the issue yet. Another example may be where
    your disk quota is filling up but is not 100% full yet, but it may do in a few
    days' time.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将警报分为三类。[13](#footnote-169) 第一类是**主动**警报，意味着您的业务服务或系统尚未处于危险之中，但在一段时间后可能会出现问题。一个好的例子是，当系统响应时间变差，但尚未到达外部用户会察觉的问题阶段。另一个例子可能是磁盘配额正在填满，但尚未100%满，可能在几天内会达到最大值。
- en: A **reactive** alert means your business service or system is in immediate danger.
    You are about to breach a service level and immediate action is needed to prevent
    the breach.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '**被动**警报意味着您的业务服务或系统正处于紧急危险中。您即将违反服务级别协议，需要立即采取行动以防止违规。'
- en: An **investigative** alert is one where your business service or system is in
    an unknown state. For example, it may be suffering a form of partial failure or
    there may be unusual errors being generated. Another example may be where an application
    is restarting too many times, which is indicative of an unusual crash situation.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '**调查性**警报是指您的业务服务或系统处于未知状态。例如，系统可能正在经历某种形式的部分故障，或可能生成了异常错误。另一个例子可能是某个应用程序重启次数过多，这通常是异常崩溃的标志。'
- en: Each of these alerts may also be directed to different teams, depending on their
    severity. Not all alerts need to be managed with the same level of urgency. For
    example, some alerts must be handled by an on-call human resource immediately,
    while for others it may be fine to handle them during business hours by an application
    business support team the following day. Let's explore how we can easily configure
    and add alerting to our applications using the OpenShift platform features to
    help us out.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这些警报中的每一个也可以根据严重性定向到不同的团队，并非所有警报都需要以相同的紧急程度进行管理。例如，一些警报必须由值班人力资源立即处理，而其他警报可能可以在下一个工作日由应用程序业务支持团队在正常工作时间内处理。让我们探讨如何利用OpenShift平台功能轻松配置并为我们的应用程序添加警报。
- en: '[13](#footnote-169-backlink) [https://www.oreilly.com/content/reduce-toil-through-better-alerting/](https://www.oreilly.com/content/reduce-toil-through-better-alerting/)'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '[13](#footnote-169-backlink) [https://www.oreilly.com/content/reduce-toil-through-better-alerting/](https://www.oreilly.com/content/reduce-toil-through-better-alerting/)'
- en: Managing Alerts
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管理警报
- en: OpenShift has platform monitoring and alerting that supports both built-in platform
    components and user workloads. The product documentation is the best place to
    start when looking to configure these.[14](#footnote-168) As we outlined earlier,
    monitoring and alerting make use of the Prometheus monitoring stack. This is combined
    with an open-source tool called Thanos[15](#footnote-167) that aggregates and
    provides access to multiple instances of Prometheus in our cluster.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift具有平台监控和警报功能，支持内置的平台组件和用户工作负载。产品文档是配置这些功能时的最佳起点。[14](#footnote-168)正如我们之前概述的那样，监控和警报利用了Prometheus监控堆栈。它与一个名为Thanos的开源工具[15](#footnote-167)结合使用，Thanos聚合并提供对我们集群中多个Prometheus实例的访问。
- en: A basic configuration for the PetBattle application suite consists of creating
    two ConfigMaps for user workload monitoring and alerting. We use ArgoCD and a
    simple kustomize YAML configuration to apply these ConfigMaps using GitOps. If
    we open up the ubiquitous journey `values-day2ops.yaml` file, we can create an
    entry for user workload monitoring.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: PetBattle应用程序套件的基本配置包括为用户工作负载监控和警报创建两个ConfigMaps。我们使用ArgoCD和一个简单的kustomize YAML配置来通过GitOps应用这些ConfigMaps。如果我们打开普遍使用的`values-day2ops.yaml`文件，我们可以为用户工作负载监控创建一个条目。
- en: '[PRE155]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: '[PRE156]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: '[PRE157]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: '[PRE158]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: '[PRE159]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: '[PRE160]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: '[PRE161]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: '[PRE162]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: '[PRE163]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: The next step is to make use of application metrics and a ServiceMonitor and
    configure specific Prometheus alerts for our PetBattle suite.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是利用应用程序指标和ServiceMonitor，并为我们的PetBattle套件配置特定的Prometheus警报。
- en: User-Defined Alerts
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用户定义的警报
- en: In the *Metrics* section, we created ServiceMonitors for our API and Tournament
    applications that allow us to collect the micrometer metrics from our Quarkus
    applications. We want to use these metrics to configure our alerts. The simplest
    approach is to browse to the Thanos query endpoint that aggregates all of our
    Prometheus metrics. You can find this in the `openshift-monitoring` project.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Metrics*部分，我们为API和Tournament应用程序创建了ServiceMonitors，允许我们从Quarkus应用程序中收集micrometer指标。我们希望使用这些指标来配置我们的警报。最简单的方法是浏览到聚合我们所有Prometheus指标的Thanos查询端点。你可以在`openshift-monitoring`项目中找到它。
- en: '[PRE164]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE164]'
- en: '[14](#footnote-168-backlink) [https://docs.openshift.com/container-platform/4.7/monitoring/configuring-the-monitoring-stack.html#configuring-the-monitoring-stack](https://docs.openshift.com/container-platform/4.7/monitoring/configuring-the-monitoring-stack.html#configuring-the-monitoring-stack)'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '[14](#footnote-168-backlink) [https://docs.openshift.com/container-platform/4.7/monitoring/configuring-the-monitoring-stack.html#configuring-the-monitoring-stack](https://docs.openshift.com/container-platform/4.7/monitoring/configuring-the-monitoring-stack.html#configuring-the-monitoring-stack)'
- en: '[15](#footnote-167-backlink) [https://github.com/thanos-io/thanos](https://github.com/thanos-io/thanos)'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '[15](#footnote-167-backlink) [https://github.com/thanos-io/thanos](https://github.com/thanos-io/thanos)'
- en: We want to create a simple reactive alert based on whether the PetBattle API,
    Tournament, and UI Pods are running in a certain project. We can make use of Kubernetes
    Pod labels and the Prometheus query language to test whether our Pods are running.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望基于PetBattle API、Tournament和UI Pods是否在某个项目中运行，创建一个简单的反应式警报。我们可以利用Kubernetes
    Pod标签和Prometheus查询语言来测试我们的Pods是否在运行。
- en: '![](img/B16297_16_03.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16297_16_03.jpg)'
- en: 'Figure 16.3: Thanos query interface'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.3：Thanos查询界面
- en: For this use case, we combine the `kube_pod_status_ready and kube_pod_labels`
    query for each Pod and namespace combination and create a PrometheusRule to alert
    us when a condition is not met. We wrapped the generation of the alerts in a Helm
    chart so we can easily template the project and alert severity values[16](#footnote-166)
    and connect the deployment with our GitOps automation.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个用例，我们将 `kube_pod_status_ready` 和 `kube_pod_labels` 查询结合起来，针对每个 Pod 和命名空间组合创建一个
    PrometheusRule，当某个条件未满足时触发警报。我们将警报的生成封装在一个 Helm chart 中，以便我们可以轻松地模板化项目和警报严重性值[16](#footnote-166)，并将部署与我们的
    GitOps 自动化连接。
- en: '[16](#footnote-166-backlink) [https://github.com/petbattle/ubiquitous-journey/blob/main/applications/alerting/chart/templates/application-alerts.yaml](https://github.com/petbattle/ubiquitous-journey/blob/main/applications/alerting/chart/templates/application-alerts.yaml)'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '[16](#footnote-166-backlink) [https://github.com/petbattle/ubiquitous-journey/blob/main/applications/alerting/chart/templates/application-alerts.yaml](https://github.com/petbattle/ubiquitous-journey/blob/main/applications/alerting/chart/templates/application-alerts.yaml)'
- en: '[PRE165]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: '[PRE166]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: '[PRE167]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE167]'
- en: '[PRE168]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE168]'
- en: '[PRE169]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE169]'
- en: '[PRE170]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE170]'
- en: '[PRE171]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE171]'
- en: '[PRE172]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE172]'
- en: '[PRE173]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: '[PRE174]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE174]'
- en: The firing alerts can be seen in the OpenShift web console as seen in *Figure
    16.4*. In this example, we have configured the `labs-dev` alerts to only have
    a severity of *info* because they are not deemed as crucial deployments in that
    environment. The severity may be set as *info, warning*, or *critical*, and we
    use *warning* for our `labs-test` *and* `labs-staging` environments, for example.
    These are arbitrary but standard severity levels, and we can use them for routing
    alerts, which we will cover in a moment.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 16.4* 所示，可以在 OpenShift Web 控制台中看到触发的警报。在此示例中，我们已将 `labs-dev` 的警报配置为仅具有
    *info* 严重性，因为在该环境中它们不被认为是至关重要的部署。严重性可以设置为 *info*、*warning* 或 *critical*，例如我们对
    `labs-test` *和* `labs-staging` 环境使用 *warning*。这些是任意但标准的严重性级别，我们可以用它们来路由警报，稍后我们将讨论这个问题。
- en: '![](img/B16297_16_04.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16297_16_04.jpg)'
- en: 'Figure 16.4: PetBattle alerts firing in OpenShift'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.4：PetBattle 警报在 OpenShift 中触发
- en: We can use the same method to create an investigative or proactive alert. This
    time we wish to measure the HTTP request time for our API application. During
    testing, we found that if API calls took longer than ~1.5 sec, the user experience
    in the PetBattle frontend was deemed too slow by end users and there was a chance
    they would disengage from using the web application altogether.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用相同的方法来创建调查性或主动性警报。这次我们希望测量 API 应用程序的 HTTP 请求时间。在测试过程中，我们发现如果 API 调用时间超过
    ~1.5 秒，PetBattle 前端的用户体验会被终端用户认为太慢，可能会导致他们完全放弃使用该 Web 应用程序。
- en: '![](img/B16297_16_05.jpg)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16297_16_05.jpg)'
- en: 'Figure 16.5: Maximum request time alert rule'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.5：最大请求时间警报规则
- en: In this alert, we use the Prometheus query language and the `http_server_requests_seconds_max`
    metric for the PetBattle API application to test whether the maximum request time
    over the last five-minute period exceeded our 1.5 sec threshold. If this alert
    starts to fire, possible remediation actions might include manually scaling up
    the number of API Pods or perhaps increasing the database resources if that is
    seen to be slow for some reason. In future iterations, we may even try to automate
    the application scale-up by using a Horizontal Pod Autoscaler, a Kubernetes construct
    that can scale our applications automatically based on metrics.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在此警报中，我们使用 Prometheus 查询语言和 `http_server_requests_seconds_max` 指标来测试 PetBattle
    API 应用程序在过去五分钟内的最大请求时间是否超过了 1.5 秒的阈值。如果此警报开始触发，可能的修复措施包括手动扩展 API Pod 的数量，或者如果数据库某些原因表现慢，可能需要增加数据库资源。在未来的迭代中，我们甚至可能尝试通过使用水平
    Pod 自动扩展器（Horizontal Pod Autoscaler）来自动化应用扩展，水平 Pod 自动扩展器是 Kubernetes 的一个构件，能够基于指标自动扩展我们的应用程序。
- en: In this way, we can continue to build on our set of alerting rules for our PetBattle
    application suite, modifying them as we run the applications in different environments,
    and learn what conditions to look out for while automating as much of the remediation
    as we can.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以继续为我们的 PetBattle 应用程序套件构建警报规则集，在不同的环境中运行应用程序时进行修改，并学习需要注意的条件，同时尽可能自动化修复过程。
- en: OpenShift Alertmanager
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenShift Alertmanager
- en: 'As we have seen, OpenShift supports three severity levels of alerting: *info*,
    *warning*, and *critical*. We can group and route alerts based on their severity
    as well as on custom labels—that is, project or application labels. In the OpenShift
    administrator console,[17](#footnote-165) you can configure the Alertmanager under
    **Cluster Settings**.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，OpenShift 支持三种警报严重性级别：*info*、*warning* 和 *critical*。我们可以根据严重性以及自定义标签（即项目或应用标签）来对警报进行分组和路由。在
    OpenShift 管理员控制台中，[17](#footnote-165) 您可以在 **集群设置** 下配置 Alertmanager。
- en: '![](img/B16297_16_06.jpg)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16297_16_06.jpg)'
- en: 'Figure 16.6: Alertmanager routing configuration'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.6：Alertmanager 路由配置
- en: Alerts may be grouped and filtered using labels and then routed to specific
    receivers, such as PagerDuty, Webhook, Email, or Slack. We can fine-tune the routing
    rules so that the correct teams receive the alerts in the correct channel, based
    on their urgency. For example, all *info* and *warning* severity alerts for the
    PetBattle UI application may be routed to the *frontend developers* Slack channel,
    whereas all *critical* alerts are routed to the on-call PagerDuty endpoint as
    well as the Slack channel.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 警报可以通过标签进行分组和过滤，然后路由到特定的接收者，如 PagerDuty、Webhook、电子邮件或 Slack。我们可以微调路由规则，以便根据警报的紧急程度，将正确的团队通过正确的渠道接收警报。例如，所有针对
    PetBattle UI 应用的*信息*和*警告*级别的警报可能会路由到*前端开发者*的 Slack 渠道，而所有*严重*级别的警报则会路由到值班 PagerDuty
    端点以及 Slack 渠道。
- en: '[17](#footnote-165-backlink) [https://docs.openshift.com/container-platform/4.7/monitoring/managing-alerts.html](https://docs.openshift.com/container-platform/4.7/monitoring/managing-alerts.html)'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '[17](#footnote-165-backlink) [https://docs.openshift.com/container-platform/4.7/monitoring/managing-alerts.html](https://docs.openshift.com/container-platform/4.7/monitoring/managing-alerts.html)'
- en: Alerting is a critical component to successfully manage the operational aspects
    of a system but you should be careful and ensure that the operations team isn't
    overwhelmed with alerts. Too many alerts or many minor or false-positive alerts
    can lead to *alert fatigue*, where it becomes an established practice within a
    team to ignore alerts, thus robbing them of their importance to the successful
    management of the system.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 警报是成功管理系统运维方面的关键组件，但你需要小心，确保运维团队不会因警报而不堪重负。过多的警报或许多次错误警报可能导致*警报疲劳*，这种情况会使团队养成忽略警报的习惯，从而剥夺警报在系统成功管理中的重要性。
- en: Service Mesh
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务网格
- en: Service mesh functionality has been one of the largest additions/extensions
    to Kubernetes in its short history. There's a lot of debate around the additional
    complexity of using a service mesh and whether all the features are even required.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格功能是 Kubernetes 在其短短历史中最大的新增功能之一。关于使用服务网格的额外复杂性以及是否需要所有功能，存在很多争论。
- en: For the purposes of this book, we're going to focus on the service mesh provided
    out of the box within OpenShift, which is based on the open-source Istio project.
    There are other implementations, such as Linkerd, SuperGloo, and Traefik, out
    there that are excellent and offer similar functionality to Istio.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的目的是专注于 OpenShift 提供的内置服务网格，它基于开源 Istio 项目。还有其他实现，如 Linkerd、SuperGloo 和 Traefik，它们也非常出色，提供与
    Istio 类似的功能。
- en: 'The OpenShift service mesh provides the following features out of the box:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 服务网格提供以下功能：
- en: '**Security**: Authentication and authorization, mutual TLS (encryption), policies'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：身份验证和授权、互信 TLS（加密）、策略'
- en: '**Traffic management**: Resiliency features, virtual services, policies, fault
    injection'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流量管理**：弹性功能、虚拟服务、策略、故障注入'
- en: '**Observability**: Service metrics, call tracing, access logs'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可观测性**：服务指标、调用追踪、访问日志'
- en: Why Service Mesh?
  id: totrans-358
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么选择服务网格？
- en: We previously talked about resiliency and how patterns like circuit breakers
    can help systems recover from downstream failures. A circuit breaker can be added
    in the scope of application code through frameworks such as **SmallRye Fault Tolerance**
    or **Spring Cloud Circuit Breaker** for Java projects; similar frameworks such
    as **Polly**[18](#footnote-164) exist for .NET, **PyBreaker**[19](#footnote-163)
    for Python, and **Opossum**[20](#footnote-162) for Node.js. A key requirement
    for all of these frameworks is that they have to be added to the existing source
    code of the application, and the application needs to be rebuilt. When using a
    service mesh, a circuit breaker is external to the application code and no changes
    are required at the application level to take advantage of this feature.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论过弹性和像断路器这样的模式如何帮助系统从下游故障中恢复。断路器可以通过框架如**SmallRye Fault Tolerance**或**Spring
    Cloud Circuit Breaker**为 Java 项目添加到应用程序代码范围内；类似的框架如**Polly**[18](#footnote-164)存在于.NET环境，**PyBreaker**[19](#footnote-163)适用于
    Python，**Opossum**[20](#footnote-162)适用于 Node.js。这些框架的一个关键要求是，它们必须添加到应用程序的现有源代码中，并且需要重建应用程序。而在使用服务网格时，断路器是外部于应用程序代码的，不需要在应用程序级别做任何修改即可利用此功能。
- en: '[18](#footnote-164-backlink) [https://github.com/App-vNext/Polly](https://github.com/App-vNext/Polly)'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '[18](#footnote-164-backlink) [https://github.com/App-vNext/Polly](https://github.com/App-vNext/Polly)'
- en: '[19](#footnote-163-backlink) [https://pypi.org/project/pybreaker/](https://pypi.org/project/pybreaker/)'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '[19](#footnote-163-backlink) [https://pypi.org/project/pybreaker/](https://pypi.org/project/pybreaker/)'
- en: '[20](#footnote-162-backlink) [https://nodeshift.dev/opossum/](https://nodeshift.dev/opossum/)'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '[20](#footnote-162-backlink) [https://nodeshift.dev/opossum/](https://nodeshift.dev/opossum/)'
- en: The same is true with **Mutual TLS** (**mTLS**), which is used for encrypting
    traffic between services. Operators such as CertManager or CertUtil can assist
    with managing and distributing certificates, but modification of the application
    code is still required to use the feature. Service meshes simplify this as the
    inter-component traffic is sent via a *sidecar proxy* and functionality such as
    mTLS is *automagically* added to this—once again, without having to change the
    application code.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '**互信TLS**（**mTLS**）也如此，它用于加密服务之间的流量。像CertManager或CertUtil这样的操作工具可以帮助管理和分发证书，但仍需要修改应用程序代码才能使用该功能。服务网格通过将组件间流量发送通过*侧车代理*来简化这一过程，像mTLS这样的功能会*自动神奇*地添加到其中——同样，无需更改应用程序代码。'
- en: The Istio component of a service mesh also manages TLS certificate generation
    and distribution so that it helps reduce the management overhead when using mTLS.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格的Istio组件还管理TLS证书的生成和分发，以帮助减少使用mTLS时的管理开销。
- en: So how does a service mesh perform all of this magical functionality? Basically,
    the service mesh operator adds a service proxy container (based on the Envoy project)
    to the application Pod and configures the application traffic to be routed through
    this proxy. The proxy registers with the Istio control plane and configuration
    settings, certificates, and routing rules are retrieved and the proxy configured.
    The Istio documentation goes into much more detail.[21](#footnote-161)
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，服务网格是如何实现所有这些神奇功能的呢？基本上，服务网格操作员将一个服务代理容器（基于Envoy项目）添加到应用程序Pod中，并配置应用流量通过该代理进行路由。该代理向Istio控制平面注册，并获取配置设置、证书和路由规则，然后进行配置。Istio文档对此有更详细的介绍。[21](#footnote-161)
- en: Aside – Sidecar Containers
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 旁注 – 侧车容器
- en: A common object that people visualize when they hear the word *sidecar* is that
    of a motorbike with a single-wheel passenger car—a *pod*—attached to it. Being
    attached to the bike, the pod goes wherever the bike goes—except in comedy sketches
    where the bike and sidecar separate and rejoin, but that's another subject entirely.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 人们在听到*侧车*这个词时，通常会联想到一辆摩托车，车上附有一个单轮的乘客车厢——一个*驾驶舱*。由于与摩托车连接，驾驶舱随摩托车一起移动——除了在喜剧小品中，摩托车和侧车分离又重新连接，但那是另一个话题。
- en: 'In Kubernetes, a sidecar is a container that runs in the same Kubernetes Pod
    as the main application container. The containers share the same network and ICP
    namespace and can also share storage. In OpenShift, when using the service mesh
    functionality, a Pod annotated with the correct annotation `sidecar.istio.io/inject:
    "true"` will have an Istio proxy automatically injected as a sidecar alongside
    the application container. All subsequent communications between the application
    and external resources will flow through this sidecar proxy and hence enable the
    usage of features such as circuit breakers, tracing, and TLS, as and when they
    are needed. As the great Freddie Mercury once said, "*It''s a kind of magic*."'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '在Kubernetes中，侧车是与主应用容器在同一Kubernetes Pod中运行的容器。这些容器共享相同的网络和ICP命名空间，也可以共享存储。在OpenShift中，使用服务网格功能时，Pod被正确注解为`sidecar.istio.io/inject:
    "true"`，会自动注入一个Istio代理作为侧车，与应用容器一起运行。应用程序与外部资源之间的所有后续通信都会通过这个侧车代理，从而使得可以根据需要使用诸如电路断路器、追踪和TLS等功能。正如伟大的Freddie
    Mercury曾经说过的，"*这是一种魔力*。" '
- en: '[PRE175]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE175]'
- en: '[PRE176]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE176]'
- en: '[PRE177]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE177]'
- en: '[PRE178]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE178]'
- en: '[PRE179]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE179]'
- en: '[PRE180]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE180]'
- en: '[PRE181]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE181]'
- en: '[PRE182]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE182]'
- en: '[PRE183]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE183]'
- en: '[PRE184]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE184]'
- en: '[PRE185]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE185]'
- en: '[PRE186]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE186]'
- en: '[PRE187]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE187]'
- en: '[PRE188]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE188]'
- en: '[PRE189]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE189]'
- en: '[PRE190]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE190]'
- en: '[PRE191]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE191]'
- en: '[PRE192]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE192]'
- en: '[PRE193]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE193]'
- en: '[PRE194]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE194]'
- en: '[PRE195]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE195]'
- en: '[PRE196]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE196]'
- en: '[21](#footnote-161-backlink) [https://istio.io/latest/docs/](https://istio.io/latest/docs/)'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '[21](#footnote-161-backlink) [https://istio.io/latest/docs/](https://istio.io/latest/docs/)'
- en: 'It is possible to have more than one sidecar container if required. Each container
    can bring different features to the application Pod: for example, one for Istio,
    another for log forwarding, another for the retrieval of security credentials,
    and so on. It''s easy to know when a Pod is running more than a single container;
    for example, the `READY` column indicates how many containers are available per
    Pod and how many are ready—that is, its readiness probe has passed.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，可以有多个侧车容器。每个容器可以为应用程序Pod带来不同的功能：例如，一个用于Istio，另一个用于日志转发，另一个用于检索安全凭证，等等。很容易知道一个Pod是否运行多个容器；例如，`READY`列显示每个Pod中有多少容器可用，以及有多少容器已经准备好——也就是说，它的就绪探针已通过。
- en: '[PRE197]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE197]'
- en: '[PRE198]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE198]'
- en: '[PRE199]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE199]'
- en: '[PRE200]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE200]'
- en: '[PRE201]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE201]'
- en: '[PRE202]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE202]'
- en: '[PRE203]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE203]'
- en: '[PRE204]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE204]'
- en: Be aware, though, that there is a temptation to try and utilize all the service
    mesh features at once, known as the *ooh… shiny* problem.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，要注意的是，存在一种诱惑，即尝试一次性使用所有服务网格功能，这被称为 *哦…好亮眼* 的问题。
- en: Here Be Dragons!
  id: totrans-402
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这里有龙！
- en: The adoption of a service mesh isn't a trivial exercise when it comes to complex
    solutions with multiple components and development teams. One thing to understand
    about a service mesh is that it crosses a lot of team boundaries and responsibilities.
    It includes features that are focused on the developer, operations, and security
    teams; all of these teams/personnel need to work together to understand and get
    the best out of using the features provided by the mesh. If you're just starting
    out, our advice is to start small and figure out what features are necessary in
    production and iterate from there.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 在涉及多个组件和开发团队的复杂解决方案时，采用服务网格并非一项简单的任务。了解服务网格的一件事是，它跨越了许多团队边界和责任范围。它包括专注于开发人员、运维和安全团队的特性；所有这些团队/人员需要协同工作，理解并充分利用网格提供的功能。如果你刚刚开始，我们的建议是从小做起，弄清楚生产环境中哪些功能是必要的，然后从那里逐步迭代。
- en: In the case of PetBattle, we decided that we were going to primarily focus on
    using some of the features in the areas of traffic management and observability.
    The rationale behind this was that Keycloak already addressed many of the security
    requirements, and we also wanted to finish the book before the end of the decade.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PetBattle 的案例中，我们决定主要专注于使用流量管理和可观察性方面的一些功能。这样做的理由是，Keycloak 已经解决了许多安全需求，同时我们也希望在十年结束前完成这本书。
- en: Service Mesh Components
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 服务网格组件
- en: 'The functionality of the service mesh is made up of a number of independent
    components:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格的功能由多个独立的组件组成：
- en: Jaeger and Elasticsearch provide the call tracing functionality and logging
    functionality.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaeger 和 Elasticsearch 提供调用跟踪功能和日志记录功能。
- en: Kiali provides the mesh visualization functionality.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kiali 提供网格可视化功能。
- en: OpenShift Service Mesh provides the core Istio functionality.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenShift 服务网格提供核心的 Istio 功能。
- en: The good news is that all of these components are installed and managed by Operators,
    so installation is reasonably straightforward. These components are installed
    via Helm, and if you want to know more about how they are installed, then the
    Red Hat OpenShift documentation will have the relevant details.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，所有这些组件都是由操作员安装和管理的，因此安装相对简单。这些组件通过 Helm 安装，如果你想了解更多关于如何安装它们的信息，可以查看 Red
    Hat OpenShift 文档中的相关内容。
- en: One key thing to note is that at the time of writing this book, OpenShift Service
    Mesh ships with a downstream version of Istio called Maistra. This is primarily
    due to the out-of-the-box multi-tenancy nature of OpenShift, as well as limiting
    the scope of Istio cluster-scoped resources. OpenShift Service Mesh also ships
    with an **Istio** **OpenShift Routing** (**IOR**) component that maps the Istio
    gateway definitions onto OpenShift routes. Note that Istio is still the upstream
    project and bugs/feature requests are fixed/implemented, as necessary.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，在撰写本书时，OpenShift 服务网格包含一个下游版本的 Istio，名为 Maistra。这主要是由于 OpenShift 的开箱即用的多租户特性，以及限制
    Istio 集群范围资源的使用。OpenShift 服务网格还包括一个 **Istio** **OpenShift 路由** (**IOR**) 组件，该组件将
    Istio 网关定义映射到 OpenShift 路由上。请注意，Istio 仍然是上游项目，必要时会修复/实现 bugs 或特性请求。
- en: 'For traffic management, Istio has the following core set of resources:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 对于流量管理，Istio 提供了以下核心资源：
- en: '**Gateways**: Controls how traffic gets into the mesh from the outside, akin
    to OpenShift routes.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网关**：控制外部流量如何进入服务网格，类似于 OpenShift 路由。'
- en: '**Virtual service**: Controls how traffic is routed within the service mesh
    to a destination service. This is where functionality such as timeouts, context-based
    routing, retries, mirroring, and so on, are configured.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟服务**：控制流量如何在服务网格内路由到目标服务。这是配置超时、基于上下文的路由、重试、镜像等功能的地方。'
- en: '**Destination rule**: Service location where traffic is routed to once traffic
    rules have been applied. Destination rules can be configured to control traffic
    aspects such as load balancing strategies, connection pools, TLS setting, and
    outlier detection (circuit breakers).'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标规则**：流量应用规则后被路由到的服务位置。目标规则可以配置以控制诸如负载均衡策略、连接池、TLS 设置和异常检测（断路器）等流量方面的内容。'
- en: There are other resources such as service entry, filters, and workloads, but
    we're not going to cover them here.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他资源，如服务入口、过滤器和工作负载，但我们在这里不会涵盖它们。
- en: PetBattle Service Mesh Resources
  id: totrans-417
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PetBattle服务网格资源
- en: We'll briefly introduce some of the resources that we use in PetBattle and explain
    how we use them.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简要介绍我们在PetBattle中使用的一些资源，并解释我们如何使用它们。
- en: Gateways
  id: totrans-419
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网关
- en: The gateway resource, as stated earlier, is used to create an ingress route
    for traffic coming into the service mesh.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，网关资源用于为进入服务网格的流量创建入口路由。
- en: '[PRE205]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE205]'
- en: '[PRE206]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE206]'
- en: '[PRE207]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE207]'
- en: '[PRE208]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE208]'
- en: '[PRE209]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE209]'
- en: '[PRE210]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE210]'
- en: '[PRE211]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE211]'
- en: '[PRE212]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE212]'
- en: '[PRE213]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE213]'
- en: '[PRE214]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE214]'
- en: '[PRE215]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE215]'
- en: '[PRE216]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE216]'
- en: '[PRE217]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE217]'
- en: '[PRE218]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE218]'
- en: '[PRE219]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE219]'
- en: '[PRE220]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE220]'
- en: '[PRE221]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE221]'
- en: A few things to note with this definition is that it will create an OpenShift
    route in the *istio-system* namespace and not the local namespace. Secondly, the
    route itself will use SSL, but it won't be able to utilize the OpenShift router
    certificates by default. Service mesh routes have to provide their own certificates.
    As part of writing this book, we took the pragmatic approach and copied the OpenShift
    router certificates into the *istio-system* namespace and provided them to the
    gateway via the *pb-ingressgateway-certs* secret. Note that this is for demonstration
    purposes only—*do not try this in production*. The correct approach for production
    is to generate and manage the PKI using as-a-service certificates.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 关于此定义有几点需要注意：它将在*istio-system*命名空间中创建一个OpenShift路由，而不是本地命名空间。其次，路由本身将使用SSL，但默认情况下无法使用OpenShift路由器证书。服务网格路由必须提供自己的证书。作为编写本书的一部分，我们采取了务实的方法，将OpenShift路由器证书复制到*istio-system*命名空间，并通过*pb-ingressgateway-certs*密钥提供给网关。请注意，这仅用于演示目的——*不要在生产环境中尝试此操作*。生产环境中的正确方法是使用“即服务”证书生成并管理PKI。
- en: Virtual services
  id: totrans-439
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟服务
- en: PetBattle contains a number of VirtualServices, such as *pet-battle-cats-tls,
    pet-battle-main-tls,* and *pet-battle-tournament-tls.*
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: PetBattle包含多个虚拟服务，如*pet-battle-cats-tls、pet-battle-main-tls*和*pet-battle-tournament-tls*。
- en: '[PRE222]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE222]'
- en: '[PRE223]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE223]'
- en: '[PRE224]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE224]'
- en: '[PRE225]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE225]'
- en: '[PRE226]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE226]'
- en: '[PRE227]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE227]'
- en: '[PRE228]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE228]'
- en: '[PRE229]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE229]'
- en: '[PRE230]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE230]'
- en: '[PRE231]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE231]'
- en: '[PRE232]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE232]'
- en: '[PRE233]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE233]'
- en: '[PRE234]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE234]'
- en: '[PRE235]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE235]'
- en: '[PRE236]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE236]'
- en: '[PRE237]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE237]'
- en: '[PRE238]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE238]'
- en: '[PRE239]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE239]'
- en: '[PRE240]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE240]'
- en: '[PRE241]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE241]'
- en: '[PRE242]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE242]'
- en: '[PRE243]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE243]'
- en: '[PRE244]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE244]'
- en: 'The VirtualServices are all similar in function in that they are all configured
    to:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 这些虚拟服务的功能类似，都是配置为：
- en: Match a specific URI; in the example above, `/cats`.
  id: totrans-465
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 匹配特定的URI；在上面的示例中是`/cats`。
- en: Once matched, route the traffic to a specific destination.
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦匹配，便将流量路由到特定的目标。
- en: Handle specific errors by performing a fixed number of request retries.
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行固定次数的请求重试来处理特定错误。
- en: Destination Rule
  id: totrans-468
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标规则
- en: Finally, the traffic is sent to a destination or even distributed to a set of
    destinations depending on the configuration. This is where DestinationRules come
    into play.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，流量被发送到一个目标，或根据配置分发到一组目标。这就是目标规则的作用。
- en: '[PRE245]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE245]'
- en: '[PRE246]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE246]'
- en: '[PRE247]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE247]'
- en: '[PRE248]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE248]'
- en: '[PRE249]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE249]'
- en: '[PRE250]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE250]'
- en: '[PRE251]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE251]'
- en: '[PRE252]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE252]'
- en: '[PRE253]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE253]'
- en: '[PRE254]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE254]'
- en: '[PRE255]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE255]'
- en: '[PRE256]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE256]'
- en: In our example, the traffic sent to a specific port is load balanced based on
    a simple strategy that selects the Pod with the least number of active requests.
    There are many load balancing strategies that can be used here, depending on the
    needs of the application—everything from simple round robin to advanced consistent
    hashing load balancing strategies, which can be used for session affinity. As
    ever, the documentation goes into far greater detail.[22](#footnote-160)
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，发送到特定端口的流量会根据一个简单的策略进行负载均衡，该策略选择活动请求最少的Pod。这里可以使用许多负载均衡策略，具体取决于应用的需求——从简单的轮询到高级的恒定哈希负载均衡策略，这些都可以用于会话亲和性。正如之前所提到的，文档中会详细讨论这一点。[22](#footnote-160)
- en: 'We can visualize the flow of traffic from the above example, as seen in *Figure
    16.7*:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过上面的示例来可视化流量的流动，如*图16.7*所示：
- en: '![](img/B16297_16_07.jpg)'
  id: totrans-484
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16297_16_07.jpg)'
- en: 'Figure 16.7: PetBattle traffic flow'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.7：PetBattle流量流动
- en: '[22](#footnote-160-backlink) [https://istio.io/latest/docs/](https://istio.io/latest/docs/)'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '[22](#footnote-160-backlink) [https://istio.io/latest/docs/](https://istio.io/latest/docs/)'
- en: Note that *Figure 16.7* shows an example of how destination rules can be used
    to send traffic to an alternative version of the service. This can be useful for
    advanced deployment strategies such as Canary, Blue/Green, and so on. We haven't
    discussed how to do this with OpenShift Service Mesh in this book, but the reader
    is encouraged to explore this area in more detail. A good place to start is the
    aforementioned Istio documentation.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*图16.7*显示了如何使用目标规则将流量发送到服务的备用版本的示例。这对于高级部署策略，如金丝雀发布、蓝绿部署等非常有用。虽然我们在本书中没有讨论如何使用OpenShift
    Service Mesh实现这一点，但鼓励读者深入探讨这个领域。一个很好的起点是上述的Istio文档。
- en: Managing all of these resources is reasonably simple when it's just a few services,
    and PetBattle utilizes service mesh functionality in a very basic manner. However,
    when there are many services and features, such as multiple destinations used
    in advanced deployment models, the amount of settings and YAML to interpret can
    be overwhelming. This is where mesh visualization functionality can be useful
    to visualize how all of this works together. For this, we use the Kiali functionality,
    which is part of OpenShift Service Mesh. *Figure 16.8* shows how PetBattle is
    visualized using Kiali.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 当只有少数服务时，管理所有这些资源相对简单，且 PetBattle 在非常基础的方式下利用了服务网格功能。然而，当服务和功能增多时，例如在高级部署模型中使用多个目的地时，需要解析的设置和
    YAML 文件量可能会变得非常庞大。这时，网格可视化功能就能派上用场，帮助可视化所有内容是如何协同工作的。为此，我们使用 Kiali 功能，它是 OpenShift
    服务网格的一部分。*图16.8* 展示了如何使用 Kiali 可视化 PetBattle。
- en: '![](img/B16297_16_08.jpg)'
  id: totrans-489
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16297_16_08.jpg)'
- en: 'Figure 16.8: Kiali service graph for PetBattle'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.8：PetBattle 的 Kiali 服务图
- en: Kiali can be very useful for diagnosing the current state of the mesh, as it
    can dynamically show where traffic is being sent as well as the state of any circuit
    breakers being used. It also integrates with Jaeger for tracing requests across
    multiple systems. Kiali can also help prevent configuration issues by semantically
    validating the deployed service mesh resources.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: Kiali 对于诊断当前网格状态非常有用，它能够动态显示流量的去向以及使用的任何断路器的状态。它还与 Jaeger 集成，用于跨多个系统追踪请求。Kiali
    还可以通过语义验证已部署的服务网格资源，帮助防止配置问题。
- en: Next we're going to explore one of the most powerful features of OpenShift 4
    - Operators.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探索 OpenShift 4 中最强大的功能之一——Operators（运维管理器）。
- en: Operators Everywhere
  id: totrans-493
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运维管理器无处不在
- en: Fundamental to the OpenShift 4 platform is the concept of *Operators*. So far,
    we have used them without talking about why we need them and what they actually
    represent on a Kubernetes platform such as OpenShift. Let's cover this briefly
    without totally rewriting the book on the subject.[23](#footnote-159)
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 4 平台的核心概念是*运维管理器*。到目前为止，我们已经使用了它们，但没有讨论为什么我们需要它们，以及它们在 Kubernetes
    平台（如 OpenShift）上实际代表什么。我们将简要介绍这一点，而不对该主题进行全面重写。[23](#footnote-159)
- en: At its heart, the Operator is a software pattern that codifies knowledge about
    the running and operation of a particular software application. That application
    could be a distributed key value store, such as etcd. It might be a web application
    such as the OpenShift web console. Fundamentally, the operator can represent *any*
    application domain that could be codified. A good analogy for an operator is the
    *expert system*, a rules-based bit of software that represents knowledge about
    a certain thing that is put to work in a meaningful way. If we take a database
    as an example, the Operator might codify what a real human database administrator
    does on a day-to-day basis, such as the deployment, running, scaling, backup,
    patching, and upgrading of that database.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，运维管理器是一种软件模式，它将关于特定软件应用程序的运行和操作的知识进行编码。这个应用程序可能是一个分布式的键值存储，例如 etcd。也可能是一个
    Web 应用程序，如 OpenShift Web 控制台。运维管理器的根本作用是表示*任何*可以被编码的应用领域。运维管理器的一个很好比喻是*专家系统*，一种基于规则的软件，代表了对某个事物的知识，并以有意义的方式投入使用。如果我们以数据库为例，运维管理器可能会编码人类数据库管理员每天需要做的事情，如数据库的部署、运行、扩展、备份、修补和升级。
- en: The physical runtime for an operator is nothing more than a Kubernetes Pod,
    that is, a collection of containers that run on a Kubernetes platform such as
    OpenShift. Operators work by extending or adding new APIs to the existing Kubernetes
    and OpenShift platform APIs. This new endpoint is called a **Custom Resource**
    (**CR**). CRs are one of the many extension mechanisms in Kubernetes.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 运维管理器的物理运行时不过是一个 Kubernetes Pod，即一组运行在 Kubernetes 平台（如 OpenShift）上的容器。运维管理器通过扩展或向现有的
    Kubernetes 和 OpenShift 平台 API 添加新 API 来工作。这个新的端点称为**自定义资源**（**CR**）。CR 是 Kubernetes
    中许多扩展机制之一。
- en: '![](img/B16297_16_09.jpg)'
  id: totrans-497
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16297_16_09.jpg)'
- en: 'Figure 16.9: The Operator pattern'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.9：运维管理器模式
- en: '[23](#footnote-159-backlink) [https://www.redhat.com/en/resources/oreilly-kubernetes-operators-automation-ebook](https://www.redhat.com/en/resources/oreilly-kubernetes-operators-automation-ebook)'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '[23](#footnote-159-backlink) [https://www.redhat.com/en/resources/oreilly-kubernetes-operators-automation-ebook](https://www.redhat.com/en/resources/oreilly-kubernetes-operators-automation-ebook)'
- en: A **Custom Resource Definition** (**CRD**) defines what the CR is. Think of
    it as the definition or schema for the CR. The Operator Pod *watches* for events
    on the platform that are related to their custom resources and takes *reconciliation*
    actions to achieve the desired state of the system. When an Operator Pod stops
    or is deleted from the cluster, the application(s) that it manages should continue
    to function. Removing a CRD from your cluster does affect the application(s) that
    it manages. In fact, deleting a CRD will in turn delete its CR instances. This
    is the Operator pattern.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '**自定义资源定义**（**CRD**）定义了 CR 是什么。可以将其视为 CR 的定义或架构。Operator Pod *监控*与其自定义资源相关的事件，并采取*协调*措施以实现系统的期望状态。当
    Operator Pod 停止或从集群中删除时，它所管理的应用程序应该继续运行。从集群中删除 CRD 会影响它所管理的应用程序。事实上，删除 CRD 会进而删除其
    CR 实例。这就是 Operator 模式。'
- en: With Operators, all of the operational experience required to run/manage a piece
    of software can be packaged up and delivered as a set of containers and associated
    resources. In fact, the whole of the OpenShift 4 platform exists as a collection
    of operators! So, as the platform owner, you are receiving the most advanced administrator
    knowledge bundled up through Operators. Even better, Operators can become more
    advanced over time as new features and capabilities are added to them. A good
    understanding of how to configure Operators is required for OpenShift platform
    administrators. This usually involves setting properties in the OpenShift cluster
    global configuration web console, setting CR property values, using ConfigMaps,
    or similar approaches. The product documentation[24](#footnote-158) is usually
    the best place to find out what these settings are for each Operator.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 Operator，运行/管理一段软件所需的所有操作经验都可以打包并作为一组容器及相关资源交付。事实上，整个 OpenShift 4 平台本身就是一系列
    Operator 的集合！因此，作为平台拥有者，你实际上是通过 Operator 获得了最先进的管理员知识。更好的是，随着新功能和能力的增加，Operator
    可以随着时间的推移变得更加先进。OpenShift 平台管理员需要对如何配置 Operator 有很好的理解。这通常涉及在 OpenShift 集群全局配置网页控制台中设置属性，设置
    CR 属性值，使用 ConfigMaps 或类似的方法。产品文档[24](#footnote-158)通常是了解每个 Operator 设置的最佳来源。
- en: '![](img/B16297_16_10.jpg)'
  id: totrans-502
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16297_16_10.jpg)'
- en: 'Figure 16.10: OpenShift Cluster Settings—configuring platform Operators'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.10：OpenShift 集群设置—配置平台 Operator
- en: '[24](#footnote-158-backlink) [https://docs.openshift.com/container-platform/4.7](https://docs.openshift.com/container-platform/4.7)'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '[24](#footnote-158-backlink) [https://docs.openshift.com/container-platform/4.7](https://docs.openshift.com/container-platform/4.7)'
- en: In OpenShift, the lifecycle management (upgrading, patching, managing) of Operators
    themselves is automated through the **Operator Lifecycle Manager** (**OLM**).
    These components make upgrading the OpenShift platform itself a lot more reliable
    and easier to manage from a user's perspective; it massively reduces the operational
    burden. Because Operators themselves are delivered as versioned images, we get
    the same benefits from immutable container images that we do for our own applications,
    i.e., the same image version can be run consistently in multiple cloud environments
    increasing quality and eliminating snowflakes (unique applications for unique
    environments).
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenShift 中，Operator 本身的生命周期管理（升级、修补、管理）是通过**Operator 生命周期管理器**（**OLM**）自动化的。这些组件使得从用户的角度来看，升级
    OpenShift 平台变得更加可靠，且更容易管理；它大大减轻了操作负担。由于 Operator 本身是作为版本化镜像交付的，我们从不可变容器镜像中获得了与我们自己应用程序相同的好处，即相同的镜像版本可以在多个云环境中一致地运行，从而提高了质量，消除了雪花现象（针对特定环境的独特应用）。
- en: And it is not just the OpenShift platform itself that can take advantage of
    Operators. The sharing and distribution of software using Operators via the Operator
    Hub[25](#footnote-157) is open to software developers and vendors from all over
    the world. We use OLM and Operator *subscriptions* to deploy these into our cluster.
    The tooling required to build and develop Operators (the SDK) is open source and
    available to all.[26](#footnote-156)
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅是 OpenShift 平台本身能够利用 Operator。通过 Operator Hub[25](#footnote-157)共享和分发软件对全球的开发者和供应商开放。我们使用
    OLM 和 Operator *订阅*将其部署到我们的集群中。构建和开发 Operator 所需的工具（SDK）是开源的，任何人都可以使用。[26](#footnote-156)
- en: So, should *all* applications be delivered as Operators? The short answer is
    no. The effort required to code, package, test, and maintain an Operator may be
    seen as overkill for many applications. For example, if your applications are
    not being distributed and shared with others, and you only need to build, package,
    deploy, and configure your application in a few clusters, there are many simpler
    ways this can be achieved, such as using container images, Kubernetes, and OpenShift
    native constructs (BuildConfigs, Deployments, ReplicaSets, ConfigMaps, Secrets,
    and more) with tools such as Helm to achieve your goals.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，*所有*应用程序都应该作为 Operators 发布吗？简短的答案是“不”。编写、打包、测试和维护一个 Operator 可能对许多应用来说显得有些过度。例如，如果你的应用并不需要与他人共享或分发，只需要在几个集群中构建、打包、部署和配置应用，那么有许多更简单的方法可以实现这一目标，比如使用容器镜像、Kubernetes
    和 OpenShift 原生构件（BuildConfigs、Deployments、ReplicaSets、ConfigMaps、Secrets 等）以及像
    Helm 这样的工具来实现目标。
- en: Operators Under the Hood
  id: totrans-508
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 后台操作符
- en: To fully understand how operators work, you need to understand how the Kubernetes
    control loop works.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 要完全理解 Operator 如何工作，你需要了解 Kubernetes 控制循环是如何工作的。
- en: Control Loops
  id: totrans-510
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 控制循环
- en: In very basic terms, core Kubernetes is just a **Key-Value** (**KV**) store—an
    etcd datastore with an API. Processes use this API to perform **Create**, **Read**,
    **Update**, and **Delete** (**CRUD**) actions on keys within the KV store. Processes
    can also register with the KV store to be notified when there are value changes
    to keys or sets of keys that they're interested in.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，Kubernetes 核心只是一个 **键值**（**KV**）存储——一个带有 API 的 etcd 数据存储。进程使用这个 API 对 KV
    存储中的键执行 **创建**、**读取**、**更新** 和 **删除**（**CRUD**）操作。进程还可以向 KV 存储注册，以便在它们感兴趣的键或键集合发生变化时收到通知。
- en: '[25](#footnote-157-backlink) [https://operatorhub.io](https://operatorhub.io)'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '[25](#footnote-157-backlink) [https://operatorhub.io](https://operatorhub.io)'
- en: '[26](#footnote-156-backlink) [https://github.com/operator-framework/operator-sdk](https://github.com/operator-framework/operator-sdk)'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '[26](#footnote-156-backlink) [https://github.com/operator-framework/operator-sdk](https://github.com/operator-framework/operator-sdk)'
- en: When these processes get a change notification, they react to that notification
    by performing some activity, such as configuring iptables rules, provisioning
    storage, and so on. These processes understand the current state of the system
    and the desired state and work toward achieving that desired state. In other words,
    these processes are performing the role of a *control loop*, meaning they attempt
    to bring the state of the system to a desired state from where it currently resides.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 当这些进程接收到变更通知时，它们会通过执行某些活动（例如配置 iptables 规则、配置存储等）来响应通知。这些进程了解系统的当前状态和期望状态，并朝着实现该期望状态的方向努力。换句话说，这些进程正在执行
    *控制循环* 的角色，意味着它们试图将系统的状态从当前状态带到期望的状态。
- en: In this example, the *process* is a controller that observes the state of a
    resource or set of resources and then makes changes to move the resource state
    closer to the desired state. As consumers of Kubernetes, we constantly use controllers.
    For example, when we instruct Kubernetes to deploy a Pod, the Pod controller works
    to make that a reality. Control loops are key to the operation on Kubernetes and
    it's a declarative and, eventually, consistent approach. For much more information,
    take a look at the Kubernetes Controller docs[27](#footnote-155) and the OpenShift
    blog site for recommendations on how to build your own Operator.[28](#footnote-154)
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，*过程*是一个控制器，它观察一个资源或一组资源的状态，然后进行更改，将资源状态向期望状态靠拢。作为 Kubernetes 的使用者，我们不断地使用控制器。例如，当我们指示
    Kubernetes 部署一个 Pod 时，Pod 控制器就会为实现这一目标而工作。控制循环是 Kubernetes 操作的关键，它是一种声明式的，并最终一致的方法。要了解更多信息，请查看
    Kubernetes 控制器文档[27](#footnote-155)和 OpenShift 博客网站，获取如何构建自己 Operator 的推荐[28](#footnote-154)。
- en: Operator Scopes
  id: totrans-516
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Operator 范围
- en: Operators can either be cluster-scoped or namespace-scoped. A cluster-scoped
    operator is installed once in a namespace and can create and manage resources
    in other namespaces; that is, cluster-wide. The OpenShift service mesh operator
    and its related operators such as Kiali and Jaeger are cluster-scoped. They are
    installed by default into the `openshift-operators` or `openshift-operators-redhat`
    namespace and create and manage resources when a related CRD is deployed in another
    namespace, such as PetBattle.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 操作符可以是集群范围的，也可以是命名空间范围的。集群范围的操作符在一个命名空间中安装一次，可以在其他命名空间中创建和管理资源；也就是说，作用于整个集群。OpenShift
    服务网格操作符及其相关操作符，如 Kiali 和 Jaeger，都是集群范围的。它们默认安装在 `openshift-operators` 或 `openshift-operators-redhat`
    命名空间中，并在部署相关 CRD 到其他命名空间时创建和管理资源，如 PetBattle。
- en: A namespace-scoped operator is one that is deployed in a namespace and only
    manages resources in that namespace. We use a number of these in PetBattle, such
    as Cert-Utils and Keycloak.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间范围的操作符是指在某个命名空间中部署，并且只管理该命名空间中的资源。我们在 PetBattle 中使用了多个这样的操作符，如 Cert-Utils
    和 Keycloak。
- en: All Operators are installed via a CRD called a **Subscription**. Without going
    into too much detail (see the official documentation for more), a Subscription
    describes how to retrieve and install an instance of an operator. The following
    is an example of a Subscription that we use to install the Grafana operator.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 所有操作符通过名为**Subscription**的 CRD 安装。简要来说（详细信息请参见官方文档），Subscription 描述了如何检索并安装操作符实例。以下是我们用来安装
    Grafana 操作符的 Subscription 示例。
- en: '[27](#footnote-155-backlink) [https://kubernetes.io/docs/concepts/architecture/controller/](https://kubernetes.io/docs/concepts/architecture/controller/)'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '[27](#footnote-155-backlink) [https://kubernetes.io/docs/concepts/architecture/controller/](https://kubernetes.io/docs/concepts/architecture/controller/)'
- en: '[28](#footnote-154-backlink) [https://www.openshift.com/blog/kubernetes-operators-best-practices](https://www.openshift.com/blog/kubernetes-operators-best-practices)'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '[28](#footnote-154-backlink) [https://www.openshift.com/blog/kubernetes-operators-best-practices](https://www.openshift.com/blog/kubernetes-operators-best-practices)'
- en: '[PRE257]'
  id: totrans-522
  prefs: []
  type: TYPE_PRE
  zh: '[PRE257]'
- en: '[PRE258]'
  id: totrans-523
  prefs: []
  type: TYPE_PRE
  zh: '[PRE258]'
- en: '[PRE259]'
  id: totrans-524
  prefs: []
  type: TYPE_PRE
  zh: '[PRE259]'
- en: '[PRE260]'
  id: totrans-525
  prefs: []
  type: TYPE_PRE
  zh: '[PRE260]'
- en: '[PRE261]'
  id: totrans-526
  prefs: []
  type: TYPE_PRE
  zh: '[PRE261]'
- en: '[PRE262]'
  id: totrans-527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE262]'
- en: '[PRE263]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE263]'
- en: '[PRE264]'
  id: totrans-529
  prefs: []
  type: TYPE_PRE
  zh: '[PRE264]'
- en: '[PRE265]'
  id: totrans-530
  prefs: []
  type: TYPE_PRE
  zh: '[PRE265]'
- en: '[PRE266]'
  id: totrans-531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE266]'
- en: '[PRE267]'
  id: totrans-532
  prefs: []
  type: TYPE_PRE
  zh: '[PRE267]'
- en: To see some of the namespace-scoped operators that PetBattle needs, run the
    following command.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看 PetBattle 需要的一些命名空间范围的操作符，请运行以下命令。
- en: '[PRE268]'
  id: totrans-534
  prefs: []
  type: TYPE_PRE
  zh: '[PRE268]'
- en: '[PRE269]'
  id: totrans-535
  prefs: []
  type: TYPE_PRE
  zh: '[PRE269]'
- en: '[PRE270]'
  id: totrans-536
  prefs: []
  type: TYPE_PRE
  zh: '[PRE270]'
- en: '[PRE271]'
  id: totrans-537
  prefs: []
  type: TYPE_PRE
  zh: '[PRE271]'
- en: '[PRE272]'
  id: totrans-538
  prefs: []
  type: TYPE_PRE
  zh: '[PRE272]'
- en: '[PRE273]'
  id: totrans-539
  prefs: []
  type: TYPE_PRE
  zh: '[PRE273]'
- en: Let us now take a look at how operators can be used by our PetBattle team.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下操作符如何被我们的 PetBattle 团队使用。
- en: Operators in PetBattle
  id: totrans-541
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PetBattle 中的操作符
- en: We use operators to create and manage resources such as Infinispan cache and
    Keycloak SSO instances. We simply install the Infinispan operator and deploy a
    relevant custom resource to tell it to create and manage a replicated cache. We
    don't have to know about spinning up Infinispan Pods or creating SSL certificates
    or provisioning storage space. The operator will do all of this for us, and if
    something fails or is accidentally deleted, the operator will look after the recreation
    of the resource. In the Infinispan example, if we delete the Infinispan K8s service,
    the operator will be notified about its deletion and recreate the service automatically.
    As developers, we don't have to worry about managing it.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用操作符来创建和管理资源，如 Infinispan 缓存和 Keycloak 单点登录实例。我们只需安装 Infinispan 操作符并部署相关的自定义资源，告诉它创建和管理一个复制缓存。我们无需了解如何启动
    Infinispan Pods、创建 SSL 证书或提供存储空间。操作符会为我们完成这些工作，如果出现故障或被意外删除，操作符会负责重新创建资源。在 Infinispan
    示例中，如果我们删除了 Infinispan K8s 服务，操作符会收到删除通知并自动重新创建该服务。作为开发者，我们无需担心其管理。
- en: It is simpler to think of Operators as *looking after stuff so you don't have
    to*. It is also possible to use multiple operators in combination to automate
    complex workflows. For example, we use Keycloak for its SSO gateway and user management
    functionality. The Keycloak instance is deployed and managed via a Keycloak Operator.
    We just need to build and send a custom resource to the API and the operator will
    do the rest. One of the resources managed by the Operator is a Kubernetes Secret
    that contains the TLS certificates and keys, which clients interacting with the
    Keycloak instance will need to use. Given that Keycloak is the security gateway
    to our application, it is prudent to ensure that all communications are encrypted.
    However, this causes issues for Java-based applications; to use SSL, the JVM requires
    that it be provided with a Java TrustStore containing the SSL/TLS certificates
    and keys so that the JVM can trust them.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 将操作符视为*代替你处理事务*是更简单的理解方式。还可以将多个操作符组合使用，以实现自动化复杂的工作流。例如，我们使用Keycloak作为其SSO网关和用户管理功能。Keycloak实例通过Keycloak操作符进行部署和管理。我们只需构建并将自定义资源发送到API，操作符会处理其余部分。操作符管理的资源之一是包含TLS证书和密钥的Kubernetes密钥，客户端在与Keycloak实例交互时需要使用这些证书和密钥。鉴于Keycloak是我们应用程序的安全网关，确保所有通信都是加密的显得尤为重要。然而，这对基于Java的应用程序来说会造成问题；要使用SSL，JVM要求提供一个包含SSL/TLS证书和密钥的Java信任存储，以便JVM能够信任它们。
- en: So, how do we take the Secret with the TLS certificates and keys and convert
    that into a TrustStore that the Java applications can use? We could do a whole
    heap of scripting with Bash, the Java Keytool, and potentially other tools to
    extract the certs/keys, creating the TrustStore, converting, and finally injecting
    the certs/keys into said TrustStore. This is manual, complex, and error-prone
    work. We will also have to recreate these TrustStores for each environment and
    handle lifecycle events such as certificate expiry.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何将包含TLS证书和密钥的密钥转换为Java应用程序可以使用的信任库呢？我们可以使用Bash脚本、Java Keytool以及可能的其他工具来提取证书/密钥，创建信任库，进行转换，最后将证书/密钥注入该信任库。这是手动的、复杂的且容易出错的工作。我们还需要为每个环境重新创建这些信任库，并处理证书过期等生命周期事件。
- en: Alternatively, we could use an operator, in this case, the *Cert-Utils* operator.
    We first install the *Cert-Utils* operator in the PetBattle namespace. This Operator
    was developed by the Red Hat Consulting PAAS Community of Practice[29](#footnote-153)
    to help manage certificates and JVM Keystores, along with TrustStores.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们可以使用一个操作符，在这种情况下是*Cert-Utils*操作符。我们首先在PetBattle命名空间中安装*Cert-Utils*操作符。这个操作符是由红帽咨询PAAS实践社区[29](#footnote-153)开发的，旨在帮助管理证书和JVM密钥库以及信任库。
- en: To use this Operator, we first create a ConfigMap containing a set of specific
    annotations. The Cert-Utils operator will detect these annotations and create
    a TrustStore containing the relevant certificates and keys; it will also add the
    TrustStore to the ConfigMap. Finally, we can mount the ConfigMap into a Deployment
    and instruct the JVM to use that TrustStore. The following resource definition
    will create the TrustStore with the relevant certificates and keys.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用此操作符，我们首先创建一个包含特定注解的ConfigMap。Cert-Utils操作符将检测这些注解，并创建一个包含相关证书和密钥的信任库；它还会将信任库添加到ConfigMap中。最后，我们可以将ConfigMap挂载到部署中，并指示JVM使用该信任库。以下资源定义将创建一个包含相关证书和密钥的信任库。
- en: '[PRE274]'
  id: totrans-547
  prefs: []
  type: TYPE_PRE
  zh: '[PRE274]'
- en: '[PRE275]'
  id: totrans-548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE275]'
- en: '[PRE276]'
  id: totrans-549
  prefs: []
  type: TYPE_PRE
  zh: '[PRE276]'
- en: '[PRE277]'
  id: totrans-550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE277]'
- en: '[PRE278]'
  id: totrans-551
  prefs: []
  type: TYPE_PRE
  zh: '[PRE278]'
- en: '[PRE279]'
  id: totrans-552
  prefs: []
  type: TYPE_PRE
  zh: '[PRE279]'
- en: '[PRE280]'
  id: totrans-553
  prefs: []
  type: TYPE_PRE
  zh: '[PRE280]'
- en: '[PRE281]'
  id: totrans-554
  prefs: []
  type: TYPE_PRE
  zh: '[PRE281]'
- en: '[PRE282]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE282]'
- en: '[29](#footnote-153-backlink) [https://github.com/redhat-cop/cert-utils-operator](https://github.com/redhat-cop/cert-utils-operator)'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '[29](#footnote-153-backlink) [https://github.com/redhat-cop/cert-utils-operator](https://github.com/redhat-cop/cert-utils-operator)'
- en: 'This does the following:'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的操作如下：
- en: The `service.beta.openshift.io/inject-cabundle` annotation will inject the service
    signing certificate bundle into the ConfigMap as a `service-sa.crt` field.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`service.beta.openshift.io/inject-cabundle`注解将把服务签名证书包注入到ConfigMap中，作为`service-sa.crt`字段。'
- en: The `cert-utils-operator.redhat-cop.io` annotation will create the Java TrustStore
    in the ConfigMap under the name `truststore.jks` with the `jkpassword` password.
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cert-utils-operator.redhat-cop.io`注解将在ConfigMap中创建名为`truststore.jks`的Java信任存储，并使用`jkpassword`密码。'
- en: In the Tournament service, the following Quarkus configuration will mount the
    `java-truststore` ConfigMap and configure the JVM accordingly.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 在Tournament服务中，以下Quarkus配置将挂载`java-truststore` ConfigMap，并相应地配置JVM。
- en: '[PRE283]'
  id: totrans-561
  prefs: []
  type: TYPE_PRE
  zh: '[PRE283]'
- en: '[PRE284]'
  id: totrans-562
  prefs: []
  type: TYPE_PRE
  zh: '[PRE284]'
- en: '[PRE285]'
  id: totrans-563
  prefs: []
  type: TYPE_PRE
  zh: '[PRE285]'
- en: '[PRE286]'
  id: totrans-564
  prefs: []
  type: TYPE_PRE
  zh: '[PRE286]'
- en: '[PRE287]'
  id: totrans-565
  prefs: []
  type: TYPE_PRE
  zh: '[PRE287]'
- en: '[PRE288]'
  id: totrans-566
  prefs: []
  type: TYPE_PRE
  zh: '[PRE288]'
- en: '[PRE289]'
  id: totrans-567
  prefs: []
  type: TYPE_PRE
  zh: '[PRE289]'
- en: '[PRE290]'
  id: totrans-568
  prefs: []
  type: TYPE_PRE
  zh: '[PRE290]'
- en: '[PRE291]'
  id: totrans-569
  prefs: []
  type: TYPE_PRE
  zh: '[PRE291]'
- en: We've only just scratched the surface of operators. OpenShift ships with a number
    of supported operators and there are many community operators available as well.
    We used many community-based operators in this book, such as the Infinispan operator
    and Keycloak operator; there are productized versions of these operators available
    as well. There are many more operators from multiple vendors available from OperatorHub.[30](#footnote-152)
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是略微触及了操作员的表面。OpenShift配备了许多受支持的操作员，并且还有许多社区操作员可供使用。在本书中，我们使用了许多基于社区的操作员，例如Infinispan操作员和Keycloak操作员；这些操作员也有产品化版本。OperatorHub中还提供了来自多个供应商的更多操作员。[30](#footnote-152)
- en: It is also possible to write your own operators if required. The OperatorFramework[31](#footnote-151)
    is an open-source SDK with which you can write your own operators using either
    Go, Ansible, or Helm.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，也可以编写自己的操作员。OperatorFramework[31](#footnote-151)是一个开源SDK，您可以使用Go、Ansible或Helm编写自己的操作员。
- en: '[30](#footnote-152-backlink) [https://operatorhub.io/](https://operatorhub.io/)'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '[30](#footnote-152-backlink) [https://operatorhub.io/](https://operatorhub.io/)'
- en: '[31](#footnote-151-backlink) [https://operatorframework.io/](https://operatorframework.io/)'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '[31](#footnote-151-backlink) [https://operatorframework.io/](https://operatorframework.io/)'
- en: Service Serving Certificate Secrets
  id: totrans-574
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 服务提供证书机密
- en: Keycloak uses an OpenShift feature called *service serving certificate secrets*.[32](#footnote-150)
    This is used for traffic encryption. Using this feature, OpenShift automatically
    generates certificates signed by the OpenShift certificate authority and stores
    them in a secret. The application, in this case Keycloak, can then mount this
    secret and use these certificates to encrypt traffic. Any application interacting
    with a Keycloak instance then just has to trust these certificates. OpenShift
    also manages the lifecycle of these certificates and automatically generates new
    certificates when the existing certificates are about to expire.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: Keycloak使用OpenShift的一个功能，称为*服务提供证书机密*。[32](#footnote-150)该功能用于流量加密。使用此功能，OpenShift会自动生成由OpenShift证书颁发机构签名的证书，并将其存储在机密中。应用程序（在此情况下是Keycloak）可以挂载该机密并使用这些证书来加密流量。任何与Keycloak实例交互的应用程序只需信任这些证书。OpenShift还会管理这些证书的生命周期，并在现有证书即将到期时自动生成新证书。
- en: 'To turn on this feature, simply add the following annotation to a service:'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用此功能，只需将以下注释添加到服务中：
- en: '[PRE292]'
  id: totrans-577
  prefs: []
  type: TYPE_PRE
  zh: '[PRE292]'
- en: 'In the case of Keycloak, the operator does this as part of its processing:'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keycloak的情况下，操作员会将此作为其处理的一部分：
- en: '[PRE293]'
  id: totrans-579
  prefs: []
  type: TYPE_PRE
  zh: '[PRE293]'
- en: '[PRE294]'
  id: totrans-580
  prefs: []
  type: TYPE_PRE
  zh: '[PRE294]'
- en: '[PRE295]'
  id: totrans-581
  prefs: []
  type: TYPE_PRE
  zh: '[PRE295]'
- en: '[PRE296]'
  id: totrans-582
  prefs: []
  type: TYPE_PRE
  zh: '[PRE296]'
- en: '[PRE297]'
  id: totrans-583
  prefs: []
  type: TYPE_PRE
  zh: '[PRE297]'
- en: '[PRE298]'
  id: totrans-584
  prefs: []
  type: TYPE_PRE
  zh: '[PRE298]'
- en: '[PRE299]'
  id: totrans-585
  prefs: []
  type: TYPE_PRE
  zh: '[PRE299]'
- en: '[PRE300]'
  id: totrans-586
  prefs: []
  type: TYPE_PRE
  zh: '[PRE300]'
- en: '[PRE301]'
  id: totrans-587
  prefs: []
  type: TYPE_PRE
  zh: '[PRE301]'
- en: 'The secret contains the actual certificate and associated key:'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 该机密包含实际的证书和相关的密钥：
- en: '[PRE302]'
  id: totrans-589
  prefs: []
  type: TYPE_PRE
  zh: '[PRE302]'
- en: '[PRE303]'
  id: totrans-590
  prefs: []
  type: TYPE_PRE
  zh: '[PRE303]'
- en: '[PRE304]'
  id: totrans-591
  prefs: []
  type: TYPE_PRE
  zh: '[PRE304]'
- en: '[PRE305]'
  id: totrans-592
  prefs: []
  type: TYPE_PRE
  zh: '[PRE305]'
- en: '[PRE306]'
  id: totrans-593
  prefs: []
  type: TYPE_PRE
  zh: '[PRE306]'
- en: '[PRE307]'
  id: totrans-594
  prefs: []
  type: TYPE_PRE
  zh: '[PRE307]'
- en: '[PRE308]'
  id: totrans-595
  prefs: []
  type: TYPE_PRE
  zh: '[PRE308]'
- en: '[PRE309]'
  id: totrans-596
  prefs: []
  type: TYPE_PRE
  zh: '[PRE309]'
- en: '[32](#footnote-150-backlink) [https://docs.openshift.com/container-platform/4.7/security/certificates/service-serving-certificate.html](https://docs.openshift.com/container-platform/4.7/security/certificates/service-serving-certificate.html)'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: '[32](#footnote-150-backlink) [https://docs.openshift.com/container-platform/4.7/security/certificates/service-serving-certificate.html](https://docs.openshift.com/container-platform/4.7/security/certificates/service-serving-certificate.html)'
- en: 'And it contains the certificate details as well:'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 它还包含证书详细信息：
- en: '[PRE310]'
  id: totrans-599
  prefs: []
  type: TYPE_PRE
  zh: '[PRE310]'
- en: '[PRE311]'
  id: totrans-600
  prefs: []
  type: TYPE_PRE
  zh: '[PRE311]'
- en: '[PRE312]'
  id: totrans-601
  prefs: []
  type: TYPE_PRE
  zh: '[PRE312]'
- en: '[PRE313]'
  id: totrans-602
  prefs: []
  type: TYPE_PRE
  zh: '[PRE313]'
- en: '[PRE314]'
  id: totrans-603
  prefs: []
  type: TYPE_PRE
  zh: '[PRE314]'
- en: '[PRE315]'
  id: totrans-604
  prefs: []
  type: TYPE_PRE
  zh: '[PRE315]'
- en: '[PRE316]'
  id: totrans-605
  prefs: []
  type: TYPE_PRE
  zh: '[PRE316]'
- en: '[PRE317]'
  id: totrans-606
  prefs: []
  type: TYPE_PRE
  zh: '[PRE317]'
- en: '[PRE318]'
  id: totrans-607
  prefs: []
  type: TYPE_PRE
  zh: '[PRE318]'
- en: '[PRE319]'
  id: totrans-608
  prefs: []
  type: TYPE_PRE
  zh: '[PRE319]'
- en: '[PRE320]'
  id: totrans-609
  prefs: []
  type: TYPE_PRE
  zh: '[PRE320]'
- en: '[PRE321]'
  id: totrans-610
  prefs: []
  type: TYPE_PRE
  zh: '[PRE321]'
- en: '[PRE322]'
  id: totrans-611
  prefs: []
  type: TYPE_PRE
  zh: '[PRE322]'
- en: '[PRE323]'
  id: totrans-612
  prefs: []
  type: TYPE_PRE
  zh: '[PRE323]'
- en: '[PRE324]'
  id: totrans-613
  prefs: []
  type: TYPE_PRE
  zh: '[PRE324]'
- en: Such Operator patterns simplify the burden of running complex middleware infrastructure
    applications on the OpenShift platform.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的操作员模式简化了在OpenShift平台上运行复杂中间件基础设施应用程序的负担。
- en: Conclusion
  id: totrans-615
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: To be able to successfully run your software at scale in production, a good
    understanding of the instrumentation that surrounds the software stack is required.
    OpenShift is a modern platform that provides all of the capabilities required
    to observe and, in a lot of cases, automatically heal your applications while
    they're running.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 要能够在生产环境中成功地大规模运行您的软件，需要对围绕软件堆栈的监控机制有很好的理解。OpenShift是一个现代化平台，提供了所有所需的功能来观察，并且在很多情况下，可以在应用程序运行时自动修复它们。
- en: In this chapter, we have discussed many common technical patterns that allow
    application developers to make use of these common platform capabilities. For
    example, one of the simplest patterns is to always log to STDOUT so the platform
    logging mechanisms can be leveraged. With containers, it becomes an antipattern
    to log to specific files mounted in a temporary filesystem within your container,
    because they are not clearly visible.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了许多常见的技术模式，这些模式使应用程序开发人员能够利用这些常见的平台功能。例如，最简单的模式之一是始终记录到STDOUT，这样平台的日志机制就可以被利用。对于容器来说，将日志记录到临时文件系统中的特定文件已成为一种反模式，因为这些文件不易查看。
- en: More complex patterns are also important to keep your business service applications
    running, even during disruption and change. Correctly configuring liveness, readiness,
    and startup probes so that your application can deploy without loss of service,
    configuring Pod disruption budgets for when nodes are restarted. Using application
    features to expose Prometheus metric endpoints for alerting and monitoring on
    the platform is a great way to alert teams when human interaction is required.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的模式对于保持业务服务应用程序的运行也非常重要，即使在发生中断和变更时。正确配置存活性、就绪性和启动探针，以确保应用程序可以部署而不会中断服务，配置
    Pod 中断预算，以应对节点重启。使用应用程序功能暴露 Prometheus 指标端点以便在平台上进行警报和监控，是一种很好的方式来提醒团队在需要人工干预时采取行动。
- en: 'The service mesh is an advanced extension to OpenShift, extrapolating many
    features that would have traditionally been packaged into your applications so
    they can be more efficiently managed at the platform level. This is a common theme:
    taking application and development cross-cutting features and leveraging them
    to the benefit of all platform services.'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格是 OpenShift 的高级扩展，提取了许多传统上会被打包到应用程序中的功能，以便能够在平台层面更高效地管理它们。这是一个常见的主题：将应用程序和开发的跨切面功能提取出来，并将其利用于所有平台服务的利益。
- en: The Operator pattern eases the operational burden of running complex middleware
    infrastructure applications on the OpenShift platform, packaging all the years
    of expert knowledge as software. It is no secret that OpenShift itself uses this
    fantastic pattern for all of its core capabilities. The real power comes in being
    able to lifecycle manage this complexity in an automated manner. Human toil is
    massively reduced because the system can self-heal and auto-upgrade without interference.
    Doing more with less is still the name of the game.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 操作符模式简化了在 OpenShift 平台上运行复杂中间件基础设施应用程序的操作负担，将多年的专家知识封装成软件。毫不奇怪，OpenShift 本身就为其所有核心功能采用了这一出色的模式。真正的强大之处在于能够以自动化的方式管理这一复杂性。由于系统能够自愈并自动升级而无需干预，人力劳动大大减少。以更少做更多的理念依然是游戏的规则。
- en: As a cross-functional product team, once you have learned and mastered these
    capabilities, it really does become possible to *give the developers the pagers*.
    The quality of any business service delivery starts with business discovery, which
    then transitions to application software, expands through to platform capabilities,
    and finally on and out into the world of networking and end user devices connected
    via the internet. Once developers and cross-functional product teams are empowered
    to build, run, and own their software—in every environment that it is required
    to run in—only then will they fully equate and connect happy customers with the
    software supply chain that they code, automate, and continuously deliver.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个跨职能的产品团队，一旦你掌握了这些能力，实际上就能够*把值班的任务交给开发人员*。任何商业服务交付的质量都始于业务发现，接着转向应用软件，扩展到平台功能，最终进入网络世界和通过互联网连接的终端设备。一旦开发人员和跨职能产品团队被授权在每一个需要运行的环境中构建、运行和拥有他们的软件，只有那时，他们才能真正地将客户满意度与他们编码、自动化并持续交付的软件供应链联系起来。
