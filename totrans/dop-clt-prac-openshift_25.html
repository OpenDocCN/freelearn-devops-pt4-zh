<html><head></head><body>
		<div>&#13;
			<div id="_idContainer560" class="Content">&#13;
			</div>&#13;
		</div>&#13;
		<div id="_idContainer561" class="Content">&#13;
			<h1 id="_idParaDest-442">17. <a id="_idTextAnchor711"/>Improve It</h1>&#13;
		</div>&#13;
		<div id="_idContainer571" class="Content">&#13;
			<p>We did it! We made it all the way around the Mobius Loop.</p>&#13;
			<p>First, we built a foundation of open culture, open leadership, and open technology. We successfully navigated our way around the Discovery Loop using practices such as the North Star and<a id="_idIndexMarker3174"/><a id="_idIndexMarker3175"/> Impact Mapping to discover our Why, and practices such as Empathy Mapping and other human-centered design tools to discover our Who. We even started the Discovery of our How by commencing some Event Storming, Non-Functional Mapping, and Metrics-Based Process Mapping. We did all of this to gather just enough information and <a id="_idIndexMarker3176"/><a id="_idIndexMarker3177"/>just enough collective, shared understanding to derive some measurable Target Outcomes.</p>&#13;
			<p>We used these Target Outcomes to guide our way through the Options Pivot. We explored several prioritization techniques and practices such as User Story Mapping, Value Slicing, impact and effort prioritization, how/now/wow prioritization, design sprints, and weighted-short-job-first to produce our<a id="_idIndexMarker3178"/><a id="_idIndexMarker3179"/> initial Product Backlog. We designed experiments that went into this backlog.</p>&#13;
			<p>We then moved to the first iteration of the Delivery Loop, where features were coded, applications were written, and we carried out experiments and<a id="_idIndexMarker3180"/><a id="_idIndexMarker3181"/> conducted research. We made use of established Agile practices to achieve this in a way where we could measure and learn against the original outcomes as quickly as possible.</p>&#13;
			<p>In the previous sections, we took a deeper dive into the technology that teams use to build, run, and own their solution.</p>&#13;
			<p>In each part of the Mobius Loop, we collected valuable information that was summarized at the end of the section on canvases. If we piece these three canvases together, we can see how<a id="_idIndexMarker3182"/><a id="_idIndexMarker3183"/> everything connects:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>Discovery:<ul style="list-style-type:disc;"><li>Who were we doing this for and why? </li>&#13;
<li>What was their problem, need, or opportunity?</li>&#13;
<li>What were the customer and organizational outcomes we set out with?</li>&#13;
<li>What was the impact on the outcomes?</li>&#13;
</ul></li>&#13;
				<li>Options:<ul style="list-style-type:disc;"><li>What were the actions we agreed to deliver?</li>&#13;
<li>What were the options that would help reach the outcomes?</li>&#13;
<li>What was the relative priority?</li>&#13;
<li>What did we learn?</li>&#13;
</ul></li>&#13;
				<li>Delivery:<ul style="list-style-type:disc;"><li>What was done?</li>&#13;
<li>What did we say we were going to research, experiment, and launch?</li>&#13;
</ul></li>&#13;
			</ul>&#13;
			<div>&#13;
				<div id="_idContainer562" class="IMG---Figure">&#13;
					<img src="../Images/B16297_17_01.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure">Figure 17.1: The Mobius Loop</p>&#13;
			<p>This chapter is called <em class="italics">Improve It</em>. Maybe the whole book should be called that because, really, continuous improvement is <a id="_idIndexMarker3184"/><a id="_idIndexMarker3185"/>what it's all about. Everything we do is focused on how we can continuously improve; whether this is the technology, our users' experience, our culture, or the metrics that we employ.</p>&#13;
			<p>In this chapter, we are going to explore what we do when we reach the end of an iteration of the Delivery Loop: What did we learn? Did we learn enough? Have we moved toward our target measurable outcomes? And, most importantly, what should we do next?</p>&#13;
			<h2 id="_idParaDest-443">W<a id="_idTextAnchor712"/><a id="_idTextAnchor713"/>hat Did We Learn?</h2>&#13;
			<p>In <em class="italics">Chapter 13</em>, <em class="italics">Measure and Learn</em>, we explored the techniques that we use to measure and learn from the increments of the products we deliver in a Delivery Loop by measuring the following:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>Feedback from Showcase and Retrospective events</li>&#13;
				<li>Learning from user testing</li>&#13;
				<li>Capturing the results of experiments run</li>&#13;
				<li>Service delivery and operational performance</li>&#13;
				<li>Service level agreements, service level indicators, and service level objectives</li>&#13;
				<li>Security</li>&#13;
				<li>Performance</li>&#13;
				<li>Culture</li>&#13;
				<li>Application metrics</li>&#13;
				<li>Infrastructure platform and resource usage</li>&#13;
			</ul>&#13;
			<p>This learning is very important and should drive conversations, inferences, and conclusions about what was learned. This is why visualizing these metrics is so powerful. Instantly, we can all see what the current measure is, what the measure was before the last iteration of the Delivery Loop, and what the target measure is to achieve the desired outcome and impact.</p>&#13;
			<p>If the conversation suggests we are not able to learn from these metrics, we need to inspect why that is. Running a deep Retrospective to ask why we are not learning enough from our Delivery Loops can be very helpful. Techniques such as the Five Whys or the Ishikawa Diagram are excellent deep retrospective approaches in facilitating these discussions and driving improvement actions that the team can put in place to facilitate learning.</p>&#13;
			<p>Ultimately, teams need to decide whether they are measuring what matters, whether the measures are accurate and reflective of their work, and whether the data is confidently taking them toward their Target Outcomes. The most important question teams should ask themselves is: <em class="italics">Did we learn enough?</em></p>&#13;
			<h2 id="_idParaDest-444">D<a id="_idTextAnchor714"/><a id="_idTextAnchor715"/>id We Learn Enough?</h2>&#13;
			<p>We have a very important decision to make at this point of the Mobius Loop. As we come out of the Delivery Loop, we can decide to go around the Delivery Loop again or we can return to the Options Pivot to revisit and reprioritize our options based on the metrics and learning captured during the Delivery Loop iteration. Otherwise, we could proceed back around the Discovery Loop.</p>&#13;
			<p>Questions we like to<a id="_idIndexMarker3186"/><a id="_idIndexMarker3187"/> ask based on our learning include the following:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>As a result of what we have delivered, have we reached an outcome? Or do we need to pull some more items from the Options list or <a id="_idIndexMarker3188"/><a id="_idIndexMarker3189"/>Product Backlog and do some more delivery? If so, turn right and go around the Delivery Loop again.</li>&#13;
				<li>Do the metrics captured from the most recent Delivery Loop suggest we've reached one or more measurable outcomes? If so, return to Discovery to validate this and work toward the next set of outcomes.</li>&#13;
				<li>Does the learning from our Delivery validate, invalidate, or improve understanding around assumptions and hypotheses made during Discovery? If so, let's go back to Discovery and update those artifacts.</li>&#13;
				<li>Has the learning from our Delivery given us some new information about the priorities of options? If so, let's go back to the Options Pivot and revisit some of those prioritization practices.</li>&#13;
				<li>Have the results from experiments given us some ideas for new or improved experiments? If so, let's go back to the Options Pivot and design those experiments.</li>&#13;
			</ul>&#13;
			<p>Over time, you will do more iterations of Discovery and Delivery Loops and spend more time in the Options Pivot. The Mobius Loop provides a <a id="_idIndexMarker3190"/><a id="_idIndexMarker3191"/>fantastic visualization as to how long you've spent on each loop, how fast you've traveled around each loop, and how often you've pivoted from one loop to another. It will also inform what level of balance you have between continuous discovery and continuous delivery.</p>&#13;
			<p>Some warning signs to look out <a id="_idIndexMarker3192"/><a id="_idIndexMarker3193"/>for include the following:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li><strong class="bold">We just keep going round and round in circles of the Delivery Loop</strong>. This suggests that we're not taking the time to revisit and re-assess outcomes and are moving toward being a feature factory and blindly building outputs.</li>&#13;
				<li><strong class="bold">We spend too much time on the Discovery Loop</strong>. This suggests that we are in a mode of <strong class="bold">analysis paralysis</strong>. We<a id="_idIndexMarker3194"/><a id="_idIndexMarker3195"/> overthink and overanalyze our Why and Who and never get to test our ideas or hypotheses. We may risk missing market windows or not delivering anything.</li>&#13;
				<li><strong class="bold">We jump from Discovery to Delivery</strong>. This suggests that we're not taking the learning from Discovery and distilling, organizing, and making some important decisions about what to deliver next or how to get knowledge and learning more quickly (through, for<a id="_idIndexMarker3196"/><a id="_idIndexMarker3197"/> example, research or experiments, as opposed to blindly building features).</li>&#13;
				<li><strong class="bold">We jump from Delivery to Discovery</strong>. This suggests we're not taking the time to factor in learning back to our options and prioritization.</li>&#13;
				<li><strong class="bold">We never move back to another part of the Mobius Loop</strong>. This suggests we're not working in an iterative or incremental way and not building learning into our system of work. This is really linear work and, as we saw in <em class="italics">Chapter 12</em>, <em class="italics">Doing Delivery</em>, when we explored <strong class="bold">Cynefin</strong>, it is only really a good solution when work is in a simple domain.</li>&#13;
			</ul>&#13;
			<p>Let's look at a story where a major pivot and shift between loops was triggered by learning.</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-445" class="Author-Heading"><a id="_idTextAnchor716"/>We Need Two Apps, Not One!</h2>&#13;
			<div>&#13;
				<div id="_idContainer563" class="IMG---Figure" style="float: right; margin: 6px; hight:6cm; width:6cm;">&#13;
					<img src="../Images/author_face_1.jpg" alt="" width="250" height="250"/>&#13;
				</div>&#13;
			</div>&#13;
			<p>This comes from one of our first European Open Innovation Labs residencies<span id="footnote-182-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="#footnote-182">1</a></span>. A start-up in Switzerland was looking to disrupt healthcare with a remote check-in application to the <strong class="bold">emergency room</strong> (<strong class="bold">ER</strong>). </p>&#13;
			<p>Together with the customer, we used Event Storming to understand the business process and identify key user flows. The team used User Story Mapping to identify the early slices that could be used to test the market, followed by Scrum to deliver a very early product in three one-week sprints.</p>&#13;
			<p>The focus on the first iteration of Delivery was all about the app that patients would use when they needed to go to the ER. It was entirely focused on their experience and most user research had been on these patient actors.</p>&#13;
			<p>When stakeholders saw the first increment of the app, there was a sudden epiphany moment. In order to fully appreciate, understand, and learn from this solution, we<a id="_idIndexMarker3198"/><a id="_idIndexMarker3199"/> would need to have prototypes for not one but two apps. The experience of doctors and nurses triaging requests would be key.</p>&#13;
			<p>This triggered an immediate shift back into the Discovery Loop to focus on these personas and processes as the team lacked insight into their needs and problems.</p>&#13;
			</div>&#13;
			<div id="footnote-182" class="_idFootnote" epub:type="footnote">&#13;
			<p class="FootNote"><a class="_idFootnoteAnchor _idGenColorInherit" href="#footnote-182-backlink">1</a>	<a href="https://www.redhat.com/en/blog/red-hat-welcomes-swiss-based-medical-company-easier-ag-waterford-its-emea-open-innovation-labs">https://www.redhat.com/en/blog/red-hat-welcomes-swiss-based-medical-company-easier-ag-waterford-its-emea-open-innovation-labs</a></p>&#13;
			</div>&#13;
			<p>This is one of many <a id="_idIndexMarker3200"/><a id="_idIndexMarker3201"/>examples where the team gathered <em class="italics">just enough</em> information and feedback to trigger a pivot and change in direction to initial thinking.</p>&#13;
			<h2 id="_idParaDest-446"><a id="_idTextAnchor717"/><a id="_idTextAnchor718"/>"Just Enough" Leads to Continuous Everything</h2>&#13;
			<p>Throughout this book, we've used the phrase <em class="italics">just enough</em> many times. Right back in the opening paragraphs, we wondered if you had just enough information from the back cover of this book to explore more. In <em class="italics">Chapter 7</em>, <em class="italics">Open Technical Practices – The Midpoint</em>, when we were building the technical foundation, we said we were building just enough of an architecture so that product developments kept moving forward. When we moved on to the Discovery Loop in <em class="italics">Chapter 8</em>, <em class="italics">Discovering the Why and Who</em>, we used practices to get just enough information to align people and get a shared understanding and enough confidence to go into the delivery of early experiments. In <em class="italics">Chapter 9</em>, <em class="italics">Discovering the How</em>, when we started to discover our How by using practices such as Event Storming, we got just enough information to be able to design how certain components would interact. And in <em class="italics">Chapter 10</em>, <em class="italics">Setting Outcomes</em>, we explained that we only ever do just enough Discovery to progress to the Options Pivot and start an iteration of Delivery.</p>&#13;
			<p>Now that we've been around the Delivery Loop, what was just enough information to start with has grown. We know much more now than when we first started. From the measures and learning captured out <a id="_idIndexMarker3202"/><a id="_idIndexMarker3203"/>of delivery practices, user testing, experimental results, and metrics systems, we now have the opportunity to revisit and update the Discovery practices. Let's look at a few examples.</p>&#13;
			<p>The Impact Map introduced in <em class="italics">Chapter 8</em>, <em class="italics">Discovering the Why and Who</em>, generated a lot of hypothesis statements. For example, in our PetBattle case study, we hypothesized that creating the Daily Tournament feature would increase site engagement by uploaders and this would help achieve our <a id="_idIndexMarker3204"/><a id="_idIndexMarker3205"/>overall goal to generate 100K in revenue with our existing and new customer base by the end of the year. Well, we've built that feature now. In fact, we created an A/B test in <em class="italics">Chapter 11</em>, <em class="italics">The Options Pivot</em>, and designed an experiment about how and when the next tournament should begin. What did the results tell us? Did we achieve the impact of more site engagement? Did this hypothesis prove true or false? Let's update the Impact Map.</p>&#13;
			<p>We met Mary, one of our users. We ran the Empathy Mapping practice and other human-centered design techniques from <em class="italics">Chapter 8</em>, <em class="italics">Discovering the Why and Who</em>. We heard from her how much she likes the top three leaderboards. We first did some user prototyping and testing with Mary. We could do another Empathy Map on the latest working software of PetBattle to capture the latest information on what she thinks, sees, hears, and says about the latest increment of the software. Let's update or create new Empathy Maps with Mary and other users.</p>&#13;
			<p>We used Event Storming to <a id="_idIndexMarker3206"/><a id="_idIndexMarker3207"/>get us just enough understanding of the business flow for the one where Mary enters the daily tournament and wins a prize. There were a number of pink square sticky notes on the Event Storm, which represented assumptions, questions, and unknowns. We know much more information now that we have delivered some features, conducted some research, run some experiments, and developed the conversation. We can update the Event Storm and perhaps even start to generate a whole new part of the system or area of functionality.</p>&#13;
			<p>Usually, we only ever have three or four Target Outcomes. Perhaps we've now met them or are close to meeting them; our learnings and measures may warrant us rethinking or rewriting Target Outcomes. Maybe we need to think about the next Target Outcomes to take our application to the next level and stay ahead of the competition.</p>&#13;
			<p>Our User Story Map, Value Slice Board, and prioritization practices were introduced in <em class="italics">Chapter 11</em>, <em class="italics">The Options Pivot</em>. We have now delivered items in the top slice or slices of value. Our learning may have triggered us to rethink existing priorities or re-slice and re-plan value delivery. Let's update these artifacts to reflect our latest viewpoints.</p>&#13;
			<p>Our Product Backlog is always <a id="_idIndexMarker3208"/><a id="_idIndexMarker3209"/>ready for more Product Backlog Refinement. With all of the updates to the Discovery and Options Pivot artifacts, it's sure to need another look and update. Let's refine the Product Backlog.</p>&#13;
			<p>So, all the artifacts we've produced from all the practices we have used are never done—they are living breathing artifacts. They should always be visible and accessible by team members and stakeholders. The more often we update these based on measures and learning, the more <a id="_idIndexMarker3210"/><a id="_idIndexMarker3211"/>valuable our top Product Backlog items will be, and the more responsive and reactive our products can be to user, market, and stakeholder needs. This is known as <strong class="bold">business agility</strong>.</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-447" class="Author-Heading"><a id="_idTextAnchor719"/>Learning from Security Experts</h2>&#13;
			<div>&#13;
				<div id="_idContainer5641" class="IMG---Figure" style="float: right; margin: 6px; hight:6cm; width:6cm;">&#13;
					<img src="../Images/author_face_1.jpg" alt="" width="250" height="250"/>&#13;
				</div>&#13;
			</div>&#13;
			<p>There's a lot of <a id="_idIndexMarker3212"/><a id="_idIndexMarker3213"/>debate in tech and Agile communities about how security fits into Agile processes. I recall <a id="_idIndexMarker3214"/><a id="_idIndexMarker3215"/>having a long conversation with the security controller of an Irish telecommunications company. He said, "<em class="italics">Agile and security do not mix and they never will.</em>"</p>&#13;
			<p>I was intrigued by this and wanted to understand (and empathize) more. Typically, he would be engaged by a project between one and four weeks before the go-live of a new system or application. He would scrutinize every framework used, the logical architecture, the physical architecture, hosting, and data storage, and would eventually run a series of security <a id="_idIndexMarker3216"/><a id="_idIndexMarker3217"/>penetration tests. Often, he would have to work nights and weekends because his security sign-off was one of the last milestones before the commercial launch of the associated product.</p>&#13;
			<p>We were determined to try and <em class="italics">shift left</em> this security analysis and the checks involved, to integrate the security controller into the process and make the penetration testing more continuous and automated.</p>&#13;
			<p>We started by inviting the security controller to all Sprint Review showcase events. He didn't show up to the first few but eventually came to one. I don't think he expected to get much out of this, but he was one of the most vocal stakeholders in the room. He quizzed us about what versions of frameworks were being used in the code, whether cross-site scripting had been considered in the UI, and where we were using open source tools in our CI/CD. We didn't have all the answers, but we did capture items on the Product Backlog to conduct some research and set up some refinement workshops in the next sprint.</p>&#13;
			<p>A couple of sprints later, we were <a id="_idIndexMarker3218"/><a id="_idIndexMarker3219"/>demonstrating the outputs, outcomes, and learning from the research items captured from earlier feedback. We were also showing how we had extended our CI/CD pipeline with automated code scanning and vulnerability checks—and we were researching automated solutions for cross-site scripting attacks.</p>&#13;
			<p>The security controller realized this was not going to be one of those projects putting him under severe pressure days before go-live. The team was learning from him and he was injecting his needs and knowledge into automation.  He would still have a week set aside to run his penetration tests but he felt much more confident. It turned out to be a very positive experience and one of the quickest security sign-offs ever done.</p>&#13;
			<p>This experience highlighted the importance of increasing confidence in important stakeholders like security controllers and sharing learning continuously between the team and stakeholders. And, of course, confidence can be measured through practices such as confidence voting!</p>&#13;
			</div>&#13;
			<p>This story highlights the importance of always improving metrics and automation. If someone is carrying out the same activities (such as security tests) repeatedly, it is a candidate for automation.</p>&#13;
			<h3 id="_idParaDest-448"><a id="_idTextAnchor720"/><a id="_idTextAnchor721"/>Always Improve Metrics and Automation</h3>&#13;
			<p>We introduced a wide range of <a id="_idIndexMarker3220"/>metrics in <em class="italics">Chapter 13</em>, <em class="italics">Measure and Learn</em>. This is not a one-time, finite list to follow as a guide, but an initial set of suggestions. Where we identify the need to measure based <a id="_idIndexMarker3221"/><a id="_idIndexMarker3222"/>on stakeholder feedback, we should add that in, especially if we set it as an opportunity to increase learning or confidence.</p>&#13;
			<p>Automating the collection, analysis, and<a id="_idIndexMarker3223"/><a id="_idIndexMarker3224"/> presentation (or visualization) of metrics is what closes feedback loops to<a id="_idIndexMarker3225"/><a id="_idIndexMarker3226"/> almost real time. Developers can get real-time feedback on the impact of code check-ins, while technical stakeholders receive real-time feedback, assurance, and confidence from the quality checks performed on built deliverables. Product Owners and business stakeholders, meanwhile, can receive such data on the usage and adoption of application features; and organizations on their overall ability and speed to deliver new features.</p>&#13;
			<p>This links nicely to how we can visualize and quantify, through metrics, the full impact of continuous delivery infrastructure.</p>&#13;
			<h3 id="_idParaDest-449"><a id="_idTextAnchor722"/><a id="_idTextAnchor723"/>Revisiting the Metrics-Based Process Map</h3>&#13;
			<p>Back in <em class="italics">Chapter 10</em>, <em class="italics">Setting Outcomes</em>, we introduced the Metrics-Based Process Mapping practice as a tool for discovering the case for continuous delivery. We have used this practice many times <a id="_idIndexMarker3227"/><a id="_idIndexMarker3228"/>with many different organizations. The metrics produced after adopting new practices, technology, and culture are mind-blowing.</p>&#13;
			<p>More recently, we've chosen not to use the practice on the first iteration of Discovery if we're working with a brand-new team. While it may appear to be a great way to capture the metrics of legacy processes, we've learned that new teams do not have the foundation of culture or psychological safety to engage in this activity. You have to ask very probing questions like <em class="italics">How long does it take you to do this process</em>? and <em class="italics">How often do you make mistakes or inaccuracies that are found in later tasks</em>, <em class="italics">meaning you need to do some rework on your bit</em>? Without safety in the culture, they can result in misleading answers and even damage the culture. However, we have found it an awesome practice to run after passing through some Delivery Loops.</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-450" class="Author-Heading"><a id="_idTextAnchor724"/>My management only really understand numbers and spreadsheets</h2>&#13;
			<div>&#13;
				<div id="_idContainer564" class="IMG---Figure" style="float: right; margin: 6px; hight:6cm; width:6cm;">&#13;
					<img src="../Images/author_face_1.jpg" alt="" width="250" height="250"/>&#13;
				</div>&#13;
			</div>&#13;
			<p>When working with a European automotive company, I ran daily Agile coaching sessions with the Product Owner <a id="_idIndexMarker3229"/><a id="_idIndexMarker3230"/>and ScrumMaster. This was an "ask me anything" type of session, which is often a great opportunity to chat about ways of working and generate different ideas to experiment with in the future.</p>&#13;
			<p>In the penultimate week of our engagement, the Product Owner said how happy he was with the team and the product that had evolved over the five preceding weeks. He loved the information radiators and visualization; he loved seeing the product come to life with Showcases every week and how the team was meeting real end users every week to test, experiment, and validate the software and approach. The technology was awesome and what the team was doing with CI/CD and other automation was just mind-blowing!</p>&#13;
			<p>But he was also concerned. This residency engagement had been run in a pop-up off-site facility. He had brought some of his peers in to "walk the walls" and see the way of working in action, but much of his leadership had not seen it. He explained how his management only really understood numbers and spreadsheets—they weren't really convinced by the use of sticky notes and colorful walls. How could he prove to them that this was truly a better way of working?</p>&#13;
			<p>This made me think of a practice in our Open Practice Library we had not yet used: Metrics-Based Process Mapping. This was exactly the tool to use for this particular problem. So, we got some sticky notes and a huge movable board.</p>&#13;
			<div>&#13;
				<div id="_idContainer565" class="IMG---Figure">&#13;
					<img src="../Images/B16297_17_02.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Figure 17.2: Using a Metrics-Based Process Map for an European automotive company</p>&#13;
			<p>We captured all the things that used to happen between their business or users requesting a feature and that feature running in production. We captured the Lead Time, Process Time, and Completeness and Accuracy metrics all as per the practice described in <em class="italics">Chapter 9</em>, <em class="italics">Discovering the How</em>.</p>&#13;
			<p>We captured these for the old way they used to deliver increments of their software and the new way. For the old way, they trawled through some ticketing systems to help them get metrics. For the new <a id="_idIndexMarker3231"/><a id="_idIndexMarker3232"/>way, they collected metrics from Jenkins, GitHub, Ansible, and other tools used during the engagement. They also visualized the old and new structures of teams. The resulting board is shown in <em class="italics">Figure 17.3</em>:</p>&#13;
			<div>&#13;
				<div id="_idContainer566" class="IMG---Figure">&#13;
					<img src="../Images/B16297_17_03.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Figure 17.3: Capturing different metrics from the legacy approach and the new approach</p>&#13;
			<p>The actual numbers and information on the sticky notes are not important for this book, but the process and<a id="_idIndexMarker3233"/><a id="_idIndexMarker3234"/> resulting visualization shows some big learnings. Each pink sticky note represents a <em class="italics">thing</em> that happens between a feature request and the feature running in production.</p>&#13;
			<p>Everything below the red line represents the old legacy approach to delivering features. The tasks were completed by people in different departments and teams—each represented by the horizontal yellow lines.</p>&#13;
			<p>As you read the pink sticky notes from left to right, every time you have to cross a yellow line to read the next sticky note to the right represents a handoff or a handover to another team. This often involved raising a ticket, booking a resource, reserving some time to communicate, and waiting or being in a queue.</p>&#13;
			<p>Everything above the red line represents the new approach to delivering this software product that had been kick-started by our engagement. It's all on one horizontal line because it is a new, long-lived, cross-functional product team doing all the work. Infrastructure, design, development, testing, deployment, and operations were all performed by this new product team.</p>&#13;
			<p>The green line represents where the pink task immediately above has been automated. This is CI/CD in<a id="_idIndexMarker3235"/><a id="_idIndexMarker3236"/> action! The pipeline has automated code scanning, builds, containerization, deployments, unit tests, end-to-end tests, contract tests, vulnerability, security tests, and UI tests. As we summed up the old ways and new ways and considered a few nuances for different types of development work this team would undertake, the measurable impact of continuous delivery was staggering. This was shown to a group of senior leaders, which brought a few gasps.</p>&#13;
			<div>&#13;
				<div id="_idContainer1010" class="IMG---Figure">&#13;
					<img src="../Images/B16297_Table_17.1.jpg" alt="" style="height:300px; width:600px;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Table 17.1: Measuring the impact of the new way of working</p>&#13;
			<p>One stakeholder asked what the main contributor to this shift in numbers was. The Product Owner explained that there were three things. Some of this was down to technology, including the OpenShift platform, being used. Some of this was down to the ways of working, including the Mobius Loop and Foundation practices that had been employed. But, most of all, it was because of this team. This long-lived product team had proven that they could not only achieve these metrics but would go on to improve them further.</p>&#13;
			<p>The three <a id="_idIndexMarker3237"/><a id="_idIndexMarker3238"/>contributing factors in the preceding story are the things we should strive to continuously improve by <a id="_idIndexMarker3239"/><a id="_idIndexMarker3240"/>continuous learning: improve the technology, improve the ways of working, and improve the team.</p>&#13;
			</div>&#13;
			<p>Most of this chapter has been about the ways of working and how we use the Mobius Loop model to <a id="_idIndexMarker3241"/><a id="_idIndexMarker3242"/>promote continuous improvement. Let's now consider the tech and then the team.</p>&#13;
			<h2 id="_idParaDest-451"><a id="_idTextAnchor725"/><a id="_idTextAnchor726"/>Improve the Technology</h2>&#13;
			<p>The challenge with writing any book about technology is that it can very quickly become obsolete—just ask two of the authors who wrote <em class="italics">DevOps with OpenShift</em> a few years before writing this. Many <a id="_idIndexMarker3243"/><a id="_idIndexMarker3244"/>of the patterns, technologies, and products have evolved since the writing of <em class="italics">DevOps with OpenShift</em>, and we fully expect that to continue in the coming years.</p>&#13;
			<p><em class="italics">Section 6, Build It, Run It, Own It</em>, was focused on the PetBattle technology and how the team would build, run, and own it in 2021. If you're reading this book in 2025, say, we are hopeful that many of the chapters in this book will still hold true, but <em class="italics">Section 6</em> may well have moved on.</p>&#13;
			<p>It's vital that teams are given ample time, space, and capacity to learn new technologies and experiment, research, and implement new products and frameworks to continuously improve the technology in the solutions they build.</p>&#13;
			<p>For a long-lived product team, this highlights the importance of great Product Ownership and well-refined Product Backlogs that can correctly articulate the value of all work items. This includes non-functional work to architectures to improve and evolve them, while also refreshing them with new technologies.</p>&#13;
			<p>Some of the approaches we use in Red Hat that we have <a id="_idIndexMarker3245"/><a id="_idIndexMarker3246"/>encouraged our customers to use include the following:</p>&#13;
			<ul style="list-style-type:disc;">&#13;
				<li>Allocating time to technology, research, and experimentation</li>&#13;
				<li>Strong regular investment in technology, training, and, where appropriate, certification</li>&#13;
				<li>Communities of practice and interest around a technology</li>&#13;
				<li>Establishing technical book clubs</li>&#13;
				<li>Organizing and hosting regular webinars, lunch and learns, and other community events</li>&#13;
				<li>Attending external technical conferences</li>&#13;
				<li>Attending internal conferences (for example, Red Hat runs a one-week internal Tech Exchange event every year for all its services and technical pre-sales staff)</li>&#13;
			</ul>&#13;
			<p>Above all, listen and learn from the team members and reserve time in each iteration to promote learning. Continuous investment in team learning is one of the strongest success factors of a high-performing, long-lived team.</p>&#13;
			<h2 id="_idParaDest-452"><a id="_idTextAnchor727"/><a id="_idTextAnchor728"/>Long Live the Team</h2>&#13;
			<p>From the outset of this book, we have <a id="_idIndexMarker3247"/><a id="_idIndexMarker3248"/>promoted the importance of long-lived, cross-functional teams. In <em class="italics">Chapter 1, Introduction – Start with Why</em>, we led <a id="_idIndexMarker3249"/><a id="_idIndexMarker3250"/>with how we want this book to help the I-shaped individual (specialists in one particular skill) to become more T-shaped (multi-functional knowledge and single skill depth), or <a id="_idIndexMarker3251"/><a id="_idIndexMarker3252"/>even M-shaped (multi-functional knowledge and multiple depth skills).</p>&#13;
			<p>Above, we explored some ways we can improve the technology. Those suggestions weren't really directly improving the tech. They were focused on the people using the technology and improving their abilities to use technology. How can we measure and learn the impact of improving team skills? What are some of the ways we can visualize this?</p>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-453" class="Author-Heading"><a id="_idTextAnchor729"/>Visualizing the Transition from I to T to M</h2>&#13;
			<p>A practice we've been improving over recent years has been capturing, measuring, and visualizing skills in a team. When we form a new team, for example, when we start one of our residency engagements, it's really helpful to understand individuals' skill sets and experience. But we want to do this in a psychologically safe way and not have new colleagues feel threatened or that they are under examination. So, we've stopped running tech-tests as it was affecting team morale. Instead, we use a simple visual such as a spider chart to visualize the skill levels (0-5) for the different capabilities needed in the cross-functional team:</p>&#13;
			<div>&#13;
				<div id="_idContainer568" class="IMG---Figure">&#13;
					<img src="../Images/B16297_17_04.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Figure 17.4: Visualizing the skills in the team using spider charts</p>&#13;
			<p>This can, of course, be done <a id="_idIndexMarker3253"/><a id="_idIndexMarker3254"/>using simple spreadsheet software:</p>&#13;
			<div>&#13;
				<div id="_idContainer569" class="IMG---Figure">&#13;
					<img src="../Images/B16297_17_05.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Figure 17.5: Using spreadsheets to analyze team skills</p>&#13;
			<p>It can also be visualized using different kinds of charts:</p>&#13;
			<div>&#13;
				<div id="_idContainer570" class="IMG---Figure">&#13;
					<img src="../Images/B16297_17_06.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p>Figure 17.6: Using histograms to analyze team skills</p>&#13;
			<p>As a long-lived team, this is a great practice to get into the habit of using. The team can regularly inspect whether there is team learning occurring and whether the team and individuals are becoming more cross-functional.</p>&#13;
			</div>&#13;
			<div style="background-color:#EEEEEE; display:block; overflow-x:auto; padding:.5em;margin: 5px;">&#13;
			<h2 id="_idParaDest-454" class="Author-Heading"><a id="_idTextAnchor730"/>Wizards and Cowboys</h2>&#13;
			<div>&#13;
			<div id="_idContainer573" class="IMG---Figure" style="float: right; margin: 6px; hight:6cm; width:6cm;">&#13;
				<img src="../Images/Donal.jpg" alt="" width="250" height="250"/>&#13;
			</div>&#13;
		</div>&#13;
			<p>Another fun learning <a id="_idIndexMarker3255"/><a id="_idIndexMarker3256"/>culture practice we've used recently is the analogy of <a id="_idIndexMarker3257"/><a id="_idIndexMarker3258"/>wizards and cowboys. In <em class="italics">Chapter 6, Open Technical Practices – Beginnings, Starting Right</em>, we explained why we are trying to eradicate the Unicorn Developer. We do not want to have heroes in the team that possess all the knowledge and are highly depended upon.</p>&#13;
			<p>When working with a geo-spatial company, we noticed in a Retrospective that there was a risk that the individuals were learning heroic things themselves and knowledge was not being shared. The team agreed to buy a wizard's hat and that, when these incidents happened, the person who had achieved something amazing would wear the wizard's hat until the knowledge had been shared in a pair or, even better, in a mobbing session.</p>&#13;
			<p>The cowboy hat was also used to highlight anti-patterns and things that people shouldn't have done (such as applying <strong class="inline">-DskipTests=true</strong> to get a build deployed without running automated tests). If such behaviors were spotted, whoever was responsible would have to wear the cowboy's hat until the anti-pattern had been shared with the whole team and they had been educated.</p>&#13;
			</div>&#13;
			&#13;
			<p>It's a bit of fun and <a id="_idIndexMarker3259"/><a id="_idIndexMarker3260"/>creates a <a id="_idIndexMarker3261"/><a id="_idIndexMarker3262"/>safe space for better team culture. It's also educational and promotes continuous learning.</p>&#13;
			<h2 id="_idParaDest-455"><a id="_idTextAnchor731"/><a id="_idTextAnchor732"/>Conclusion</h2>&#13;
			<p>In this chapter, we explored what to do when we come out of a Delivery Loop iteration to ensure we improve as we either go round the Delivery Loop again, return to the Options Pivot, or go back to the Discovery Loop. We looked at how we take metrics and learning from Delivery, assessing what we learned and whether this is enough to decide the next steps to be taken.</p>&#13;
			<p>We are now operating in a continuous flow of innovation from Discovery to Delivery and back again. We started with <em class="italics">just enough</em> and <em class="italics">just in time</em> information to get going. We're learning all the time.</p>&#13;
			<p>We looked at how we can measure the improvements in our new system and ways of working by returning to the Metrics-Based Process Mapping practice to quantify improvements in the technology, ways of working, and the team. We recognized the importance of continuous learning <a id="_idIndexMarker3263"/><a id="_idIndexMarker3264"/>and continuous improvement in all of these.</p>&#13;
			<p>In the final chapter of this book, we will look at ways to sustain everything we've covered in this book. There has been a great focus on one team and one dream. This has been intentional. As we look to sustain this, we will also see how we can re-use all of the patterns, approaches, and practices used throughout this book to grow, mature, and scale a product-centric mindset to applications and platforms—even to leadership and strategy.</p>&#13;
		</div>&#13;
</body></html>