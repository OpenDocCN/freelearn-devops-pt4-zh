- en: 17\. Improve It
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We did it! We made it all the way around the Mobius Loop.
  prefs: []
  type: TYPE_NORMAL
- en: First, we built a foundation of open culture, open leadership, and open technology.
    We successfully navigated our way around the Discovery Loop using practices such
    as the North Star and Impact Mapping to discover our Why, and practices such as
    Empathy Mapping and other human-centered design tools to discover our Who. We
    even started the Discovery of our How by commencing some Event Storming, Non-Functional
    Mapping, and Metrics-Based Process Mapping. We did all of this to gather just
    enough information and just enough collective, shared understanding to derive
    some measurable Target Outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: We used these Target Outcomes to guide our way through the Options Pivot. We
    explored several prioritization techniques and practices such as User Story Mapping,
    Value Slicing, impact and effort prioritization, how/now/wow prioritization, design
    sprints, and weighted-short-job-first to produce our initial Product Backlog.
    We designed experiments that went into this backlog.
  prefs: []
  type: TYPE_NORMAL
- en: We then moved to the first iteration of the Delivery Loop, where features were
    coded, applications were written, and we carried out experiments and conducted
    research. We made use of established Agile practices to achieve this in a way
    where we could measure and learn against the original outcomes as quickly as possible.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous sections, we took a deeper dive into the technology that teams
    use to build, run, and own their solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In each part of the Mobius Loop, we collected valuable information that was
    summarized at the end of the section on canvases. If we piece these three canvases
    together, we can see how everything connects:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Discovery:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who were we doing this for and why?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What was their problem, need, or opportunity?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What were the customer and organizational outcomes we set out with?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What was the impact on the outcomes?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Options:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What were the actions we agreed to deliver?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What were the options that would help reach the outcomes?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What was the relative priority?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What did we learn?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Delivery:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What was done?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What did we say we were going to research, experiment, and launch?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B16297_17_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.1: The Mobius Loop'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is called *Improve It*. Maybe the whole book should be called that
    because, really, continuous improvement is what it's all about. Everything we
    do is focused on how we can continuously improve; whether this is the technology,
    our users' experience, our culture, or the metrics that we employ.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to explore what we do when we reach the end of
    an iteration of the Delivery Loop: What did we learn? Did we learn enough? Have
    we moved toward our target measurable outcomes? And, most importantly, what should
    we do next?'
  prefs: []
  type: TYPE_NORMAL
- en: What Did We Learn?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In *Chapter 13*, *Measure and Learn*, we explored the techniques that we use
    to measure and learn from the increments of the products we deliver in a Delivery
    Loop by measuring the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Feedback from Showcase and Retrospective events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning from user testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capturing the results of experiments run
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service delivery and operational performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service level agreements, service level indicators, and service level objectives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Culture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infrastructure platform and resource usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This learning is very important and should drive conversations, inferences,
    and conclusions about what was learned. This is why visualizing these metrics
    is so powerful. Instantly, we can all see what the current measure is, what the
    measure was before the last iteration of the Delivery Loop, and what the target
    measure is to achieve the desired outcome and impact.
  prefs: []
  type: TYPE_NORMAL
- en: If the conversation suggests we are not able to learn from these metrics, we
    need to inspect why that is. Running a deep Retrospective to ask why we are not
    learning enough from our Delivery Loops can be very helpful. Techniques such as
    the Five Whys or the Ishikawa Diagram are excellent deep retrospective approaches
    in facilitating these discussions and driving improvement actions that the team
    can put in place to facilitate learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ultimately, teams need to decide whether they are measuring what matters, whether
    the measures are accurate and reflective of their work, and whether the data is
    confidently taking them toward their Target Outcomes. The most important question
    teams should ask themselves is: *Did we learn enough?*'
  prefs: []
  type: TYPE_NORMAL
- en: Did We Learn Enough?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have a very important decision to make at this point of the Mobius Loop.
    As we come out of the Delivery Loop, we can decide to go around the Delivery Loop
    again or we can return to the Options Pivot to revisit and reprioritize our options
    based on the metrics and learning captured during the Delivery Loop iteration.
    Otherwise, we could proceed back around the Discovery Loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Questions we like to ask based on our learning include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: As a result of what we have delivered, have we reached an outcome? Or do we
    need to pull some more items from the Options list or Product Backlog and do some
    more delivery? If so, turn right and go around the Delivery Loop again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do the metrics captured from the most recent Delivery Loop suggest we've reached
    one or more measurable outcomes? If so, return to Discovery to validate this and
    work toward the next set of outcomes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the learning from our Delivery validate, invalidate, or improve understanding
    around assumptions and hypotheses made during Discovery? If so, let's go back
    to Discovery and update those artifacts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Has the learning from our Delivery given us some new information about the priorities
    of options? If so, let's go back to the Options Pivot and revisit some of those
    prioritization practices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have the results from experiments given us some ideas for new or improved experiments?
    If so, let's go back to the Options Pivot and design those experiments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Over time, you will do more iterations of Discovery and Delivery Loops and spend
    more time in the Options Pivot. The Mobius Loop provides a fantastic visualization
    as to how long you've spent on each loop, how fast you've traveled around each
    loop, and how often you've pivoted from one loop to another. It will also inform
    what level of balance you have between continuous discovery and continuous delivery.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some warning signs to look out for include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**We just keep going round and round in circles of the Delivery Loop**. This
    suggests that we''re not taking the time to revisit and re-assess outcomes and
    are moving toward being a feature factory and blindly building outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**We spend too much time on the Discovery Loop**. This suggests that we are
    in a mode of **analysis paralysis**. We overthink and overanalyze our Why and
    Who and never get to test our ideas or hypotheses. We may risk missing market
    windows or not delivering anything.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**We jump from Discovery to Delivery**. This suggests that we''re not taking
    the learning from Discovery and distilling, organizing, and making some important
    decisions about what to deliver next or how to get knowledge and learning more
    quickly (through, for example, research or experiments, as opposed to blindly
    building features).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**We jump from Delivery to Discovery**. This suggests we''re not taking the
    time to factor in learning back to our options and prioritization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**We never move back to another part of the Mobius Loop**. This suggests we''re
    not working in an iterative or incremental way and not building learning into
    our system of work. This is really linear work and, as we saw in *Chapter 12*,
    *Doing Delivery*, when we explored **Cynefin**, it is only really a good solution
    when work is in a simple domain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at a story where a major pivot and shift between loops was triggered
    by learning.
  prefs: []
  type: TYPE_NORMAL
- en: We Need Two Apps, Not One!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](img/author_face_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This comes from one of our first European Open Innovation Labs residencies[1](#footnote-182).
    A start-up in Switzerland was looking to disrupt healthcare with a remote check-in
    application to the **emergency room** (**ER**).
  prefs: []
  type: TYPE_NORMAL
- en: Together with the customer, we used Event Storming to understand the business
    process and identify key user flows. The team used User Story Mapping to identify
    the early slices that could be used to test the market, followed by Scrum to deliver
    a very early product in three one-week sprints.
  prefs: []
  type: TYPE_NORMAL
- en: The focus on the first iteration of Delivery was all about the app that patients
    would use when they needed to go to the ER. It was entirely focused on their experience
    and most user research had been on these patient actors.
  prefs: []
  type: TYPE_NORMAL
- en: When stakeholders saw the first increment of the app, there was a sudden epiphany
    moment. In order to fully appreciate, understand, and learn from this solution,
    we would need to have prototypes for not one but two apps. The experience of doctors
    and nurses triaging requests would be key.
  prefs: []
  type: TYPE_NORMAL
- en: This triggered an immediate shift back into the Discovery Loop to focus on these
    personas and processes as the team lacked insight into their needs and problems.
  prefs: []
  type: TYPE_NORMAL
- en: '[1](#footnote-182-backlink) [https://www.redhat.com/en/blog/red-hat-welcomes-swiss-based-medical-company-easier-ag-waterford-its-emea-open-innovation-labs](https://www.redhat.com/en/blog/red-hat-welcomes-swiss-based-medical-company-easier-ag-waterford-its-emea-open-innovation-labs)'
  prefs: []
  type: TYPE_NORMAL
- en: This is one of many examples where the team gathered *just enough* information
    and feedback to trigger a pivot and change in direction to initial thinking.
  prefs: []
  type: TYPE_NORMAL
- en: '"Just Enough" Leads to Continuous Everything'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughout this book, we've used the phrase *just enough* many times. Right
    back in the opening paragraphs, we wondered if you had just enough information
    from the back cover of this book to explore more. In *Chapter 7*, *Open Technical
    Practices – The Midpoint*, when we were building the technical foundation, we
    said we were building just enough of an architecture so that product developments
    kept moving forward. When we moved on to the Discovery Loop in *Chapter 8*, *Discovering
    the Why and Who*, we used practices to get just enough information to align people
    and get a shared understanding and enough confidence to go into the delivery of
    early experiments. In *Chapter 9*, *Discovering the How*, when we started to discover
    our How by using practices such as Event Storming, we got just enough information
    to be able to design how certain components would interact. And in *Chapter 10*,
    *Setting Outcomes*, we explained that we only ever do just enough Discovery to
    progress to the Options Pivot and start an iteration of Delivery.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've been around the Delivery Loop, what was just enough information
    to start with has grown. We know much more now than when we first started. From
    the measures and learning captured out of delivery practices, user testing, experimental
    results, and metrics systems, we now have the opportunity to revisit and update
    the Discovery practices. Let's look at a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: The Impact Map introduced in *Chapter 8*, *Discovering the Why and Who*, generated
    a lot of hypothesis statements. For example, in our PetBattle case study, we hypothesized
    that creating the Daily Tournament feature would increase site engagement by uploaders
    and this would help achieve our overall goal to generate 100K in revenue with
    our existing and new customer base by the end of the year. Well, we've built that
    feature now. In fact, we created an A/B test in *Chapter 11*, *The Options Pivot*,
    and designed an experiment about how and when the next tournament should begin.
    What did the results tell us? Did we achieve the impact of more site engagement?
    Did this hypothesis prove true or false? Let's update the Impact Map.
  prefs: []
  type: TYPE_NORMAL
- en: We met Mary, one of our users. We ran the Empathy Mapping practice and other
    human-centered design techniques from *Chapter 8*, *Discovering the Why and Who*.
    We heard from her how much she likes the top three leaderboards. We first did
    some user prototyping and testing with Mary. We could do another Empathy Map on
    the latest working software of PetBattle to capture the latest information on
    what she thinks, sees, hears, and says about the latest increment of the software.
    Let's update or create new Empathy Maps with Mary and other users.
  prefs: []
  type: TYPE_NORMAL
- en: We used Event Storming to get us just enough understanding of the business flow
    for the one where Mary enters the daily tournament and wins a prize. There were
    a number of pink square sticky notes on the Event Storm, which represented assumptions,
    questions, and unknowns. We know much more information now that we have delivered
    some features, conducted some research, run some experiments, and developed the
    conversation. We can update the Event Storm and perhaps even start to generate
    a whole new part of the system or area of functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, we only ever have three or four Target Outcomes. Perhaps we've now
    met them or are close to meeting them; our learnings and measures may warrant
    us rethinking or rewriting Target Outcomes. Maybe we need to think about the next
    Target Outcomes to take our application to the next level and stay ahead of the
    competition.
  prefs: []
  type: TYPE_NORMAL
- en: Our User Story Map, Value Slice Board, and prioritization practices were introduced
    in *Chapter 11*, *The Options Pivot*. We have now delivered items in the top slice
    or slices of value. Our learning may have triggered us to rethink existing priorities
    or re-slice and re-plan value delivery. Let's update these artifacts to reflect
    our latest viewpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Our Product Backlog is always ready for more Product Backlog Refinement. With
    all of the updates to the Discovery and Options Pivot artifacts, it's sure to
    need another look and update. Let's refine the Product Backlog.
  prefs: []
  type: TYPE_NORMAL
- en: So, all the artifacts we've produced from all the practices we have used are
    never done—they are living breathing artifacts. They should always be visible
    and accessible by team members and stakeholders. The more often we update these
    based on measures and learning, the more valuable our top Product Backlog items
    will be, and the more responsive and reactive our products can be to user, market,
    and stakeholder needs. This is known as **business agility**.
  prefs: []
  type: TYPE_NORMAL
- en: Learning from Security Experts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](img/author_face_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There's a lot of debate in tech and Agile communities about how security fits
    into Agile processes. I recall having a long conversation with the security controller
    of an Irish telecommunications company. He said, "*Agile and security do not mix
    and they never will.*"
  prefs: []
  type: TYPE_NORMAL
- en: I was intrigued by this and wanted to understand (and empathize) more. Typically,
    he would be engaged by a project between one and four weeks before the go-live
    of a new system or application. He would scrutinize every framework used, the
    logical architecture, the physical architecture, hosting, and data storage, and
    would eventually run a series of security penetration tests. Often, he would have
    to work nights and weekends because his security sign-off was one of the last
    milestones before the commercial launch of the associated product.
  prefs: []
  type: TYPE_NORMAL
- en: We were determined to try and *shift left* this security analysis and the checks
    involved, to integrate the security controller into the process and make the penetration
    testing more continuous and automated.
  prefs: []
  type: TYPE_NORMAL
- en: We started by inviting the security controller to all Sprint Review showcase
    events. He didn't show up to the first few but eventually came to one. I don't
    think he expected to get much out of this, but he was one of the most vocal stakeholders
    in the room. He quizzed us about what versions of frameworks were being used in
    the code, whether cross-site scripting had been considered in the UI, and where
    we were using open source tools in our CI/CD. We didn't have all the answers,
    but we did capture items on the Product Backlog to conduct some research and set
    up some refinement workshops in the next sprint.
  prefs: []
  type: TYPE_NORMAL
- en: A couple of sprints later, we were demonstrating the outputs, outcomes, and
    learning from the research items captured from earlier feedback. We were also
    showing how we had extended our CI/CD pipeline with automated code scanning and
    vulnerability checks—and we were researching automated solutions for cross-site
    scripting attacks.
  prefs: []
  type: TYPE_NORMAL
- en: The security controller realized this was not going to be one of those projects
    putting him under severe pressure days before go-live. The team was learning from
    him and he was injecting his needs and knowledge into automation. He would still
    have a week set aside to run his penetration tests but he felt much more confident.
    It turned out to be a very positive experience and one of the quickest security
    sign-offs ever done.
  prefs: []
  type: TYPE_NORMAL
- en: This experience highlighted the importance of increasing confidence in important
    stakeholders like security controllers and sharing learning continuously between
    the team and stakeholders. And, of course, confidence can be measured through
    practices such as confidence voting!
  prefs: []
  type: TYPE_NORMAL
- en: This story highlights the importance of always improving metrics and automation.
    If someone is carrying out the same activities (such as security tests) repeatedly,
    it is a candidate for automation.
  prefs: []
  type: TYPE_NORMAL
- en: Always Improve Metrics and Automation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We introduced a wide range of metrics in *Chapter 13*, *Measure and Learn*.
    This is not a one-time, finite list to follow as a guide, but an initial set of
    suggestions. Where we identify the need to measure based on stakeholder feedback,
    we should add that in, especially if we set it as an opportunity to increase learning
    or confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Automating the collection, analysis, and presentation (or visualization) of
    metrics is what closes feedback loops to almost real time. Developers can get
    real-time feedback on the impact of code check-ins, while technical stakeholders
    receive real-time feedback, assurance, and confidence from the quality checks
    performed on built deliverables. Product Owners and business stakeholders, meanwhile,
    can receive such data on the usage and adoption of application features; and organizations
    on their overall ability and speed to deliver new features.
  prefs: []
  type: TYPE_NORMAL
- en: This links nicely to how we can visualize and quantify, through metrics, the
    full impact of continuous delivery infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting the Metrics-Based Process Map
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Back in *Chapter 10*, *Setting Outcomes*, we introduced the Metrics-Based Process
    Mapping practice as a tool for discovering the case for continuous delivery. We
    have used this practice many times with many different organizations. The metrics
    produced after adopting new practices, technology, and culture are mind-blowing.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, we've chosen not to use the practice on the first iteration of
    Discovery if we're working with a brand-new team. While it may appear to be a
    great way to capture the metrics of legacy processes, we've learned that new teams
    do not have the foundation of culture or psychological safety to engage in this
    activity. You have to ask very probing questions like *How long does it take you
    to do this process*? and *How often do you make mistakes or inaccuracies that
    are found in later tasks*, *meaning you need to do some rework on your bit*? Without
    safety in the culture, they can result in misleading answers and even damage the
    culture. However, we have found it an awesome practice to run after passing through
    some Delivery Loops.
  prefs: []
  type: TYPE_NORMAL
- en: My management only really understand numbers and spreadsheets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](img/author_face_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: When working with a European automotive company, I ran daily Agile coaching
    sessions with the Product Owner and ScrumMaster. This was an "ask me anything"
    type of session, which is often a great opportunity to chat about ways of working
    and generate different ideas to experiment with in the future.
  prefs: []
  type: TYPE_NORMAL
- en: In the penultimate week of our engagement, the Product Owner said how happy
    he was with the team and the product that had evolved over the five preceding
    weeks. He loved the information radiators and visualization; he loved seeing the
    product come to life with Showcases every week and how the team was meeting real
    end users every week to test, experiment, and validate the software and approach.
    The technology was awesome and what the team was doing with CI/CD and other automation
    was just mind-blowing!
  prefs: []
  type: TYPE_NORMAL
- en: But he was also concerned. This residency engagement had been run in a pop-up
    off-site facility. He had brought some of his peers in to "walk the walls" and
    see the way of working in action, but much of his leadership had not seen it.
    He explained how his management only really understood numbers and spreadsheets—they
    weren't really convinced by the use of sticky notes and colorful walls. How could
    he prove to them that this was truly a better way of working?
  prefs: []
  type: TYPE_NORMAL
- en: 'This made me think of a practice in our Open Practice Library we had not yet
    used: Metrics-Based Process Mapping. This was exactly the tool to use for this
    particular problem. So, we got some sticky notes and a huge movable board.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_17_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.2: Using a Metrics-Based Process Map for an European automotive company'
  prefs: []
  type: TYPE_NORMAL
- en: We captured all the things that used to happen between their business or users
    requesting a feature and that feature running in production. We captured the Lead
    Time, Process Time, and Completeness and Accuracy metrics all as per the practice
    described in *Chapter 9*, *Discovering the How*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We captured these for the old way they used to deliver increments of their
    software and the new way. For the old way, they trawled through some ticketing
    systems to help them get metrics. For the new way, they collected metrics from
    Jenkins, GitHub, Ansible, and other tools used during the engagement. They also
    visualized the old and new structures of teams. The resulting board is shown in
    *Figure 17.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_17_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.3: Capturing different metrics from the legacy approach and the new
    approach'
  prefs: []
  type: TYPE_NORMAL
- en: The actual numbers and information on the sticky notes are not important for
    this book, but the process and resulting visualization shows some big learnings.
    Each pink sticky note represents a *thing* that happens between a feature request
    and the feature running in production.
  prefs: []
  type: TYPE_NORMAL
- en: Everything below the red line represents the old legacy approach to delivering
    features. The tasks were completed by people in different departments and teams—each
    represented by the horizontal yellow lines.
  prefs: []
  type: TYPE_NORMAL
- en: As you read the pink sticky notes from left to right, every time you have to
    cross a yellow line to read the next sticky note to the right represents a handoff
    or a handover to another team. This often involved raising a ticket, booking a
    resource, reserving some time to communicate, and waiting or being in a queue.
  prefs: []
  type: TYPE_NORMAL
- en: Everything above the red line represents the new approach to delivering this
    software product that had been kick-started by our engagement. It's all on one
    horizontal line because it is a new, long-lived, cross-functional product team
    doing all the work. Infrastructure, design, development, testing, deployment,
    and operations were all performed by this new product team.
  prefs: []
  type: TYPE_NORMAL
- en: The green line represents where the pink task immediately above has been automated.
    This is CI/CD in action! The pipeline has automated code scanning, builds, containerization,
    deployments, unit tests, end-to-end tests, contract tests, vulnerability, security
    tests, and UI tests. As we summed up the old ways and new ways and considered
    a few nuances for different types of development work this team would undertake,
    the measurable impact of continuous delivery was staggering. This was shown to
    a group of senior leaders, which brought a few gasps.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_Table_17.1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 17.1: Measuring the impact of the new way of working'
  prefs: []
  type: TYPE_NORMAL
- en: One stakeholder asked what the main contributor to this shift in numbers was.
    The Product Owner explained that there were three things. Some of this was down
    to technology, including the OpenShift platform, being used. Some of this was
    down to the ways of working, including the Mobius Loop and Foundation practices
    that had been employed. But, most of all, it was because of this team. This long-lived
    product team had proven that they could not only achieve these metrics but would
    go on to improve them further.
  prefs: []
  type: TYPE_NORMAL
- en: 'The three contributing factors in the preceding story are the things we should
    strive to continuously improve by continuous learning: improve the technology,
    improve the ways of working, and improve the team.'
  prefs: []
  type: TYPE_NORMAL
- en: Most of this chapter has been about the ways of working and how we use the Mobius
    Loop model to promote continuous improvement. Let's now consider the tech and
    then the team.
  prefs: []
  type: TYPE_NORMAL
- en: Improve the Technology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The challenge with writing any book about technology is that it can very quickly
    become obsolete—just ask two of the authors who wrote *DevOps with OpenShift*
    a few years before writing this. Many of the patterns, technologies, and products
    have evolved since the writing of *DevOps with OpenShift*, and we fully expect
    that to continue in the coming years.
  prefs: []
  type: TYPE_NORMAL
- en: '*Section 6, Build It, Run It, Own It*, was focused on the PetBattle technology
    and how the team would build, run, and own it in 2021\. If you''re reading this
    book in 2025, say, we are hopeful that many of the chapters in this book will
    still hold true, but *Section 6* may well have moved on.'
  prefs: []
  type: TYPE_NORMAL
- en: It's vital that teams are given ample time, space, and capacity to learn new
    technologies and experiment, research, and implement new products and frameworks
    to continuously improve the technology in the solutions they build.
  prefs: []
  type: TYPE_NORMAL
- en: For a long-lived product team, this highlights the importance of great Product
    Ownership and well-refined Product Backlogs that can correctly articulate the
    value of all work items. This includes non-functional work to architectures to
    improve and evolve them, while also refreshing them with new technologies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the approaches we use in Red Hat that we have encouraged our customers
    to use include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Allocating time to technology, research, and experimentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strong regular investment in technology, training, and, where appropriate, certification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communities of practice and interest around a technology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establishing technical book clubs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organizing and hosting regular webinars, lunch and learns, and other community events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attending external technical conferences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attending internal conferences (for example, Red Hat runs a one-week internal
    Tech Exchange event every year for all its services and technical pre-sales staff)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Above all, listen and learn from the team members and reserve time in each iteration
    to promote learning. Continuous investment in team learning is one of the strongest
    success factors of a high-performing, long-lived team.
  prefs: []
  type: TYPE_NORMAL
- en: Long Live the Team
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From the outset of this book, we have promoted the importance of long-lived,
    cross-functional teams. In *Chapter 1, Introduction – Start with Why*, we led
    with how we want this book to help the I-shaped individual (specialists in one
    particular skill) to become more T-shaped (multi-functional knowledge and single
    skill depth), or even M-shaped (multi-functional knowledge and multiple depth
    skills).
  prefs: []
  type: TYPE_NORMAL
- en: Above, we explored some ways we can improve the technology. Those suggestions
    weren't really directly improving the tech. They were focused on the people using
    the technology and improving their abilities to use technology. How can we measure
    and learn the impact of improving team skills? What are some of the ways we can
    visualize this?
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the Transition from I to T to M
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A practice we''ve been improving over recent years has been capturing, measuring,
    and visualizing skills in a team. When we form a new team, for example, when we
    start one of our residency engagements, it''s really helpful to understand individuals''
    skill sets and experience. But we want to do this in a psychologically safe way
    and not have new colleagues feel threatened or that they are under examination.
    So, we''ve stopped running tech-tests as it was affecting team morale. Instead,
    we use a simple visual such as a spider chart to visualize the skill levels (0-5)
    for the different capabilities needed in the cross-functional team:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_17_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.4: Visualizing the skills in the team using spider charts'
  prefs: []
  type: TYPE_NORMAL
- en: 'This can, of course, be done using simple spreadsheet software:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_17_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.5: Using spreadsheets to analyze team skills'
  prefs: []
  type: TYPE_NORMAL
- en: 'It can also be visualized using different kinds of charts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16297_17_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.6: Using histograms to analyze team skills'
  prefs: []
  type: TYPE_NORMAL
- en: As a long-lived team, this is a great practice to get into the habit of using.
    The team can regularly inspect whether there is team learning occurring and whether
    the team and individuals are becoming more cross-functional.
  prefs: []
  type: TYPE_NORMAL
- en: Wizards and Cowboys
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](img/Donal.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Another fun learning culture practice we've used recently is the analogy of
    wizards and cowboys. In *Chapter 6, Open Technical Practices – Beginnings, Starting
    Right*, we explained why we are trying to eradicate the Unicorn Developer. We
    do not want to have heroes in the team that possess all the knowledge and are
    highly depended upon.
  prefs: []
  type: TYPE_NORMAL
- en: When working with a geo-spatial company, we noticed in a Retrospective that
    there was a risk that the individuals were learning heroic things themselves and
    knowledge was not being shared. The team agreed to buy a wizard's hat and that,
    when these incidents happened, the person who had achieved something amazing would
    wear the wizard's hat until the knowledge had been shared in a pair or, even better,
    in a mobbing session.
  prefs: []
  type: TYPE_NORMAL
- en: The cowboy hat was also used to highlight anti-patterns and things that people
    shouldn't have done (such as applying `-DskipTests=true` to get a build deployed
    without running automated tests). If such behaviors were spotted, whoever was
    responsible would have to wear the cowboy's hat until the anti-pattern had been
    shared with the whole team and they had been educated.
  prefs: []
  type: TYPE_NORMAL
- en: It's a bit of fun and creates a safe space for better team culture. It's also
    educational and promotes continuous learning.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we explored what to do when we come out of a Delivery Loop
    iteration to ensure we improve as we either go round the Delivery Loop again,
    return to the Options Pivot, or go back to the Discovery Loop. We looked at how
    we take metrics and learning from Delivery, assessing what we learned and whether
    this is enough to decide the next steps to be taken.
  prefs: []
  type: TYPE_NORMAL
- en: We are now operating in a continuous flow of innovation from Discovery to Delivery
    and back again. We started with *just enough* and *just in time* information to
    get going. We're learning all the time.
  prefs: []
  type: TYPE_NORMAL
- en: We looked at how we can measure the improvements in our new system and ways
    of working by returning to the Metrics-Based Process Mapping practice to quantify
    improvements in the technology, ways of working, and the team. We recognized the
    importance of continuous learning and continuous improvement in all of these.
  prefs: []
  type: TYPE_NORMAL
- en: In the final chapter of this book, we will look at ways to sustain everything
    we've covered in this book. There has been a great focus on one team and one dream.
    This has been intentional. As we look to sustain this, we will also see how we
    can re-use all of the patterns, approaches, and practices used throughout this
    book to grow, mature, and scale a product-centric mindset to applications and
    platforms—even to leadership and strategy.
  prefs: []
  type: TYPE_NORMAL
