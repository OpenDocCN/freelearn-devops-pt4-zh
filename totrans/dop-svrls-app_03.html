<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Applying DevOps to AWS Lambda Applications</h1>
                </header>
            
            <article>
                
<p>Let's briefly look at what we learned about AWS Lambda functions. Amazon Web Services was the first web service platform to launch the serverless computing module Lambda, written in <strong>Lambda functions</strong>.<strong> </strong>Lambda functions are stateless and have no affinity with the underlying infrastructure. Lambda functions are executed in response to the events. These events could be an HTTP request, a change of the data in the S3 bucket, a change in a DynamoDB table, or a change in Kinesis or SNS. Lambda functions replicate faster in response to events, and descale as the number of events goes down. </p>
<p>In this chapter, we will be covering various methods of deploying Lambda functions, looking at how we can painlessly deploy to multiple environments, unit test, system test, and integration test the Lambda function. We will also learn various deployment best practices and go through a few example recipes using these best practices. We will see how we can manage the AWS Lambda logs and move them to the ELK stack. </p>
<p>We will explore the following topics in this chapter:</p>
<ul>
<li>Manual deployment of a Lambda function</li>
<li>AWS Lambda with DevOps</li>
<li>Serverless with CodeStar</li>
<li>Blue and green deployment with AWS Lambda</li>
<li>The GitHub and Jenkins pipeline using Serverless Framework</li>
<li>Setting up Jenkins for a serverless application</li>
<li>Unit testing a deployed application</li>
<li>Integrating CloudWatch with ELK</li>
</ul>
<p><span>I will be creating a simple application that is used in our day-to-day work. The application is a thumbnail creator, and is a Node.js application that uses two S3 buckets. One bucket is for uploading the actual images and the other is for the thumbnails. The moment an image is uploaded into the images bucket, an event is triggered that calls a function to resize the image and upload it to the thumbnails bucket. We will first look at how we can manually execute this sequence of events, and then we will learn how we can streamline the process by automating the deployment process. In the section dealing with DevOps, we will talk about setting up an assembly line with the development environment, automated testing and deployment, applying a CI/CD pipeline, logging, and monitoring.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Manual deployment of a Lambda function</h1>
                </header>
            
            <article>
                
<p>The Node.js Lambda application that we will be using here is already part of AWS tutorials. We will learn how to create, deploy, and execute a Lambda application via the AWS portal. The prerequisite for this tutorial is for you to have an AWS account; we will be using a free AWS subscription throughout this chapter. The next step is to set up AWS CLI.</p>
<div class="packt_quote packt_infobox">You can create an AWS free account and an AWS CLI through the following links: <br/>
<a href="https://portal.aws.amazon.com/billing/signup#/start">https://portal.aws.amazon.com/billing/signup#/start</a><br/>
<a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html">https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html</a></div>
<p>Go through the following steps:</p>
<ol>
<li>Once the AWS account and CLIs are in place, sign in to AWS Console (<a href="https://aws.amazon.com/console/">https://aws.amazon.com/console/</a>), and then we will create an IAM user with the name <kbd>adminuser</kbd> by logging into your AWS account then either clicking on the IAM link or searching for the link through the services:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cf57886f-e0d2-4131-bccd-ceb032dfecc7.png" style="width:29.33em;height:5.17em;"/></div>
<ol start="2">
<li class="mce-root">Then we click on the <span class="packt_screen">Users</span> link on the left-hand side, where a new page will open. Then we click on <span class="packt_screen">Add User</span>. Add a user with the name <span class="packt_screen">adminuser </span>and select both the access type <span class="packt_screen">Programmatic access </span>and <span class="packt_screen">AWS Management Console access</span>. In the console's <span class="packt_screen">Password</span> field, add your custom password, uncheck the <span class="packt_screen">Require password reset</span> checkbox, and then click <span class="packt_screen">Next:Permissions</span>. Let's create a group by clicking on the <span class="packt_screen">Create Group </span>button. We will give the group the name of <kbd>administrators</kbd>. Next, let's select the <span class="packt_screen">AdminstrativeAccess </span>checkbox to provide full access to the group and then click on <span class="packt_screen">Create Group</span>. Now that we have a group created, let's click on <span class="packt_screen">Next:Review</span>. Then, we will review the user, which the user have created and has been added to the administrator group. Now click on the <span class="packt_screen">Create User</span> button. Once the user is created, we should be able to see the user in the list, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8671fd9f-547b-4b42-9545-86d29be225d3.png" style="width:30.92em;height:9.08em;"/></div>
<div class="packt_quote packt_infobox">We have created a user with administrative rights just for our tutorials, but in the real world the role and policies will be more restricted for the sake of security. </div>
<ol start="3">
<li>We need to create two buckets in AWS S3. The buckets need to be created through the <kbd>adminuser</kbd> <span>login, so let's log in to AWS Console using the new user that we have created. Click on the</span> <span class="packt_screen">adminuser </span><span>and select the</span> <span class="packt_screen">Security credentials</span><span> tab. Next, let's copy the </span>Console <span>URL for logging in</span><span>, then open it on the new browser tab. Feed in the username and password for the new user that we have created and click on</span> <span class="packt_screen">Sign In</span>. <span>Once you are logged in, search for</span> <span class="packt_screen">S3</span> <span>in the</span> <span class="packt_screen">AWS Services</span><span>. Then go to <span class="packt_screen">S3 Console Management</span> and click on the</span> <span class="packt_screen">Create bucket</span> button, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f9900192-269b-421a-9b38-b3f4336f9dfb.png" style="width:20.83em;height:7.17em;"/></div>
<p class="mce-root" style="padding-left: 90px">Let's add a unique bucket and region name of <span class="packt_screen">US East</span> by default. The two buckets should be named <kbd>Source</kbd><span class="packt_screen"> </span>and <kbd>SourceResized</kbd><span class="packt_screen">.</span> Source<span class="packt_screen"> </span>is the placeholder name that should be replaced with the actual bucket name—for example, <kbd>my-image-bucket76</kbd><span class="packt_screen"> </span>and <kbd>my-image-bucket76resized</kbd><strong>. </strong>So, <kbd>my-image-bucket76</kbd> will be the source bucket and <kbd>my-image-bucket76resized</kbd> will be the target bucket, as shown in the following screenshot:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/0a635283-5f83-40ce-938e-e15cc9d8e3d0.png" style="width:23.33em;height:16.83em;"/></div>
<div class="mce-root CDPAlignLeft CDPAlign packt_infobox"><span>The bucket name must be unique, as AWS allows only universally unique bucket names.</span></div>
<ol start="4">
<li class="mce-root">Once both the buckets are successfully created, we can upload an image for the Lambda function to resize and push to the resized bucket. Let's upload a JPG image into the <kbd>my-source-bucket76</kbd><em><strong><span> </span></strong></em>source. Click on the bucket name, then upload an image to this bucket. This will redirect you to the bucket page. Click on the <span class="packt_screen">Upload</span> button and a popup will pop up. Then, select <span class="packt_screen">Add files</span> to browse for an image file from the local directory and then upload the image to the S<span class="packt_screen">3</span> bucket.</li>
<li>The next step is to create a Lambda function and run it manually. Here, we have to first follow three steps to create the deployment package, and then create the execution role (IAM role) and a Lambda function and test them. The deployment package is a ZIP file containing the Lambda function and its dependencies:
<ul>
<li>Let's create a deployment package. I will be using Node.js as the language for this practice application, but we can use Java and Python as well (it depends on the developer's preference).</li>
<li>A prerequisite for creating this deployment package is to have Node.js version 6.0 (<a href="https://nodejs.org/en/download/">https://nodejs.org/en/download/</a>) or later installed on you local environment. You should also make sure that npm is installed. </li>
<li>Then go through the following steps. We are downloading the npm libraries for the image resizing of the Node.js Lambda function, using the first of the two following commands. The second command will download the required libraries:</li>
</ul>
</li>
</ol>
<pre style="padding-left: 120px"><strong>$ mkdir tutorial1; cd tutorial1<br/></strong><strong>$ npm install async gm</strong></pre>
<ul>
<li style="list-style-type: none">
<ul>
<li>Open your favorite editor and copy the following script into the file named <kbd>CreateThumbnails.js</kbd>:</li>
</ul>
</li>
</ul>
<pre style="padding-left: 120px">// dependencies<br/>var async = require('async');<br/>var AWS = require('aws-sdk');<br/>var gm = require('gm')<br/>            .subClass({ imageMagick: true }); // Enable ImageMagick integration.<br/>var util = require('util');<br/>// constants<br/>var MAX_WIDTH  = 100;<br/>var MAX_HEIGHT = 100;<br/>// get reference to S3 client<br/>var s3 = new AWS.S3();<br/>exports.handler = function(event, context, callback) {<br/>    // Read options from the event.<br/>    console.log("Reading options from event:\n", util.inspect(event, {depth: 5}));<br/>    var srcBucket = event.Records[0].s3.bucket.name;<br/>    // Object key may have spaces or unicode non-ASCII characters.<br/>    var srcKey    =<br/>    decodeURIComponent(event.Records[0].s3.object.key.replace(/\+/g, " ")); <br/>    var dstBucket = srcBucket + "resized";<br/>    var dstKey    = "resized-" + srcKey;</pre>
<div class="packt_infobox" style="padding-left: 60px"><span>You can find the gist for <kbd>CreateThumbnails.js</kbd> at </span><a href="https://gist.github.com/shzshi/6e1cf435a4c1aa979e3a9a243c13c44a">https://gist.github.com/shzshi/6e1cf435a4c1aa979e3a9a243c13c44a</a>.</div>
<ul>
<li style="list-style-type: none">
<ul>
<li>As a sanity check, validate that the source and destination are different buckets:</li>
</ul>
</li>
</ul>
<pre style="padding-left: 90px">    if (srcBucket == dstBucket) {<br/>       callback("Source and destination buckets are the same.");<br/>        return;<br/>    }</pre>
<ul>
<li style="list-style-type: none">
<ul>
<li>Infer the image type:</li>
</ul>
</li>
</ul>
<pre style="padding-left: 120px">var typeMatch = srcKey.match(/\.([^.]*)$/);<br/>    if (!typeMatch) {<br/>        callback("Could not determine the image type.");<br/>        return;<br/>    }<br/>    var imageType = typeMatch[1];<br/>    if (imageType != "jpg" &amp;&amp; imageType != "png") {<br/>        callback('Unsupported image type: ${imageType}');<br/>        return;<br/>    }</pre>
<ul>
<li style="list-style-type: none">
<ul>
<li>Download the image from S3, transform it, and upload it to a different S3 bucket:</li>
</ul>
</li>
</ul>
<pre style="padding-left: 120px">async.waterfall([<br/>        function download(next) {<br/>            // Download the image from S3 into a buffer.<br/>            s3.getObject({<br/>                    Bucket: srcBucket,<br/>                    Key: srcKey<br/>                },<br/>                next);<br/>            },<br/>        function transform(response, next) {<br/>            gm(response.Body).size(function(err, size) {</pre>
<ul>
<li style="list-style-type: none">
<ul>
<li>Infer the scaling factor to avoid stretching the image unnaturally:</li>
</ul>
</li>
</ul>
<pre style="padding-left: 120px">var scalingFactor = Math.min(<br/>                    MAX_WIDTH / size.width,<br/>                    MAX_HEIGHT / size.height<br/>                );<br/>                var width  = scalingFactor * size.width;<br/>                var height = scalingFactor * size.height;</pre>
<ul>
<li style="list-style-type: none">
<ul>
<li>Transform the image buffer in memory:</li>
</ul>
</li>
</ul>
<pre style="padding-left: 120px">this.resize(width, height)<br/>                    .toBuffer(imageType, function(err, buffer) {<br/>                        if (err) {<br/>                            next(err);<br/>                        } else {<br/>                            next(null, response.ContentType, buffer);<br/>                        }<br/>                    });<br/>            });<br/>        },<br/>        function upload(contentType, data, next) {</pre>
<ul>
<li style="list-style-type: none">
<ul>
<li>Stream the transformed image to a different S3 bucket:</li>
</ul>
</li>
</ul>
<pre style="padding-left: 120px">s3.putObject({<br/>                    Bucket: dstBucket,<br/>                    Key: dstKey,<br/>                    Body: data,<br/>                    ContentType: contentType<br/>                },<br/>                next);<br/>            }<br/>        ], function (err) {<br/>            if (err) {<br/>                console.error(<br/>                    'Unable to resize ' + srcBucket + '/' + srcKey +<br/>                    ' and upload to ' + dstBucket + '/' + dstKey +<br/>                    ' due to an error: ' + err<br/>                );<br/>            } else {<br/>                console.log(<br/>                    'Successfully resized ' + srcBucket + '/' + srcKey<br/>+<br/>                    ' and uploaded to ' + dstBucket + '/' + dstKey<br/>                );<br/>            }<br/>            callback(null, "message");<br/>        }<br/>    );<br/>};</pre>
<p style="padding-left: 90px">Now we should see two  items in our <kbd>tutorials1</kbd> folder, namely the <kbd>CreateThumbnail.js</kbd> and <kbd>node_modules</kbd><em><strong> </strong></em>folders. Let's zip all these into a file named <kbd>tutorialsimg.zip</kbd><span>. T</span><span>his will be our Lambda function deployment package:</span></p>
<pre style="padding-left: 90px">$ cd <span>tutorials1</span><br/>$ zip -r ../tutorialsimg.zip *</pre>
<ol start="6">
<li>Next, we will create the execution role for the Lambda function in IAM. L<span>og into AWS Console, search for <span class="packt_screen">IAM services</span>, and </span><span>then go into IAM and click on the </span><span class="packt_screen">Roles</span> button.<span> Click on</span> <span class="packt_screen">Create Role</span><span>, select the </span><span class="packt_screen">AWS service, </span><span>and then</span> choose <span class="packt_screen">Lambda</span> as the service<span>. Then click on the</span> <span class="packt_screen">Next: Permission</span><strong> </strong><span>button. Then search for <span class="packt_screen">AWSLambda</span> in the</span> <span class="packt_screen">Policy Type</span> <span>box and check the </span><span class="packt_screen">AWSLambdaExecute </span><span>checkbox. Then search for</span> <span class="packt_screen">AmazonS3FullAccess </span><span>and select it. Then click on the </span><span class="packt_screen">Next:Review</span> button<span>. Next, on the <span class="packt_screen">Create Role</span> page, add the role name </span><span class="packt_screen">myLambdaRole</span><span>, add a description, and then click on</span> <span class="packt_screen">Create Role</span><span>. Now we have the role that contains the policies that we can use to execute the Lambda function in order to make changes to the content of the S3 bucket. </span></li>
<li>The next step is to deploy the Lambda functions and node modules on the AWS Lambda portal. Let's go to the AWS Console's home page, and under <span class="packt_screen">Services</span>, let's search for Lambda. We will be redirected to the Lambda home page. Click on <span class="packt_screen">Create Function</span>, then choose <span class="packt_screen">Author from scratch</span> in the <span class="packt_screen">Functions Information</span> section. Let's add the function name of <kbd>myfirstLambdafunction</kbd>, <span class="packt_screen">Runtime</span> as <strong>Node.js 6.10</strong>.<strong> </strong>Choose the existing role name of <span class="packt_screen">myLambdaRole </span>and click on <span class="packt_screen">Create Function</span>. Now we will be redirected to the Lambda function designer page. We will upload our Lambda function to the portal. Scroll down a bit and go to the <span class="packt_screen">Function Code </span>section, then select <span class="packt_screen">C<span class="packt_screen">ode entry type</span></span><strong> </strong>as <strong>U</strong><span class="packt_screen">pload a .ZIP file</span>, <span class="packt_screen">R<strong><span class="packt_screen">untime</span></strong></span><strong><span class="packt_screen"> </span></strong>as <span class="packt_screen">Node.js 6.10</span>, and type into the <span class="packt_screen">Handler </span>field the text <span class="packt_screen">CreateThumbnail.handler</span>. Now let's click on the <span class="packt_screen">Upload </span>button, select the file named <span class="packt_screen">tutorialsimg.zip</span><em><strong> </strong></em>and upload it. Once the package is uploaded successfully, we should be able to see the <span class="packt_screen">CreateThumbnail.js</span> file in the function editor with the <span class="packt_screen">node_modules</span> folder, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9fa6af60-bc13-49c1-82a2-0644a678f8d0.png" style="width:65.33em;height:24.33em;"/></div>
<ol start="8">
<li>Now that the function and node modules are uploaded, we will create an event to trigger the function. If we scroll to the top right-hand side of the screen, we will see a drop-down menu appear that we can use to add the event. Select <span class="packt_screen">Configure Test Events</span><strong>.</strong> A pop-up box will appear. Give the event the name of <kbd>myThumbnailEvent</kbd><span class="packt_screen"> </span>and in the text field, add the following listed JSON file. Make sure that you replace <kbd>my-source-bucket76</kbd> with your source bucket's name and <kbd>Baby.jpg</kbd> with your image name. Then go ahead and click <span class="packt_screen">Save</span>:<br/>
<div class="packt_infobox">You can find the event JSON file at <a href="https://gist.github.com/shzshi/7a498513ae43b6572c219843bbba277d">https://gist.github.com/shzshi/7a498513ae43b6572c219843bbba277d</a><span>.</span></div>
</li>
</ol>
<pre style="padding-left: 60px">{<br/>  "Records": [<br/>    {<br/>      "eventVersion": "2.0",<br/>      "eventSource": "aws:s3",<br/>      "awsRegion": "us-west-2",<br/>      "eventTime": "1970-01-01T00:00:00.000Z",<br/>      "eventName": "ObjectCreated:Put",<br/>      "userIdentity": {<br/>        "principalId": "AIDAJDPLRKLG7UEXAMPLE"<br/>      },<br/>      "requestParameters": {<br/>        "sourceIPAddress": "127.0.0.1"<br/>      },<br/>      "responseElements": {<br/>        "x-amz-request-id": "C3D13FE58DE4C810",<br/>        "x-amz-id-2": "FMyUVURIY8/IgAtTv8xRjskZQpcIZ9KG4V5Wp6S7S/JRWeUWerMUE5JgHvANOjpD"<br/>      },<br/>      "s3": {<br/>        "s3SchemaVersion": "1.0",<br/>        "configurationId": "testConfigRule",<br/>        "bucket": {<br/>          "name": "<em><strong>my-source-bucket76</strong></em>",<br/>          "ownerIdentity": {<br/>            "principalId": "A3NL1KOZZKExample"<br/>          },<br/>          "arn": "arn:aws:s3:::<em><strong>my-source-bucket76</strong></em>"<br/>        },<br/>        "object": {<br/>          "key": "<em><strong>Baby.jpg</strong></em>",<br/>          "size": 1024,<br/>          "eTag": "d41d8cd98f00b204e9800998ecf8427e",<br/>          "versionId": "096fKKXTRTtl3on89fVO.nfljtsv6qko"<br/>        }<br/>      }<br/>    }<br/>  ]<br/>}</pre>
<ol start="9">
<li>Now we have deployed the function, and have created S3 buckets and an event. Now let's invoke the function and see if our image in the source bucket is resized and pushed to the resized S3 bucket. To do this, click on <span class="packt_screen">Test</span>. We should see that the function has successfully executed in the logs (as shown in the following log text). If you refresh the resized named S3 bucket, you should be able to see resized image file. You can just download the resized file and see whether the resizing worked. We can also add the S3 put trigger to automatically trigger this <kbd>CreateThumbnail</kbd> function when any image file is uploaded to the source S3 bucket. </li>
</ol>
<div class="invoke-output packt_quote">eTag: 'd41d8cd98f00b204e9800998ecf8427e', versionId: '096fKKXTRTtl3on89fVO.nfljtsv6qko' } } } ] } 2018-06-14T21:07:25.469Z ea822830-7016-11e8-b407-9514918aacd8<br/>
<br/>
Successfully resized my-source-bucket76/Baby.jpg and uploaded to my-source-bucket76resized/resized-Baby.jpg END RequestId: ea822830-7016-11e8-b407-9514918aacd8</div>
<p>In this exercise, we learned how to create, build, deploy, and invoke the Lambda function manually, and we had to go through number of steps to get this working. Now let's say that we need to deploy hundreds or thousands of such Lambda functions for a banking application to the portal. To do this task manually, we would require lots of resources and time. This is where DevOps comes in really handy to make our life faster and more easy. Let's look closer at how we can use DevOps to automate all of the steps involved and make it much simpler to build, test, and deploy our Lambda functions. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AWS Lambda with DevOps</h1>
                </header>
            
            <article>
                
<p>To start implementing DevOps for AWS Lambda, we first create an assembly line. An assembly line outlines the stages involved when a developer creates code, tests the code, and then commits the code into a repository. The source code is pulled from the repository, and is then built and tested. After this, the static code analysis takes place. Once it is deployed into a production-like environment, acceptance tests are run against it. This is how the application is monitored, and how logging is managed. We will look at these stages using the recipes in this section. We will also look at two perspectives of DevOps—one through AWS's own set of tools, and the other through serverless frameworks, such as GitHub, Jenkins, Mocha (for testing), and JSHint (for source code analysis).</p>
<p>So the first step is to set up a local development environment where we can create a folder structure, add and change the source code, add images, and so on. We can run the source code locally, execute tests, and debug them for errors and failures. We will do this using a Node.js tutorial. We will set up a Node.js project from scratch with unit testing, source code analysis, and acceptance testing.</p>
<p>The Node.js application that I have created is a simple task manager. The underlying architecture is AWS Lambda functions with an API gateway to add, update, delete, and list the DynamoDB table. In short, we will be doing the <kbd>create</kbd>, <kbd>read</kbd>, <kbd>update</kbd>, and <kbd>delete</kbd> functions through AWS Lambda functions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Serverless frameworks with AWS CodePipeline</h1>
                </header>
            
            <article>
                
<p>As I have mentioned, our first approach will be through AWS's own DevOps tools. We will start with CodePipeline, which is an indigenous cloud-based tool of AWS that can help you to build, develop, and deploy applications quickly on AWS. We can set up continuous delivery very quickly with CodePipeline. It has its own dashboard, and it integrates easily with tools such as JIRA, Jenkins, GitHub, and other project-management tools. Let's look how we can use this for Lambda functions. We will be using the thumbnail application that was created earlier in the chapter. </p>
<p class="mce-root">The prerequisites for these recipes are as follows:</p>
<ul>
<li class="mce-root"><strong>AWS account and login credentials for AWS Console:</strong> Most of the setup for the first part of the tutorials will be done through AWS Console. We will using the same <span class="packt_screen">adminuser</span> account that we created earlier in the chapter.</li>
<li class="mce-root"><strong>GitHub repository:</strong> You need to create a repository and copy all the files and folders from the following repository into your repository:<br/>
<kbd>https://github.com/shzshi/aws-lambda-thumbnail.git</kbd></li>
<li><strong>CloudFormation service role:</strong> Go to the home page of AWS Console and search for <span class="packt_screen">IAM.</span> On the <span class="packt_screen">IAM</span> page, select <strong>Roles</strong> and then click on <span class="packt_screen">Create role</span>. On the <span class="packt_screen">Create role</span> page, select <span class="packt_screen">AWS Service</span> and choose <span class="packt_screen">CloudFormation </span>as the service. Then click on <span class="packt_screen">Next:Permission</span>, and on the permission policy page, select the <span class="packt_screen">AWSLambdaExecute </span>policy and click on <span class="packt_screen">Next:Review</span>. Once the review page is open, name the role as <kbd>myCloudFormationRole</kbd> and then click on <span class="packt_screen">Create role</span>. Now, for this service role, we need to add additional policies to execute the pipeline, so let's go to the roles. We will see our role in the list; let's click on it. In the <span class="packt_screen">Role Summary</span> page, click on <span class="packt_screen">Add inline policy</span>, and in the <span class="packt_screen">Create Policy</span> page, click on the <span class="packt_screen">JSON </span>tab and then replace the existing JSON script with the script within <kbd>cloudformationpolicy.json</kbd>, which is in the <kbd>aws-lambda-thumbnail</kbd> repository. Click on <span class="packt_screen">Review policy</span>. Now let's name the policy <kbd>myThumbnailPipelinePolicy</kbd>,<span> so that we have a service role for CloudFormation. </span></li>
<li><strong>Bucket for the CloudFormation package:</strong> We need to create a bucket for the CloudFormation package, so let's go to the home page of AWS Console and search for <span class="packt_screen">S3</span> in services. Next, let's create a bucket with the name of<span class="packt_screen"> my-cloud-formation-bucket</span>. This bucket is used for packaging our artifacts when we run the pipeline. </li>
</ul>
<p class="mce-root">Let's go through the following steps: </p>
<ol>
<li class="mce-root">To add and retrieve CodeStar permission for the <span class="packt_screen">adminuser</span>, go to AWS Console (<a href="https://console.aws.amazon.com/console/home">https://console.aws.amazon.com/console/home</a>) and log in with your root account credentials. This means that we need to log in as a free account user (we created this free account at the start of the chapter). If you go to the <span class="packt_screen">adminuser </span>login page, you will see a link at the bottom named <span class="packt_screen">Sign-in using root account credentials</span>. Once you are logged in, go to <span class="packt_screen">IAM services</span> and click on <span class="packt_screen">Users</span>. You should be able to see the <span class="packt_screen">adminuser</span> in the list. Now click on the <span class="packt_screen">adminuser </span>link to the <span class="packt_screen">Security credentials</span> tab. Scroll down to find the <span class="packt_screen">HTTPS Git credentials for AWS CodeCommit</span> section and<strong><span class="ng-scope"> </span></strong>click on the <span><span class="packt_screen">Generate </span></span>button. The credentials to authenticate AWS CodeCommit will then be generated. Copy or download the credentials. If you have not copied the access key ID and the secret access key, please generate new ones using the <span class="packt_screen">Create access key </span>button. Save the details for both of these somewhere for later use.</li>
<li>Let's now log into the console as <span class="packt_screen">adminuser</span>. Search for <span class="packt_screen">CodePipeline</span> from the home page. A page will open. On this page, click on <span class="packt_screen">Create pipeline</span>. You will then be redirected to the <span class="packt_screen">Create Pipeline</span> page. Let's name the pipeline <span class="packt_screen">myServerlessThumbnailPipeline </span>and click on <span class="packt_screen">Next Step</span>.</li>
<li>In <span class="packt_screen">Source Provider</span>, let's select <span class="packt_screen">GitHub</span>. We will then be asked to connect to GitHub; go ahead and connect using your credentials. The <span class="packt_screen">Repository </span>should be the one that we created as a prerequisite, and the <span class="packt_screen">Branch</span> should be the branch where our files are residing (for example, master). Once the details are added, click on <span class="packt_screen">Next step</span>. While setting up <span class="packt_screen">C<span class="packt_screen">odeBuild</span></span>, a role was created, so we need to add an extra policy for this. </li>
<li>In the <span class="packt_screen">Build provider, </span>select <span class="packt_screen">AWS CodeBuild</span>, and then in the <span class="packt_screen">Configure your Project</span> section, select <span class="packt_screen">Create a new build project</span>. Let's add the project details: The project name should be <kbd>myThumbnailCodeBuild</kbd>, the environment image should be <span class="packt_screen">Use an image managed by AWS CodeBuild</span>, the operating system should be <span class="packt_screen">Ubuntu</span>, the runtime should be <span class="packt_screen">Node.js</span>, and the version should be <span class="packt_screen">N<span class="packt_screen">o</span></span><span class="packt_screen">de.js 6.3.1</span>. Keep the rest of the details as their default values and click on <span class="packt_screen">Save build project</span>. We have successfully created a AWS CodeBuild project. However, it has also created a service role, and we need to add an additional policy for the CodeBuild project role. So let's open a new tab on the browser and log into AWS Console as <span class="packt_screen">adminuser</span>. Then, in services, search for <span class="packt_screen">IAM, </span>and on the <span class="packt_screen">IAM</span> page, go to <span class="packt_screen">Roles </span>and select the service role with the name<span class="packt_screen"> code-build-myThumbnailCodeBuild-service-role</span> or something similar.</li>
</ol>
<p style="padding-left: 90px">Now click on <span class="packt_screen">Add inline policy</span> and then click on the <span class="packt_screen">Create Policy</span> page. Choose the <span class="packt_screen">S3 </span>service and the <span class="packt_screen">PubObject</span> action from the <span class="packt_screen">Write</span> access level, and select the <span class="packt_screen">Resources</span> as <span class="packt_screen">All resources</span>. Finally, click <span class="packt_screen">Review Policy</span>. Name the policy <kbd>myThumbnailCodeBuildPolicy</kbd>. In the <span class="packt_screen">Summary</span> section, we should be able to see S3. Click on <span class="packt_screen">Create Policy</span>. Now we have a new policy for <span class="packt_screen">S3</span> that has been added to the <span class="packt_screen">CodeBuild</span> role. Let's go back to the create CodePipeline page. Click <span class="packt_screen">Next step</span>. </p>
<ol start="5">
<li>In the <span class="packt_screen">Deploy</span> template, let's set <span class="packt_screen">Deployment provider</span> as <span class="packt_screen">AWS CloudFormation</span>. Now that we're in the CloudFormation section, let's add all the details as shown in the following screenshot. The template file is basically an export file that will be used by CloudFormation: </li>
</ol>
<div class="CDPAlignCenter CDPAlign"> <strong>  <img src="assets/2395b0dc-31a5-4a28-b4d7-c112c5d8286a.png" style="width:40.33em;height:30.58em;"/></strong></div>
<ol start="6">
<li>Next, we will create a role to give permission to the AWS CodePipeline to use the resources. Click on <span class="packt_screen">Create role</span><strong> </strong>and go through the steps as it prompts you. Once the role is created, click on <span class="packt_screen">Next</span>. Then review the pipeline, and click on <span class="packt_screen">Create pipeline</span>. Our pipeline will trigger automatically. Our first stages of the pipeline worked fine. If we go to CloudFormation (<span class="packt_screen">Services</span> | <span class="packt_screen">CloudFormation</span>), we should be able to see the stack for the thumbnail that was created. If you tick the checkbox and select the events, you should be able to see the events that ran, as well as some other details. Let's go ahead and add more stages to the pipeline that we can approve, and then deploy the function. </li>
<li>Now, we will edit the pipeline and add some stages to deploy the thumbnail Lambda function. Let's click on <span class="packt_screen">Edit </span>and scroll to the bottom. Click on <span class="packt_screen"><span class="packt_screen">+</span> Stage</span>. Let's add a stage called <span class="packt_screen">Approval</span>, so that we can review and approve our work before deployment. Click on <span class="packt_screen">Action </span>and select <span class="packt_screen">approval</span> from the <span class="packt_screen">Action</span> category of the drop-down list. Let's name it as <kbd>Approval</kbd>. We can also add an SNS topic to get approval emails. To do this, let's go ahead and use the default values for the rest and click on <span class="packt_screen">Add action</span>.</li>
<li>The next stage to add is the deployment of the function from the Git repository to the Lambda function. C<span>lick on </span><span class="packt_screen">+ Stage</span><span>. Let's add a stage named </span><span class="packt_screen">Deploy</span>,<span> click on </span><span class="packt_screen">Action</span>, <span>and select </span><span class="packt_screen">Deploy </span><span>from the <span class="packt_screen">Action</span> category drop-down list. Next, let's name the action <span class="packt_screen">myDeploy</span> and select</span> <span class="packt_screen">AWS CloudFormation</span> <span>as the</span> <span class="packt_screen">Deployment Provider</span><span>. In the <span class="packt_screen">CloudFormation</span> section, let's add an action mode called </span><span class="packt_screen">execute a change set</span><span> and select the stack name </span><span class="packt_screen">mythumbnailstack </span><span>and the change set name </span><span class="packt_screen">mythumbnailchangeset</span>. Let's <span>leave the rest of the details as their defaults and click on</span> <span class="packt_screen">Add action</span>. </li>
<li><span>Now we have added two stages, let's save the pipeline by clicking</span> <span class="packt_screen">Save Pipelineline changes</span>.<span> We will be asked to continue; let's go ahead and click </span><span class="packt_screen">Save and Continue</span><span>. This time, the pipeline won't trigger automatically, so let's click on</span> <span class="packt_screen">Release change</span><strong> </strong><span>to start the pipeline. Once the pipeline has successfully completed, we should see all the stages in green, as shown in the following screenshot: </span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><span><br/>
<img src="assets/5fa9b772-911d-404c-8256-10978d51732a.png" style="width:21.75em;height:51.83em;"/></span></div>
<ol start="10">
<li>Let's check whether the function has been created, and try executing it. Let's go to the AWS Console home page and search for Lambda. We should be able to see that a <span class="packt_screen">thumbnail</span> function has been created. Let's open the Lambda function. On the <span class="packt_screen">Function</span> page, let's scroll to the top right-hand side of the page, where we will see a drop-down menu that we can use to add the event. Select<span> </span><span class="packt_screen">Configure Test Events</span>. A pop-up box will appear containing the name of the <span class="packt_screen">myThumbnailEvent </span>event and a text field. In the text field, add the following JSON file. Make sure that you replace the<span> </span><kbd>my-source-bucket76</kbd><span> </span>with your source bucket name and<span> </span><kbd>Baby.jpg</kbd><span> </span>with your image name. Then go ahead and<span> click on </span><span class="packt_screen">Save</span>:</li>
</ol>
<div class="packt_infobox">The event JSON file can be found at <a href="https://gist.github.com/shzshi/7a498513ae43b6572c219843bbba277d"><span class="underline">https://gist.github.com/shzshi/7a498513ae43b6572c219843bbba277d</span></a><span>.</span></div>
<ol start="11">
<li>Now we have deployed our function, and created our S3 buckets <span>and an event. Let's invoke the function. Click on </span><span class="packt_screen">Test</span><span>. Now you should see that the function has successfully executed. You will see the details in the logs, and if you refresh the S3 bucket named </span><span class="packt_screen">resized</span><span>, we should be able to see a resized image file. You can just download the resized file and see whether the resizing worked. We can also add an S3 put trigger in order to automatically trigger this <span class="packt_screen">CreateThumbnail</span> function when any image file is uploaded to the source S3 bucket. </span></li>
</ol>
<p>In this tutorial, we learned how to use CodePipeline, which is a CD platform of AWS, to deploy Lambda functions. It was pretty quick in deploying a function from GitHub into Lambda using different combinations of tools by AWS. But the cons of these tools are that you are charged for their use, and we have to really get our heads around CloudFormation and the roles. Now let's look at how to set up a pipeline using open source tools. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Continuous integration and continuous deployment with Lambda</h1>
                </header>
            
            <article>
                
<p>In this section, we will be using Jenkins, a serverless framework, and other open source software to set up Continuous Integration and Continuous Deployment. I have the whole project set up in a Git repository (<span><a href="https://github.com/shzshi/aws-lambda-dynamodb-mytasks.git">https://github.com/shzshi/aws-lambda-dynamodb-mytasks.git</a>)</span><span>, or we can go through the following steps listed in this section.</span></p>
<p><span>T</span>he application that we are using for this tutorial will create a Lambda function and an AWS API gateway for the task where we can test our Lambda function, which will manage tasks using CRUD operations to DynamoDB. <br/>
<a href="https://github.com/shzshi/aws-lambda-dynamodb-mytasks.git"/></p>
<p>First, let's create a folder structure using a serverless framework to create a template function, as shown in the following code. I am assuming that you are using Linux Terminal, and that all the instructions are Linux-Terminal-based:</p>
<pre><strong>$ serverless create --template aws-nodejs --path AWSLambdaMyTask</strong><br/><strong>$ cd AWSLambdaMyTask</strong></pre>
<p><span>We will see three files created w</span>ithin the <kbd>AWSLambdaMyTask</kbd> folder. This is a sample template for Node.js using a serverless framework. We will be modifying these files as per our example's need, as shown in the following code:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6e0bcfc9-8230-47e3-aaa8-55e1623674be.png"/></div>
<ol>
<li>Let's create two more folders within the <kbd>AWSLambdaMyTask</kbd> folder, namely <kbd>src</kbd> and <kbd>test</kbd>. The <kbd>src</kbd> phrase is our source code folder and <kbd>test</kbd> is the folder for our test cases, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ mkdir test</strong><br/><strong>$ mkdir src </strong> </pre>
<ol start="2">
<li>Then we will create a file called <kbd>package.json</kbd> using the editor. This file will hold the metadata that is relevant to the project. Copy the following content into the file. Please make whatever changes you need:</li>
</ol>
<pre style="padding-left: 60px">{<br/>  "name": "AWS-serverless-with-dynamodb",<br/>  "version": "1.0.0",<br/>  "description": "Serverless CRUD service exposing a REST HTTP interface",<br/>  "author": "Shashikant Bangera",<br/>  "dependencies": {<br/>    "uuid": "^3.0.1"<br/>  },<br/>  "keywords": [<br/>    "AWS",<br/>    "Deployment",<br/>    "CD/CI",<br/>    "serverless",<br/>    "task"<br/>  ],<br/>  "repository": {<br/>    "type": "git",<br/>    "url": ""<br/>  },<br/>  "bugs": {<br/>    "url": ""<br/>  },<br/>  "author": "Shashikant Bangera &lt;shzshi@gmail.com&gt;",<br/>  "license": "MIT",<br/>  "devDependencies": {<br/>    "AWS-sdk": "^2.6.7",<br/>    "request": "^2.79.0",<br/>    "mocha": "^3.2.0",<br/>    "serverless": "^1.7.0"<br/>},<br/>  "scripts": {<br/>    "test": "./node_modules/.bin/mocha"<br/>  }<br/>}</pre>
<ol start="3">
<li>Let's edit the <kbd>serverless.yml</kbd> file, as per our needs, as shown in the following snippet. You can find the file in the mentioned GitHub repo: <a href="https://github.com/shzshi/aws-lambda-dynamodb-mytasks/blob/master/serverless.yml">https://github.com/shzshi/aws-lambda-dynamodb-mytasks/blob/master/serverless.yml</a>:</li>
</ol>
<pre style="padding-left: 60px"># Welcome to Serverless!<br/>#<br/># This file is the main config file for your service.<br/># It's very minimal at this point and uses default values.<br/># You can always add more config options for more control.<br/># We've included some commented out config examples here.<br/># Just uncomment any of them to get that config option.<br/>#<br/># For full config options, check the docs:<br/>#    docs.serverless.com<br/>#<br/># Happy Coding!<br/>service: AWSLambdaMyTask<br/>frameworkVersion: "&gt;=1.1.0 &lt;2.0.0"<br/>provider:<br/>  name: AWS<br/>  runtime: nodejs4.3<br/>  environment:<br/>    DYNAMODB_TABLE: ${self:service}-${opt:stage, self:provider.stage}<br/>  iamRoleStatements:<br/>    - Effect: Allow<br/>      Action:<br/>        - dynamodb:Query<br/>        - dynamodb:Scan<br/>        - dynamodb:GetItem<br/>        - dynamodb:PutItem<br/>        - dynamodb:UpdateItem<br/>        - dynamodb:DeleteItem<br/>      Resource: "arn:AWS:dynamodb:${opt:region, self:provider.region}:*:table/${self:provider.environment.DYNAMODB_TABLE}"<br/>functions:<br/>  create:<br/>    handler: src/mytasks/create.create<br/>---<br/>  list:<br/>    handler: src/mytasks/list.list<br/>---<br/>  get:<br/>    handler: src/mytasks/get.get<br/>---<br/>  update:<br/>    handler: src/mytasks/update.update<br/>---<br/>  delete:<br/>    handler: src/mytasks/delete.delete<br/>---<br/>resources:<br/>  Resources:<br/>    mytasksDynamoDbTable:<br/>      Type: 'AWS::DynamoDB::Table'<br/>      DeletionPolicy: Retain<br/>      Properties:<br/>        AttributeDefinitions:<br/>          -<br/>            AttributeName: id<br/>            AttributeType: S<br/>        KeySchema:<br/>          -<br/>            AttributeName: id<br/>            KeyType: HASH<br/>        ProvisionedThroughput:<br/>          ReadCapacityUnits: 1<br/>          WriteCapacityUnits: 1<br/>        TableName: ${self:provider.environment.DYNAMODB_TABLE}</pre>
<ol start="4">
<li>Let's move the <kbd>src</kbd> directory, and create a file named <kbd>package.json</kbd> and a folder named <kbd>mytasks</kbd>, as shown in the following code. The <kbd>mytasks</kbd> folder will have Node.js files to create, delete, get, list, and update the DynamoDB table on the AWS:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cd src</strong><br/><strong>$ mkdir mytasks</strong><br/><strong>$ vim package.json</strong></pre>
<ol start="5">
<li>Copy the <span>following </span>content to <kbd>package.json</kbd>:</li>
</ol>
<pre style="padding-left: 60px">{<br/>  "name": "src",<br/>  "version": "1.0.0",<br/>  "description": "",<br/>  "main": "index.js",<br/>  "scripts": {<br/>    "test": "echo \"Error: no test specified\" &amp;&amp; exit 1"<br/>  },<br/>  "keywords": [],<br/>  "author": "",<br/>  "license": "MIT",<br/>  "dependencies": {<br/>    "uuid": "^2.0.3"<br/>  }<br/>}</pre>
<p style="padding-left: 90px"><span><span>Go to the </span></span> <kbd>mytasks</kbd> folder and create a <kbd>create.js</kbd> file to create, update, list, get, and delete the DynamoDB tables. The <kbd>create.js</kbd> files are handlers for functions in Lambda.</p>
<ol start="6">
<li>Add the following content to <kbd>src\mytasks\create.js</kbd>:</li>
</ol>
<pre style="padding-left: 60px">'use strict';<br/>const uuid = require('uuid');<br/>const AWS = require('AWS-sdk');<br/>const dynamoDb = new AWS.DynamoDB.DocumentClient();<br/>module.exports.create = (event, context, callback) =&gt; {<br/>  const timestamp = new Date().getTime();<br/>  const data = JSON.parse(event.body);<br/>  if (typeof data.text !== 'string') {<br/>    console.error('Validation Failed');<br/>    callback(null, {<br/>      statusCode: 400,<br/>      headers: { 'Content-Type': 'text/plain' },<br/>      body: 'Couldn\'t create the task item.',<br/>    });<br/>    return;<br/>  }<br/>  const params = {<br/>    TableName: process.env.DYNAMODB_TABLE,<br/>    Item: {<br/>      id: uuid.v1(),<br/>      text: data.text,<br/>      checked: false,<br/>      createdAt: timestamp,<br/>      updatedAt: timestamp,<br/>    },<br/>  };<br/>  // write the task to the database<br/>  dynamoDb.put(params, (error) =&gt; {<br/>    // handle potential errors<br/>    if (error) {<br/>      console.error(error);<br/>      callback(null, {<br/>        statusCode: error.statusCode || 501,<br/>        headers: { 'Content-Type': 'text/plain' },<br/>        body: 'Couldn\'t create the task item.',<br/>      });<br/>      return;<br/>    }<br/>    // create a response<br/>    const response = {<br/>      statusCode: 200,<br/>      body: JSON.stringify(params.Item),<br/>    };<br/>    callback(null, response);<br/>  });<br/>};</pre>
<ol start="7">
<li>Add the following content to <kbd>src\mytasks\delete.js</kbd>:</li>
</ol>
<pre style="padding-left: 60px">'use strict';<br/>const AWS = require('AWS-sdk'); // eslint-disable-line import/no-extraneous-dependencies<br/>const dynamoDb = new AWS.DynamoDB.DocumentClient();<br/>module.exports.delete = (event, context, callback) =&gt; {<br/>  const params = {<br/>    TableName: process.env.DYNAMODB_TABLE,<br/>    Key: {<br/>      id: event.pathParameters.id,<br/>    },<br/>  };<br/>  // delete the task from the database<br/>  dynamoDb.delete(params, (error) =&gt; {<br/>    // handle potential errors<br/>    if (error) {<br/>      console.error(error);<br/>      callback(null, {<br/>        statusCode: error.statusCode || 501,<br/>        headers: { 'Content-Type': 'text/plain' },<br/>        body: 'Couldn\'t remove the task item.',<br/>      });<br/>      return;<br/>    }<br/>    // create a response<br/>    const response = {<br/>statusCode: 200,<br/>      body: JSON.stringify({}),<br/>    };<br/>    callback(null, response);<br/>  });<br/>};</pre>
<ol start="8">
<li>Add the following content to <kbd>src\mytasks\get.js</kbd>:</li>
</ol>
<pre style="padding-left: 60px">'use strict';<br/>const AWS = require('AWS-sdk'); // eslint-disable-line import/no-extraneous-dependencies<br/>const dynamoDb = new AWS.DynamoDB.DocumentClient();<br/>module.exports.get = (event, context, callback) =&gt; {<br/>  const params = {<br/>    TableName: process.env.DYNAMODB_TABLE,<br/>    Key: {<br/>      id: event.pathParameters.id,<br/>    },<br/>  };<br/>  // fetch task from the database<br/>  dynamoDb.get(params, (error, result) =&gt; {<br/>    // handle potential errors<br/>if (error) {<br/>      console.error(error);<br/>      callback(null, {<br/>        statusCode: error.statusCode || 501,<br/>        headers: { 'Content-Type': 'text/plain' },<br/>        body: 'Couldn\'t fetch the task item.',<br/>      });<br/>      return;<br/>    }<br/>    // create a response<br/>    const response = {<br/>      statusCode: 200,<br/>      body: JSON.stringify(result.Item),<br/>    };<br/>    callback(null, response);<br/>  });<br/>};</pre>
<ol start="9">
<li>Add the following content to <kbd>src\mytasks\list.js</kbd>:</li>
</ol>
<pre style="padding-left: 60px">'use strict';<br/>const AWS = require('AWS-sdk'); // eslint-disable-line import/no-extraneous-dependencies<br/>const dynamoDb = new AWS.DynamoDB.DocumentClient();<br/>const params = {<br/>  TableName: process.env.DYNAMODB_TABLE,<br/>};<br/>module.exports.list = (event, context, callback) =&gt; {<br/>  // fetch all tasks from the database<br/>  dynamoDb.scan(params, (error, result) =&gt; {<br/>    // handle potential errors<br/>    if (error) {<br/>      console.error(error);<br/>      callback(null, {<br/>        statusCode: error.statusCode || 501,<br/>        headers: { 'Content-Type': 'text/plain' },<br/>        body: 'Couldn\'t fetch the tasks.',<br/>      });<br/>      return;<br/>    }<br/>    // create a response<br/>    const response = {<br/>      statusCode: 200,<br/>      body: JSON.stringify(result.Items),<br/>    };<br/>    callback(null, response);<br/>  });<br/>};</pre>
<ol start="10">
<li>Add the following content to <kbd>src\mytasks\update.js</kbd>:</li>
</ol>
<pre style="padding-left: 60px">'use strict';<br/>const AWS = require('AWS-sdk'); // eslint-disable-line import/no-extraneous-dependencies<br/>const dynamoDb = new AWS.DynamoDB.DocumentClient();<br/>module.exports.update = (event, context, callback) =&gt; {<br/>  const timestamp = new Date().getTime();<br/>  const data = JSON.parse(event.body);<br/>  // validation<br/>  if (typeof data.text !== 'string' || typeof data.checked !== 'boolean') {<br/>    console.error('Validation Failed');<br/>    callback(null, {<br/>      statusCode: 400,<br/>      headers: { 'Content-Type': 'text/plain' },<br/>      body: 'Couldn\'t update the task item.',<br/>    });<br/>    return;<br/>  }<br/>  const params = {<br/>    TableName: process.env.DYNAMODB_TABLE,<br/>    Key: {<br/>id: event.pathParameters.id,<br/>    },<br/>    ExpressionAttributeNames: {<br/>      '#task_text': 'text',<br/>    },<br/>    ExpressionAttributeValues: {<br/>      ':text': data.text,<br/>      ':checked': data.checked,<br/>      ':updatedAt': timestamp,    },<br/>    UpdateExpression: 'SET #task_text = :text, checked = :checked,<br/>updatedAt = :updatedAt',<br/>    ReturnValues: 'ALL_NEW',<br/>  };<br/>  // update the task in the database<br/>  dynamoDb.update(params, (error, result) =&gt; {<br/>    // handle potential errors<br/>    if (error) {<br/>      console.error(error);<br/>      callback(null, {<br/>        statusCode: error.statusCode || 501,<br/>        headers: { 'Content-Type': 'text/plain' },<br/>        body: 'Couldn\'t fetch the task item.',<br/>      });<br/>      return;<br/>    }<br/>// create a response<br/>    const response = {<br/>      statusCode: 200,<br/>      body: JSON.stringify(result.Attributes),<br/>    };<br/>    callback(null, response);<br/>  });<br/>};</pre>
<p style="padding-left: 90px">Now we will create test cases to unit test the code that we created. We will be using Mocha for unit testing and run the APIs again. Let's create a file called <kbd>data</kbd> in the <kbd>test</kbd> folder, as shown in the following screenshot. This will have the JSON data that the unit test will run on: </p>
<pre style="padding-left: 90px"><strong>$ mkdir test/data</strong></pre>
<ol start="11">
<li>Next, let's add the <kbd>test/createDelete.js</kbd> file, which will create DynamoDB data and delete it, once the test is complete, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">var assert = require('assert');<br/>var request = require('request');<br/>var fs = require('fs');<br/>describe('Create, Delete', function() {<br/>            this.timeout(5000);<br/>    it('should create a new Task, &amp; delete it', function(done) {<br/>                        // Build and log the path<br/>                        var path = "https://" + process.env.TASKS_ENDPOINT + "/mytasks";<br/>                        // Fetch the comparison payload<br/>                        require.extensions['.txt'] = function (module, filename) {<br/>                            module.exports = fs.readFileSync(filename, 'utf8');<br/>                        };<br/>                        var desiredPayload = require("./data/newTask1.json");<br/>                        // Create the new Task<br/>                        var options = {'url' : path, 'form': JSON.stringify(desiredPayload)};<br/>                        request.post(options, function (err, res, body){<br/>                                    if(err){<br/>                                                throw new Error("Create call failed: " + err);<br/>                                    }<br/>                                    assert.equal(200, res.statusCode, "Create Status Code != 200 (" + res.statusCode + ")");<br/>                                    var task = JSON.parse(res.body);<br/>                                    // Now delete the task<br/>                                    var deletePath = path + "/" +<br/>task.id;<br/>                                   request.del(deletePath, function (err, res, body){<br/>                                                if(err){<br/>                                                            throw new Error("Delete call failed: " + err);<br/>                                                }<br/>                                                assert.equal(200, res.statusCode, "Delete Status Code != 200 (" + res.statusCode + ")");<br/>                                                done();<br/>                                    });<br/>                        });<br/>    });<br/>});</pre>
<ol start="12">
<li>Now add the <kbd>test/createListDelete.js</kbd> file, which will create the DynamoDB data, list it, and then delete it once the test is complete, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">var assert = require('assert');<br/>var request = require('request');<br/>var fs = require('fs');<br/>describe('Create, List, Delete', function() {<br/>            this.timeout(5000);<br/>    it('should create a new task, list it, &amp; delete it', function(done) {<br/>                        // Build and log the path<br/>----<br/>                        // Fetch the comparison payload<br/>                        require.extensions['.txt'] = function (module, filename) {<br/>----<br/>                        // Create the new Task<br/>                        var options = {'url' : path, 'form': JSON.stringify(desiredPayload)};<br/>                        request.post(options, function (err, res, body){<br/>                                    if(err){<br/>                                                throw new Error("Create call failed: " + err);<br/>                                    }<br/>                                    assert.equal(200, res.statusCode, "Create Status Code != 200 (" + res.statusCode + ")");<br/>// Read the list, see if the new item is there at the end<br/>                                    request.get(path, function (err,<br/>res, body){<br/>                                                if(err){<br/>                                                            throw new Error("List call failed: " + err);<br/>                                                }<br/>                                                assert.equal(200, res.statusCode, "List Status Code != 200 (" + res.statusCode + ")");<br/>                                                var taskList = JSON.parse(res.body);<br/>                                                if(taskList[taskList.length-1].text = desiredPayload.text)     {<br/>                                                            // Item found, delete it<br/>-----<br/>                                                                        assert.equal(200, res.statusCode, "Delete Status Code != 200 (" + res.statusCode + ")");<br/>                                                                        done();<br/>                                                            });<br/>                                                } else {<br/>                                                            // Item not found, fail test<br/>                                                            assert.equal(true, false, "New item not found in list.");<br/>                                                            done();<br/>                                                }<br/>});<br/>                        });<br/>    });<br/>});</pre>
<ol start="13">
<li>Let's add the <kbd>test/createReadDelete.js</kbd> file, which will create the DynamoDB data, read it, and then delete it once the test is complete, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">var assert = require('assert');<br/>var request = require('request');<br/>var fs = require('fs');<br/>describe('Create, Read, Delete', function() {<br/>            this.timeout(5000);<br/>    it('should create a new Todo, read it, &amp; delete it', function(done) {<br/>                        // Build and log the path<br/>                        var path = "https://" + process.env.TASKS_ENDPOINT + "/mytasks";<br/>                        // Fetch the comparison payload<br/>                        require.extensions['.txt'] = function (module, filename) {<br/>                            module.exports = fs.readFileSync(filename, 'utf8');<br/>                        };<br/>                        var desiredPayload = require("./data/newTask1.json");<br/>                        // Create the new todo<br/>                        var options = {'url' : path, 'form': JSON.stringify(desiredPayload)};<br/>                        request.post(options, function (err, res, body){<br/>if(err){<br/>                                                throw new Error("Create call failed: " + err);<br/>                                    }<br/>                                    assert.equal(200, res.statusCode, "Create Status Code != 200 (" + res.statusCode + ")");<br/>                                    var todo = JSON.parse(res.body);<br/>                                    // Read the item<br/>                                    var specificPath = path + "/" + todo.id;<br/>                                    request.get(path, function (err, res, body){<br/>                                                if(err){<br/>                                                            throw new Error("Read call failed: " + err);<br/>                                                }<br/>                                                assert.equal(200,<br/>res.statusCode, "Read Status Code != 200 (" + res.statusCode + ")");<br/>                                                var todoList = JSON.parse(res.body);<br/>                                                if(todoList.text = desiredPayload.text)          <br/><br/>                                                            // Item found, delete it<br/>                                                            request.del(specificPath, function (err, res, body){<br/>                                                                        if(err){<br/>                                                                                    throw new Error("Delete call failed: " + err);<br/>                                                                        }<br/>                                                                        assert.equal(200, res.statusCode, "Delete Status Code != 200 (" + res.statusCode + ")");<br/>                                                                        done();<br/>                                                            });<br/>} else {<br/>                                                            // Item not found, fail test<br/>                                                            assert.equal(true, false, "New item not found in list.");<br/>                                                            done();<br/>                                                }<br/>                                    });<br/>                        });<br/>    });<br/>});</pre>
<p style="padding-left: 90px">Now we will create two test data files—<kbd>newTask1.json</kbd> and <kbd>newTask2.json</kbd>—that can be used for unit testing.</p>
<ol start="14">
<li>Let's create <kbd>data/newTask1.json</kbd> using the aforementioned data, as follows:</li>
</ol>
<pre style="padding-left: 60px">{ "text": "Learn Serverless" }</pre>
<ol start="15">
<li>Add the following JSON data to <kbd>data/newTask2</kbd>:</li>
</ol>
<pre style="padding-left: 60px">{ "text": "Test Serverless" }</pre>
<p>The project folder should now look like the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cf7304ea-a05f-4992-9159-6b49a986f525.png" style="width:14.08em;height:33.58em;"/></div>
<p>We need to create a repository on Git to push all the code that we created previously so that we can set up CI to the serverless project. I am assuming that Git has already been installed on the local server and that the Git repository is already in place. In my case, I have the following Git repository set up. I will <kbd>git clone</kbd>, add files and folders, and then push everything to the Git repository:</p>
<pre><strong>$ git clone https://github.com/shzshi/AWS-lambda-dynamodb-mytasks.git</strong></pre>
<p>There will be a folder with the name <kbd>AWS-lambda-dynamodb-mytasks</kbd>. Go into that directory, copy all the files that we created earlier, and then push it to the repository, as shown in the following code:</p>
<pre><strong>$ git add .</strong><br/><strong>$ git commit –m “my first commit”</strong><br/><strong>$ git push origin master</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up Jenkins for a serverless application</h1>
                </header>
            
            <article>
                
<p>Assuming that we already have Jenkins up and running, we need to install Node.js, and then we need to install Mocha on the Jenkins server <span>for unit testing</span><span>. After this, we need to install a serverless framework. You can use the Dockerfile (<a href="https://github.com/shzshi/aws-lambda-dynamodb-mytasks/blob/master/Dockerfile">https://github.com/shzshi/aws-lambda-dynamodb-mytasks/blob/master/Dockerfile</a>) from the aforementioned GitHub repository for Jenkins and serverless frameworks. If you are using Docker, you don't need to follow the steps for installing Node.js on Jenkins. </span></p>
<p>Go through the following steps to install Node.js on the Jenkins node:</p>
<pre><strong>$ curl -sL https://deb.nodesource.com/setup_6.x | sudo -E bash -</strong><br/><strong>$ sudo apt-get install -y nodejs</strong><br/><strong>$ sudo npm install -g serverless</strong></pre>
<p>Then go to the browser and open the Jenkins home page. Click on the <span class="packt_screen">New item </span><span>link.</span><span> This will open a new page, that will allow you to create a job with a name of your choosing. Select</span> <span class="packt_screen">Freestyle project</span><span>, which is the default selection, and click</span> <span class="packt_screen">OK</span> <span>to go ahead, as shown in the following screenshot:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0a92ecb3-6084-4dac-8eda-2c8e0cd8ad0b.png" style="width:56.42em;height:21.92em;"/></div>
<p>Now, we need to integrate Git source code with Jenkins and then build, deploy, and test our serverless application. First, let's add the Git repository to the Jenkins job, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/278ae203-16ec-4421-8b38-90bf8e85c3c7.png" style="width:56.75em;height:26.50em;"/></div>
<p>We need to parameterize the build to add <kbd>AWS_ACCESS_KEY_ID</kbd> and <kbd>AWS_SECRET_ACCESS_KEY</kbd>, as shown in the following screenshot. We will get <kbd>AWS_ACCESS_KEY_ID</kbd><span> and <kbd>AWS_SECRET_ACCESS_KEY </kbd></span>after we create an IAM user for the Serverless Framework to work:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4c4009df-eccb-4473-8e31-47ba94379e46.png" style="width:47.17em;height:34.58em;"/></div>
<p>Now go to <span class="packt_screen">Build </span> |<span class="packt_screen">Add build step</span> | <span class="packt_screen">Execute shell</span> | <span class="packt_screen">Execute build step</span> | <span class="packt_screen">Add build step</span> from the drop-down menu, which will open Command Prompt, where we will add the command that we need to run, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5beca5c5-748c-4f41-9cdb-32a674694cfc.png" style="width:58.08em;height:22.25em;"/></div>
<p>Once the build is successful, we will have successfully deployed the application to the AWS S3 bucket that was created by Serverless Framework. We will also have exposed the API and the functions, allowing them to be used by the application to perform CRUD functions, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/068f685e-1a3b-4527-969b-914006932438.png" style="width:58.08em;height:44.33em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Automated testing for Lambda functions</h1>
                </header>
            
            <article>
                
<p>In the previous recipes, we looked at how we can automate builds and deploy Lambda functions through Jenkins and Serverless Frameworks. But we can also unit test the deployed Lambda functions through Jenkins. In the following recipes, we will see how to unit test the Lambda function to check whether the function is deployed perfectly and works fine. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unit testing a deployed application</h1>
                </header>
            
            <article>
                
<p>Once the application is deployed, we can run the unit test on it. In order to try out some unit tests, I have created three unit tests in the <kbd>test</kbd> folder, and we will be testing them using Mocha. Let's create another job in Jenkins for setting up unit testing. Again, it would be a freestyle job in Jenkins, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4642c508-69e5-4d89-8215-3e1a47778795.png" style="width:62.08em;height:24.33em;"/></div>
<p>We add the Git repository path to Jenkins where our test cases are residing, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7643b548-96cb-4766-9a45-bcf48a9b399c.png" style="width:55.58em;height:23.42em;"/></div>
<p>Then we add an execute shell to run the unit test using Mocha, as shown in the following screenshot:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/0f958db1-3fca-4036-8585-4f961f5489fc.png" style="width:57.67em;height:31.42em;"/></div>
<p>As you can see in the following screenshot, our test cases are passing, which proves that our functions are running perfectly fine:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7101231a-051f-454c-b273-3f4c75ba6dba.png" style="width:63.92em;height:35.92em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AWS Lambda pipeline</h1>
                </header>
            
            <article>
                
<p>In this section of the chapter, we will move to the continuous delivery pipeline and do what we have been doing again: creating different environments, deploying to multiple environments, unit testing, system testing, and adding an approval process. In this pipeline, we will be using Jenkins, Groovy, Serverless Framework, and Docker to set up a Serverless Framework environment. Make sure that you have Docker installed on your machine.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Prerequisites</h1>
                </header>
            
            <article>
                
<p>We need to create a user for each stage or environment so that we can isolate environments and stage the deployment for each environment. To do this, go through the following steps for each user:</p>
<ol>
<li>Log in to an AWS account as the root user, and go to the <span class="packt_screen">IAM</span> (<span class="packt_screen">Identity and Access Management</span>) page.</li>
<li>Click on<span> </span><span class="packt_screen">Users </span>on the left-hand-side bar, then click on the <span class="packt_screen">Add User</span><span> </span>button and add the username <span class="packt_screen">dev-serverless</span>. Enable<span> </span><span class="packt_screen">programmatic access</span><strong> </strong>by checking the<strong> </strong>checkbox. Then click on the <span class="packt_screen">Next:Permissions </span>button. </li>
<li>On the <span class="packt_screen">Permissions</span> page, select<span> </span><span class="packt_screen">Attach existing policies directly</span>, and search for and select the<span> </span><span class="packt_screen">AdministratorAccess </span>checkbox. Then click on<span> </span><span class="packt_screen">Next:Review</span>.</li>
<li>Now check that everything is good and then click on<span> </span><span class="packt_screen">Create User</span>. This will create a user and show us the <span class="packt_screen">access key id </span>and <span class="packt_screen">secret access key</span>. Copy these these keys somewhere temporarily. </li>
<li>Now that we have the keys, we export them as environment variables so that they will be accessed by the framework to perform the required functions. </li>
<li>Repeat the preceding five steps for the <span class="packt_screen">sit-serverless</span> and <span class="packt_screen">prod-serverless</span> users. </li>
<li>CloudBees AWS Credentials Jenkins Plugin.</li>
</ol>
<p>Now, go through the following steps to create the pipeline:</p>
<ol>
<li>Git clone the following repository into a directory:</li>
</ol>
<pre style="padding-left: 60px">$ git clone https://github.com/shzshi/aws-lambda-dynamodb-mytasks.git</pre>
<ol start="2">
<li>Go into this directory and build the Docker image with the Dockerfile provided. With <kbd>docker images</kbd>, we should be able to see the Docker image with the <kbd>name docker build --rm -f Dockerfile -t aws-lambda-dynamodb-mytasks:latest</kbd>, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px"><span>$ cd </span>aws-lambda-dynamodb-mytasks<br/>$ docker build --rm -f Dockerfile -t aws-lambda-dynamodb-mytasks:latest .<br/>$ docker images</pre>
<ol start="3">
<li>Next, we will run the container and open Jenkins on the browser. The initial installation password can be found in the container run logs as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">$ mkdir jenkins<br/>$ docker run --rm -it -p 50000:50000 -p 8080:8080 -v &lt;FULL_PATH_TO_JENKINS_FOLDER&gt;/jenkins:/var/jenkins_home  aws-lambda-dynamodb-mytasks:latest</pre>
<ol start="4">
<li>Go to the browser and open <kbd>http://localhost:8080</kbd>. Copy the password from the container run; it should be something like the following output. Once you are logged in, install the suggested plugin and create a Jenkins user for future logins. </li>
</ol>
<div class="packt_quote"><em>Jenkins initial setup will be required. An admin user will have been created and a password generated. Use the following password to proceed to installation:</em><br/>
<br/>
<em>6050bfe89a9b463c8e2784060e2225b6</em><br/>
<br/>
<em>This may also be found at /var/jenkins_home/secrets/initialAdminPassword<span>.</span></em></div>
<p>Once Jenkins is up and running, we can go ahead and create a pipeline job, so click on <span class="packt_screen">New Item</span>, enter the item name as <kbd>my-serverless-pipeline</kbd>,<span class="packt_screen"> </span>and select the <span class="packt_screen">pipeline</span> project. In <span class="packt_screen">Job Configure</span>, select the <span class="packt_screen">This project is parameterized</span> <span>checkbox, and </span><span>then in</span> <span class="packt_screen">Add Parameter</span>,<strong> </strong><span>select</span> <span class="packt_screen">Credentials Parameter</span><strong> </strong>and <span>then go to the </span><span class="packt_screen">Default Value</span><span> </span><span>section</span><span> </span><span>of the <span class="packt_screen">Credentials Parameter</span> and click on</span> <span class="packt_screen">Add</span>. T<span>hen select</span> <span class="packt_screen">Jenkins</span>.<strong> </strong><span>This will open the <span class="packt_screen">Jenkins Credentials Provider</span> page. On this page in the </span><span class="packt_screen">Kind </span><span>drop-down menu, </span><span>select <span class="packt_screen">AWS Credentials</span> and add the users</span> <kbd>dev-serverless</kbd><span>,</span> <kbd>sit-serverless</kbd><span>, and</span> <kbd>prod-serverless</kbd>, as shown in the following screenshot<span>. Then, click</span> <span class="packt_screen">Add</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/37b9cac0-a83f-46cf-815b-b4f69eabe7d2.png" style="width:53.92em;height:19.17em;"/></div>
<p class="CDPAlignLeft CDPAlign">Once all the AWS credentials are added, pull them into the <span class="packt_screen">Credentials</span> parameter for AWS, as shown in the following screenshot. Make sure that all three types of credential parameter are added, namely <kbd>dev</kbd>, <kbd>sit</kbd>,<span class="packt_screen"> </span>and <kbd>prod</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4ec305eb-f033-49a6-97f9-0ec28aca345a.png" style="width:49.08em;height:22.75em;"/></p>
<p>You need to create your own Git repository and push the files from the repository named <span><a href="https://github.com/shzshi/aws-lambda-dynamodb-mytasks.git">https://github.com/shzshi/aws-lambda-dynamodb-mytasks.git</a>. Then, open the Jenkins file in your favorite editor and comment in all the system test code for the entire environment, as shown in the following code. The reason we are doing this is because we will be exporting the API gateway endpoint to execute the system test for the entire environment: </span></p>
<div>
<pre><span>stage (</span><span>'System Test on Dev'</span><span>) {<br/></span>steps {<br/>    <span>withCredentials([[$class: </span><span>'AmazonWebServicesCredentialsBinding'</span><span>, accessKeyVariable: </span><span>'AWS_ACCESS_KEY_ID'</span><span>, credentialsId: </span><span>'dev-serverless'</span><span>, secretKeyVariable: </span><span>'AWS_SECRET_ACCESS_KEY'</span><span>]]) {<br/>    </span>sh <span>''' <br/>        </span><span><strong>export TASKS_ENDPOINT=6pgn5wuqeh.execute-api.us-east-1.amazonaws.com/dev</strong><br/></span><span>./node_modules/mocha/bin/mocha ./test/*.js<br/></span><span>'''<br/></span><span>   }<br/></span><span>  }<br/></span><span>}<br/></span></pre></div>
<p>Now click on the <span class="packt_screen">Pipeline</span> tab and select <span class="packt_screen">Pipeline script the SCM</span> in the definition. Set the <span class="packt_screen">SCM</span> as <span class="packt_screen">Git </span>and in the <span class="packt_screen">Repository URL</span> field, add the Git repository path created by you. This repository has a Jenkins file, a Lambda function, and a test folder. Leave the rest as default and click on <span class="packt_screen">Save</span>.<strong> </strong>Now our pipeline is saved. It is time to run the pipeline. Jenkinsfile is a great script that will orchestrate the pipeline for us. </p>
<p>Click <span class="packt_screen">Build with Parameters</span>. You will then see the environment-based build parameter that will be required for our pipeline to build, test, and deploy our code. </p>
<p>The first run should be without any testing in place, and should only deploy functions and the API gateway for the tasks on the AWS Cloud. The console output will provide us with the endpoints for the tasks, as shown in the following code: </p>
<pre class="console-output">endpoints:
  POST - https://6pgn5wuqeh.execute-api.us-east-1.amazonaws.com/dev/mytasks
  GET - https://6pgn5wuqeh.execute-api.us-east-1.amazonaws.com/dev/mytasks
  GET - https://6pgn5wuqeh.execute-api.us-east-1.amazonaws.com/dev/mytasks/{id}
  PUT - https://6pgn5wuqeh.execute-api.us-east-1.amazonaws.com/dev/mytasks/{id}
  DELETE - https://6pgn5wuqeh.execute-api.us-east-1.amazonaws.com/dev/mytasks/{id}</pre>
<p>Replace the task endpoint in the Jenkinsfile for the entire environment with the API gateway path listed on the console, as shown in the following code. Then save the Jenkinsfile and push it to the Git repository created by you. The reason we are adding the endpoints late in the build is because API gateway endpoints are created dynamically. But we can also have a static endpoint URL with custom domain names featured in the API gateway: </p>
<div>
<pre><span><strong>export TASKS_ENDPOINT=6pgn5wuqeh.execute-api.us-east-1.amazonaws.com/dev<br/></strong></span></pre></div>
<p>Build the job by clicking on <span class="packt_screen">Build with Parameters</span>. Here, we should be able to see that the system test is running along the deployment steps, and the pipeline should be green, as shown in the following screenshot: </p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/23e7d668-800e-4cd9-847f-041ce1177650.png" style="width:60.00em;height:16.83em;"/></p>
<p class="mce-root"/>
<p>In the preceding recipe, we learned how Lambda functions and the API gateway call tasks. We also learned how they are deployed to AWS through Serverless Framework and Jenkins via different environments, such as <span class="packt_screen">dev</span>, <span class="packt_screen">sit</span>, and <span class="packt_screen">prod</span>. We also created and tested a system test. The deployment and execution will deploy the Lambda function and API gateway to the AWS cloud, and every system test will execute the Lambda function to perform CRUD operations on DynamoDB. So if you go into DynamoDB, you should see three tables that are created for each environment. You should also be able to see different functions and the API gateway for each environment. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment methods</h1>
                </header>
            
            <article>
                
<p>Deployment into production has its pains. We take a lot of precautions while deploying into production. We introduce lots of testing on lower environments so that most of the bugs and performance problems are taken care of early on. But we are still nervous when deploying into production, as we are never 100 percent sure whether the deployed version will work perfectly fine. If we are using deployment techniques, then we minimize the deployment failure substantially. There are varieties of deployment technique, but canary and blue-green are the most popular ones. We will look at examples of both deployment techniques for AWS Lambda.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Canary deployment</h1>
                </header>
            
            <article>
                
<p>Canary deployment is a deployment technique involving a gradual shift in production traffic from version A to version B, where version B is the latest version and version A is the previous version. AWS has recently introduced traffic shifting for Lambda functions aliases. An alias is a pointer to a specific version of the Lambda functions, which basically means that we can split the traffic of the functions between two different versions by specifying the percentage of incoming traffic that we want to direct to the new release. Lambda will automatically load balance requests between versions when aliases are invoked. So instead of replacing one function with another version, both versions can coexist and can be monitored as to how they perform. </p>
<p>All of this sounds awesome, but doing all this it not that easy. Fortunately, AWS already has a service that will help us with this problem—CodeDeploy. To use canary deployment with the AWS CodeDeploy service, we need to create a variety of resources. We need to create a CodeDeploy application, a deployment group, and aliases for the functions. We also need to create new permissions and replace all the event sources to trigger the aliases instead of the latest functions. But this can be much easier if we use the canary deployment plugin with Serverless Framework. Let's learn how we can achieve this using an example.</p>
<div class="packt_infobox">The code that we will be working on in the following recipe is available at <span>:<a href="https://github.com/shzshi/my-canary-deployment.git">https://github.com/shzshi/my-canary-deployment.git</a></span><span>.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up a simple environment </h1>
                </header>
            
            <article>
                
<p>Perform the <span><span>following</span></span> steps:</p>
<ol>
<li>Let's create a simple serverless service with the following command. Two files will be created named <kbd>handler.js</kbd> and <kbd>serverless.yml</kbd>:</li>
</ol>
<pre style="padding-left: 60px">$ serverless create --template aws-nodejs --path my-canary-deployment</pre>
<ol start="2">
<li>Now replace the content of <kbd>serverless.yml</kbd> with the following code. Make sure that it is indented properly. We are creating a service with a function and an API gateway: </li>
</ol>
<pre style="padding-left: 60px"><span class="hljs-attr">service:</span><span> </span><span class="hljs-string">my-canary-deployment</span><span> <br/></span><span class="hljs-attr">provider:</span><span> </span><span class="hljs-attr"> <br/>    name:</span><span> </span><span class="hljs-string">aws</span><span> </span><span class="hljs-attr"> <br/>    runtime:</span><span> </span><span class="hljs-string">nodejs6.10</span><span> <br/><br/></span><span class="hljs-attr">plugins:</span><span> </span><span class="hljs-bullet"> <br/>    -</span><span> </span><span class="hljs-string">serverless-plugin-canary-deployments</span><span> <br/></span><span class="hljs-attr">functions:</span><span> </span><span class="hljs-attr"> <br/>    hello:</span><span> </span><span class="hljs-attr"> <br/>        handler:</span><span> </span><span class="hljs-string">handler.hello</span><span> </span><span class="hljs-attr"> <br/>        events:</span><span> </span><span class="hljs-attr"> <br/>            - http:</span><span> </span><span class="hljs-string">get</span><span> </span><span class="hljs-string">hello</span></pre>
<ol start="3">
<li>Let's replace the content of <kbd>handler.js</kbd> with the following content: </li>
</ol>
<pre style="padding-left: 60px"><span class="hljs-built_in">module</span><span>.exports.hello = </span><span class="hljs-function">(<span class="hljs-params">event, context, callback</span>) =&gt;</span><span> { <br/>    </span><span class="hljs-keyword">const</span><span> response = { <br/>        </span><span class="hljs-attr">statusCode</span><span>: </span><span class="hljs-number">200</span><span>, <br/>        </span><span class="hljs-attr">body</span><span>: </span><span class="hljs-string">'Go Serverless v1.0! Your function executed successfully!'</span><span> <br/>    };<br/><br/>    callback(</span><span class="hljs-literal">null</span><span>, response);<br/>};</span></pre>
<ol start="4">
<li>Create a <kbd>package.json</kbd> file, as shown in the following code: </li>
</ol>
<div>
<pre style="padding-left: 60px"><span>{<br/>    </span><span>"name"</span><span>: </span><span>"my-canary-deployment"</span><span>,<br/>    </span><span>"version"</span><span>: </span><span>"1.0.0"</span><span>,<br/>    </span><span>"description"</span><span>: </span><span>""</span><span>,<br/>    </span><span>"main"</span><span>: </span><span>"handler.js"</span><span>,<br/>    </span><span>"scripts"</span><span>: {<br/>        </span><span>"test"</span><span>: </span><span>"echo </span><span>\"</span><span>Error: no test specified</span><span>\"</span><span> &amp;&amp; exit 1"<br/>    </span><span>},<br/>    </span><span>"author"</span><span>: </span><span>"shashikant bangera"</span><span>,<br/>    </span><span>"devDependencies"</span><span>: {<br/>        </span><span>"serverless-plugin-aws-alerts"</span><span>: </span><span>"^1.2.4"</span><span>,<br/>        </span><span>"serverless-plugin-canary-deployments"</span><span>: </span><span>"^0.4.0"<br/>    </span><span>}<br/></span><span>}</span></pre></div>
<ol start="5">
<li>Let's deploy this function to the AWS, as shown in the following code. Make sure that you have configured your AWS access and secret key before deploying: </li>
</ol>
<pre style="padding-left: 60px">$ npm install <br/>$ serverless deploy -v</pre>
<ol start="6">
<li>Once successfully deployed, let's invoke the function and let it verify the execution. We will be provided with the service endpoint, so let's invoke the function with the endpoint, as shown in the following code: </li>
</ol>
<pre style="padding-left: 60px">$ curl https://&lt;my-service-endpoint&gt;-east-1.amazonaws.com/dev/hello<br/><br/><span>Go Serverless v1.0! Your function executed successfully!</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up canary deployment </h1>
                </header>
            
            <article>
                
<p>Once the initial setup is complete, we will tell the serverless canary deployment plugin to split the traffic between the last two versions and gradually shift more traffic to the new version until it receives all the load. </p>
<p>The following are the three types of gradual deployment that we can implement using AWS's CodeDeploy: </p>
<ul>
<li><strong>Canary:</strong> The traffic is shifted to a new version during a certain period of time, and when the time elapses, all the traffic will have moved to the newer version</li>
<li><strong>Linear:</strong> The traffic is shifted to the new version incrementally at intervals until it gets all of the traffic</li>
<li><strong>All at once:</strong> All the traffic is shifted to the new version at once</li>
</ul>
<p>We need to be specific as to which parameter and type of deployment we will use by choosing any of the aforementioned options for using CodeDeploy:</p>
<p><kbd><span>Canary10Percent30Minutes</span></kbd><span>,</span> <kbd><span>Canary10Percent5Minutes</span></kbd><span>,</span> <kbd><span>Canary10Percent10Minutes</span></kbd><span>,</span> <kbd><span>Canary10Percent15Minutes</span></kbd><span>,</span> <kbd><span>Linear10PercentEvery10Minutes</span></kbd><span>,</span> <kbd><span>Linear10PercentEvery1Minute</span></kbd><span>,</span> <kbd><span>Linear10PercentEvery2Minutes</span></kbd><span>,</span> <kbd><span>Linear10PercentEvery3Minutes</span></kbd> <span>or</span> <kbd><span>AllAtOnce</span></kbd><span>.</span></p>
<p>For our tutorial, we will use <kbd>Linear10PercentEvery1Minute</kbd><span>,</span> <span>which means that the traffic that the new version of the function will receive will be increased by 10 percent increments every minute, until it reaches 100%. To do this, we need to set the </span>type <span>and alias (the name of the alias that we want to create) under <kbd>deploymentSettings</kbd> in the function. Let's update the files, and redeploy and invoke the function to see how the traffic moves: </span></p>
<ol>
<li>Add the deployment settings within the <kbd>serverless.yml</kbd>, as shown in the following code: </li>
</ol>
<pre style="padding-left: 60px">service: my-canary-deployment<br/>provider:<br/> name: aws<br/> runtime: nodejs6.10<br/><br/>plugins:<br/> - serverless-plugin-canary-deployments<br/>functions:<br/> hello:<br/>     handler: handler.hello<br/>     events:<br/>         - http: get hello<br/>     <strong>deploymentSettings:</strong><br/><strong>         type: Linear10PercentEvery1Minute</strong><br/><strong>         alias: Live</strong></pre>
<ol start="2">
<li>Update the <kbd>handler.js</kbd> with the following code: </li>
</ol>
<pre style="padding-left: 60px">module.exports.hello = (event, context, callback) =&gt; {<br/>    const response = {<br/>    statusCode: 200,<br/>    <strong>body: 'Hey new version is 1.26.1 !'</strong><br/>    };<br/>    callback(null, response);<br/>};</pre>
<ol start="3">
<li>Let's deploy the function. You will see from the following code that the deployment will trigger <kbd>CodeDeploy</kbd> with linear deployment, and requests will be load balanced between the two functions: </li>
</ol>
<pre style="padding-left: 60px">$ serverless deploy -v<br/><br/>Serverless: Packaging service...<br/>---<br/>Serverless: Checking Stack update progress...<br/>CloudFormation - UPDATE_IN_PROGRESS - AWS::CloudFormation::Stack - my-canary-deployment-dev<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::IAM::Role - CodeDeployServiceRole<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::CodeDeploy::Application - MycanarydeploymentdevDeploymentApplication<br/>CloudFormation - UPDATE_IN_PROGRESS - AWS::Lambda::Function - HelloLambdaFunction<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::CodeDeploy::Application - MycanarydeploymentdevDeploymentApplication<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::IAM::Role - CodeDeployServiceRole<br/>CloudFormation - CREATE_COMPLETE - AWS::CodeDeploy::Application - MycanarydeploymentdevDeploymentApplication<br/>CloudFormation - UPDATE_COMPLETE - AWS::Lambda::Function - HelloLambdaFunction<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - HelloLambdaVersionW59z2S8rIu6lAv3dCyvgKLndpEosDs1l1kpbg6Lrg<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - HelloLambdaVersionW59z2S8rIu6lAv3dCyvgKLndpEosDs1l1kpbg6Lrg<br/>CloudFormation - CREATE_COMPLETE - AWS::Lambda::Version - HelloLambdaVersionW59z2S8rIu6lAv3dCyvgKLndpEosDs1l1kpbg6Lrg<br/>CloudFormation - CREATE_COMPLETE - AWS::IAM::Role - CodeDeployServiceRole<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::CodeDeploy::DeploymentGroup - HelloLambdaFunctionDeploymentGroup<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::CodeDeploy::DeploymentGroup - HelloLambdaFunctionDeploymentGroup<br/>CloudFormation - CREATE_COMPLETE - AWS::CodeDeploy::DeploymentGroup - HelloLambdaFunctionDeploymentGroup<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Alias - HelloLambdaFunctionAliasLive<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Alias - HelloLambdaFunctionAliasLive<br/>CloudFormation - CREATE_COMPLETE - AWS::Lambda::Alias - HelloLambdaFunctionAliasLive<br/>CloudFormation - UPDATE_IN_PROGRESS - AWS::ApiGateway::Method - ApiGatewayMethodHelloGet<br/>CloudFormation - UPDATE_COMPLETE - AWS::ApiGateway::Method - ApiGatewayMethodHelloGet<br/>CloudFormation - UPDATE_IN_PROGRESS - AWS::Lambda::Permission - HelloLambdaPermissionApiGateway<br/>CloudFormation - UPDATE_IN_PROGRESS - AWS::Lambda::Permission - HelloLambdaPermissionApiGateway<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::ApiGateway::Deployment - ApiGatewayDeployment1530401347906<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::ApiGateway::Deployment - ApiGatewayDeployment1530401347906<br/>CloudFormation - CREATE_COMPLETE - AWS::ApiGateway::Deployment - ApiGatewayDeployment1530401347906<br/>CloudFormation - UPDATE_COMPLETE - AWS::Lambda::Permission - HelloLambdaPermissionApiGateway<br/>CloudFormation - UPDATE_COMPLETE_CLEANUP_IN_PROGRESS - AWS::CloudFormation::Stack - my-canary-deployment-dev<br/>CloudFormation - DELETE_IN_PROGRESS - AWS::Lambda::Permission - HelloLambdaPermissionApiGateway<br/>CloudFormation - DELETE_IN_PROGRESS - AWS::ApiGateway::Deployment - ApiGatewayDeployment1530398711004<br/>CloudFormation - DELETE_SKIPPED - AWS::Lambda::Version - HelloLambdaVersionjAYLrhwIiK3mGoae1oyrqMYfnXsHGIk0IuE6gh2dWdA<br/>CloudFormation - DELETE_COMPLETE - AWS::ApiGateway::Deployment - ApiGatewayDeployment1530398711004<br/>CloudFormation - DELETE_COMPLETE - AWS::Lambda::Permission - HelloLambdaPermissionApiGateway<br/>CloudFormation - UPDATE_COMPLETE - AWS::CloudFormation::Stack - my-canary-deployment-dev</pre>
<ol start="4">
<li>Let's invoke the function and see whether it is load balanced, as shown in the following code: </li>
</ol>
<pre style="padding-left: 60px">$ curl https://&lt;api-gateway-path&gt;.amazonaws.com/dev/hello<br/>Go Serverless v1.0! Your function executed successfully!<br/><br/>$ curl https://&lt;api-gateway-path&gt;.amazonaws.com/dev/hello<br/>Hey new version is 1.26.1!</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Making sure the deployment works fine</h1>
                </header>
            
            <article>
                
<p>Our function deployed without any hassle, but how do we ensure that whole system is behaving correctly? To ensure everything is working fine, we can provide CodeDeploy with a list of variables to track during the deployment process, then cancel it, and shift all the traffic to the old version if the <kbd>ALARM</kbd> is triggered. With serverless, we can set the alarm using another plugin. Let's have a look at how to do this: </p>
<ol>
<li>Update the <kbd>serverless.yml</kbd> to set the alarm, as shown in the following code: </li>
</ol>
<pre style="padding-left: 60px">service: my-canary-deployment<br/>provider:<br/> name: aws<br/> runtime: nodejs6.10<br/><br/>plugins:<br/>    <strong>- serverless-plugin-aws-alerts</strong><br/>    - serverless-plugin-canary-deployments<br/><br/><strong>custom:</strong><br/><strong>    alerts:</strong><br/><strong>        dashboards: true</strong><br/><br/>functions:<br/>    hello:<br/>         handler: handler.hello<br/>         events:<br/>             - http: get hello<br/>         <strong>alarms:</strong><br/><strong>             - name: test</strong><br/><strong>                 namespace: 'AWS/Lambda'</strong><br/><strong>                 metric: Errors</strong><br/><strong>                 threshold: 1</strong><br/><strong>                 statistic: Minimum</strong><br/><strong>                 period: 60</strong><br/><strong>                 evaluationPeriods: 1</strong><br/><strong>                 comparisonOperator: GreaterThanOrEqualToThreshold</strong><br/>         deploymentSettings:<br/>             type: Linear10PercentEvery1Minute<br/>             alias: Live<br/>             <strong>alarms:</strong><br/><strong>                 - HelloTestAlarm</strong></pre>
<ol start="2">
<li>Let's deploy the function and see how it works, as shown in the following code: </li>
</ol>
<pre style="padding-left: 60px">$ serverless deploy -v<br/><br/>Serverless: Packaging service...<br/>Serverless: Excluding development dependencies...<br/>Serverless: Uploading CloudFormation file to S3...<br/>Serverless: Uploading artifacts...<br/>Serverless: Uploading service .zip file to S3 (611 B)...<br/>Serverless: Validating template...<br/>Serverless: Updating Stack...<br/>Serverless: Checking Stack update progress...<br/>CloudFormation - UPDATE_IN_PROGRESS - AWS::CloudFormation::Stack - my-canary-deployment-dev<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::CloudWatch::Dashboard - AlertsDashboard<br/>CloudFormation - UPDATE_IN_PROGRESS - AWS::Lambda::Function - HelloLambdaFunction<br/>CloudFormation - UPDATE_COMPLETE - AWS::Lambda::Function - HelloLambdaFunction<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::CloudWatch::Alarm - HelloTestAlarm<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::CloudWatch::Alarm - HelloTestAlarm<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - HelloLambdaVersionja19jdYXntxmsiUagZLZfDEMTshQJ8ApOagyYwmXE<br/>CloudFormation - CREATE_COMPLETE - AWS::CloudWatch::Alarm - HelloTestAlarm<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - HelloLambdaVersionja19jdYXntxmsiUagZLZfDEMTshQJ8ApOagyYwmXE<br/>CloudFormation - CREATE_COMPLETE - AWS::Lambda::Version - HelloLambdaVersionja19jdYXntxmsiUagZLZfDEMTshQJ8ApOagyYwmXE<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::CloudWatch::Dashboard - AlertsDashboard<br/>CloudFormation - CREATE_COMPLETE - AWS::CloudWatch::Dashboard - AlertsDashboard<br/>CloudFormation - UPDATE_IN_PROGRESS - AWS::CodeDeploy::DeploymentGroup - HelloLambdaFunctionDeploymentGroup<br/>CloudFormation - UPDATE_COMPLETE - AWS::CodeDeploy::DeploymentGroup - HelloLambdaFunctionDeploymentGroup<br/>CloudFormation - UPDATE_IN_PROGRESS - AWS::Lambda::Alias - HelloLambdaFunctionAliasLive<br/>CloudFormation - UPDATE_IN_PROGRESS - AWS::Lambda::Alias - HelloLambdaFunctionAliasLive<br/>CloudFormation - UPDATE_COMPLETE - AWS::Lambda::Alias - HelloLambdaFunctionAliasLive<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::ApiGateway::Deployment - ApiGatewayDeployment1530403890255<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::ApiGateway::Deployment - ApiGatewayDeployment1530403890255<br/>CloudFormation - CREATE_COMPLETE - AWS::ApiGateway::Deployment - ApiGatewayDeployment1530403890255<br/>CloudFormation - UPDATE_COMPLETE_CLEANUP_IN_PROGRESS - AWS::CloudFormation::Stack - my-canary-deployment-dev<br/>CloudFormation - DELETE_IN_PROGRESS - AWS::ApiGateway::Deployment - ApiGatewayDeployment1530401797330<br/>CloudFormation - DELETE_COMPLETE - AWS::ApiGateway::Deployment - ApiGatewayDeployment1530401797330<br/>CloudFormation - DELETE_SKIPPED - AWS::Lambda::Version - HelloLambdaVersionW59z2S8rIu6lAv3dCyvgKLndpEosDs1l1kpbg6Lrg<br/>CloudFormation - UPDATE_COMPLETE - AWS::CloudFormation::Stack - my-canary-deployment-dev</pre>
<p>The deploy step will create an alarm in CodeDeploy and the CloudWatch dashboard, where we can see different graphs representing invocations and errors. You can log into AWS Console and go to CodeDeploy and CloudWatch to see how the alarm is created, and to see what the dashboard looks like. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying CodeDeploy hooks </h1>
                </header>
            
            <article>
                
<p>So now we have all the tools to minimize the impact of possible bugs or failures. However, we can also avoid invoking a function version that contained errors by running the CodeDeploy hooks first. Hooks are Lambda functions triggered by CodeDeploy before and after traffic shifting takes place. It expects to get notified about the success or failure of the hooks, only continuing to the next step if they succeed. They are good for running integration tests and checking that everything fits together in the cloud, since it will automatically rollback at a failure.</p>
<p>Let's have look at how to create hooks by going through the following steps: </p>
<ol>
<li>Let's update the <kbd>serverless.yml</kbd> to add hook details. We need to grant our functions access to CodeDeploy so that we can use CodeDeploy's SDK in the hooks, as shown in the following code: </li>
</ol>
<div>
<pre style="padding-left: 60px"><span>service</span><span>: </span><span>my-canary-deployment</span><span> <br/></span><span>provider</span><span>: <br/></span><span>    name</span><span>: </span><span>aws</span><span><br/>    </span><span>runtime</span><span>: </span><span>nodejs6.10<br/>    </span><strong><span>iamRoleStatements</span><span>:<br/>    </span><span>- </span><span>Effect</span><span>: </span><span>Allow<br/>    </span><span>Action</span><span>:<br/>        </span><span>- </span><span>codedeploy:*<br/>    </span><span>Resource</span><span>:<br/>        </span><span>- </span><span>"*"</span></strong><span> <br/></span><span>plugins</span><span>: <br/>    </span><span>- </span><span>serverless-plugin-aws-alerts<br/>    </span><span>- </span><span>serverless-plugin-canary-deployments<br/><br/></span><span>custom</span><span>:<br/>    </span><span>alerts</span><span>:<br/>        </span><span>dashboards</span><span>: </span><span>true<br/><br/></span><span>functions</span><span>:<br/>    </span><span>hello</span><span>:<br/>        </span><span>handler</span><span>: </span><span>handler.hello<br/>        </span><span>events</span><span>:<br/>            </span><span>- </span><span>http</span><span>: </span><span>get hello<br/>        </span><span>alarms</span><span>:<br/>            </span><span>- </span><span>name</span><span>: </span><span>test<br/>            </span><span>namespace</span><span>: </span><span>'AWS/Lambda'<br/>            </span><span>metric</span><span>: </span><span>Errors<br/>            </span><span>threshold</span><span>: </span><span>1<br/>            </span><span>statistic</span><span>: </span><span>Minimum<br/>            </span><span>period</span><span>: </span><span>60<br/>            </span><span>evaluationPeriods</span><span>: </span><span>1<br/>            </span><span>comparisonOperator</span><span>: </span><span>GreaterThanOrEqualToThreshold<br/>        </span><span>deploymentSettings</span><span>:<br/>            </span><span>type</span><span>: </span><span>Linear10PercentEvery1Minute<br/>            </span><span>alias</span><span>: </span><span>Live<br/>            </span><strong><span>preTrafficHook</span><span>: </span><span>preHook<br/>            </span><span>postTrafficHook</span><span>: </span></strong><span><strong>postHook</strong><br/>            </span><span>alarms</span><span>:<br/>                </span><span>- </span><span>HelloTestAlarm<br/>    </span><strong><span>preHook</span><span>:<br/>        </span><span>handler</span><span>: </span><span>hooks.pre<br/></span></strong></pre>
<pre style="padding-left: 60px"><strong><span>    </span><span>postHook</span><span>:<br/>        </span><span>handler</span><span>: </span><span>hooks.post</span></strong></pre></div>
<ol start="2">
<li>Next, we will create the hooks. Let's create a new file named <kbd>hooks.js</kbd> within the service directory and add the following hook content: </li>
</ol>
<pre style="padding-left: 60px">const aws = require('aws-sdk');<br/>const codedeploy = new aws.CodeDeploy({apiVersion: '2014-10-06'});<br/>module.exports.pre = (event, context, callback) =&gt; {<br/> var deploymentId = event.DeploymentId;<br/> var lifecycleEventHookExecutionId = event.LifecycleEventHookExecutionId;<br/> <br/>console.log('We are running some integration tests before we start shifting traffic...');<br/><br/>var params = {<br/> deploymentId: deploymentId,<br/> lifecycleEventHookExecutionId: lifecycleEventHookExecutionId,<br/> status: 'Succeeded' // status can be 'Succeeded' or 'Failed'<br/> };<br/><br/>return codedeploy.putLifecycleEventHookExecutionStatus(params).promise()<br/> .then(data =&gt; callback(null, 'Validation test succeeded'))<br/> .catch(err =&gt; callback('Validation test failed'));<br/>};<br/><br/>module.exports.post = (event, context, callback) =&gt; {<br/> var deploymentId = event.DeploymentId;<br/> var lifecycleEventHookExecutionId = event.LifecycleEventHookExecutionId;<br/> console.log('Check some stuff after traffic has been shifted...');<br/> var params = {<br/> deploymentId: deploymentId,<br/> lifecycleEventHookExecutionId: lifecycleEventHookExecutionId,<br/> status: 'Succeeded' // status can be 'Succeeded' or 'Failed'<br/> };<br/> return codedeploy.putLifecycleEventHookExecutionStatus(params).promise()<br/> .then(data =&gt; callback(null, 'Validation test succeeded'))<br/> .catch(err =&gt; callback('Validation test failed'));<br/>};</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="3">
<li>Now that we have created the hooks, let's deploy them and see how they function using the following code: </li>
</ol>
<pre style="padding-left: 60px">$ serverless deploy -v<br/>Serverless: Packaging service...<br/>Serverless: Excluding development dependencies...<br/>Serverless: Uploading CloudFormation file to S3...<br/>Serverless: Uploading artifacts...<br/>Serverless: Uploading service .zip file to S3 (1.12 KB)...<br/>Serverless: Validating template...<br/>Serverless: Updating Stack...<br/>Serverless: Checking Stack update progress...<br/>CloudFormation - UPDATE_IN_PROGRESS - AWS::CloudFormation::Stack - my-canary-deployment-dev<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::Logs::LogGroup - PreHookLogGroup<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::Logs::LogGroup - PostHookLogGroup<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::Logs::LogGroup - PreHookLogGroup<br/>CloudFormation - UPDATE_IN_PROGRESS - AWS::IAM::Role - IamRoleLambdaExecution<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::Logs::LogGroup - PostHookLogGroup<br/>CloudFormation - UPDATE_IN_PROGRESS - AWS::CloudWatch::Dashboard - AlertsDashboard<br/>CloudFormation - CREATE_COMPLETE - AWS::Logs::LogGroup - PreHookLogGroup<br/>CloudFormation - CREATE_COMPLETE - AWS::Logs::LogGroup - PostHookLogGroup<br/>CloudFormation - UPDATE_COMPLETE - AWS::CloudWatch::Dashboard - AlertsDashboard<br/>CloudFormation - UPDATE_COMPLETE - AWS::IAM::Role - IamRoleLambdaExecution<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Function - PostHookLambdaFunction<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Function - PreHookLambdaFunction<br/>CloudFormation - UPDATE_IN_PROGRESS - AWS::Lambda::Function - HelloLambdaFunction<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Function - PreHookLambdaFunction<br/>CloudFormation - UPDATE_COMPLETE - AWS::Lambda::Function - HelloLambdaFunction<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Function - PostHookLambdaFunction<br/>CloudFormation - CREATE_COMPLETE - AWS::Lambda::Function - PostHookLambdaFunction<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - HelloLambdaVersionUutX83EhRCt0XFaMjWRyD8vAkoceeNRnZXqaeuCkFo<br/>CloudFormation - CREATE_COMPLETE - AWS::Lambda::Function - PreHookLambdaFunction<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - PostHookLambdaVersionI0mvapPGCoXwaOw5oaAUMCSPtZYGXKy9YJABLTU<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - HelloLambdaVersionUutX83EhRCt0XFaMjWRyD8vAkoceeNRnZXqaeuCkFo<br/>CloudFormation - CREATE_COMPLETE - AWS::Lambda::Version - HelloLambdaVersionUutX83EhRCt0XFaMjWRyD8vAkoceeNRnZXqaeuCkFo<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - PostHookLambdaVersionI0mvapPGCoXwaOw5oaAUMCSPtZYGXKy9YJABLTU<br/>CloudFormation - CREATE_COMPLETE - AWS::Lambda::Version - PostHookLambdaVersionI0mvapPGCoXwaOw5oaAUMCSPtZYGXKy9YJABLTU<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - PreHookLambdaVersionG9WhVjc3o3mP7moxWMFYoj0jCN4eO1jJBUgGLJmcJA<br/>CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - PreHookLambdaVersionG9WhVjc3o3mP7moxWMFYoj0jCN4eO1jJBUgGLJmcJA<br/>CloudFormation - CREATE_COMPLETE - AWS::Lambda::Version - PreHookLambdaVersionG9WhVjc3o3mP7moxWMFYoj0jCN4eO1jJBUgGLJmcJA<br/>CloudFormation - UPDATE_IN_PROGRESS - AWS::Lambda::Alias - HelloLambdaFunctionAliasLive<br/>CloudFormation - UPDATE_IN_PROGRESS - AWS::Lambda::Alias - HelloLambdaFunctionAliasLive</pre>
<p><span>While the deployment is continuing on the CLI, we should log into the AWS Cloud console and go to <span class="packt_screen">Services</span> |</span> <span class="packt_screen">CloudFormation</span><span>. Select the stack named </span><span class="packt_screen">my-canary-deployment-dev </span><span>and scroll down and select the</span> <span class="packt_screen">CodeDeploy</span> <span>link in the <span class="packt_screen">Status Reason</span> column. We should be able to see the gradual shifting of traffic with prehook, traffic shifting, and posthook execution, finally completing the whole stack and, lastly, the deployment.</span></p>
<p>Here, we learned how we can set up canary deployments using Serverless Framework, various plugins, and AWS CodeDeploy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Blue-green deployment</h1>
                </header>
            
            <article>
                
<p>Like canary deployment, blue–green deployment is another type of methodology for safe deployment for production. In canary deployment, we shifted the traffic from one version to the next version gradually until we completely moved it to the latest version. But in the case of blue–green deployment, we create two different environments. One environment is used for going live and the other for staging the new version. So a blue–green deployment setup would create a separate region for staging and production and then route the traffic from one region to another with deploying the latest version. </p>
<p>Let's say that I have a <strong>blue</strong> region (<kbd>us-east-1</kbd>), which is the production region, and that it is live and has the current version of the Lambda functions deployed. Now let's say that I also have a new version out, so I will deploy the Lambda function (the new version) into a <strong>green </strong>region (<kbd>us-east-2</kbd>), and that this will act as my staging environment. I will perform all the testing, and once satisfied, I will redirect all the traffic to the <strong>green</strong> <span>region </span><span>(</span><kbd>us-east-2</kbd><span>). Now that my staging is live, I will go ahead and deploy the latest </span><span>version to the</span><span> </span><span>blue</span><span> region </span><span>(<kbd>us-east-1</kbd>), and my functions will be tested for bugs and problems. But let's say that, unfortunately, some serious bugs are discovered in</span> the <strong>blue</strong> region <span>(<kbd>us-east-1</kbd>).</span></p>
<p><span>Then the code is rolled back, all the traffic is once again pointed to the</span> <strong>blue</strong> region <span>(<kbd>us-east-1</kbd>), and the <strong>green</strong> region (<kbd>us-east-2</kbd>) becomes the staging environment again:</span></p>
<ul>
<li><strong>Blue: </strong><kbd>$ serverless deploy --stage prod --region us-east-1</kbd> </li>
<li><strong>Green:</strong> <kbd>$ serverless deploy --stage prod --region us-east-2</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Integration of CloudWatch with ELK</h1>
                </header>
            
            <article>
                
<p>I have been using ELK for quite a long time. It was daily work for me, as AWS Lambda logs are shipped to CloudWatch, but as my company uses ELK for centralized log management, I now like to push all the logs <span>from CloudWatch </span><span>to ELK.</span></p>
<p>So I decided to ship the CloudWatch logs to ELK. Lambda logs can be shipped directly to Elasticsearch or to Redis for Logstash to pick it up. There is a plugin available that will help us to ship the Lambda CloudWatch logs to ELK. We will now look at how to configure this. We will be using a Docker ELK image to set up ELK locally and then <span>connect to AWS CloudWatch</span><span> </span><span>through the Logstash plugin. Then we will push the logs to Elasticsearch. Let's go through the following steps: </span></p>
<ol>
<li>Get the Docker image for ELK, as shown in the following code. If you already have an ELK account set up, then you don't need to follow this step: </li>
</ol>
<pre class="invoke-output" style="padding-left: 60px"><span>$ docker pull sebp/elk</span><br/>$ docker run --rm -it -p 5044:5044 -p 5601:5601 -p 9200:9200 -p 9300:9300 sebp/elk:latest</pre>
<p style="padding-left: 90px">Now, if you go to your browser and open the link <kbd>http://localhost:5601/</kbd>, then you should be able to see the Kibana dashboard. </p>
<ol start="2">
<li>Install the <kbd>logstash-input-cloudwatch_logs</kbd> plugin. Ssh into your Docker container that was created in the previous step and install the plugin, as shown in the following code: </li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker exec -it &lt;container&gt; bash<br/>$ /opt/logstash/bin/logstash-plugin install logstash-input-cloudwatch_logs<br/><br/>Validating logstash-input-cloudwatch_logs<br/>Installing logstash-input-cloudwatch_logs<br/>Installation successful<br/></strong></pre>
<ol start="3">
<li>Once the plugin is successfully installed, we need to create a Logstash config file that will help us in shipping the CloudWatch logs. Let's open the editor and add the following config file and name it <kbd>cloud-watch-lambda.conf</kbd>. We need to replace <kbd>access_key_id</kbd> and <kbd>secret_access_key</kbd> as per the AWS IAM user and also update the log group. I have added three <kbd>grok</kbd> filters:
<ul>
<li>The first filter matches the generic log message, where we strip the timestamp and pull out the <kbd>[lambda][request_id]</kbd> field for indexing</li>
<li>The second <kbd>grok</kbd> filter handles the <kbd>START</kbd> and <kbd>END</kbd> log messages</li>
<li>The third filter handles the <kbd>REPORT</kbd> messages and gives us the most important fields</li>
</ul>
</li>
</ol>
<pre style="padding-left: 120px">[lambda][duration]<br/>[lambda][billed_duration]<br/>[lambda][memory_size]<br/>[lambda][memory_used]<br/>input {<br/>    cloudwatch_logs {<br/>        <strong>log_group =&gt; "/AWS/lambda/my-lambda"</strong><br/><strong>        access_key_id =&gt; "AKIAXXXXXX"</strong><br/><strong>        secret_access_key =&gt; "SECRET"</strong><br/>        type =&gt; "lambda"<br/>    }<br/>}</pre>
<p style="padding-left: 90px">Let's name this<kbd> cloud-watch-lambda.conf </kbd>and place it in the <kbd>/etc/logstash/conf.d</kbd> file of the Docker container. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="4">
<li>Now let's restart the Logstash service, as shown in the following code. Once this is done, you should be able to see that the logs have been pulled into our ELK container: </li>
</ol>
<pre style="padding-left: 60px">$ /etc/init.d/logstash restart</pre>
<ol start="5">
<li> Open the browser and go to the page <kbd>http://localhost:5601</kbd> and we should be able to see the logs streaming into ELK from cloudwatch and this can be refined further with ELK filter and regular expression. </li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/917f3c3b-6a6d-4add-9b2c-2142581673cb.png"/></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned how to integrate DevOps with various out-of-the-box tools within AWS Lambda and with open source tools. In the next chapter, we will learn how to set up CI and CD with Azure functions with the help of Serverless Framework, as well as how to monitor and log with Azure functions.</p>


            </article>

            
        </section>
    </body></html>