- en: Applying DevOps to AWS Lambda Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 DevOps 应用到 AWS Lambda 应用程序
- en: Let's briefly look at what we learned about AWS Lambda functions. Amazon Web
    Services was the first web service platform to launch the serverless computing
    module Lambda, written in **Lambda functions**.Lambda functions are stateless
    and have no affinity with the underlying infrastructure. Lambda functions are
    executed in response to the events. These events could be an HTTP request, a change
    of the data in the S3 bucket, a change in a DynamoDB table, or a change in Kinesis
    or SNS. Lambda functions replicate faster in response to events, and descale as
    the number of events goes down.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简要回顾一下关于 AWS Lambda 函数的内容。亚马逊网络服务是第一个推出无服务器计算模块 Lambda 的网络服务平台，Lambda 函数是用
    **Lambda 函数** 编写的。Lambda 函数是无状态的，并且与底层基础设施没有依赖关系。Lambda 函数是响应事件执行的，这些事件可能是一个 HTTP
    请求、S3 桶中的数据变化、DynamoDB 表的变化，或者 Kinesis 或 SNS 的变化。Lambda 函数会在事件发生时快速复制，并且在事件数量减少时会缩减规模。
- en: In this chapter, we will be covering various methods of deploying Lambda functions,
    looking at how we can painlessly deploy to multiple environments, unit test, system
    test, and integration test the Lambda function. We will also learn various deployment
    best practices and go through a few example recipes using these best practices.
    We will see how we can manage the AWS Lambda logs and move them to the ELK stack.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍部署 Lambda 函数的各种方法，探讨如何轻松地将其部署到多个环境，进行单元测试、系统测试和集成测试。我们还将学习各种部署最佳实践，并通过几个示例来演示这些最佳实践。我们将看到如何管理
    AWS Lambda 日志，并将其移到 ELK 堆栈中。
- en: 'We will explore the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨以下主题：
- en: Manual deployment of a Lambda function
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动部署 Lambda 函数
- en: AWS Lambda with DevOps
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 DevOps 的 AWS Lambda
- en: Serverless with CodeStar
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CodeStar 的无服务器架构
- en: Blue and green deployment with AWS Lambda
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AWS Lambda 进行蓝绿部署
- en: The GitHub and Jenkins pipeline using Serverless Framework
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Serverless Framework 的 GitHub 和 Jenkins 管道
- en: Setting up Jenkins for a serverless application
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为无服务器应用程序设置 Jenkins
- en: Unit testing a deployed application
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对已部署应用程序进行单元测试
- en: Integrating CloudWatch with ELK
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 CloudWatch 与 ELK 集成
- en: I will be creating a simple application that is used in our day-to-day work.
    The application is a thumbnail creator, and is a Node.js application that uses
    two S3 buckets. One bucket is for uploading the actual images and the other is
    for the thumbnails. The moment an image is uploaded into the images bucket, an
    event is triggered that calls a function to resize the image and upload it to
    the thumbnails bucket. We will first look at how we can manually execute this
    sequence of events, and then we will learn how we can streamline the process by
    automating the deployment process. In the section dealing with DevOps, we will
    talk about setting up an assembly line with the development environment, automated
    testing and deployment, applying a CI/CD pipeline, logging, and monitoring.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我将创建一个简单的应用程序，这是我们日常工作中使用的应用程序。该应用程序是一个缩略图创建器，使用的是 Node.js 应用程序，涉及两个 S3 桶。一个桶用于上传实际的图像，另一个用于存放缩略图。当一张图片上传到图像桶时，会触发一个事件，调用一个函数来调整图片大小并将其上传到缩略图桶中。我们首先看看如何手动执行这一系列事件，然后学习如何通过自动化部署过程来简化这个过程。在
    DevOps 相关部分，我们将讨论如何搭建开发环境、自动化测试与部署、应用 CI/CD 管道、日志记录和监控。
- en: Manual deployment of a Lambda function
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动部署 Lambda 函数
- en: The Node.js Lambda application that we will be using here is already part of
    AWS tutorials. We will learn how to create, deploy, and execute a Lambda application
    via the AWS portal. The prerequisite for this tutorial is for you to have an AWS
    account; we will be using a free AWS subscription throughout this chapter. The
    next step is to set up AWS CLI.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在此处使用的 Node.js Lambda 应用程序已经是 AWS 教程的一部分。我们将学习如何通过 AWS 门户创建、部署和执行 Lambda
    应用程序。本教程的前提是你拥有一个 AWS 账户；在本章中我们将使用 AWS 免费订阅。下一步是设置 AWS CLI。
- en: 'You can create an AWS free account and an AWS CLI through the following links:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下链接创建 AWS 免费账户并设置 AWS CLI：
- en: '[https://portal.aws.amazon.com/billing/signup#/start](https://portal.aws.amazon.com/billing/signup#/start)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://portal.aws.amazon.com/billing/signup#/start](https://portal.aws.amazon.com/billing/signup#/start)'
- en: '[https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html)'
- en: 'Go through the following steps:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照以下步骤操作：
- en: 'Once the AWS account and CLIs are in place, sign in to AWS Console ([https://aws.amazon.com/console/](https://aws.amazon.com/console/)),
    and then we will create an IAM user with the name `adminuser` by logging into
    your AWS account then either clicking on the IAM link or searching for the link
    through the services:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 AWS 账户和 CLI 设置好后，登录 AWS 控制台 ([https://aws.amazon.com/console/](https://aws.amazon.com/console/))，然后我们将通过登录您的
    AWS 账户创建名为 `adminuser` 的 IAM 用户，您可以点击 IAM 链接或通过服务搜索该链接：
- en: '![](img/cf57886f-e0d2-4131-bccd-ceb032dfecc7.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf57886f-e0d2-4131-bccd-ceb032dfecc7.png)'
- en: 'Then we click on the Users link on the left-hand side, where a new page will
    open. Then we click on Add User. Add a user with the name adminuser and select
    both the access type Programmatic access and AWS Management Console access. In
    the console''s Password field, add your custom password, uncheck the Require password
    reset checkbox, and then click Next:Permissions. Let''s create a group by clicking
    on the Create Group button. We will give the group the name of `administrators`.
    Next, let''s select the AdminstrativeAccess checkbox to provide full access to
    the group and then click on Create Group. Now that we have a group created, let''s click
    on Next:Review. Then, we will review the user, which the user have created and
    has been added to the administrator group. Now click on the Create User button. Once
    the user is created, we should be able to see the user in the list, as shown in
    the following screenshot:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们点击左侧的 Users 链接，打开一个新页面。接着点击 Add User，添加一个名为 adminuser 的用户，并选择两种访问类型：编程访问（Programmatic
    access）和 AWS 管理控制台访问（AWS Management Console access）。在控制台的密码字段中，输入自定义密码，取消勾选“需要重置密码”复选框，然后点击
    Next:Permissions。接下来，我们点击 Create Group 按钮来创建一个新组。我们将该组命名为 `administrators`。然后，勾选
    AdminstrativeAccess 复选框，为该组提供完全访问权限，然后点击 Create Group。现在，我们已经创建了一个组，点击 Next:Review。接下来，我们将审核该用户，确保该用户已被添加到管理员组中。然后点击
    Create User 按钮。一旦用户创建完成，我们应该能在列表中看到该用户，如下图所示：
- en: '![](img/8671fd9f-547b-4b42-9545-86d29be225d3.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8671fd9f-547b-4b42-9545-86d29be225d3.png)'
- en: We have created a user with administrative rights just for our tutorials, but
    in the real world the role and policies will be more restricted for the sake of
    security.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为教程创建了一个具有管理员权限的用户，但在实际操作中，出于安全考虑，角色和策略会更加受限。
- en: 'We need to create two buckets in AWS S3\. The buckets need to be created through
    the `adminuser` login, so let''s log in to AWS Console using the new user that
    we have created. Click on the adminuser and select the Security credentials tab.
    Next, let''s copy the Console URL for logging in, then open it on the new browser
    tab. Feed in the username and password for the new user that we have created and
    click on Sign In. Once you are logged in, search for S3 in the AWS Services. Then
    go to S3 Console Management and click on the Create bucket button, as shown in
    the following screenshot:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要在 AWS S3 中创建两个桶。这些桶需要通过 `adminuser` 登录创建，因此我们需要使用刚才创建的新用户登录到 AWS 控制台。点击
    adminuser 并选择 Security credentials 选项卡。接下来，复制控制台 URL 以进行登录，然后在新的浏览器标签页中打开它。输入我们刚才创建的新用户的用户名和密码，点击
    Sign In 登录。一旦登录，搜索 AWS 服务中的 S3。然后进入 S3 控制台管理，点击 Create bucket 按钮，如下图所示：
- en: '![](img/f9900192-269b-421a-9b38-b3f4336f9dfb.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9900192-269b-421a-9b38-b3f4336f9dfb.png)'
- en: 'Let''s add a unique bucket and region name of US East by default. The two buckets
    should be named `Source` and `SourceResized`. Source is the placeholder name that
    should be replaced with the actual bucket name—for example, `my-image-bucket76` and
    `my-image-bucket76resized`**. **So, `my-image-bucket76` will be the source bucket
    and `my-image-bucket76resized` will be the target bucket, as shown in the following
    screenshot:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，我们将添加一个唯一的桶和区域名称，区域为美国东部。两个桶的名称应分别为 `Source` 和 `SourceResized`。`Source`
    是占位符名称，应替换为实际的桶名称——例如，`my-image-bucket76` 和 `my-image-bucket76resized`**。**因此，`my-image-bucket76`
    将是源桶，而 `my-image-bucket76resized` 将是目标桶，如下图所示：
- en: '![](img/0a635283-5f83-40ce-938e-e15cc9d8e3d0.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a635283-5f83-40ce-938e-e15cc9d8e3d0.png)'
- en: The bucket name must be unique, as AWS allows only universally unique bucket
    names.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 桶名称必须唯一，因为 AWS 只允许使用全局唯一的桶名称。
- en: Once both the buckets are successfully created, we can upload an image for the
    Lambda function to resize and push to the resized bucket. Let's upload a JPG image
    into the `my-source-bucket76`source. Click on the bucket name, then upload an
    image to this bucket. This will redirect you to the bucket page. Click on the
    Upload button and a popup will pop up. Then, select Add files to browse for an
    image file from the local directory and then upload the image to the S3 bucket.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦两个存储桶都成功创建，我们可以上传一张图片，让 Lambda 函数进行调整大小，并将其推送到调整后的存储桶。让我们将一张 JPG 图片上传到 `my-source-bucket76`
    存储桶中。点击存储桶名称，然后上传一张图片到这个存储桶。这样会将您重定向到存储桶页面。点击上传按钮，然后会弹出一个窗口。接着，选择“添加文件”以浏览本地目录中的图片文件，并将图片上传到
    S3 存储桶中。
- en: 'The next step is to create a Lambda function and run it manually. Here, we
    have to first follow three steps to create the deployment package, and then create
    the execution role (IAM role) and a Lambda function and test them. The deployment
    package is a ZIP file containing the Lambda function and its dependencies:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是创建一个 Lambda 函数并手动运行它。在这里，我们首先需要按照三个步骤创建部署包，然后创建执行角色（IAM 角色）和 Lambda 函数并进行测试。部署包是一个
    ZIP 文件，包含 Lambda 函数及其依赖项：
- en: Let's create a deployment package. I will be using Node.js as the language for
    this practice application, but we can use Java and Python as well (it depends
    on the developer's preference).
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们创建一个部署包。我将使用 Node.js 作为此实践应用的语言，但我们也可以使用 Java 和 Python（这取决于开发人员的偏好）。
- en: A prerequisite for creating this deployment package is to have Node.js version
    6.0 ([https://nodejs.org/en/download/](https://nodejs.org/en/download/)) or later
    installed on you local environment. You should also make sure that npm is installed.
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建此部署包的前提是您的本地环境中安装了 Node.js 6.0 或更高版本（[https://nodejs.org/en/download/](https://nodejs.org/en/download/)）。您还应确保已安装
    npm。
- en: 'Then go through the following steps. We are downloading the npm libraries for
    the image resizing of the Node.js Lambda function, using the first of the two
    following commands. The second command will download the required libraries:'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后按照以下步骤进行操作。我们将下载用于 Node.js Lambda 函数的图像调整大小的 npm 库，使用以下两个命令中的第一个。第二个命令将下载所需的库：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Open your favorite editor and copy the following script into the file named
    `CreateThumbnails.js`:'
  id: totrans-35
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打开您最喜欢的编辑器，并将以下脚本复制到名为 `CreateThumbnails.js` 的文件中：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can find the gist for `CreateThumbnails.js` at [https://gist.github.com/shzshi/6e1cf435a4c1aa979e3a9a243c13c44a](https://gist.github.com/shzshi/6e1cf435a4c1aa979e3a9a243c13c44a).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 [https://gist.github.com/shzshi/6e1cf435a4c1aa979e3a9a243c13c44a](https://gist.github.com/shzshi/6e1cf435a4c1aa979e3a9a243c13c44a)
    找到 `CreateThumbnails.js` 的要点。
- en: 'As a sanity check, validate that the source and destination are different buckets:'
  id: totrans-38
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为基本检查，请验证源和目标是否是不同的存储桶：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Infer the image type:'
  id: totrans-40
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推断图片类型：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Download the image from S3, transform it, and upload it to a different S3 bucket:'
  id: totrans-42
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 S3 下载图片，进行转换，并上传到另一个 S3 存储桶：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Infer the scaling factor to avoid stretching the image unnaturally:'
  id: totrans-44
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推断缩放因子，以避免不自然地拉伸图片：
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Transform the image buffer in memory:'
  id: totrans-46
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在内存中转换图片缓冲区：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Stream the transformed image to a different S3 bucket:'
  id: totrans-48
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将转换后的图片流式传输到另一个 S3 存储桶：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now we should see two  items in our `tutorials1` folder, namely the `CreateThumbnail.js` and `node_modules`folders.
    Let''s zip all these into a file named `tutorialsimg.zip`. This will be our Lambda
    function deployment package:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应该能在 `tutorials1` 文件夹中看到两个项目，即 `CreateThumbnail.js` 和 `node_modules` 文件夹。我们将所有这些文件压缩成一个名为
    `tutorialsimg.zip` 的文件。这将是我们的 Lambda 函数部署包：
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we will create the execution role for the Lambda function in IAM. Log
    into AWS Console, search for IAM services, and then go into IAM and click on the Roles
    button. Click on Create Role, select the AWS service, and then choose Lambda as
    the service. Then click on the Next: Permissionbutton. Then search for AWSLambda
    in the Policy Type box and check the AWSLambdaExecute checkbox. Then search for
    AmazonS3FullAccess and select it. Then click on the Next:Review button. Next,
    on the Create Role page, add the role name myLambdaRole, add a description, and
    then click on Create Role. Now we have the role that contains the policies that
    we can use to execute the Lambda function in order to make changes to the content
    of the S3 bucket.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将在 IAM 中创建 Lambda 函数的执行角色。登录到 AWS 控制台，搜索 IAM 服务，然后进入 IAM 并点击“角色”按钮。点击“创建角色”，选择“AWS
    服务”，然后选择 Lambda 作为服务。然后点击“下一步：权限”按钮。接着在策略类型框中搜索 AWSLambda，并勾选AWSLambdaExecute复选框。然后搜索
    AmazonS3FullAccess 并选择它。然后点击“下一步：审查”按钮。接着，在创建角色页面中，添加角色名称myLambdaRole，添加描述，然后点击“创建角色”。现在我们拥有了包含可以用于执行
    Lambda 函数的策略的角色，以便对 S3 存储桶的内容进行更改。
- en: 'The next step is to deploy the Lambda functions and node modules on the AWS
    Lambda portal. Let''s go to the AWS Console''s home page, and under Services,
    let''s search for Lambda. We will be redirected to the Lambda home page. Click
    on Create Function, then choose Author from scratch in the Functions Information
    section. Let''s add the function name of `myfirstLambdafunction`, Runtime as **Node.js
    6.10**.Choose the existing role name of myLambdaRole and click on Create Function.
    Now we will be redirected to the Lambda function designer page. We will upload
    our Lambda function to the portal. Scroll down a bit and go to the Function Code section, then
    select Code entry typeas **U**pload a .ZIP file, R**untime**as Node.js 6.10, and
    type into the Handler field the text CreateThumbnail.handler. Now let''s click
    on the Upload button, select the file named tutorialsimg.zipand upload it. Once
    the package is uploaded successfully, we should be able to see the CreateThumbnail.js
    file in the function editor with the node_modules folder, as shown in the following
    screenshot:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是在 AWS Lambda 门户上部署 Lambda 函数和节点模块。我们首先进入 AWS 控制台首页，在服务下搜索 Lambda。我们将被重定向到
    Lambda 首页。点击“创建函数”，然后在函数信息部分选择“从头开始编写”。我们将函数命名为`myfirstLambdafunction`，运行时选择**Node.js
    6.10**。选择现有角色名称为myLambdaRole并点击“创建函数”。现在我们将被重定向到 Lambda 函数设计页面。我们将把 Lambda 函数上传到门户。稍微向下滚动，进入“函数代码”部分，然后选择代码输入类型为**上传
    .ZIP 文件**，运行时为 Node.js 6.10，并在处理程序字段中输入文本`CreateThumbnail.handler`。现在点击上传按钮，选择名为tutorialsimg.zip的文件并上传。当包成功上传后，我们应该能够在函数编辑器中看到CreateThumbnail.js文件和node_modules文件夹，如下截图所示：
- en: '![](img/9fa6af60-bc13-49c1-82a2-0644a678f8d0.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9fa6af60-bc13-49c1-82a2-0644a678f8d0.png)'
- en: 'Now that the function and node modules are uploaded, we will create an event
    to trigger the function. If we scroll to the top right-hand side of the screen,
    we will see a drop-down menu appear that we can use to add the event. Select Configure
    Test Events**.** A pop-up box will appear. Give the event the name of `myThumbnailEvent` and
    in the text field, add the following listed JSON file. Make sure that you replace `my-source-bucket76`
    with your source bucket''s name and `Baby.jpg` with your image name. Then go ahead
    and click Save:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，函数和节点模块已经上传，我们将创建一个事件来触发该函数。如果我们滚动到屏幕的右上角，我们将看到一个下拉菜单出现，我们可以用它来添加事件。选择“配置测试事件**”。**
    一个弹出框将出现。为事件命名为`myThumbnailEvent`，并在文本字段中添加以下列出的 JSON 文件。确保将`my-source-bucket76`替换为你的源存储桶名称，将`Baby.jpg`替换为你的图像名称。然后继续点击保存：
- en: You can find the event JSON file at [https://gist.github.com/shzshi/7a498513ae43b6572c219843bbba277d](https://gist.github.com/shzshi/7a498513ae43b6572c219843bbba277d).
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在[https://gist.github.com/shzshi/7a498513ae43b6572c219843bbba277d](https://gist.github.com/shzshi/7a498513ae43b6572c219843bbba277d)找到事件
    JSON 文件。
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now we have deployed the function, and have created S3 buckets and an event.
    Now let's invoke the function and see if our image in the source bucket is resized
    and pushed to the resized S3 bucket. To do this, click on Test. We should see
    that the function has successfully executed in the logs (as shown in the following
    log text). If you refresh the resized named S3 bucket, you should be able to see
    resized image file. You can just download the resized file and see whether the
    resizing worked. We can also add the S3 put trigger to automatically trigger this
    `CreateThumbnail` function when any image file is uploaded to the source S3 bucket.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'eTag: ''d41d8cd98f00b204e9800998ecf8427e'', versionId: ''096fKKXTRTtl3on89fVO.nfljtsv6qko''
    } } } ] } 2018-06-14T21:07:25.469Z ea822830-7016-11e8-b407-9514918aacd8'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'Successfully resized my-source-bucket76/Baby.jpg and uploaded to my-source-bucket76resized/resized-Baby.jpg
    END RequestId: ea822830-7016-11e8-b407-9514918aacd8'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we learned how to create, build, deploy, and invoke the Lambda
    function manually, and we had to go through number of steps to get this working.
    Now let's say that we need to deploy hundreds or thousands of such Lambda functions
    for a banking application to the portal. To do this task manually, we would require
    lots of resources and time. This is where DevOps comes in really handy to make
    our life faster and more easy. Let's look closer at how we can use DevOps to automate
    all of the steps involved and make it much simpler to build, test, and deploy
    our Lambda functions.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: AWS Lambda with DevOps
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To start implementing DevOps for AWS Lambda, we first create an assembly line.
    An assembly line outlines the stages involved when a developer creates code, tests
    the code, and then commits the code into a repository. The source code is pulled
    from the repository, and is then built and tested. After this, the static code
    analysis takes place. Once it is deployed into a production-like environment,
    acceptance tests are run against it. This is how the application is monitored,
    and how logging is managed. We will look at these stages using the recipes in
    this section. We will also look at two perspectives of DevOps—one through AWS's
    own set of tools, and the other through serverless frameworks, such as GitHub,
    Jenkins, Mocha (for testing), and JSHint (for source code analysis).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: So the first step is to set up a local development environment where we can
    create a folder structure, add and change the source code, add images, and so
    on. We can run the source code locally, execute tests, and debug them for errors
    and failures. We will do this using a Node.js tutorial. We will set up a Node.js
    project from scratch with unit testing, source code analysis, and acceptance testing.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: The Node.js application that I have created is a simple task manager. The underlying
    architecture is AWS Lambda functions with an API gateway to add, update, delete,
    and list the DynamoDB table. In short, we will be doing the `create`, `read`,
    `update`, and `delete` functions through AWS Lambda functions.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我创建的 Node.js 应用是一个简单的任务管理器。其底层架构是 AWS Lambda 函数，配合 API 网关用于添加、更新、删除和列出 DynamoDB
    表。简而言之，我们将通过 AWS Lambda 函数实现 `create`、`read`、`update` 和 `delete` 功能。
- en: Serverless frameworks with AWS CodePipeline
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与 AWS CodePipeline 的无服务器框架
- en: As I have mentioned, our first approach will be through AWS's own DevOps tools.
    We will start with CodePipeline, which is an indigenous cloud-based tool of AWS
    that can help you to build, develop, and deploy applications quickly on AWS. We
    can set up continuous delivery very quickly with CodePipeline. It has its own
    dashboard, and it integrates easily with tools such as JIRA, Jenkins, GitHub,
    and other project-management tools. Let's look how we can use this for Lambda
    functions. We will be using the thumbnail application that was created earlier
    in the chapter.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，我们的第一种方法将通过 AWS 自己的 DevOps 工具来实现。我们将从 CodePipeline 开始，它是 AWS 原生的基于云的工具，可以帮助你在
    AWS 上快速构建、开发和部署应用程序。我们可以通过 CodePipeline 快速设置持续交付。它有自己的仪表盘，且能轻松与 JIRA、Jenkins、GitHub
    和其他项目管理工具集成。让我们来看看如何将其应用于 Lambda 函数。我们将使用本章前面创建的缩略图应用。
- en: 'The prerequisites for these recipes are as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作的先决条件如下：
- en: '**AWS account and login credentials for AWS Console:** Most of the setup for
    the first part of the tutorials will be done through AWS Console. We will using
    the same adminuser account that we created earlier in the chapter.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS 账户和 AWS 控制台登录凭证：** 本教程第一部分的大部分设置将通过 AWS 控制台完成。我们将使用之前在本章创建的相同 adminuser
    账户。'
- en: '**GitHub repository:** You need to create a repository and copy all the files
    and folders from the following repository into your repository:'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GitHub 仓库：** 你需要创建一个仓库，并将以下仓库中的所有文件和文件夹复制到你的仓库中：'
- en: '`https://github.com/shzshi/aws-lambda-thumbnail.git`'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`https://github.com/shzshi/aws-lambda-thumbnail.git`'
- en: '**CloudFormation service role:** Go to the home page of AWS Console and search
    for IAM. On the IAM page, select **Roles** and then click on Create role. On the
    Create role page, select AWS Service and choose CloudFormation as the service.
    Then click on Next:Permission, and on the permission policy page, select the AWSLambdaExecute policy
    and click on Next:Review. Once the review page is open, name the role as `myCloudFormationRole`
    and then click on Create role. Now, for this service role, we need to add additional
    policies to execute the pipeline, so let''s go to the roles. We will see our role
    in the list; let''s click on it. In the Role Summary page, click on Add inline
    policy, and in the Create Policy page, click on the JSON tab and then replace
    the existing JSON script with the script within `cloudformationpolicy.json`, which
    is in the `aws-lambda-thumbnail` repository. Click on Review policy. Now let''s
    name the policy `myThumbnailPipelinePolicy`, so that we have a service role for
    CloudFormation.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CloudFormation 服务角色：** 进入 AWS 控制台首页并搜索 IAM。在 IAM 页面，选择 **角色**，然后点击创建角色。在创建角色页面，选择
    AWS 服务，并选择 CloudFormation 作为服务。接着点击下一步：权限，在权限策略页面，选择 AWSLambdaExecute 策略，然后点击下一步：审核。一旦审核页面打开，给角色命名为
    `myCloudFormationRole`，然后点击创建角色。现在，对于这个服务角色，我们需要添加额外的策略来执行管道，所以我们需要进入角色页面。我们将在列表中看到我们的角色，点击它。在角色摘要页面，点击添加内联策略，在创建策略页面，点击
    JSON 标签，然后将现有的 JSON 脚本替换为 `cloudformationpolicy.json` 中的脚本，`cloudformationpolicy.json`
    文件位于 `aws-lambda-thumbnail` 仓库中。点击审核策略。现在我们将策略命名为 `myThumbnailPipelinePolicy`，这样我们就为
    CloudFormation 创建了一个服务角色。'
- en: '**Bucket for the CloudFormation package:** We need to create a bucket for the
    CloudFormation package, so let''s go to the home page of AWS Console and search
    for S3 in services. Next, let''s create a bucket with the name of my-cloud-formation-bucket. This
    bucket is used for packaging our artifacts when we run the pipeline.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CloudFormation 包的存储桶：** 我们需要为 CloudFormation 包创建一个存储桶，所以我们需要进入 AWS 控制台首页并在服务中搜索
    S3。接着，我们创建一个名为 `my-cloud-formation-bucket` 的存储桶。这个存储桶用于在我们运行管道时打包我们的工件。'
- en: 'Let''s go through the following steps:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照以下步骤进行操作：
- en: To add and retrieve CodeStar permission for the adminuser, go to AWS Console
    ([https://console.aws.amazon.com/console/home](https://console.aws.amazon.com/console/home))
    and log in with your root account credentials. This means that we need to log
    in as a free account user (we created this free account at the start of the chapter).
    If you go to the adminuser login page, you will see a link at the bottom named Sign-in
    using root account credentials. Once you are logged in, go to IAM services and
    click on Users. You should be able to see the adminuser in the list. Now click
    on the adminuser link to the Security credentials tab. Scroll down to find the
    HTTPS Git credentials for AWS CodeCommit section andclick on the Generate button. The
    credentials to authenticate AWS CodeCommit will then be generated. Copy or download
    the credentials. If you have not copied the access key ID and the secret access
    key, please generate new ones using the Create access key button. Save the details
    for both of these somewhere for later use.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要为 adminuser 添加并获取 CodeStar 权限，请访问 AWS 控制台（[https://console.aws.amazon.com/console/home](https://console.aws.amazon.com/console/home)），并使用您的根账户凭证登录。这意味着我们需要作为一个免费账户用户登录（我们在章节开始时创建了这个免费账户）。如果你进入
    adminuser 登录页面，你会看到页面底部有一个名为“使用根账户凭证登录”的链接。登录后，进入 IAM 服务并点击“用户”。你应该能够在列表中看到 adminuser。现在点击
    adminuser 链接，进入“安全凭证”标签。向下滚动，找到“HTTPS Git 凭证用于 AWS CodeCommit”部分，然后点击“生成”按钮。此时将生成用于认证
    AWS CodeCommit 的凭证。复制或下载这些凭证。如果你没有复制访问密钥 ID 和密钥访问密钥，请使用“创建访问密钥”按钮生成新的密钥。将这两个密钥的详细信息保存以供以后使用。
- en: Let's now log into the console as adminuser. Search for CodePipeline from the
    home page. A page will open. On this page, click on Create pipeline. You will
    then be redirected to the Create Pipeline page. Let's name the pipeline myServerlessThumbnailPipeline and
    click on Next Step.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们以 adminuser 身份登录控制台。从主页搜索 CodePipeline。页面将会打开。在这个页面中，点击“创建管道”。然后你将被重定向到“创建管道”页面。我们将管道命名为
    `myServerlessThumbnailPipeline` 并点击“下一步”。
- en: In Source Provider, let's select GitHub. We will then be asked to connect to
    GitHub; go ahead and connect using your credentials. The Repository should be
    the one that we created as a prerequisite, and the Branch should be the branch
    where our files are residing (for example, master). Once the details are added,
    click on Next step. While setting up CodeBuild, a role was created, so we need
    to add an extra policy for this.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在源提供程序中，选择 GitHub。接下来我们将被要求连接到 GitHub，请使用你的凭证进行连接。仓库应为我们之前作为先决条件创建的仓库，分支应为我们的文件所在的分支（例如，master）。输入完细节后，点击“下一步”。在设置
    CodeBuild 时，创建了一个角色，因此我们需要为此角色添加额外的策略。
- en: 'In the Build provider, select AWS CodeBuild, and then in the Configure your
    Project section, select Create a new build project. Let''s add the project details:
    The project name should be `myThumbnailCodeBuild`, the environment image should
    be Use an image managed by AWS CodeBuild, the operating system should be Ubuntu,
    the runtime should be Node.js, and the version should be Node.js 6.3.1\. Keep
    the rest of the details as their default values and click on Save build project.
    We have successfully created a AWS CodeBuild project. However, it has also created
    a service role, and we need to add an additional policy for the CodeBuild project
    role. So let''s open a new tab on the browser and log into AWS Console as adminuser.
    Then, in services, search for IAM, and on the IAM page, go to Roles and select
    the service role with the name code-build-myThumbnailCodeBuild-service-role or
    something similar.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在构建提供程序中，选择 AWS CodeBuild，然后在配置项目部分中，选择创建一个新的构建项目。现在我们来添加项目细节：项目名称应为`myThumbnailCodeBuild`，环境镜像应选择“使用
    AWS CodeBuild 管理的镜像”，操作系统应选择 Ubuntu，运行时应选择 Node.js，版本应为 Node.js 6.3.1。其余的细节保持默认值，然后点击“保存构建项目”。我们已成功创建了一个
    AWS CodeBuild 项目。不过，它也创建了一个服务角色，我们需要为 CodeBuild 项目角色添加一个额外的策略。所以现在我们打开浏览器的新标签页，并以
    adminuser 身份登录 AWS 控制台。接着，在服务中搜索 IAM，在 IAM 页面中，进入角色并选择名为 `code-build-myThumbnailCodeBuild-service-role`
    或类似名称的服务角色。
- en: Now click on Add inline policy and then click on the Create Policy page. Choose
    the S3 service and the PubObject action from the Write access level, and select
    the Resources as All resources. Finally, click Review Policy. Name the policy `myThumbnailCodeBuildPolicy`.
    In the Summary section, we should be able to see S3\. Click on Create Policy.
    Now we have a new policy for S3 that has been added to the CodeBuild role. Let's
    go back to the create CodePipeline page. Click Next step.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在点击“Add inline policy”，然后点击“Create Policy”页面。选择 S3 服务和“PubObject”操作（在写入访问级别下），并选择资源为所有资源。最后，点击“Review
    Policy”。将策略命名为`myThumbnailCodeBuildPolicy`。在摘要部分，我们应该能够看到 S3。点击“Create Policy”。现在我们已经为
    S3 添加了一个新策略，并将其添加到 CodeBuild 角色中。接下来，我们回到创建 CodePipeline 页面。点击“Next step”。
- en: 'In the Deploy template, let''s set Deployment provider as AWS CloudFormation.
    Now that we''re in the CloudFormation section, let''s add all the details as shown
    in the following screenshot. The template file is basically an export file that
    will be used by CloudFormation:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在“Deploy template”中，我们将“Deployment provider”设置为 AWS CloudFormation。现在我们进入 CloudFormation
    部分，按以下截图添加所有详细信息。模板文件基本上是一个导出文件，将由 CloudFormation 使用：
- en: '**  ![](img/2395b0dc-31a5-4a28-b4d7-c112c5d8286a.png)**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '** ![](img/2395b0dc-31a5-4a28-b4d7-c112c5d8286a.png)**'
- en: Next, we will create a role to give permission to the AWS CodePipeline to use
    the resources. Click on Create roleand go through the steps as it prompts you.
    Once the role is created, click on Next. Then review the pipeline, and click on
    Create pipeline. Our pipeline will trigger automatically. Our first stages of
    the pipeline worked fine. If we go to CloudFormation (Services | CloudFormation),
    we should be able to see the stack for the thumbnail that was created. If you
    tick the checkbox and select the events, you should be able to see the events
    that ran, as well as some other details. Let's go ahead and add more stages to
    the pipeline that we can approve, and then deploy the function.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个角色，以授予 AWS CodePipeline 使用资源的权限。点击“Create role”并按照提示完成步骤。角色创建后，点击“Next”。然后审查管道并点击“Create
    pipeline”。我们的管道将自动触发。管道的第一个阶段正常工作。如果我们进入 CloudFormation（服务 | CloudFormation），应该能看到为缩略图创建的堆栈。如果勾选复选框并选择事件，我们应该能看到已运行的事件以及其他一些详细信息。接下来，我们将继续向管道添加更多可以批准的阶段，然后部署该函数。
- en: Now, we will edit the pipeline and add some stages to deploy the thumbnail Lambda
    function. Let's click on Edit and scroll to the bottom. Click on + Stage. Let's
    add a stage called Approval, so that we can review and approve our work before
    deployment. Click on Action and select approval from the Action category of the
    drop-down list. Let's name it as `Approval`. We can also add an SNS topic to get
    approval emails. To do this, let's go ahead and use the default values for the
    rest and click on Add action.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将编辑管道并添加一些阶段来部署缩略图 Lambda 函数。点击“编辑”，并向下滚动至底部。点击“+ 阶段”。我们添加一个名为“Approval”的阶段，以便在部署之前审查和批准我们的工作。点击“Action”，从下拉列表中的“Action”类别中选择“approval”。我们将其命名为`Approval`。我们还可以添加一个
    SNS 主题来接收批准邮件。为此，我们继续使用默认值并点击“Add action”。
- en: The next stage to add is the deployment of the function from the Git repository
    to the Lambda function. Click on + Stage. Let's add a stage named Deploy, click
    on Action, and select Deploy from the Action category drop-down list. Next, let's
    name the action myDeploy and select AWS CloudFormation as the Deployment Provider.
    In the CloudFormation section, let's add an action mode called execute a change
    set and select the stack name mythumbnailstack and the change set name mythumbnailchangeset.
    Let's leave the rest of the details as their defaults and click on Add action.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一阶段是将函数从 Git 仓库部署到 Lambda 函数。点击“+ 阶段”。我们添加一个名为“Deploy”的阶段，点击“Action”，并从下拉列表中的“Action”类别中选择“Deploy”。接下来，我们将操作命名为“myDeploy”，并选择
    AWS CloudFormation 作为部署提供商。在 CloudFormation 部分，我们添加一个名为“execute a change set”的操作模式，并选择堆栈名称“mythumbnailstack”和更改集名称“mythumbnailchangeset”。我们将其余细节保持为默认值并点击“Add
    action”。
- en: 'Now we have added two stages, let''s save the pipeline by clicking Save Pipelineline
    changes. We will be asked to continue; let''s go ahead and click Save and Continue.
    This time, the pipeline won''t trigger automatically, so let''s click on Release
    changeto start the pipeline. Once the pipeline has successfully completed, we
    should see all the stages in green, as shown in the following screenshot:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经添加了两个阶段，点击“Save Pipeline changes”保存管道。系统会提示我们继续操作，点击“Save and Continue”。这一次，管道不会自动触发，因此我们需要点击“Release
    change”来启动管道。管道成功完成后，我们应该能看到所有阶段都显示为绿色，如下图所示：
- en: '![](img/5fa9b772-911d-404c-8256-10978d51732a.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5fa9b772-911d-404c-8256-10978d51732a.png)'
- en: 'Let''s check whether the function has been created, and try executing it. Let''s
    go to the AWS Console home page and search for Lambda. We should be able to see
    that a thumbnail function has been created. Let''s open the Lambda function. On
    the Function page, let''s scroll to the top right-hand side of the page, where
    we will see a drop-down menu that we can use to add the event. Select Configure
    Test Events. A pop-up box will appear containing the name of the myThumbnailEvent event
    and a text field. In the text field, add the following JSON file. Make sure that
    you replace the `my-source-bucket76` with your source bucket name and `Baby.jpg` with
    your image name. Then go ahead and click on Save:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查一下函数是否已经创建，并尝试执行它。我们进入 AWS 控制台主页并搜索 Lambda。我们应该能看到已创建一个缩略图函数。点击打开 Lambda
    函数。在函数页面上，向页面的右上角滚动，我们会看到一个下拉菜单，点击后可以添加事件。选择“配置测试事件”。弹出的框中会显示事件名称“myThumbnailEvent”以及一个文本字段。在文本字段中，添加以下
    JSON 文件。确保将 `my-source-bucket76` 替换为你的源存储桶名称，将 `Baby.jpg` 替换为你的图像文件名。然后点击“保存”。
- en: The event JSON file can be found at [https://gist.github.com/shzshi/7a498513ae43b6572c219843bbba277d](https://gist.github.com/shzshi/7a498513ae43b6572c219843bbba277d).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 事件 JSON 文件可以在 [https://gist.github.com/shzshi/7a498513ae43b6572c219843bbba277d](https://gist.github.com/shzshi/7a498513ae43b6572c219843bbba277d)
    找到。
- en: Now we have deployed our function, and created our S3 buckets and an event.
    Let's invoke the function. Click on Test. Now you should see that the function
    has successfully executed. You will see the details in the logs, and if you refresh
    the S3 bucket named resized, we should be able to see a resized image file. You
    can just download the resized file and see whether the resizing worked. We can
    also add an S3 put trigger in order to automatically trigger this CreateThumbnail
    function when any image file is uploaded to the source S3 bucket.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经部署了函数，创建了 S3 存储桶和事件。让我们来调用这个函数。点击“测试”。现在你应该能看到函数成功执行的结果。你将看到日志中的详细信息，如果你刷新名为“resized”的
    S3 存储桶，我们应该能看到一张已调整大小的图像文件。你可以直接下载这个调整大小后的文件，查看调整是否成功。我们还可以添加一个 S3 放置触发器，以便在任何图像文件上传到源
    S3 存储桶时自动触发这个 CreateThumbnail 函数。
- en: In this tutorial, we learned how to use CodePipeline, which is a CD platform
    of AWS, to deploy Lambda functions. It was pretty quick in deploying a function
    from GitHub into Lambda using different combinations of tools by AWS. But the
    cons of these tools are that you are charged for their use, and we have to really
    get our heads around CloudFormation and the roles. Now let's look at how to set
    up a pipeline using open source tools.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们学习了如何使用 AWS 的 CD 平台 CodePipeline 来部署 Lambda 函数。使用 AWS 提供的各种工具组合，从 GitHub
    部署函数到 Lambda 非常迅速。但这些工具的缺点是你需要为它们的使用付费，而且我们必须真正理解 CloudFormation 和角色的配置。现在让我们来看看如何使用开源工具设置一个流水线。
- en: Continuous integration and continuous deployment with Lambda
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Lambda 实现持续集成和持续部署
- en: In this section, we will be using Jenkins, a serverless framework, and other
    open source software to set up Continuous Integration and Continuous Deployment.
    I have the whole project set up in a Git repository ([https://github.com/shzshi/aws-lambda-dynamodb-mytasks.git](https://github.com/shzshi/aws-lambda-dynamodb-mytasks.git)),
    or we can go through the following steps listed in this section.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将使用 Jenkins、无服务器框架和其他开源软件来设置持续集成和持续部署。我已将整个项目设置在一个 Git 仓库中（[https://github.com/shzshi/aws-lambda-dynamodb-mytasks.git](https://github.com/shzshi/aws-lambda-dynamodb-mytasks.git)），或者我们可以按照本部分列出的以下步骤进行操作。
- en: The application that we are using for this tutorial will create a Lambda function
    and an AWS API gateway for the task where we can test our Lambda function, which
    will manage tasks using CRUD operations to DynamoDB.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程中使用的应用程序将创建一个 Lambda 函数和一个 AWS API 网关，以便我们测试 Lambda 函数，该函数将使用 CRUD 操作管理任务，并将数据存储在
    DynamoDB 中。
- en: 'First, let''s create a folder structure using a serverless framework to create
    a template function, as shown in the following code. I am assuming that you are
    using Linux Terminal, and that all the instructions are Linux-Terminal-based:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用无服务器框架创建一个文件夹结构，以创建一个模板函数，如下面的代码所示。我假设你使用的是 Linux 终端，并且所有的指令都是基于 Linux
    终端的：
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will see three files created within the `AWSLambdaMyTask` folder. This is
    a sample template for Node.js using a serverless framework. We will be modifying
    these files as per our example''s need, as shown in the following code:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在`AWSLambdaMyTask`文件夹中看到创建的三个文件。这是一个使用无服务器框架的Node.js示例模板。我们将根据示例的需要修改这些文件，如下所示：
- en: '![](img/6e0bcfc9-8230-47e3-aaa8-55e1623674be.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e0bcfc9-8230-47e3-aaa8-55e1623674be.png)'
- en: 'Let''s create two more folders within the `AWSLambdaMyTask` folder, namely `src`
    and `test`. The `src` phrase is our source code folder and `test` is the folder
    for our test cases, as shown in the following code:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在`AWSLambdaMyTask`文件夹中再创建两个文件夹，分别是`src`和`test`。`src`文件夹是我们的源代码文件夹，`test`文件夹是我们的测试用例文件夹，如下所示：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then we will create a file called `package.json` using the editor. This file
    will hold the metadata that is relevant to the project. Copy the following content
    into the file. Please make whatever changes you need:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将使用编辑器创建一个名为`package.json`的文件。这个文件将保存与项目相关的元数据。请将以下内容复制到该文件中，并根据需要进行修改：
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let''s edit the `serverless.yml` file, as per our needs, as shown in the following
    snippet. You can find the file in the mentioned GitHub repo: [https://github.com/shzshi/aws-lambda-dynamodb-mytasks/blob/master/serverless.yml](https://github.com/shzshi/aws-lambda-dynamodb-mytasks/blob/master/serverless.yml):'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据我们的需求，让我们编辑`serverless.yml`文件，如下所示。你可以在下面提到的GitHub仓库中找到这个文件：[https://github.com/shzshi/aws-lambda-dynamodb-mytasks/blob/master/serverless.yml](https://github.com/shzshi/aws-lambda-dynamodb-mytasks/blob/master/serverless.yml)：
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s move the `src` directory, and create a file named `package.json` and
    a folder named `mytasks`, as shown in the following code. The `mytasks` folder
    will have Node.js files to create, delete, get, list, and update the DynamoDB
    table on the AWS:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们移动`src`目录，并创建一个名为`package.json`的文件和一个名为`mytasks`的文件夹，如下所示。`mytasks`文件夹将包含Node.js文件，用于在AWS上创建、删除、获取、列出和更新DynamoDB表：
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Copy the following content to `package.json`:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下内容复制到`package.json`：
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Go to the  `mytasks` folder and create a `create.js` file to create, update,
    list, get, and delete the DynamoDB tables. The `create.js` files are handlers
    for functions in Lambda.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 进入`mytasks`文件夹，创建一个`create.js`文件，用于创建、更新、列出、获取和删除DynamoDB表。`create.js`文件是Lambda函数的处理程序。
- en: 'Add the following content to `src\mytasks\create.js`:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下内容添加到`src\mytasks\create.js`：
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Add the following content to `src\mytasks\delete.js`:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下内容添加到`src\mytasks\delete.js`：
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Add the following content to `src\mytasks\get.js`:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下内容添加到`src\mytasks\get.js`：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Add the following content to `src\mytasks\list.js`:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下内容添加到`src\mytasks\list.js`：
- en: '[PRE19]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Add the following content to `src\mytasks\update.js`:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下内容添加到`src\mytasks\update.js`：
- en: '[PRE20]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now we will create test cases to unit test the code that we created. We will
    be using Mocha for unit testing and run the APIs again. Let''s create a file called `data`
    in the `test` folder, as shown in the following screenshot. This will have the
    JSON data that the unit test will run on:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将创建测试用例来单元测试我们创建的代码。我们将使用Mocha进行单元测试，并重新运行API。让我们在`test`文件夹中创建一个名为`data`的文件夹，如下所示。该文件夹将包含单元测试将要使用的JSON数据：
- en: '[PRE21]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, let''s add the `test/createDelete.js` file, which will create DynamoDB
    data and delete it, once the test is complete, as shown in the following code:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们添加`test/createDelete.js`文件，该文件将在测试完成后创建DynamoDB数据并删除它，如下所示：
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now add the `test/createListDelete.js` file, which will create the DynamoDB
    data, list it, and then delete it once the test is complete, as shown in the following
    code:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在添加`test/createListDelete.js`文件，该文件将在测试完成后创建DynamoDB数据、列出它并删除它，如下所示：
- en: '[PRE23]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s add the `test/createReadDelete.js` file, which will create the DynamoDB
    data, read it, and then delete it once the test is complete, as shown in the following
    code:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们添加`test/createReadDelete.js`文件，该文件将在测试完成后创建DynamoDB数据、读取它并删除它，如下所示：
- en: '[PRE24]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now we will create two test data files—`newTask1.json` and `newTask2.json`—that
    can be used for unit testing.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将创建两个测试数据文件——`newTask1.json`和`newTask2.json`——可以用于单元测试。
- en: 'Let''s create `data/newTask1.json` using the aforementioned data, as follows:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用上述数据创建`data/newTask1.json`，如下所示：
- en: '[PRE25]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Add the following JSON data to `data/newTask2`:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下JSON数据添加到`data/newTask2`：
- en: '[PRE26]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The project folder should now look like the following screenshot:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 项目文件夹现在应该像以下截图所示：
- en: '![](img/cf7304ea-a05f-4992-9159-6b49a986f525.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf7304ea-a05f-4992-9159-6b49a986f525.png)'
- en: 'We need to create a repository on Git to push all the code that we created
    previously so that we can set up CI to the serverless project. I am assuming that
    Git has already been installed on the local server and that the Git repository
    is already in place. In my case, I have the following Git repository set up. I
    will `git clone`, add files and folders, and then push everything to the Git repository:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在Git上创建一个仓库，将之前创建的所有代码推送到仓库，以便我们可以为无服务器项目设置CI。我假设Git已经安装在本地服务器上，且Git仓库已经建立。在我的例子中，我已经设置了以下Git仓库。我将执行`git
    clone`，添加文件和文件夹，然后将所有内容推送到Git仓库：
- en: '[PRE27]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'There will be a folder with the name `AWS-lambda-dynamodb-mytasks`. Go into
    that directory, copy all the files that we created earlier, and then push it to
    the repository, as shown in the following code:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 将会有一个名为`AWS-lambda-dynamodb-mytasks`的文件夹。进入该目录，复制我们之前创建的所有文件，然后将它们推送到仓库，如以下代码所示：
- en: '[PRE28]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Setting up Jenkins for a serverless application
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为无服务器应用程序设置Jenkins
- en: Assuming that we already have Jenkins up and running, we need to install Node.js,
    and then we need to install Mocha on the Jenkins server for unit testing. After
    this, we need to install a serverless framework. You can use the Dockerfile ([https://github.com/shzshi/aws-lambda-dynamodb-mytasks/blob/master/Dockerfile](https://github.com/shzshi/aws-lambda-dynamodb-mytasks/blob/master/Dockerfile))
    from the aforementioned GitHub repository for Jenkins and serverless frameworks.
    If you are using Docker, you don't need to follow the steps for installing Node.js
    on Jenkins.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经启动并运行了Jenkins，我们需要安装Node.js，然后在Jenkins服务器上安装Mocha进行单元测试。之后，我们需要安装Serverless
    Framework。你可以使用上面提到的GitHub仓库中的Dockerfile（[https://github.com/shzshi/aws-lambda-dynamodb-mytasks/blob/master/Dockerfile](https://github.com/shzshi/aws-lambda-dynamodb-mytasks/blob/master/Dockerfile)）为Jenkins和Serverless框架配置。如果你使用Docker，则无需按照Jenkins上安装Node.js的步骤操作。
- en: 'Go through the following steps to install Node.js on the Jenkins node:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤在Jenkins节点上安装Node.js：
- en: '[PRE29]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then go to the browser and open the Jenkins home page. Click on the New item link. This
    will open a new page, that will allow you to create a job with a name of your
    choosing. Select Freestyle project, which is the default selection, and click
    OK to go ahead, as shown in the following screenshot:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后打开浏览器并访问Jenkins主页。点击“New item”链接。这将打开一个新页面，允许你创建一个自定义名称的作业。选择“Freestyle project”，这是默认选择，然后点击“OK”继续，如下截图所示：
- en: '![](img/0a92ecb3-6084-4dac-8eda-2c8e0cd8ad0b.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a92ecb3-6084-4dac-8eda-2c8e0cd8ad0b.png)'
- en: 'Now, we need to integrate Git source code with Jenkins and then build, deploy,
    and test our serverless application. First, let''s add the Git repository to the
    Jenkins job, as shown in the following screenshot:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要将Git源代码与Jenkins集成，然后构建、部署并测试我们的无服务器应用程序。首先，让我们将Git仓库添加到Jenkins作业中，如下截图所示：
- en: '![](img/278ae203-16ec-4421-8b38-90bf8e85c3c7.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/278ae203-16ec-4421-8b38-90bf8e85c3c7.png)'
- en: 'We need to parameterize the build to add `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`,
    as shown in the following screenshot. We will get `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY `after
    we create an IAM user for the Serverless Framework to work:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将构建参数化，添加`AWS_ACCESS_KEY_ID`和`AWS_SECRET_ACCESS_KEY`，如下截图所示。在我们为Serverless
    Framework创建IAM用户后，将获得`AWS_ACCESS_KEY_ID`和`AWS_SECRET_ACCESS_KEY`：
- en: '![](img/4c4009df-eccb-4473-8e31-47ba94379e46.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4c4009df-eccb-4473-8e31-47ba94379e46.png)'
- en: 'Now go to Build  |Add build step | Execute shell | Execute build step | Add
    build step from the drop-down menu, which will open Command Prompt, where we will
    add the command that we need to run, as shown in the following screenshot:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，转到“Build | Add build step | Execute shell | Execute build step | Add build
    step”从下拉菜单中选择，这将打开命令提示符，我们将在其中添加需要运行的命令，如下截图所示：
- en: '![](img/5beca5c5-748c-4f41-9cdb-32a674694cfc.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5beca5c5-748c-4f41-9cdb-32a674694cfc.png)'
- en: 'Once the build is successful, we will have successfully deployed the application
    to the AWS S3 bucket that was created by Serverless Framework. We will also have
    exposed the API and the functions, allowing them to be used by the application
    to perform CRUD functions, as shown in the following screenshot:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦构建成功，我们就成功将应用程序部署到Serverless Framework创建的AWS S3桶中。我们还将暴露API和函数，允许它们被应用程序使用，以执行CRUD操作，如下截图所示：
- en: '![](img/068f685e-1a3b-4527-969b-914006932438.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/068f685e-1a3b-4527-969b-914006932438.png)'
- en: Automated testing for Lambda functions
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Lambda函数的自动化测试
- en: In the previous recipes, we looked at how we can automate builds and deploy
    Lambda functions through Jenkins and Serverless Frameworks. But we can also unit
    test the deployed Lambda functions through Jenkins. In the following recipes,
    we will see how to unit test the Lambda function to check whether the function
    is deployed perfectly and works fine.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的配方中，我们查看了如何通过Jenkins和Serverless Framework自动化构建和部署Lambda函数。但我们还可以通过Jenkins对已部署的Lambda函数进行单元测试。在接下来的配方中，我们将看到如何对Lambda函数进行单元测试，检查该函数是否完美部署并正常工作。
- en: Unit testing a deployed application
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署应用程序的单元测试
- en: 'Once the application is deployed, we can run the unit test on it. In order
    to try out some unit tests, I have created three unit tests in the `test` folder,
    and we will be testing them using Mocha. Let''s create another job in Jenkins
    for setting up unit testing. Again, it would be a freestyle job in Jenkins, as
    shown in the following screenshot:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦应用程序部署完成，我们可以在其上运行单元测试。为了进行单元测试，我在`test`文件夹中创建了三个单元测试，我们将使用Mocha来测试它们。接下来，在Jenkins中创建另一个任务来设置单元测试。这个任务同样是Jenkins中的一个自由风格任务，如下图所示：
- en: '![](img/4642c508-69e5-4d89-8215-3e1a47778795.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4642c508-69e5-4d89-8215-3e1a47778795.png)'
- en: 'We add the Git repository path to Jenkins where our test cases are residing,
    as shown in the following screenshot:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将测试用例所在的Git仓库路径添加到Jenkins中，如下图所示：
- en: '![](img/7643b548-96cb-4766-9a45-bcf48a9b399c.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7643b548-96cb-4766-9a45-bcf48a9b399c.png)'
- en: 'Then we add an execute shell to run the unit test using Mocha, as shown in
    the following screenshot:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们添加一个执行Shell命令来使用Mocha运行单元测试，如下图所示：
- en: '![](img/0f958db1-3fca-4036-8585-4f961f5489fc.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f958db1-3fca-4036-8585-4f961f5489fc.png)'
- en: 'As you can see in the following screenshot, our test cases are passing, which
    proves that our functions are running perfectly fine:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，我们的测试用例通过了，这证明我们的函数运行得非常正常：
- en: '![](img/7101231a-051f-454c-b273-3f4c75ba6dba.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7101231a-051f-454c-b273-3f4c75ba6dba.png)'
- en: AWS Lambda pipeline
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS Lambda流水线
- en: 'In this section of the chapter, we will move to the continuous delivery pipeline
    and do what we have been doing again: creating different environments, deploying
    to multiple environments, unit testing, system testing, and adding an approval
    process. In this pipeline, we will be using Jenkins, Groovy, Serverless Framework,
    and Docker to set up a Serverless Framework environment. Make sure that you have
    Docker installed on your machine.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的这一部分，我们将转向持续交付流水线，继续我们之前做的事情：创建不同的环境，部署到多个环境，进行单元测试，系统测试，并添加审批流程。在这个流水线中，我们将使用Jenkins、Groovy、Serverless
    Framework和Docker来设置Serverless Framework环境。请确保你已在机器上安装了Docker。
- en: Prerequisites
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前提条件
- en: 'We need to create a user for each stage or environment so that we can isolate
    environments and stage the deployment for each environment. To do this, go through
    the following steps for each user:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为每个阶段或环境创建一个用户，以便隔离环境，并为每个环境设定部署的阶段。为此，请为每个用户执行以下步骤：
- en: Log in to an AWS account as the root user, and go to the IAM (Identity and Access
    Management) page.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以root用户登录AWS账户，并进入IAM（身份和访问管理）页面。
- en: Click on Users on the left-hand-side bar, then click on the Add User button and
    add the username dev-serverless. Enable programmatic accessby checking thecheckbox.
    Then click on the Next:Permissions button.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧栏点击“用户”，然后点击“添加用户”按钮，添加用户名dev-serverless。通过勾选复选框启用程序化访问。然后点击“下一步：权限”按钮。
- en: On the Permissions page, select Attach existing policies directly, and search
    for and select the AdministratorAccess checkbox. Then click on Next:Review.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在权限页面，选择“直接附加现有策略”，搜索并选择“AdministratorAccess”复选框。然后点击“下一步：审核”。
- en: Now check that everything is good and then click on Create User. This will create
    a user and show us the access key id and secret access key. Copy these these keys
    somewhere temporarily.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在检查一切是否正常，然后点击“创建用户”。这将创建一个用户，并展示给我们“访问密钥ID”和“秘密访问密钥”。将这些密钥临时复制到其他地方。
- en: Now that we have the keys, we export them as environment variables so that they
    will be accessed by the framework to perform the required functions.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经获取了密钥，将它们作为环境变量导出，这样框架就能访问这些变量，执行所需的功能。
- en: Repeat the preceding five steps for the sit-serverless and prod-serverless users.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对sit-serverless和prod-serverless用户重复前面五个步骤。
- en: CloudBees AWS Credentials Jenkins Plugin.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CloudBees AWS Credentials Jenkins插件。
- en: 'Now, go through the following steps to create the pipeline:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，按照以下步骤创建流水线：
- en: 'Git clone the following repository into a directory:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Git克隆以下仓库到一个目录：
- en: '[PRE30]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Go into this directory and build the Docker image with the Dockerfile provided.
    With `docker images`, we should be able to see the Docker image with the `name docker
    build --rm -f Dockerfile -t aws-lambda-dynamodb-mytasks:latest`, as shown in the
    following code:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入此目录并使用提供的 Dockerfile 构建 Docker 镜像。通过 `docker images`，我们应该能看到名为 `docker build
    --rm -f Dockerfile -t aws-lambda-dynamodb-mytasks:latest` 的 Docker 镜像，如以下代码所示：
- en: '[PRE31]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, we will run the container and open Jenkins on the browser. The initial
    installation password can be found in the container run logs as shown in the following
    code:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将运行容器并在浏览器中打开 Jenkins。初始安装密码可以在容器运行日志中找到，如以下代码所示：
- en: '[PRE32]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Go to the browser and open `http://localhost:8080`. Copy the password from the
    container run; it should be something like the following output. Once you are
    logged in, install the suggested plugin and create a Jenkins user for future logins.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开浏览器并访问 `http://localhost:8080`。从容器运行中复制密码，它应该类似于以下输出。一旦你登录，安装推荐的插件并为将来登录创建一个
    Jenkins 用户。
- en: '*Jenkins initial setup will be required. An admin user will have been created
    and a password generated. Use the following password to proceed to installation:*'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '*Jenkins 初始设置将是必需的。一个管理员用户已经被创建，并且密码已生成。使用以下密码继续安装：*'
- en: '*6050bfe89a9b463c8e2784060e2225b6*'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '*6050bfe89a9b463c8e2784060e2225b6*'
- en: '*This may also be found at /var/jenkins_home/secrets/initialAdminPassword.*'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '*这也可以在 /var/jenkins_home/secrets/initialAdminPassword 中找到。*'
- en: 'Once Jenkins is up and running, we can go ahead and create a pipeline job,
    so click on New Item, enter the item name as `my-serverless-pipeline`, and select
    the pipeline project. In Job Configure, select the This project is parameterized checkbox,
    and then in Add Parameter,select Credentials Parameterand then go to the Default
    Value section of the Credentials Parameter and click on Add. Then select Jenkins.This
    will open the Jenkins Credentials Provider page. On this page in the Kind drop-down
    menu, select AWS Credentials and add the users `dev-serverless`, `sit-serverless`,
    and `prod-serverless`, as shown in the following screenshot. Then, click Add:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Jenkins 启动并运行，我们可以继续创建一个流水线作业，所以点击 New Item，输入项目名称为 `my-serverless-pipeline`，并选择流水线项目。在
    Job Configure 中，选择 This project is parameterized 复选框，然后在 Add Parameter 中选择 Credentials
    Parameter，接着进入 Credentials Parameter 的 Default Value 部分并点击 Add。然后选择 Jenkins。这将打开
    Jenkins Credentials Provider 页面。在该页面的 Kind 下拉菜单中，选择 AWS Credentials 并添加用户 `dev-serverless`、`sit-serverless`
    和 `prod-serverless`，如以下截图所示。然后点击 Add：
- en: '![](img/37b9cac0-a83f-46cf-815b-b4f69eabe7d2.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37b9cac0-a83f-46cf-815b-b4f69eabe7d2.png)'
- en: 'Once all the AWS credentials are added, pull them into the Credentials parameter
    for AWS, as shown in the following screenshot. Make sure that all three types
    of credential parameter are added, namely `dev`, `sit`, and `prod`:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有 AWS 凭证被添加，拉取它们到 AWS 的 Credentials 参数中，如以下截图所示。确保所有三种凭证参数都已添加，即 `dev`、`sit`
    和 `prod`：
- en: '![](img/4ec305eb-f033-49a6-97f9-0ec28aca345a.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ec305eb-f033-49a6-97f9-0ec28aca345a.png)'
- en: 'You need to create your own Git repository and push the files from the repository
    named [https://github.com/shzshi/aws-lambda-dynamodb-mytasks.git](https://github.com/shzshi/aws-lambda-dynamodb-mytasks.git).
    Then, open the Jenkins file in your favorite editor and comment in all the system
    test code for the entire environment, as shown in the following code. The reason
    we are doing this is because we will be exporting the API gateway endpoint to
    execute the system test for the entire environment:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要创建自己的 Git 仓库，并推送文件到名为 [https://github.com/shzshi/aws-lambda-dynamodb-mytasks.git](https://github.com/shzshi/aws-lambda-dynamodb-mytasks.git)
    的仓库中。然后，在你喜欢的编辑器中打开 Jenkins 文件，并取消注释整个环境的所有系统测试代码，如以下代码所示。我们这么做的原因是我们将导出 API 网关端点来执行整个环境的系统测试：
- en: '[PRE33]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Now click on the Pipeline tab and select Pipeline script the SCM in the definition.
    Set the SCM as Git and in the Repository URL field, add the Git repository path
    created by you. This repository has a Jenkins file, a Lambda function, and a test
    folder. Leave the rest as default and click on Save.Now our pipeline is saved.
    It is time to run the pipeline. Jenkinsfile is a great script that will orchestrate
    the pipeline for us.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在点击 Pipeline 标签，并选择 Pipeline script 作为定义中的 SCM。将 SCM 设置为 Git，并在 Repository
    URL 字段中添加你创建的 Git 仓库路径。这个仓库中包含了 Jenkins 文件、Lambda 函数和测试文件夹。其余部分保持默认设置并点击 Save。现在我们的流水线已经保存。接下来是运行流水线。Jenkinsfile
    是一个出色的脚本，它将为我们协调流水线的执行。
- en: Click Build with Parameters. You will then see the environment-based build parameter
    that will be required for our pipeline to build, test, and deploy our code.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“使用参数构建”。然后你会看到我们的管道构建、测试和部署代码所需要的基于环境的构建参数。
- en: 'The first run should be without any testing in place, and should only deploy
    functions and the API gateway for the tasks on the AWS Cloud. The console output
    will provide us with the endpoints for the tasks, as shown in the following code:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次运行应该没有任何测试，仅仅是部署函数和API网关到AWS云中的任务。控制台输出将提供任务的端点，如下代码所示：
- en: '[PRE34]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Replace the task endpoint in the Jenkinsfile for the entire environment with
    the API gateway path listed on the console, as shown in the following code. Then
    save the Jenkinsfile and push it to the Git repository created by you. The reason
    we are adding the endpoints late in the build is because API gateway endpoints
    are created dynamically. But we can also have a static endpoint URL with custom
    domain names featured in the API gateway:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 用控制台中列出的API网关路径替换Jenkinsfile中整个环境的任务端点，如下代码所示。然后保存Jenkinsfile并将其推送到你创建的Git仓库中。我们之所以在构建后期添加端点，是因为API网关端点是动态创建的。但我们也可以拥有一个带有自定义域名的静态端点URL，API网关中提供了这一功能：
- en: '[PRE35]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Build the job by clicking on Build with Parameters. Here, we should be able
    to see that the system test is running along the deployment steps, and the pipeline
    should be green, as shown in the following screenshot:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“使用参数构建”来构建作业。在这里，我们应该能够看到系统测试正在与部署步骤一起运行，管道应显示为绿色，如下截图所示：
- en: '![](img/23e7d668-800e-4cd9-847f-041ce1177650.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/23e7d668-800e-4cd9-847f-041ce1177650.png)'
- en: In the preceding recipe, we learned how Lambda functions and the API gateway
    call tasks. We also learned how they are deployed to AWS through Serverless Framework
    and Jenkins via different environments, such as dev, sit, and prod. We also created
    and tested a system test. The deployment and execution will deploy the Lambda
    function and API gateway to the AWS cloud, and every system test will execute
    the Lambda function to perform CRUD operations on DynamoDB. So if you go into
    DynamoDB, you should see three tables that are created for each environment. You
    should also be able to see different functions and the API gateway for each environment.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们学习了Lambda函数如何与API网关进行任务调用。我们还了解了它们如何通过Serverless框架和Jenkins部署到AWS，涉及不同的环境，如开发（dev）、系统集成（sit）和生产（prod）。我们还创建并测试了系统测试。部署和执行将把Lambda函数和API网关部署到AWS云中，并且每个系统测试将执行Lambda函数对DynamoDB进行CRUD操作。因此，如果你进入DynamoDB，你应该看到为每个环境创建的三个表。你还应该能够看到每个环境的不同功能和API网关。
- en: Deployment methods
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署方法
- en: Deployment into production has its pains. We take a lot of precautions while
    deploying into production. We introduce lots of testing on lower environments
    so that most of the bugs and performance problems are taken care of early on.
    But we are still nervous when deploying into production, as we are never 100 percent
    sure whether the deployed version will work perfectly fine. If we are using deployment
    techniques, then we minimize the deployment failure substantially. There are varieties
    of deployment technique, but canary and blue-green are the most popular ones.
    We will look at examples of both deployment techniques for AWS Lambda.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 部署到生产环境总是伴随着一定的困难。在部署到生产环境时，我们会采取很多预防措施。我们在低环境中进行大量的测试，以便尽早解决大部分的漏洞和性能问题。但是我们在部署到生产环境时仍然感到紧张，因为我们永远无法百分之百确定部署的版本是否能完美运行。如果我们使用了部署技术，那么就能大大减少部署失败的风险。部署技术有很多种，但金丝雀部署和蓝绿部署是最受欢迎的两种。我们将一起看一下这两种部署技术在AWS
    Lambda中的应用实例。
- en: Canary deployment
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 金丝雀部署
- en: Canary deployment is a deployment technique involving a gradual shift in production
    traffic from version A to version B, where version B is the latest version and
    version A is the previous version. AWS has recently introduced traffic shifting
    for Lambda functions aliases. An alias is a pointer to a specific version of the
    Lambda functions, which basically means that we can split the traffic of the functions
    between two different versions by specifying the percentage of incoming traffic
    that we want to direct to the new release. Lambda will automatically load balance
    requests between versions when aliases are invoked. So instead of replacing one
    function with another version, both versions can coexist and can be monitored
    as to how they perform.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀部署是一种部署技术，涉及将生产流量逐步从版本 A 转移到版本 B，其中版本 B 是最新版本，版本 A 是之前的版本。AWS 最近引入了 Lambda
    函数别名的流量转移。别名是指向特定版本 Lambda 函数的指针，这基本上意味着我们可以通过指定要转移到新版本的流量百分比，将函数的流量在两个不同版本之间进行拆分。当调用别名时，Lambda
    会自动在版本之间负载均衡请求。因此，取代用另一个版本替换一个函数，两者可以共存，并且可以监控它们的表现。
- en: All of this sounds awesome, but doing all this it not that easy. Fortunately,
    AWS already has a service that will help us with this problem—CodeDeploy. To use
    canary deployment with the AWS CodeDeploy service, we need to create a variety
    of resources. We need to create a CodeDeploy application, a deployment group,
    and aliases for the functions. We also need to create new permissions and replace
    all the event sources to trigger the aliases instead of the latest functions.
    But this can be much easier if we use the canary deployment plugin with Serverless
    Framework. Let's learn how we can achieve this using an example.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切听起来很棒，但要做到这一点并不容易。幸运的是，AWS 已经有一个可以帮助我们解决这个问题的服务——CodeDeploy。要在 AWS CodeDeploy
    服务中使用金丝雀部署，我们需要创建各种资源。我们需要创建一个 CodeDeploy 应用程序、一个部署组以及函数的别名。我们还需要创建新的权限，并将所有事件源替换为触发别名，而不是最新的函数。但如果我们使用与
    Serverless Framework 配合使用的金丝雀部署插件，这一切会变得更简单。让我们通过一个例子学习如何实现这一点。
- en: The code that we will be working on in the following recipe is available at :[https://github.com/shzshi/my-canary-deployment.git](https://github.com/shzshi/my-canary-deployment.git).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的食谱中使用的代码可以在以下网址找到：[https://github.com/shzshi/my-canary-deployment.git](https://github.com/shzshi/my-canary-deployment.git)。
- en: Setting up a simple environment
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置一个简单的环境
- en: 'Perform the following steps:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'Let''s create a simple serverless service with the following command. Two files
    will be created named `handler.js` and `serverless.yml`:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用以下命令创建一个简单的 serverless 服务。将创建两个文件，分别为 `handler.js` 和 `serverless.yml`：
- en: '[PRE36]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now replace the content of `serverless.yml` with the following code. Make sure
    that it is indented properly. We are creating a service with a function and an
    API gateway:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在用以下代码替换 `serverless.yml` 的内容。确保它正确缩进。我们正在创建一个包含函数和 API 网关的服务：
- en: '[PRE37]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let''s replace the content of `handler.js` with the following content:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们用以下内容替换 `handler.js` 的内容：
- en: '[PRE38]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Create a `package.json` file, as shown in the following code:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 `package.json` 文件，如以下代码所示：
- en: '[PRE39]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Let''s deploy this function to the AWS, as shown in the following code. Make
    sure that you have configured your AWS access and secret key before deploying:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将此功能部署到 AWS，如以下代码所示。确保在部署之前已经配置好你的 AWS 访问密钥和秘密密钥：
- en: '[PRE40]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Once successfully deployed, let''s invoke the function and let it verify the
    execution. We will be provided with the service endpoint, so let''s invoke the
    function with the endpoint, as shown in the following code:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署成功后，让我们调用该函数并验证执行情况。我们将获得服务端点，所以让我们使用端点调用该函数，如以下代码所示：
- en: '[PRE41]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Setting up canary deployment
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置金丝雀部署
- en: Once the initial setup is complete, we will tell the serverless canary deployment
    plugin to split the traffic between the last two versions and gradually shift
    more traffic to the new version until it receives all the load.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦初始设置完成，我们将告诉 serverless 金丝雀部署插件将流量在最后两个版本之间分配，并逐渐将更多流量转移到新版本，直到它接收所有负载。
- en: 'The following are the three types of gradual deployment that we can implement
    using AWS''s CodeDeploy:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们可以使用 AWS 的 CodeDeploy 实现的三种渐进式部署类型：
- en: '**Canary:** The traffic is shifted to a new version during a certain period
    of time, and when the time elapses, all the traffic will have moved to the newer
    version'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金丝雀：** 流量在一段时间内转移到新版本，时间一到，所有流量将转移到新版本'
- en: '**Linear:** The traffic is shifted to the new version incrementally at intervals
    until it gets all of the traffic'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性：** 流量会按间隔逐渐转移到新版本，直到新版本接收到所有流量。'
- en: '**All at once:** All the traffic is shifted to the new version at once'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一次性全部转移：** 所有流量会一次性转移到新版本。'
- en: 'We need to be specific as to which parameter and type of deployment we will
    use by choosing any of the aforementioned options for using CodeDeploy:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要明确指定我们将使用的参数和部署类型，可以选择上述任何一种 CodeDeploy 部署选项：
- en: '`Canary10Percent30Minutes`, `Canary10Percent5Minutes`, `Canary10Percent10Minutes`,
    `Canary10Percent15Minutes`, `Linear10PercentEvery10Minutes`, `Linear10PercentEvery1Minute`,
    `Linear10PercentEvery2Minutes`, `Linear10PercentEvery3Minutes` or `AllAtOnce`.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`Canary10Percent30Minutes`、`Canary10Percent5Minutes`、`Canary10Percent10Minutes`、`Canary10Percent15Minutes`、`Linear10PercentEvery10Minutes`、`Linear10PercentEvery1Minute`、`Linear10PercentEvery2Minutes`、`Linear10PercentEvery3Minutes`
    或 `AllAtOnce`。'
- en: 'For our tutorial, we will use `Linear10PercentEvery1Minute`, which means that
    the traffic that the new version of the function will receive will be increased
    by 10 percent increments every minute, until it reaches 100%. To do this, we need
    to set the type and alias (the name of the alias that we want to create) under
    `deploymentSettings` in the function. Let''s update the files, and redeploy and
    invoke the function to see how the traffic moves:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的教程，我们将使用 `Linear10PercentEvery1Minute`，这意味着新版本的函数将每分钟增加 10% 的流量，直到达到 100%。为了实现这一点，我们需要在函数中设置类型和别名（我们想要创建的别名名称）下的
    `deploymentSettings`。让我们更新文件，并重新部署并调用函数，看看流量是如何变化的：
- en: 'Add the deployment settings within the `serverless.yml`, as shown in the following
    code:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `serverless.yml` 中添加部署设置，如下方代码所示：
- en: '[PRE42]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Update the `handler.js` with the following code:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码更新 `handler.js`：
- en: '[PRE43]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let''s deploy the function. You will see from the following code that the deployment
    will trigger `CodeDeploy` with linear deployment, and requests will be load balanced
    between the two functions:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们部署函数。您将看到以下代码触发 `CodeDeploy` 进行线性部署，并且请求将在两个函数之间进行负载均衡：
- en: '[PRE44]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Let''s invoke the function and see whether it is load balanced, as shown in
    the following code:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们调用函数，看看它是否已经实现负载均衡，如下方代码所示：
- en: '[PRE45]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Making sure the deployment works fine
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确保部署正常工作
- en: 'Our function deployed without any hassle, but how do we ensure that whole system
    is behaving correctly? To ensure everything is working fine, we can provide CodeDeploy
    with a list of variables to track during the deployment process, then cancel it,
    and shift all the traffic to the old version if the `ALARM` is triggered. With
    serverless, we can set the alarm using another plugin. Let''s have a look at how
    to do this:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的函数部署没有任何问题，但我们如何确保整个系统的正确运行呢？为了确保一切正常工作，我们可以向 CodeDeploy 提供一个变量列表，在部署过程中进行追踪，然后取消部署，如果触发了
    `ALARM`，则将所有流量切换到旧版本。在无服务器架构中，我们可以使用另一个插件来设置警报。让我们看看如何操作：
- en: 'Update the `serverless.yml` to set the alarm, as shown in the following code:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 `serverless.yml` 来设置警报，如下方代码所示：
- en: '[PRE46]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s deploy the function and see how it works, as shown in the following
    code:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们部署函数，看看它是如何工作的，如下方代码所示：
- en: '[PRE47]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The deploy step will create an alarm in CodeDeploy and the CloudWatch dashboard,
    where we can see different graphs representing invocations and errors. You can
    log into AWS Console and go to CodeDeploy and CloudWatch to see how the alarm
    is created, and to see what the dashboard looks like.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 部署步骤将在 CodeDeploy 和 CloudWatch 仪表板中创建一个警报，在那里我们可以看到不同的图表，表示调用和错误。您可以登录 AWS 控制台，进入
    CodeDeploy 和 CloudWatch，查看警报是如何创建的，并查看仪表板的样子。
- en: Deploying CodeDeploy hooks
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署 CodeDeploy 钩子
- en: So now we have all the tools to minimize the impact of possible bugs or failures.
    However, we can also avoid invoking a function version that contained errors by
    running the CodeDeploy hooks first. Hooks are Lambda functions triggered by CodeDeploy
    before and after traffic shifting takes place. It expects to get notified about
    the success or failure of the hooks, only continuing to the next step if they
    succeed. They are good for running integration tests and checking that everything
    fits together in the cloud, since it will automatically rollback at a failure.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们拥有了所有工具来尽量减少可能的错误或故障的影响。然而，我们也可以通过首先运行 CodeDeploy 钩子来避免调用包含错误的函数版本。钩子是由
    CodeDeploy 在流量切换之前和之后触发的 Lambda 函数。它期望收到钩子成功或失败的通知，只有在成功的情况下才会继续进行下一步。这些钩子非常适合运行集成测试并检查一切是否在云端配合得当，因为一旦失败，系统会自动回滚。
- en: 'Let''s have look at how to create hooks by going through the following steps:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下步骤来看看如何创建钩子：
- en: 'Let''s update the `serverless.yml` to add hook details. We need to grant our
    functions access to CodeDeploy so that we can use CodeDeploy''s SDK in the hooks,
    as shown in the following code:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们更新`serverless.yml`以添加钩子详细信息。我们需要授予我们的函数访问CodeDeploy的权限，这样我们就可以在钩子中使用CodeDeploy的SDK，如以下代码所示：
- en: '[PRE48]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Next, we will create the hooks. Let''s create a new file named `hooks.js` within
    the service directory and add the following hook content:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建钩子。让我们在服务目录中创建一个名为`hooks.js`的新文件，并添加以下钩子内容：
- en: '[PRE50]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now that we have created the hooks, let''s deploy them and see how they function
    using the following code:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经创建了钩子，让我们通过以下代码进行部署并查看它们的功能：
- en: '[PRE51]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: While the deployment is continuing on the CLI, we should log into the AWS Cloud
    console and go to Services | CloudFormation. Select the stack named my-canary-deployment-dev and
    scroll down and select the CodeDeploy link in the Status Reason column. We should
    be able to see the gradual shifting of traffic with prehook, traffic shifting,
    and posthook execution, finally completing the whole stack and, lastly, the deployment.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 当CLI上的部署仍在继续时，我们应该登录到AWS Cloud控制台并转到“服务”|“CloudFormation”。选择名为my-canary-deployment-dev的堆栈，向下滚动并选择状态原因列中的CodeDeploy链接。我们应该能够看到流量逐步转移的过程，包括预钩子、流量转移和后钩子执行，最终完成整个堆栈的部署。
- en: Here, we learned how we can set up canary deployments using Serverless Framework,
    various plugins, and AWS CodeDeploy.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们学习了如何使用Serverless框架、各种插件和AWS CodeDeploy来设置金丝雀部署。
- en: Blue-green deployment
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝绿部署
- en: Like canary deployment, blue–green deployment is another type of methodology
    for safe deployment for production. In canary deployment, we shifted the traffic
    from one version to the next version gradually until we completely moved it to
    the latest version. But in the case of blue–green deployment, we create two different
    environments. One environment is used for going live and the other for staging
    the new version. So a blue–green deployment setup would create a separate region
    for staging and production and then route the traffic from one region to another
    with deploying the latest version.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 像金丝雀部署一样，蓝绿部署是另一种用于生产环境安全部署的方法。在金丝雀部署中，我们逐渐将流量从一个版本转移到下一个版本，直到完全转移到最新版本。但在蓝绿部署中，我们创建了两个不同的环境，一个用于上线，另一个用于暂存新版本。因此，蓝绿部署设置会为暂存和生产创建独立的区域，然后将流量从一个区域路由到另一个区域，同时部署最新版本。
- en: Let's say that I have a **blue** region (`us-east-1`), which is the production
    region, and that it is live and has the current version of the Lambda functions
    deployed. Now let's say that I also have a new version out, so I will deploy the
    Lambda function (the new version) into a **green **region (`us-east-2`), and that
    this will act as my staging environment. I will perform all the testing, and once
    satisfied, I will redirect all the traffic to the **green** region (`us-east-2`).
    Now that my staging is live, I will go ahead and deploy the latest version to
    the blue region (`us-east-1`), and my functions will be tested for bugs and problems.
    But let's say that, unfortunately, some serious bugs are discovered in the **blue** region (`us-east-1`).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我有一个**蓝色**区域(`us-east-1`)，这是生产区域，并且它是实时的，已部署当前版本的Lambda函数。现在假设我也发布了一个新版本，所以我将Lambda函数（新版本）部署到**绿色**区域(`us-east-2`)，它将作为我的暂存环境。我将在此环境中进行所有测试，一旦满意，就将所有流量重定向到**绿色**区域(`us-east-2`)。现在我的暂存环境已上线，我将继续将最新版本部署到蓝色区域(`us-east-1`)，并测试我的函数以查找漏洞和问题。但假设不幸的是，在**蓝色**区域(`us-east-1`)发现了一些严重的漏洞。
- en: 'Then the code is rolled back, all the traffic is once again pointed to the **blue** region (`us-east-1`),
    and the **green** region (`us-east-2`) becomes the staging environment again:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，代码被回滚，所有流量再次指向**蓝色**区域(`us-east-1`)，而**绿色**区域(`us-east-2`)再次成为暂存环境：
- en: '**Blue: **`$ serverless deploy --stage prod --region us-east-1`'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**蓝色：**`$ serverless deploy --stage prod --region us-east-1`'
- en: '**Green:** `$ serverless deploy --stage prod --region us-east-2`'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**绿色：**`$ serverless deploy --stage prod --region us-east-2`'
- en: Integration of CloudWatch with ELK
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将CloudWatch与ELK集成
- en: I have been using ELK for quite a long time. It was daily work for me, as AWS
    Lambda logs are shipped to CloudWatch, but as my company uses ELK for centralized
    log management, I now like to push all the logs from CloudWatch to ELK.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经使用 ELK 很长时间了。它曾是我的日常工作，因为 AWS Lambda 日志会被发送到 CloudWatch，但由于我的公司使用 ELK 来集中管理日志，现在我希望将所有
    CloudWatch 日志推送到 ELK。
- en: 'So I decided to ship the CloudWatch logs to ELK. Lambda logs can be shipped
    directly to Elasticsearch or to Redis for Logstash to pick it up. There is a plugin
    available that will help us to ship the Lambda CloudWatch logs to ELK. We will
    now look at how to configure this. We will be using a Docker ELK image to set
    up ELK locally and then connect to AWS CloudWatch through the Logstash plugin.
    Then we will push the logs to Elasticsearch. Let''s go through the following steps:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我决定将 CloudWatch 日志传输到 ELK。Lambda 日志可以直接传输到 Elasticsearch 或 Redis，由 Logstash
    拿取。有一个插件可以帮助我们将 Lambda CloudWatch 日志传输到 ELK。接下来我们将了解如何配置它。我们将使用 Docker ELK 镜像在本地设置
    ELK，然后通过 Logstash 插件连接到 AWS CloudWatch，最后将日志推送到 Elasticsearch。让我们按照以下步骤操作：
- en: 'Get the Docker image for ELK, as shown in the following code. If you already
    have an ELK account set up, then you don''t need to follow this step:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取 ELK 的 Docker 镜像，如以下代码所示。如果你已经设置了 ELK 账户，则无需执行此步骤：
- en: '[PRE52]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Now, if you go to your browser and open the link `http://localhost:5601/`, then
    you should be able to see the Kibana dashboard.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你打开浏览器并访问链接 `http://localhost:5601/`，你应该能看到 Kibana 仪表板。
- en: 'Install the `logstash-input-cloudwatch_logs` plugin. Ssh into your Docker container
    that was created in the previous step and install the plugin, as shown in the
    following code:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 `logstash-input-cloudwatch_logs` 插件。通过 SSH 登录到你在上一步创建的 Docker 容器并安装插件，如以下代码所示：
- en: '[PRE53]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Once the plugin is successfully installed, we need to create a Logstash config
    file that will help us in shipping the CloudWatch logs. Let''s open the editor
    and add the following config file and name it `cloud-watch-lambda.conf`. We need
    to replace `access_key_id` and `secret_access_key` as per the AWS IAM user and
    also update the log group. I have added three `grok` filters:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 插件成功安装后，我们需要创建一个 Logstash 配置文件，帮助我们将 CloudWatch 日志传输到目标位置。打开编辑器，添加以下配置文件并命名为
    `cloud-watch-lambda.conf`。我们需要根据 AWS IAM 用户替换 `access_key_id` 和 `secret_access_key`，并更新日志组。我添加了三个
    `grok` 过滤器：
- en: The first filter matches the generic log message, where we strip the timestamp
    and pull out the `[lambda][request_id]` field for indexing
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个过滤器匹配通用的日志消息，其中我们会去除时间戳，并提取出 `[lambda][request_id]` 字段进行索引
- en: The second `grok` filter handles the `START` and `END` log messages
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个 `grok` 过滤器处理 `START` 和 `END` 日志消息
- en: The third filter handles the `REPORT` messages and gives us the most important
    fields
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个过滤器处理 `REPORT` 消息，并提供最重要的字段
- en: '[PRE54]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Let's name this` cloud-watch-lambda.conf `and place it in the `/etc/logstash/conf.d` file
    of the Docker container.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其命名为 `cloud-watch-lambda.conf` 并将其放置在 Docker 容器的 `/etc/logstash/conf.d` 文件夹中。
- en: 'Now let''s restart the Logstash service, as shown in the following code. Once
    this is done, you should be able to see that the logs have been pulled into our
    ELK container:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们重启 Logstash 服务，如以下代码所示。完成后，你应该能看到日志已被拉入我们的 ELK 容器：
- en: '[PRE55]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Open the browser and go to the page `http://localhost:5601` and we should be
    able to see the logs streaming into ELK from cloudwatch and this can be refined
    further with ELK filter and regular expression.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开浏览器并访问页面 `http://localhost:5601`，我们应该能看到 CloudWatch 的日志流式传输到 ELK，并且可以使用 ELK
    过滤器和正则表达式进一步优化此过程。
- en: '![](img/917f3c3b-6a6d-4add-9b2c-2142581673cb.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/917f3c3b-6a6d-4add-9b2c-2142581673cb.png)'
- en: Summary
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to integrate DevOps with various out-of-the-box
    tools within AWS Lambda and with open source tools. In the next chapter, we will
    learn how to set up CI and CD with Azure functions with the help of Serverless
    Framework, as well as how to monitor and log with Azure functions.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何将 DevOps 与 AWS Lambda 内置的各种工具以及开源工具集成。在下一章中，我们将学习如何通过 Serverless
    Framework 设置 CI 和 CD，并学习如何在 Azure Functions 中进行监控和日志记录。
