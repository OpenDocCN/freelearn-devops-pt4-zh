- en: Best Practices and the Future of DevOps with Serverless
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we learned what serverless is and the multiple different
    service providers for serverless features. We also looked at how to build, test,
    and deploy a serverless application, as well as how to monitor and log the execution.
    But this chapter is pretty interesting, because in this chapter, we will go one
    step further to learn some best practices for serverless. We will also learn the
    best practices for building, deploying, monitoring, logging, and securing our
    serverless application. We will also examine how DevOps works with serverless.
  prefs: []
  type: TYPE_NORMAL
- en: Important aspects of DevOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The two important tenets of DevOps are automation and process. We have to automate
    every bit of development from the nonproduction environment to production, and
    at the same time we have to maintain continuous feedback, with information moving
    back and forth, while also logging everything. Let's look at some best practices
    of how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Collaboration and tools strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The DevOps team needs to come up with a common tool strategy across the organization,
    and they should collaborate with different teams—such as development, testing,
    and infrastructure—and agree upon the business objectives of DevOps. There should
    be seamless collaboration and integration between the teams. The objective is
    to automate everything, so the ideal goal should be one-click deployment from
    development to production, with very minimal human intervention.
  prefs: []
  type: TYPE_NORMAL
- en: Agile development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many Agile methodologies available. Scrum, XP, and Kanban are a few
    of the more popular ones. You can use one of these development methodologies to
    give you a more flexible option to plan, create a faster output, and give you
    clear focus and transparency throughout the development of your project. Agile
    methodologies help to limit the work in progress, which in turn helps maintain
    a balanced flow so that we don't attempt to do too much at once.
  prefs: []
  type: TYPE_NORMAL
- en: Version control everything
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we talk about version control, the first thing that comes to our mind is
    application source code versioning, which is a practice that we are well accustomed
    to. But with source code versioning, we should version control the database, the
    build artifacts, the dependencies, and everything that is involved in the application.
    This ensures that every aspect of the application can be historically tracked
    through the single origin that is the repository.
  prefs: []
  type: TYPE_NORMAL
- en: Capture every request
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We should make sure that no ad hoc work or changes happen outside of DevOps,
    and that there should be a change request raised and maintained for every functional
    and nonfunctional requirement. These change requests should be captured and maintained
    within the tool.
  prefs: []
  type: TYPE_NORMAL
- en: Automate test and source code analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we talk about automating the test, this does not mean just automating the
    testing. The test automation should include provisioning test data and running
    the standard test to ensure that the standard of the code meets the enterprise
    service level agreement. The testing must be continuous and should run 100 or
    1,000 times, and then, upon successful completion, should automatically be promoted
    to a higher environment. The quality of the code should be checked regularly and
    returned back to the developer to be reworked if the code coverage, source code
    analysis, and performance of the code are not up to the baseline.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous feedback
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The feedback loop is very a important aspect of DevOps. It plays a very important
    role for the success of DevOps. The feedback loop consists of automated communication
    between an issue registered during development and the human element of the DevOps
    process. It should be managed by a tool, through which issues must be registered
    manually or via an automated mechanism. The issue should also be tagged with an
    artifact that developers can use to trace what occurred, why it occurred, and
    where in the process it occurred. The tool should help to define the chain of
    communication with all automated and human elements in the loop.
  prefs: []
  type: TYPE_NORMAL
- en: Time to market and the cycle time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During the DevOps process, we will get a rich array of metrics and real-time
    reports about our process, and we will want to measure the time to market and
    the cycle time of the application, because these two metrics play a very important
    role in how fast and efficiently we make new features available to customers.
    Time to market measures end-to-end efficiency in bringing valuable new features
    to market, and the cycle time is the measurement of the engineering team's process,
    which, once the feature is defined, indicates when it becomes available to move
    to production. It helps in understanding the efficiency of the team and improving
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Log metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We should always track the productivity of DevOps processes, both automated
    and manual, and determine whether they working in favor of the organization. We
    need to define which metrics are relevant for the DevOps process, such as the
    deployment speed, the testing errors found, or the build time. With this definition,
    automated processes can correct these issues without manual intervention.
  prefs: []
  type: TYPE_NORMAL
- en: We skimmed through general DevOps practices that can be applied to application
    development, but when it comes to serverless applications, we need to add more
    specific best practices in terms of tools and process. We will look into each
    service provider and funnel the best practices for the serverless applications,
    as well for DevOps with serverless.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for Serverless
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we know, the serverless architecture consists of a small piece of code called
    a function, which runs in a stateless container. One major purpose of this architecture
    is to scale and descale as and when required. So, bearing this in mind, our best
    practices are more or less focused on this aspect of serverless. So let's look
    at a few of the best practices involved with the serverless concept.
  prefs: []
  type: TYPE_NORMAL
- en: One function, one task
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we start building functions, we might end up with monolithic functions
    behind the proxy route and use a `switch` statement. So, if we have one or a few
    functions to run our whole app, then we are actually scaling the whole application
    instead of scaling a specific element of the application. This should be avoided,
    as scaling would be a problem in this instance, and we also might end up with
    large and complex functions.
  prefs: []
  type: TYPE_NORMAL
- en: Functions call other functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We should avoid calling a function within another function because we will end
    up paying more for this, and debugging would be a headache. We lose the value
    of isolating the functions. If required, it would be ideal to trigger another
    function if more work has to be done.
  prefs: []
  type: TYPE_NORMAL
- en: Minimize libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we write a function, we might need libraries to make it run. But adding
    a huge set of libraries to the function would make it really slow. There are two
    types of starts when using functions—one is the cold start, which is function
    that is started for the first time, and the other is the warm start, which is
    function that has already been started and is ready to be executed from the pool.
    So in theory, a larger number of libraries will be slower during the cold start
    and will also impact the scaling of the application. With each increase in scale,
    the function has to perform a cold start, and so, as the cold start is slower,
    the scaling is slower.
  prefs: []
  type: TYPE_NORMAL
- en: Also, as the number of libraries rises, the less secure our code will be, so
    security testing has to be performed for each set of libraries added. They also
    need to be trusted before you actually start using them.
  prefs: []
  type: TYPE_NORMAL
- en: With HTTP –  one function per route
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We should avoid using single-function proxy if possible while using http route,
    as this hinders in scaling of the function, and also makes debugging tedious.
    There are occasions where we can avoid it, where a functionality of a series of
    routes is tied to a single table, and it's very much decoupled from the rest of
    the application.
  prefs: []
  type: TYPE_NORMAL
- en: Database connection to RDBMS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of a serverless application is designed to work very well with services,
    so establishing the connection to RDMBS within the function could be troublesome,
    as the parts of the services are still relied upon to provide the fast responses.
  prefs: []
  type: TYPE_NORMAL
- en: As function scales with RDBMS connections, the number of connections grows with
    the scale,  and we might end up introducing a bottleneck and an I/O wait into
    the cold start of the function.
  prefs: []
  type: TYPE_NORMAL
- en: So Serverless architecture makes us rethink the data layer, so if we try using
    serverless with the existing data layer (RDBMS), then you will see a performance
    lag in your overall application performance. This might also be the reason why
    DynamoDB works so well with Serverless.
  prefs: []
  type: TYPE_NORMAL
- en: Use messages and queues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A serverless application works efficiently when the application is asynchronous,
    but with web applications, this is not that simple, because web applications require
    lots of request–response interactions and lots of querying. Going back to functions
    not calling other functions, it’s important to point out that this is how you
    chain functions together. Queues act as circuit breakers in the chaining scenario,
    so if the function fails, we can easily drill down to the failed queue and investigate
    the failure, which could be the result of pushing messages that failed to a dead-letter
    queue.
  prefs: []
  type: TYPE_NORMAL
- en: With applications that have serverless as the backend, it would be ideal to
    use CQRS, which separates out the conceptual model of input and output in a single
    model into separate models for the purposes of updates and displaying changes
    to the user.
  prefs: []
  type: TYPE_NORMAL
- en: Data in motion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a serverless system, the data is in motion. It flows through the system,
    but it might end up in a datalake. Nevertheless, while in a serverless system
    it should in flow, so make sure that you keep all the data in motion instead of
    a datalake. Avoid querying from the datalake while using the serverless environment.
    It is not always possible to keep data in motion, but it is best to try.
  prefs: []
  type: TYPE_NORMAL
- en: Measure the scale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main feature of serverless is efficient scaling, but we should code it so
    that it scales efficiently. We could avoid calling a direct data connection within
    the function, or avoid adding huge numbers of dependency libraries. So, in short,
    the lighter the function, the faster the cold start is, and eventually it will
    scale much faster. Also make sure that functions are load tested before they reach
    production for efficient scaling.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we looked into the various best practices that make serverless
    functions perform and scale very efficiently. Although this is not the complete
    list, these are a few of the very important ones. In the next section, we will
    learn the best practices for each cloud provider and also understand how best
    practices with DevOps will help make serverless development and deployment faster
    and more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: DevOps best practices and troubleshooting for AWS Lambda
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned the best practices for using DevOps and designing the
    architecture of a serverless application. We also looked at the best ways of writing
    functions to make them scalable and have a faster cold start. Going forward, we
    will learn how to apply the best practices for a specific cloud provider.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Lambda is a very popular and mature serverless platform. Because of this,
    most serverless best practices are aligned with Lambda, and there are a number
    of tools and processes that align functions perfectly well with AWS Lambda. In
    this section, when we talk about DevOps, we will look at the source code versioning,
    building, testing, the package itself, releasing, monitoring, security, and controlling
    the cost aspects of DevOps. All of these aspects of DevOps are stitched together
    to perform as a single platform. This needs a lot of hard work and patience from
    us if we are to reach the nirvana of DevOps.
  prefs: []
  type: TYPE_NORMAL
- en: Source code versioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Source code versioning is a very important part of development. If the code
    is not versioned, then it might as well not exist. There are many tools available
    for source code versioning, but these days, Git and Apache Subversion are the
    most popular ones. It is always good to have one repository for each application
    and then create multiple branches for development. With respect to Git, when we
    create a repository, there is already a master branch. The master holds the golden
    copy of the code, which is usually used as a benchmark and can also be used as
    a production copy. We can also create many other branches for the feature, release,
    and development versions of the code for efficient development and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: So the best practices concerning the source code management is to commit often
    and also make sure that you raise the pull request every time you start your day.
    This allows us to avoid wasting time on merging the code and also prevents us
    from breaking the build. Usually, we have a huge team of developers in various
    parts of the globe building different modules for the application. If we forgot
    to commit one day, then the next morning we would have to waste lots of time merging
    our code before we actually started development. We should also make sure that
    we always inspect before we commit, because otherwise we might commit a whole
    bunch of junk, unwanted libraries, or JAR or debug files into the repository,
    which will clog the repository, as well as make our function bigger, eventually
    degrading the performance.
  prefs: []
  type: TYPE_NORMAL
- en: We should also make sure that we add the commit message while we commit the
    code. The comment should explain why we committed the code because this will help
    us to easily trace whether something failed, and also makes it easier for us to
    roll back if we committed buggy code, or if some functionality changes. When we
    add the commit message, we should make sure that it is not too generic or lacking
    in information. We should also ensure that it is fixed, that it works, and that
    it has no typos, because this won't help us to trace it back if we have introduced
    a bug or an issue. In addition, it is not good practice to have the same commit
    message appear after a previous commit, as we usually commit because something
    has changed in the code compared to the previous commit.
  prefs: []
  type: TYPE_NORMAL
- en: We should avoid committing a build artifact or compiled dependencies like `.dll`
    or `.jar` into the source control as this could well irritate your coworkers,
    as they will have to check out a huge list of files or have his on local environment
    corrupted by downloading these dependencies. Another way to avoid commit issues
    would be to write precommit hooks, which can mitigate most of the commit issues.
  prefs: []
  type: TYPE_NORMAL
- en: We have talked about versioning, but we should also talk about the version database
    component. Like the UI, application, and testing code, the database component
    should be versioned for stable deployment and application. If we don't version
    the database, then we might run the application with old data or an old configuration.
    It is also easier to track what DDL and DML statement were used in a particular
    release version.
  prefs: []
  type: TYPE_NORMAL
- en: So, in short, we should always remember to commit regularly, know what are we
    committing, add a valid commit message, and ensure that sure we do it ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS also supports the versioning of the functions. The Lambda function is versioned
    through publishing. One of the AWS Lambda consoles is specifically dedicated to
    publishing the new version of the Lambda function, and we can create multiple
    versions of the same function. Each Lambda function version is provided with a
    unique ARN, and becomes immutable postpublishing. Lambda also allows us to create
    aliases (as shown in the following screenshot), which are pointers to specific
    Lambda function versions. Each alias will have a unique ARN and can only point
    to one function version, and not to another alias:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01781fec-f67b-44c9-bc46-5cdaa3281039.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's look at an example of AWS Lambda versioning. Say that we have an AWS S3
    bucket that is an event source for a Lambda function, so whenever any new item
    is added to the bucket, the Lambda function is triggered and executed. The event
    source mapping information is stored in the bucket notification configuration,
    and in that configuration, we can identify the Lambda function ARN that S3 can
    invoke. But the newly published version is not automatically updated, so we have
    to make sure that this is updated every time a new version of the function is
    created.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we are using the Serverless Framework, the new version is created through
    the deployment. There is also the provision to roll back to a previous version
    through the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Build
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In programming terms, the build is the version of the program or product, but
    the term is also used to refer to the continuous integration of DevOps. So we
    should be sure of the build of the code is built each time code is committed into
    the Git repository. The reason for this is because there are many developers involved
    in such projects, working individually on their own machines, which might work
    fine. But until the code is committed and built on the continuous integration
    server, which will trigger source code analysis and unit testing on the committed
    code, we won't know if we have pushed broken code. So it is essential to trigger
    a build with every commit.
  prefs: []
  type: TYPE_NORMAL
- en: The artifact of a successful build should always be versioned. The best practice
    is to version them on the Nexus repository, so that all the nonproduction builds
    should be pushed into snapshot repositories of the nexus and the release candidate
    should be pushed into the release repository of the nexus. This process will help
    us to keep the release build separate from the snapshot build, which is temporary
    and should be regularly purged, as it is created for each build and will not be
    needed later.
  prefs: []
  type: TYPE_NORMAL
- en: The build should also trigger source code analysis, which could be linting for
    a Node.js and Python application, and if you are using Java or C#, then there
    are lots of tools available for source code analysis. The source code analysis
    will make sure that the proper coding standards are followed by developers. It
    also performs security checks and checks for cyclomatic complexity within the
    code. The tools will generate reports as well, and we can also set a threshold
    for the source code analysis pass parameters that will eventually pass the build. So
    if the source code analysis checks are below the baseline set, then the build
    will fail and then the developer will have to fix the issue.
  prefs: []
  type: TYPE_NORMAL
- en: If we are using Java as a language for serverless, then there are many open
    source tools available for us to use, such as PMD, Checkmarx, Checkstyle, FindBugs,
    and SonarQube. **SonarQube** is a popular tool for source code analysis. It supports
    lots of languages, such as Java, C# , Node.js, and Python. It also has a lovely
    dashboard and is pretty easy to install and configure. The SonarQube official
    image is there on Docker Hub. You can set it up and give it a try.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the SonarQube image on Docker Hub at [https://hub.docker.com/_/sonarqube/](https://hub.docker.com/_/sonarqube/).
  prefs: []
  type: TYPE_NORMAL
- en: Test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Testing is one of the essential parts of the development cycle. Testing ensures
    that bug-free and secure code is deployed to the higher environment, and we should
    make sure that the testing should be automated as much as possible. Testing should
    be combined with the continuous integration process and eventually with continuous
    deployment. The benefit of testing is that it saves lots time and money for organizations
    because it mitigates most of the bugs and errors at the initial stage of development.
    There are many different levels of testing, such as unit testing, integration
    testing, functional testing, and performance testing.  Let's look at the best
    practices around these levels.
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unit testing is the foundation level of testing when testing software. It basically
    means testing the units of your code to check the correctness of its functioning.
    During unit testing, the code is tested in a test environment using simulated
    input. The output of the execution is then compared with the expected output,
    and if the output matches the expected output, then the test passes.
  prefs: []
  type: TYPE_NORMAL
- en: 'With respect to the serverless concept, we do not have to worry about Lambda
    functions or handlers or events; we just need to organize our code base for easy
    integration of unit testing. If you look at the following code, you can see that
    we are separating our core logic into separate modules instead of within the handler
    so that they can be unit tested individually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: So, it always better to separate your business logic so that it is independent
    of the service provider, reusable, and more easily testable. With this separation
    of business logic, the unit testing would be easier to write and run, and it would
    also take less effort to move to a different provider.
  prefs: []
  type: TYPE_NORMAL
- en: There are many unit-testing frameworks available for Node.js and Python, but
    the ones we will look at are **Mocha** and **Jest**. There are serverless framework
    plugins available for these two frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the Mocha serverless plugin installation and usage.
    It can be installed locally as a service or included within the `serverless.yml`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Likewise, we can add the Jest serverless plugin into the `serverless.yml`,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, **Nose** is a popular framework for Python serverless applications.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to this, we can use **LocalStack **for mocking an AWS environment.
    LocalStack helps us to create a local testing environment that is similar to AWS.
    It spins up a core Cloud API for us to test our functions locally.
  prefs: []
  type: TYPE_NORMAL
- en: Integration testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Integration testing is basically when tests cover more than one unit. Our Lambda
    function must be integrated with third-party code dependencies, which have to
    be tested thoroughly. That's where integration testing comes into play. Integration
    testing plays a very important role in the serverless world, but it is also expensive
    if you are always doing it over the cloud. But we can reduce the cost by mocking,
    as we talked about when looking at the LocalStack framework. Using this method,
    we can put it to use mocking a large number of AWS resources.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we can invoke the Lambda function locally using the `serverless invoke
    local`, and if our functions need to read/ write from DynamoDB, then this can
    also be mocked using `dynamodb-local` Node.js libraries.
  prefs: []
  type: TYPE_NORMAL
- en: While these methods don’t emulate Lambda 100%, we’ll still be able to find issues
    in our code base quickly without having to wait for a deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Performance testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance testing is a type of testing where we check how our application
    is performing with an expected workload. This is a very essential type of testing
    for our type of application because performance testing will provide us with an
    opportunity to improve the performance of our functions, and will also help to
    improve the speed of scaling. Scaling is a very important feature of functions,
    but it is our responsibility to code it in a fashion that will help it scale faster.
    This is where performance testing will help.
  prefs: []
  type: TYPE_NORMAL
- en: One of the very popular tools for performance testing is **Jmeter**. It is widely
    used across the industry. The Jmeter stress test will examine load-testing KPIs,
    such as the response time, error rate, memory leaks, slowness, security issues,
    and data corruption. Jmeter requires a performance parameter script. We can start
    creating the script with the BlazeMeter Chrome extension.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another popular tool is **Artillery**, which can be used for performance testing,
    load testing, and functional testing. It is an open source tool, and is built
    over Node.js. It can generate a load for an application and can simulate virtual
    users. You can run through this simple installation using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: There is also a combination of Serverless Framework and Artillery, called `serverless-artillery`.
    This package will help us to deploy the function and run the performance test
    through a single Node.js package, and makes it easier to integrate it into our
    CI/CD pipeline. More information can be found at: [//github.com/Nordstrom/serverless-artillery](http://://github.com/Nordstrom/serverless-artillery).
  prefs: []
  type: TYPE_NORMAL
- en: 'Testing is very important in order to weed out defects and errors in the early
    stages of development, but we have to make sure that it is introduced very early
    in the development cycle, and should be performed until production. But it is
    equally important to automate it, and it should be integrated with the continuous
    integration and continuous delivery cycle. But serverless testing has its challenges,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Serverless architecture is actually an integration of separate, distributed
    services that need to be tested both together and independently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can test the serverless functions locally, but it is hard to emulate them
    completely locally, and it is equally important to test them over the cloud. This
    process should be automated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The serverless architecture can feature event-driven, asynchronous workflows,
    and testing them thoroughly is not that easy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we think of monitoring, the first thing that comes into our mind is observing
    and tracking the operations and activities of the software application regularly
    and setting up an alarm to trigger upon a system or application failure. Monitoring
    should begin right from the start of the development, in such instances as the
    monitoring of the speed of the application or monitoring how many resources a
    developed piece of code is using. This monitoring system should be coupled with
    a notification system, triggering a notification such as an email when memory
    is used beyond a defined threshold.
  prefs: []
  type: TYPE_NORMAL
- en: But monitoring serverless functions is more tricky. In server monitoring, we
    monitor the performance of the server, network latency, and CPU, but these details
    are irrelevant with respect to serverless as the infrastructure is completely
    managed by the service provider. But then what should we monitor with respect
    to serverless? We can still monitor memory and concurrency rate. Although the
    service provider handles the provisioning and execution of Lambda functions, there
    is still a limit to the amount of memory that can be used and the concurrent executions
    allocated to the function.
  prefs: []
  type: TYPE_NORMAL
- en: To perform serverless monitoring, there are number of tools available. There
    are a few out-of-the-box tools from AWS, such as **AWS CloudWatch**, which involves
    all the AWS resources feeding into AWS CloudWatch. CloudWatch will be our first
    tool to monitor the Lambda functions. CloudWatch tracks metrics such as the latency
    of execution, number of functions executed, and errors made during the execution.
    But we can go beyond this by setting up custom metrics within CloudWatch. We also
    looked at the CloudWatch dashboard for use with the Lambda function.
  prefs: []
  type: TYPE_NORMAL
- en: AWS provides another powerful tool for the application performance of the lambda
    function, and that tool is X-ray. X-ray is a tracing tool that is integrated with
    AWS Lambda out of the box. It provides an end-to-end view of requests as they
    move. With X-ray, we can analyse how lambda functions and their connected services
    are performing. We can identify and troubleshoot the root causes of performance
    issues and errors, and with the map view of the application, we can see all of
    its components.
  prefs: []
  type: TYPE_NORMAL
- en: Thundra is another monitoring tool that can be used as an application performance
    monitoring tool for AWS Lambda. It asynchronously publishes the data from CloudWatch
    into Thundra using functions and agents to send the monitoring data into the application.
    It has some really good charts that cover many metrics, such as the invocation
    counts, cold-start invocation counts and durations, error counts by error types
    and function names, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To get the latest version of the Lambda function for execution, they should
    be deployed on the cloud. In this way, if our deployment package is heavier, then
    deployment will be slower. This will eventually degrade the performance of the
    function as it unpacks when it is invoked. The packing of our function also affects
    our execution. For example, if we are using Java, then packaging the function
    with all the dependencies grouped together as one function is slower compared
    to putting the dependencies within the `lib` folder.
  prefs: []
  type: TYPE_NORMAL
- en: There are many tools that are available for deployment, but Serverless Framework
    is one of the main candidates for this job. Of course, there is lot that we still
    have to achieve, but this is still a far more mature solution than any other open
    source option available on the market.
  prefs: []
  type: TYPE_NORMAL
- en: Also with respect to deployment , it should be orchestrated and should go through
    lots of approval and change request process with respect to UAT and prod environment.
    Deployment in production should be managed through service-now change request
    and CAB approvals.
  prefs: []
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The runtime language for Lambda provides a mechanism for your function to deliver
    logged statements to CloudWatch logs. We need any type of application to make
    the best use of its logs, not just serverless applications. With other applications,
    we can retrieve the logs from the server in which the application is deployed.
    However, with serverless applications that is not possible, as there is no server,
    and present, we don't have the ability to "step through" the code of a live, running
    lambda function. As a result, we are heavily dependent on the logs that we create
    to inform our investigation of our functions' behavior. Because of this, we have
    to make sure that any logs generated should have the right balance of verbosity
    to triage the issues without demanding too much computing time. It is recommended
    that we use an environment variable to create a `loglevel` variable.
  prefs: []
  type: TYPE_NORMAL
- en: The appropriate use of log levels can ensure that we have the ability to selectively
    incur the additional compute and storage cost only during an operational triage that
    our function can refer to so that it can determine which log statements to create
    during runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Security should be first in our list of priorities while designing and implementing
    functions. One of the major differences between serverless applications and server-based
    applications is that with a serverless application, we do not have control over
    the servers, so we must bake in most of the securities within our code. So while
    coding the serverless application, we should make sure that we write a secure
    code so that all the third-parties' security configuration is validated. Let's
    look at some of the best practices that are essential for implementing this secure
    code.
  prefs: []
  type: TYPE_NORMAL
- en: An IAM role per function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is advisable that you create a role for each function, because this would
    decouple the IAM roles. This would also enable least privilege to be provided
    to individual functions. Say, for example, that a function is using a KMS key
    within its code. If we have a common role for the KMS key, then all the other
    functions would have access to the KMS key.
  prefs: []
  type: TYPE_NORMAL
- en: No long-lived credentials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using temporary AWS credentials with lambda function code is always secure. This
    is where static analysis configuration plays an important role. It best to create
    an AWS service client within the function code through AWS SDK without providing
    any credentials. The SDK should automatically manage the retrieval and rotation
    of the credentials for the alloted role.
  prefs: []
  type: TYPE_NORMAL
- en: Do not persist secrets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is best practice to not persist the secrets. However, our function might
    need some secrets to be long lived, such as database credentials and dependency
    service access keys. Because of this requirement, it is recommended that you encrypt
    these secrets. There are a few options available to us, such as using the lambda
    environment variable with encryption and the Amazon EC2 systems manager's parameter
    store.
  prefs: []
  type: TYPE_NORMAL
- en: Secrets should not be saved or persisted on memory. Instead, the function should
    retrieve temporary credentials and keep rotating them, revoking them from time
    to time. API interaction with the lambda function should be authenticated and
    authorized.
  prefs: []
  type: TYPE_NORMAL
- en: Lambda in VPC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When using Lambda functions in VPC, we should follow the best practices of using network
    security by using least-privilege security groups, lambda-function-specific subnets,
    and route tables to allow traffic specific to our lambda function, if we have
    used resources from the VPC.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Lambda best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we learned about the best practice for DevOps with respect to AWS Lambda,
    but I feel that this chapter cannot end without knowing the best practices for
    the lambda function. These best practices will eventually help us to develop efficient
    lambda functions, and will also help us to achieve faster deployment. Let's look
    at these best practices individually.
  prefs: []
  type: TYPE_NORMAL
- en: Keep the handler independent of business logic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is always advisable to keep business logic outside the handler because this
    will decouple our business logic from the lambda function runtime environment,
    reuse the business logic functions, and make it easier to test our written unit
    test, as we discussed earlier in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Keep warm containers alive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we code, we have to make sure that our code is be written in such a way
    that it uses the resources of a warm container instead of doing a cold start.
    This means that we have to scope our variables in such a way that they can be
    reused multiple times on subsequent invocations wherever possible. We should reuse
    our connection (such as an HTTP or a database connection) that was established
    in a previous invocation.
  prefs: []
  type: TYPE_NORMAL
- en: Dependency control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lambda needs many libraries during invocation. Lambda will always look for the
    latest libraries and security updates, but this will also bring changes in the
    behavior of the lambda function. As a result, it is best to package the dependencies
    along with the functions and deploy them. They should also be controlled and managed
    for the sake of performance within the lower environments.
  prefs: []
  type: TYPE_NORMAL
- en: Shorter timeout for dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to configure a shorter timeout for all external dependencies. We cannot
    allow them to run while they are continuously looking for dependencies because
    Lambda is billed based on the duration of the execution of the function. The larger
    the execution time, the higher the are charges.
  prefs: []
  type: TYPE_NORMAL
- en: Exception handling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We should evaluate what the failure behavior of our functions should be, and
    accordingly, our functions should throw the correct exceptions back for faster
    resolution.
  prefs: []
  type: TYPE_NORMAL
- en: Recursive coding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We should avoid recursive code within our lambda function because the function
    will call itself recursively until a criteria is met. But this will lead to multiple
    volumes of invocation and increased cost. If we still accidentally do it, then
    we should set the function's concurrent execution limit to `0` to immediately
    throttle all invocations to the function while you update the code.
  prefs: []
  type: TYPE_NORMAL
- en: High availability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we go into production, high availability becomes essential, and with respect
    to serverless applications, high availability depends on the number of zones in
    which the lambda functions can be executed. If our function uses the default network,
    then it is automatically able to execute within all available zones in that region.
    Nothing else is required to configure the high availability for the function in
    the default network environment. Therefore, while designing the VPC, it is important
    to include the subnets from multiple availability zones.
  prefs: []
  type: TYPE_NORMAL
- en: Runtime language
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Choosing a runtime language is quite tricky when we start designing a serverless
    application. But we can always seek comfort in our skills sets when we decide
    on the runtime language. But the runtime language also determines the performance
    of our application. For example, if we use Java or .NET (the compiled language)
    as our runtime language, then this incurs the largest initial startup cost for
    a container's first invocation, but the performance will improve for subsequent
    invocations. Interpreted languages, such as Node.js or Python, have very fast
    initial invocation times compared to compiled languages, but they cannot output
    the maximum performance compared to complied languages. So if the application
    is latency sensitive or very spiky, then it is recommended that you use an interpreted
    language, and if the application does not experience large peaks or valleys within
    its traffic, or if user experience is not blocked with the lambda functions response
    time, then we can choose any language as the runtime language.
  prefs: []
  type: TYPE_NORMAL
- en: Code for cold and warm containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The performance of the lambda function is dictated by the logic that we code
    within it and the dependencies we call. So the best practice to code the lambda
    function would be to consider the cold and warm start of the containers.
  prefs: []
  type: TYPE_NORMAL
- en: For improved performance for warm starting the container, make sure that you
    store and reference any externalized configurations or dependencies that your
    code retrieves locally. Limit the reinitialization of the variables/objects on
    every invocation that uses global and static variables. Keep the HTTP and database
    connections alive.
  prefs: []
  type: TYPE_NORMAL
- en: For better performance for cold starting the container, it is always better
    to use the default network environment, unless connectivity to a resource within
    a VPC via a private IP is required. Choose an interpreted runtime language over
    a complied language. Keep the function code package as small as possible, as this
    will reduce the time to download, unpack the code from the S3 bucket, and invoke
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Cost optimizing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s an antipattern to assume that the smallest resource size available to your
    function will provide the lowest total cost. If your function’s resource size
    is too small, you could pay more because of a longer execution time than if more
    resources were available that allowed your function to complete more quickly.
  prefs: []
  type: TYPE_NORMAL
- en: You don’t need to implement all use cases through a series of blocking/synchronous
    API requests and responses. If you are able to design your application to be asynchronous,
    you might find that each decoupled component of your architecture takes less compute
    time to conduct its work than tightly coupled components that spend CPU cycles
    awaiting responses to synchronous requests. Many of the Lambda event sources fit
    well with distributed systems, and can be used to integrate your modular and decoupled
    functions in a more cost-effective manner.
  prefs: []
  type: TYPE_NORMAL
- en: Some Lambda event sources allow you to define the batch size for the number
    of records that are delivered on each function invocation (for example, Kinesis
    and DynamoDB). You should run tests to find the optimal number of records for
    each batch size so that the polling frequency of each event source is tuned to
    how quickly your function can complete its task.
  prefs: []
  type: TYPE_NORMAL
- en: The variety of event sources available to integrate with Lambda means that you
    often have a variety of solutions available to meet your requirements. Depending
    on your use case and requirements (request scale, volume of data, latency required,
    and so on), there might be a nontrivial difference in the total cost of your architecture
    based on which AWS services you choose as the components that surround your Lambda
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for Azure functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With respect to other cloud providers, such as Azure, the best practices should
    align with the Lambda function. In this section, we will look at a few of these
    best practices for Azure that are closely related to Lambda functions.
  prefs: []
  type: TYPE_NORMAL
- en: Avoid large and long-running functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large and long-running functions can cause unexpected timeout issues  and A
    function can become large because of many Node.js dependencies. Specially Importing
    dependencies can also cause increased load times that result in unexpected timeouts.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever possible, we should refactor large functions into smaller function
    sets that work together and return responses fast.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-function communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we are not using durable functions or logic apps to integrate multiple functions,
    it is generally a best practice to use storage queues for cross-function communication.
    The storage queues are cheaper and much easier to provision.
  prefs: []
  type: TYPE_NORMAL
- en: Individual messages in a storage queue are limited in size to 64 KB. If we need
    to pass larger messages between functions, an Azure Service Bus queue could be
    used to support message
  prefs: []
  type: TYPE_NORMAL
- en: functions should be be stateless
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is recommended that functions are stateless and idempotent where ever possible.
    Associate any required state information with your data. For example, an order
    that is being processed will likely have an associated state member. A function
    could process an order based on that state while the function itself remains stateless.
  prefs: []
  type: TYPE_NORMAL
- en: and very importantly it is recommended that idempotent functions have timer
    triggers. For example, if you have something that absolutely must run once a day,
    we should write it so that it can run at any time during the day with the same
    results. The function can skip when there is no work for a particular day. Also,
    if a previous run failed to complete, then the next run should pick up where it
    left off.
  prefs: []
  type: TYPE_NORMAL
- en: functions is defensive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is always good to design our functions with the ability to continue from
    a previous fail point during the next execution.  It is recommended to write the
    function with proper error handling considering networking outages, reached quota
    limits, or made any other kind of mistake. All of these issues can affect our
    function at any time. we need to design our functions to be prepared for such
    things.
  prefs: []
  type: TYPE_NORMAL
- en: Let assume our code fails after inserting 2000 items into the a queue for processing,
    we should be able to track the items in a set that is been completed or else we
    might end up inserting them again.
  prefs: []
  type: TYPE_NORMAL
- en: How does your code react if a failure occurs after inserting 5,000 of these
    items into a queue for processing? You should track items in a set that you’ve
    completed. Otherwise, you might insert them again next time. This can have a serious
    impact on your work flow.
  prefs: []
  type: TYPE_NORMAL
- en: The same function app should not have code for test and production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We should not mix, test and production functions or resources within the same
    function app, the performance might be degraded because functions within the function
    app shares resources, for example memory is shared. So we should be careful what
    we load in our production function apps as memory is averaged across each function
    in the function app
  prefs: []
  type: TYPE_NORMAL
- en: Use asynchronous code, but avoid blocking calls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Asynchronous programming is a recommended best practice. However, always avoid
    referencing the `Result` property, or calling the `Wait` method on a `Task` instance.
    This approach can lead to thread exhaustion.
  prefs: []
  type: TYPE_NORMAL
- en: Configure host behaviors to better handle concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `host.json` file in the function app allows for the configuration of the
    host runtime and trigger behaviors. In addition to batching behaviors, you can
    manage concurrency for a number of triggers. Often, adjusting the values in these
    options can help each instance scale appropriately for the demands of the invoked
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: The settings in the `hosts` file apply across all functions within the app,
    within a *single instance* of the function. For example, if you had a function
    app with 2 HTTP functions and concurrent requests set to 25, a request to either
    HTTP trigger would count towards the shared 25 concurrent requests. If that function
    app scaled to 10 instances, the 2 functions would effectively allow 250 concurrent
    requests (10 instances X 25 concurrent requests per instance).
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for Google Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the best practices are same across the Serverless functions even though
    the service providers are different , but I would still like to list some of them
    here.
  prefs: []
  type: TYPE_NORMAL
- en: Code idempotent functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The functions should produce the same result even after calling them multiple
    times. This helps in retrying an invocation if the previous invocation fails partway
    through our code.
  prefs: []
  type: TYPE_NORMAL
- en: Signal the completion of function calls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Signal the completion of your function, failing to do so can result in your
    function executing until a timeout is hit. If a timeout occurs, you will be charged
    for the entire timeout time. Timeouts may also cause subsequent invocations to
    require a cold start, which results in additional latency.
  prefs: []
  type: TYPE_NORMAL
- en: Do not start background activities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A function invocation finishes once termination is signaled. Any code run after
    graceful termination cannot access the CPU, and will not make any progress. In
    addition, when a subsequent invocation is executed in the same environment, your
    background activity resumes, interfering with the new invocation. This may lead
    to unexpected behavior and errors that are hard to diagnose. Accessing the network
    after a function finishes usually leads to connections being reset (and the `ECONNRESET`
    error code).
  prefs: []
  type: TYPE_NORMAL
- en: Background activity is anything that happens after your function has terminated.
    It can often be detected in logs from individual invocations by finding anything
    that is logged after the line saying that the invocation finished. Background
    activity can sometimes be buried deeper in the code, especially when asynchronous
    operations such as callbacks or timers are present. Review your code to make sure
    that all asynchronous operations finish before you terminate the function.
  prefs: []
  type: TYPE_NORMAL
- en: Always delete temporary files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Local disk storage in the temporary directory is an in-memory filesystem. Files
    that you write consume memory available to your function, and sometimes persist
    between invocations. Failing to explicitly delete these files may eventually lead
    to an out-of-memory error and a subsequent cold start.
  prefs: []
  type: TYPE_NORMAL
- en: You can see the memory used by an individual function by selecting it in the
    list of functions ([https://console.cloud.google.com/getting-started](https://console.cloud.google.com/getting-started))
    in the GCP console and choosing the Memory usage plot.
  prefs: []
  type: TYPE_NORMAL
- en: Do not attempt to write outside of the temporary directory, and be sure to use
    platform/OS-independent methods to construct file paths.
  prefs: []
  type: TYPE_NORMAL
- en: You can bypass the size restrictions on temporary files by using `pipelining`.
    For example, you can process a file on Cloud Storage by creating a read stream,
    passing it through a stream-based process, and writing the output stream directly
    to Cloud Storage.
  prefs: []
  type: TYPE_NORMAL
- en: Local development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Function deployment takes a bit of time, so it is often faster to test the code
    of your function locally using a **shim**.
  prefs: []
  type: TYPE_NORMAL
- en: Error reporting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Do not throw uncaught exceptions, because they force cold starts in future invocations.
    See the **Error Reporting guide** ([https://cloud.google.com/functions/docs/monitoring/error-reporting](https://cloud.google.com/functions/docs/monitoring/error-reporting))
    for information on how to properly report errors.
  prefs: []
  type: TYPE_NORMAL
- en: Use SendGrid to send emails
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cloud Functions does not allow outbound connections on port 25, so you cannot
    make nonsecure connections to an SMTP server. Instead, you should use `SendGrid`
    to send emails.
  prefs: []
  type: TYPE_NORMAL
- en: Use dependencies wisely
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Because functions are stateless, the execution environment is often initialized
    from scratch (during a cold start). When a cold start occurs, the global context
    of the function is evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: If your functions import modules, the load time for those modules can add to
    the invocation latency during a cold start. You can reduce this latency, as well
    as the time needed to deploy your function, by loading the dependencies correctly
    and not loading dependencies that your function doesn't use.
  prefs: []
  type: TYPE_NORMAL
- en: Use global variables to reuse objects in future invocations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is no guarantee that the state of a Cloud function will be preserved for
    future invocations. However, Cloud Functions often recycles the execution environment
    of a previous invocation. If you declare a variable with a global scope, its value
    can be reused in subsequent invocations without having to be recomputed.
  prefs: []
  type: TYPE_NORMAL
- en: This way, you can cache objects that may be expensive to recreate on each function
    invocation. Moving such objects from the function body to a global scope may result
    in significant performance improvements.
  prefs: []
  type: TYPE_NORMAL
- en: Do lazy initialization of global variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you initialize variables with a global scope, the initialization code will
    always be executed via a cold start invocation, increasing your function's latency.
    If some objects are not used in all code paths, consider initializing them lazily
    on demand.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As its name implies, this chapter was all about learning the troubleshooting
    techniques and best practices to apply when using DevOps to construct serverless
    architecture. In the next chapter, we will talk about the various use cases of
    AWS Lambda, Azure Functions, and open source versions, and how DevOps fits in
    to it all.
  prefs: []
  type: TYPE_NORMAL
