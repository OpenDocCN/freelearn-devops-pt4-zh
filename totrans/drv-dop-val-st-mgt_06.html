<html><head></head><body>
		<div id="_idContainer050">
			<h1 id="_idParaDest-103"><em class="italic"><a id="_idTextAnchor126"/></em><a href="B17087_05_Final_PD_epub.xhtml#_idTextAnchor126"><em class="italic">Chapter 5</em></a>: Driving Business Value through a DevOps Pipeline</h1>
			<p>In the previous four chapters, you learned there are many different ways to define the term <em class="italic">value</em> and the importance of understanding its use in context. Therefore, we spent some time learning about these terms to ensure that we have a common semantic understanding when communicating about using <strong class="bold">value stream management</strong> (<strong class="bold">VSM</strong>) and DevOps to deliver customer-centric value.</p>
			<p>You learned how Lean and Agile practices complement each other to help an organization deliver customer-centric value. You also learned why we need to take a systems-thinking view to improve value delivery within large and complex organizations.</p>
			<p>This chapter explains how and why IT organizations represent very complex systems across two primary functions: <strong class="bold">development</strong> and <strong class="bold">operations</strong>. In a traditional IT shop, the development and operations organizations function as separate departments with different activities, and each has a different focus and culture. This type of separation of people and responsibilities only adds to the complexity of the IT organization.</p>
			<p>This chapter will explain why this functional split in the IT organization can become a problem. You will also learn about a collaboration and integration strategy that can be used to resolve such issues. That strategy is called <strong class="bold">DevOps</strong>, which is a concatenation of development and operations.</p>
			<p>The topics presented in this chapter are a necessary precursor to our discussions on the implementation of VSM tools and methods, which we'll cover in <em class="italic">Section 2</em> of this book.</p>
			<p>In this chapter, we're going to cover the following topics:</p>
			<ul>
				<li>Breaking down barriers</li>
				<li>Improving flows with DevOps pipelines</li>
				<li>Understanding virtualization</li>
				<li>Defining <strong class="bold">continuous integration</strong> (<strong class="bold">CI</strong>)</li>
				<li>Defining <strong class="bold">continuous delivery</strong> (<strong class="bold">CD</strong>)</li>
				<li>Enabling CI/CD and DevOps pipeline flows</li>
				<li>Understanding the scope of DevOps</li>
				<li>Integrating <strong class="bold">IT service management</strong> (<strong class="bold">ITSM</strong>)</li>
				<li>Moving beyond projects and into products</li>
			</ul>
			<p>In this last chapter of <em class="italic">Section 1</em> of this book, you will gain an appreciation of the disparate technologies, portfolio-level investments, and genuine complexities involved in developing a competitive DevOps pipeline capability. Let's start this introduction to DevOps by understanding the business drivers that led to its evolution in the software industry.</p>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor127"/>Breaking down barriers</h1>
			<p>Nowadays, with a quick search on the internet, you will find many industry analysts and other commentators who will agree that DevOps has become the table stakes to effectively compete in our modern digital economy (Dietrich, 2019). Those organizations that master the integration and automation of tools of activities across the IT value streams of development and operations have orders of magnitude better velocity in software delivery and better quality and efficiency.</p>
			<p>Just as Lean practices transformed the competitive landscape in manufacturing and other services-based companies, DevOps has similarly transformed the IT industry. Specifically, DevOps pipelines implement a software development strategy that is equivalent to the Lean production flow concepts in manufacturing and other industries. As a result, those organizations that effectively implement DevOps pipelines have significant competitive advantages when responding to new market opportunities, evolving competitive pressures, and changing customer needs.</p>
			<p>Moreover, DevOps is an amalgamation of Lean and Agile practices or, for short, Lean-Agile. Agile provides values and principles to guide customer-centric software development practices; Lean production concepts provide proven approaches to eliminate waste and achieve efficient software value delivery. As you will learn in <em class="italic">Section 2</em> of this book, modern VSM methods and tools enable organizations to implement Lean transformations across their IT value streams.</p>
			<p>DevOps concepts began to emerge in 2008. Specifically, Andrew Shafer and Patrick Debois were given credit for initially discussing these concepts in a private meeting during the Agile Conference held in Toronto that same year. DevOps then became popularized a bit later when Patrick Debois organized the first <em class="italic">DevOpsDays</em> conference, held in Belgium in 2009.</p>
			<p>It's important <a id="_idIndexMarker403"/>to note that DevOps began as a collaboration strategy in <strong class="bold">Agile systems administration</strong>. The goal was to overcome conflicts between Agile-based software development teams, who could now deliver new software products and features with increased frequency (that is, increased <em class="italic">velocity</em>), and the traditionally risk-averse system administration organizations.</p>
			<p>CI capabilities, which we'll <a id="_idIndexMarker404"/>introduce later in this chapter (see <em class="italic">Defining CI</em>), allowed developers to increase the velocity of application delivery to the IT operations' function. However, there was very little in the way of integrated processes or cultures of collaboration to facilitate the frequent releases of new software products into an organization's test and production environments.</p>
			<p>In a traditional IT organization, development and operations are two separate functions. They have different goals and objectives. They also have different mindsets. Software developers thrive in a world of change, delivering new features and capabilities continuously. That's a good thing because customers and users want new features that add value, and the sooner the better.</p>
			<p>On the other hand, systems administrators don't care for change because they are responsible for ensuring all networks, systems, and applications are running, stable, and secure. In short, changes can break their systems, infrastructures, and security. And their hesitancy is a good thing, as we need our networks and software to work and be secure.</p>
			<p>Think of the difference in cultures like this: developers thrive in making changes, and they are rewarded when they release new functionality that supports the needs of customers and users. In contrast, any changes in operations are scary – as those changes can break their deployed networks, systems, and applications. Even worse than this is that the systems administrators will receive the blame when the systems go down, and they alone feel all of the pressure until the systems are back up and running.</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor128"/>Sharing accountability</h2>
			<p>First and foremost, DevOps is a communications and collaboration strategy. In this context, DevOps' objective is to get the two teams working together in a coordinated manner, with information <a id="_idIndexMarker405"/>flowing both ways. Developers need to know why their new releases fail in production. In contrast, an operations team needs detailed information regarding installation configurations, system administration and support information, and whether the product releases have been thoroughly tested via systems testing, security testing, performance testing, load testing, and stress testing. However, such collaborations are nearly impossible when the development and operations teams remain separated by responsibilities, desired outcomes, and their differing measurements of success.</p>
			<p>Development teams frequently want to put out new releases that they've built and tested and believe are ready for release. But since the operations team traditionally works apart from the development team, the operations group will be reluctant to put out a new release until they have confirmed the product won't fail or cause other issues with systems configurations, performance, or security in their production environments.</p>
			<p>While the development team implements the new changes, the operations department is held accountable for ensuring everything works properly in deployment. For example, the operations <a id="_idIndexMarker406"/>team might conduct tests, such as <strong class="bold">user acceptance testing (UAT)</strong>, that should have been completed earlier in development. If the software fails, the operations team has to go back to development with bug and defect lists, and then try to make them a development priority.</p>
			<p>Other tests, such as <strong class="bold">performance testing</strong>, <strong class="bold">load testing</strong>, <strong class="bold">stress testing</strong>, and <strong class="bold">systems testing</strong>, require the duplication of the organization's production environments. If the <a id="_idIndexMarker407"/>development team cannot fully replicate the <a id="_idIndexMarker408"/>production environments in their testing environments, they might fail <a id="_idIndexMarker409"/>to discover potential problems with scaling the applications. That means <a id="_idIndexMarker410"/>performance and integration issues might not be uncovered until after the application has been moved into production. Even then, it could take some time before a set of events triggers a failure. As a result, it might look to be a failure of the operations team when, in fact, it was a failure to thoroughly test the system in a production-like environment.</p>
			<p>Ideally, the development or operations team will build a preproduction or staging environment to mimic its production environments. When that's not possible, the operations team could be forced to run a limited set of tests directly on their production environments, and by doing so, hope for the best but be prepared for the worst.</p>
			<p>All of the operation team's testing takes time to plan and execute. In addition to this, those tests won't occur until after the development team has already moved on to develop new features and functions during their next set of sprints. Any bugs or defects discovered by the operations team have to flow back into the product backlog for reprioritization and scheduling, potentially delaying the release of new features that customers are expecting. Moreover, the responsibility and blame for any failed releases tend to shift to the operations department.</p>
			<p>This cultural logjam cannot be fixed if the development and operations teams remain separated in any fashion. Leveraging the Lean-Agile concepts you learned in <a href="B17087_02_Final_PD_epub.xhtml#_idTextAnchor062"><em class="italic">Chapter 2</em></a>, <em class="italic">Building on a Lean-Agile Foundation</em>, the organization needs to integrate, streamline, and orchestrate the flow of work and information across both the IT development and operations groups.</p>
			<p>Organizations <a id="_idIndexMarker411"/>must remove the barriers that keep these functions apart. For smaller organizations, eliminating communication barriers can be as simple as getting the two teams to communicate together well before each release. However, larger organizations might have multiple product lines, multiple software product development teams, and even different operations support teams. In those situations, communication, integration, and synchronization challenges grow exponentially. The only practical way to remove the wedges between development and operations in larger organizations is to integrate the two functions within product teams or product lines and have everyone on the product teams be equally responsible for each release's velocity and quality.</p>
			<p>The practical ramifications of such a strategy are that the activities across development and operations must be linked, streamlined, and synchronized to each product under development. In other words, development and operations need to interoperate as a single product team.</p>
			<p>Product team members share accountability for each new feature release from ideation to delivery. The product teams are also accountable for effectively supporting operations-oriented activities across the product's life.</p>
			<p>Another set of issues arises from the two IT organizations – development and operations – having different velocities. Traditionally, an operations team requires more time to manage the risks associated with deploying a new release into the organization's production environments. This mismatch in velocities creates a bottleneck that slows down the release of new features into the organization's production environments.</p>
			<p>In the next section, you will learn how modern <strong class="bold">CI</strong> and <strong class="bold">CD</strong> capabilities address these issues.</p>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor129"/>Improving flows with CI/CD pipelines</h1>
			<p>In <a href="B17087_06_Final_PD_epub.xhtml#_idTextAnchor165"><em class="italic">Chapter 6</em></a><em class="italic">, Launching the VSM Initiative (VSM Steps 1-3), </em>through to <a href="B17087_11_Final_PD_epub.xhtml#_idTextAnchor291"><em class="italic">Chapter 11</em></a>, <em class="italic">Identifying VSM Tool Types and Capabilities</em>, we will use the concepts you learn in this section as a use case to introduce how you can use an eight-step VSM methodology to improve work and information flow across a CI/CD pipeline. However, before we get to that use case, we need to have a basic understanding of the purpose of a CI/CD pipeline, its component activities, and the complexities of implementing a fully integrated and automated toolchain. Those are the topics that we'll cover in this subsection. </p>
			<p>CI/CD toolchains <a id="_idIndexMarker412"/>enable the pipeline flows of work items and information during the software development life cycle. Another helpful way to think about CI/CD pipelines is that they enable the implementation of Lean-oriented production concepts across IT value streams. Before we discuss the components of a CI/CD pipeline, let's review the purposes of its two constituent elements:</p>
			<ul>
				<li><strong class="bold">CI</strong>: This provides <a id="_idIndexMarker413"/>the infrastructure that allows several software developers or even different development teams to implement and test code changes to a software product under development.</li>
				<li><strong class="bold">CD</strong>: This enables <a id="_idIndexMarker414"/>the automated provisioning of development, test, and <a id="_idIndexMarker415"/>production environments as <strong class="bold">configurable items</strong>.</li>
			</ul>
			<p>There are three critical capabilities and related tools that support the implementation of CI/CD pipelines. These <a id="_idIndexMarker416"/>include the following:</p>
			<ul>
				<li><strong class="bold">Configuration management</strong> (<strong class="bold">CM</strong>)</li>
				<li>Task management/automation</li>
				<li>Containerization</li>
			</ul>
			<p>Let's take a look at these three enabling technologies and tools in more detail.</p>
			<h1 id="_idParaDest-107"><a id="_idTextAnchor130"/>Tooling to establish CI/CD pipelines</h1>
			<p>CM helps us to track and manage the proper versions of CI that make up each software release. A configurable item is a system component or associated information artifact that has been uniquely identified <a id="_idIndexMarker417"/>for version and change control and identification purposes. <strong class="bold">Source control management</strong> (<strong class="bold">SCM</strong>) tools help developers maintain version control of source code and other CIs.</p>
			<p>Git and GitHub <a id="_idIndexMarker418"/>are two of the better-known SCM tools. But there are other tools, such as <strong class="bold">Apache Subversion</strong> (<strong class="bold">SVN</strong>), Azure DevOps Server (formerly Team Foundation Server), Bazaar, Bitbucket Server, CVS, GitLab, Gerrit, Kallithea, Mercurial, Monotone, Perforce <a id="_idIndexMarker419"/>Helix Core, Rational ClearCase, and <strong class="bold">Revision Control System</strong> (<strong class="bold">RCS</strong>).</p>
			<p><strong class="bold">Task management</strong> tools facilitate <a id="_idIndexMarker420"/>the automation of CI/CD and DevOps workflows. In both CI/CD and DevOps platforms, the software industry refers to automated workflows as pipelines. Typically, a CI/CD workflow automates the pipeline activities of planning, designing, developing, testing, provisioning, and delivering software releases. In addition, task management supports tracking a work item's progress, monitoring and analyzing key metrics across the pipeline, and reporting the results.</p>
			<p>One of the better-known task <a id="_idIndexMarker421"/>management tools is Jenkins, which lauds its community as providing <a id="_idIndexMarker422"/>the industry-leading open source automation server. Jenkins is used to automate software build, test, and deployment processes in a CI/CD environment. Though free, Jenkins is considered by some to be outdated and cumbersome to use. There are alternatives to Jenkins, including AutoRABIT, Bamboo, Bitrise, Buddy, Buildkite, CircleCI, CruiseControl, FinalBuilder, GitLab CI, GoCD, Integrity, Strider, TeamCity, UrbanCode, and Werker.</p>
			<p><strong class="bold">Containerization</strong> is a mechanism <a id="_idIndexMarker423"/>that is used to package an application's code and its related configuration files, libraries, and other dependencies to run an application in its target hardware environments. Conceptually, containers implement a virtualization strategy to maximize the utilization of computing resources.</p>
			<p>Before virtualization, organizations had to dedicate servers to run specific applications, such as email, web-based applications, and backend business applications. Having dedicated application servers is extraordinarily inefficient and inflexible.</p>
			<p>Two of the better-known container technologies are <strong class="bold">Docker</strong> and <strong class="bold">Kubernetes</strong>, and they work together. Docker is a <a id="_idIndexMarker424"/>software tool that developers use to build and deploy containers, while <strong class="bold">Kubernetes</strong> (that is, <strong class="bold">k8s</strong> or <strong class="bold">Kube</strong>) orchestrates and manages <a id="_idIndexMarker425"/>multiple containers in clusters. Orchestration is necessary to schedule and automate the deployment, management, and scaling of containerized applications.</p>
			<p>As with SCM and task management, there are alternative tools available in the industry for both Docker <a id="_idIndexMarker426"/>and Kubernetes. Docker alternatives include Canonical (Ubuntu), <strong class="bold">Linux Containers</strong> (<strong class="bold">LXD</strong>), CoreOS rkt, <strong class="bold">Open Container Initiative</strong> (<strong class="bold">OCI</strong>), LXC Linux <a id="_idIndexMarker427"/>Containers, Mesos Containerizer, and OpenVZ. The <a id="_idIndexMarker428"/>Kubernetes alternatives <a id="_idIndexMarker429"/>include <strong class="bold">Amazon ECS</strong> (<strong class="bold">Elastic Container Service</strong>), AWS Fargate, AZK, <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>), Cloudify, Containership, <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>), OpenShift, Marathon, Minikube, Nomad, and Rancher.</p>
			<p>If you think <a id="_idIndexMarker430"/>the number of tools available to support these three technologies is daunting, just wait until we examine the larger tool options that are available to support the entire DevOps toolchain. These tools are a small subset of the DevOps pipeline tools that are available as commercial and open source offerings. Later, in <a href="B17087_11_Final_PD_epub.xhtml#_idTextAnchor291"><em class="italic">Chapter 11</em></a>, <em class="italic">Identifying VSM Tool Types and Capabilities</em>, you will learn that there are 17 categories of tools and more than 400 offerings available to support the full scope of a DevOps <strong class="bold">value stream delivery platform</strong> (<strong class="bold">VSDP</strong>).</p>
			<p>We'll revisit <a id="_idIndexMarker431"/>all three of these technologies later in this chapter. However, before we get into the details of containerization, you need to <a id="_idIndexMarker432"/>understand the basic concepts behind virtualization.</p>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor131"/>Understanding virtualization</h1>
			<p>IT organizations, especially larger ones, need to maximize their computing resources' flexibility, utilization, and scalability. These goals are difficult, if not impossible, to achieve without virtualization. Virtualization is an approach taken by IT organizations to simplify their operations and respond faster to changing business demands.</p>
			<p>Virtualization offers a practical approach to distribute an application across any number of computing devices. For example, in many cases, one computing device is not sufficient to run a business application due to the high demand loads. In a related example, application demand loads can vary over time across the organization's applications. Virtualization offers an approach to reapportion loads across servers as demands vary, enabling high availability for demand-critical applications while also streamlining the process to deploy and migrate applications.</p>
			<p>Additionally, modern data centers employ servers deployed in racks to maximize their computing resource utilization. Virtualization makes it possible to coordinate the use of rack-mounted servers to utilize those resources maximally. These rack-based server strategies reduce power consumption for the computing devices and air conditioning needs; additionally, they reduce the land and facility space requirements of the data centers.</p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor132"/>Virtualizing data center resources</h2>
			<p>Virtualization <a id="_idIndexMarker433"/>creates a logical (virtual) computing environment that sits on top of a physical computing environment. Each virtualized environment mimics the hardware, <strong class="bold">operating systems</strong> (<strong class="bold">OSes</strong>), storage devices, and other system <a id="_idIndexMarker434"/>and security components that are necessary to run a specific software application. </p>
			<p>Virtualization allows IT organizations to partition a single physical computer or racks of servers into <strong class="bold">virtual machines</strong> (<strong class="bold">VMs</strong>). Each VM operates independently and can run different OSes or applications while sharing a single host machine's resources. </p>
			<p>The primary <a id="_idIndexMarker435"/>benefit of virtualization is that each physical computing system can manage multiple virtual environments, thereby maximizing its utilization. Moreover, the IT department can automate building and taking down virtual environments to match demand loads and business application needs, maximizing the IT organization's responsiveness and flexibility.</p>
			<p>Virtualization concepts employ a specific semantic description to distinguish between physical versus virtualized environments, as hosts versus guest machines. Host machines are the physical machines used for virtualization, and guest machines are the VMs. The host versus guest machine terminology makes it easier to distinguish the OS that runs on the physical machine from the OSes that run on its VMs.</p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor133"/>Employing hypervisor software for virtualization</h2>
			<p><em class="italic">Figure 5.1</em> shows a traditional application server architecture on the left and a virtualized server on the right. The diagram clearly shows that the traditional model requires a separate computer as a server for each application requirement. In contrast, the virtualized host machine shares its resources among all the virtualized guest machines and their applications:</p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/B17087_Figure_5.1.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – Traditional (left) versus virtualized (right) servers</p>
			<p>In the original virtualization model, <strong class="bold">hypervisor</strong> software (that is, <em class="italic">virtual machine monitor</em>, <em class="italic">VMM</em>, or <em class="italic">virtualizer</em>) is installed <a id="_idIndexMarker436"/>on a host machine to enable multiple VMs to operate as guest machines on one physical server. The hypervisor <a id="_idIndexMarker437"/>software is a lightweight OS that serves as an abstraction layer separating the applications and their required OS from the server's OS.</p>
			<p>Hypervisors work in two types of operating modes:</p>
			<ul>
				<li><strong class="bold">Bare-metal hypervisors</strong> run multiple <a id="_idIndexMarker438"/>OSes on <a id="_idIndexMarker439"/>top of one hardware server.</li>
				<li><strong class="bold">Hosted hypervisors</strong> are <a id="_idIndexMarker440"/>installed on top of the <a id="_idIndexMarker441"/>hardware's standard OS but isolate the virtualized applications' OSes.</li>
			</ul>
			<p><em class="italic">Figure 5.2</em> depicts the two standard hypervisor software implementation models, that is, bare-metal and hosted:</p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/B17087_Figure_5.2.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 – Hypervisor software implementation models</p>
			<p>The benefits <a id="_idIndexMarker442"/>of using hypervisor software for virtualization <a id="_idIndexMarker443"/>include the following:</p>
			<ul>
				<li>It offers greater speed, efficiency, and flexibility in provisioning VMs instead of installing one or more physical servers for every software application.</li>
				<li>It allows multiple OSes to reside on the same host machine. Therefore, software applications do not need to be rewritten to run on the host machine's OS.</li>
				<li>All virtualized applications share the same virtual computing, storage, and memory resources, thereby reducing computing equipment needs, computer room space, energy costs, and equipment maintenance.</li>
				<li>It improves disaster recovery by making it simpler and faster to create and recover snapshot images.</li>
				<li>It simplifies the process of creating test environments as VMs.</li>
			</ul>
			<p>To put the virtualization's power into perspective, it's not unusual to have thousands of servers dedicated to supporting just one critical business application in large enterprises. Indeed, the largest commercial data centers might have more than a million servers deployed on tens of thousands of racks running any number of applications for any number of customers.</p>
			<p>These data centers offer a network of remote servers hosted on the internet as a cloud-based service. Over time, the data centers could employ different technologies. The virtualization of servers is critical to the efficient storage, management, and processing of data for external customers regardless of the underlying physical environments.</p>
			<p>However, as it turns out, hypervisor-based virtualization is not a perfect solution. Since the hypervisor <a id="_idIndexMarker444"/>software emulates virtual hardware, the hypervisor must include all the guest machines' application OS and system functionality, making them relatively inefficient.</p>
			<p>In the following subsection, you will learn how containers resolve these issues by sharing a lightweight OS.</p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor134"/>Using containers for virtualization</h1>
			<p>Both containers and hypervisors make applications faster, more portable, and more efficient to deploy. However, they achieve those objectives differently. You've already learned that hypervisor software implements a light OS over a host machine's environment. In contrast, a container's OS is smaller and more efficient than hypervisor software. Containers package an application and its dependencies and run them as an OS process on the host machine. </p>
			<p>A container package can run anywhere a container engine is installed. For example, please refer to <em class="italic">Figure 5.2</em> for a graphical depiction of the container-based architecture, and then compare it with the hypervisor virtualization architectures shown in the following figure:</p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/B17087_Figure_5.3.jpg" alt="Figure 5.3 – Container-based virtualization&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – Container-based virtualization</p>
			<p>At first glance, the container-based virtualization model looks relatively similar to the hosted <a id="_idIndexMarker445"/>hypervisor model. They both provide an abstraction layer between the host OS and the applications. However, VMs with hypervisors isolate the hardware and its OS to run the virtualized applications' full OSes. In contrast, the container engines provide an abstraction layer on top of the hardware's OSes in order to run the applications directly with their OS of preference without utilizing the OS installed on the VMs.</p>
			<p>A commercial OS, such as Linux, Windows, or macOS, needs to provide numerous common services to support computer applications that are running on the hardware where the OS is installed. However, most of those services are not required by any single application. Therefore, containers do not contain a complete OS – only the bare-bones elements that are necessary to run the application it supports.</p>
			<p>The container-based virtualization approach is much more lightweight and flexible than hypervisor-based VMs. For example, where a VM might take up tens of gigabytes of space, a container might only require tens of megabytes. Additionally, containers tend to be more secure since each container's OS is self-contained, thereby offering fewer entry points for malicious actors (for instance, through malware or intrusion attacks).</p>
			<p>Metaphorically, containers serve a transport function that mimics the shipping containers used to move physical products by ships, trains, and trucks (for instance, one container type from the origin to the final destination). In the software-based analogy, developers build products and deploy them to their target host environments via their containers. But in our software-based variant, the containers transport the application together with just the resources it needs to execute during runtime as virtualized guests on the targeted host's physical environment. </p>
			<p>These resources include the code, runtime, system libraries, system tools, and configuration settings. The containers are constructed as <em class="italic">images</em> that are separate from their runtime environment. Thus, they can be deployed anywhere – just as long as the target environment has a container engine installed, such as Docker Engine.</p>
			<p>A modern approach <a id="_idIndexMarker446"/>to software development, especially in DevOps-oriented pipelines, defines and creates very small pieces of code as independent services, called microservices. The microservices-based development strategy allows the rapid coding, testing, and deployment of new functionality into production – often, this can be multiple times per day. Conceptually, the microservices approach mimics the concept of single-piece flow concepts in Lean production practices. </p>
			<p>You will learn about the value of implementing single-piece flows in <a href="B17087_07_Final_PD_epub.xhtml#_idTextAnchor183"><em class="italic">Chapter 7</em></a>, <em class="italic">Mapping the Current State (VSM Step 4)</em>, and <a href="B17087_08_Final_PD_epub.xhtml#_idTextAnchor209"><em class="italic">Chapter 8</em></a>, <em class="italic">Identifying Lean Metrics (VSM Step 5)</em>, of this book. For now, it's essential to understand that single-piece flows represent the most efficient Lean development processes. </p>
			<p>Container engines perform two critical services: <strong class="bold">clustering</strong> and <strong class="bold">orchestration</strong>. Clustering connects <a id="_idIndexMarker447"/>two or more servers as a single virtualized computer. The clustering of servers allows them to operate in parallel, and the container engines manage load balancing and fault tolerance activities across the cluster of servers.</p>
			<p>Container orchestration <a id="_idIndexMarker448"/>automates the deployment, management, scaling, and networking of containers. Orchestration is critical when scheduling between hundreds or thousands of individual containers consisting of microservices that operate across multiple clusters.</p>
			<p>When using software containers, software developers do not need to worry about deployments across multiple types of production environments nor the virtualization of hardware resources; the containers have everything they need to run the application on a desktop, an organization's backend servers, or computing environments available in the cloud.</p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor135"/>Having it both ways</h2>
			<p>Still, an IT organization could deploy both hypervisor and container-based virtualization strategies to maximize their flexibility when deploying and managing applications. Both hypervisors and container engines can live on the same physical server.</p>
			<p>Containers do well in cloud-based environments and when developers want to build fine-grained <a id="_idIndexMarker449"/>services, called microservices. IT shops without many legacy applications might prefer to go <a id="_idIndexMarker450"/>down this route right from the beginning as microservices offer the greatest speed and flexibility to build, test, and deploy new IT services.</p>
			<p>On the other hand, VMs provide all the management capabilities and security tools available within <a id="_idIndexMarker451"/>a mature OS. VMs provide a <strong class="bold">hardware abstraction layer</strong> (<strong class="bold">HAL</strong>) that eliminates software application compatibility issues with the underlying hardware. VMs effectively utilize memory capacities and multiple cores in CPUs that allow the consolidation of numerous applications and tasks across each physical system. In fact, VMs are optimal for running applications requiring persistent and high-transaction volume workloads. For example, applications with large transactional databases – think of bank ATMs that require resilient and <a id="_idIndexMarker452"/>persistent backends – cannot lose data, and have high <strong class="bold">input/output</strong> (<strong class="bold">I/O</strong>) transaction requirements. Finally, some third-party applications have not and might not adopt the container model. </p>
			<p>Now that you understand the necessary tooling required to support CI/CD, we can introduce the types of activities included in CI and CD processes. We'll start by defining the activities of CI.</p>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor136"/>Defining CI</h1>
			<p>Fundamentally, CI is a development <a id="_idIndexMarker453"/>approach that speeds up the velocity of software development. CI enforces a discipline that merges all developers' working copies of their code to a shared repository several times a day on a technical level. The purpose of this is to verify each incremental code integration's functionality through software build and test processes when developing the code. The goal is to ensure the main software code is always in a working and potentially deployable state.</p>
			<p>A mature CI pipeline includes automated build and automated test capabilities; though, these two capabilities were not part of the original definition. Today, CI workflows encompass the process of taking each new code <em class="italic">commit</em> from the main branch (that is, the mainline code, trunk, or master) and running the appropriate steps to verify that commit.</p>
			<p>A basic CI pipeline spans the following software development activities, as shown in <em class="italic">Figure 5.4</em>:</p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/B17087_Figure_5.4.jpg" alt="Figure 5.4 – CI pipeline flow&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4 – CI pipeline flow</p>
			<p>The preceding diagram is a high-level view of a more complicated process. As an example of the <a id="_idIndexMarker454"/>complexities, the following tasks are usually marshaled via a CI automation server, such as Jenkins:</p>
			<ul>
				<li>Move the source code to the version control system.</li>
				<li>Manage the version control system's push, pull, and merge functions.</li>
				<li>Execute the software build process (for example, compiling the source code, linking the object files and libraries, and packaging the libraries and tools).</li>
				<li>Execute static code analysis.</li>
				<li>Run automated unit tests.</li>
				<li>Execute code coverage analysis.</li>
				<li>Provision test servers.</li>
				<li>Set up test fixtures (for example, code that sets up the test environment and then returns it to its original state once the test is complete).</li>
				<li>Run automated tests.</li>
				<li>Publish logs and reports.</li>
				<li>Send information to the developers.</li>
			</ul>
			<p>The CI process appears to be a lot of work and a highly complex process – and it is when implemented as a manual process. However, as an automated CI process, the complete feedback loop should run for less than 10–20 minutes. The goal is to make the process so quick and straightforward that developers won't hesitate to initiate the process several times per day.</p>
			<p>The CI strategy addresses <a id="_idIndexMarker455"/>two fundamental problems. The first is to make sure every new piece of code implements its functionality correctly, according to its requirements and acceptance criteria, before making the change part of the main code. The second is to ensure the newly integrated code doesn't cause problems or bugs in the application's mainline code.</p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor137"/>Encouraging frequent testing</h2>
			<p>It should be pretty clear that, fundamentally, CI is a process used to develop and test small increments <a id="_idIndexMarker456"/>of new software functionality rapidly and frequently. This strategy supports the first and seventh principles of the <em class="italic">Manifesto for Agile Software Development</em> (Beck et al., 2001).</p>
			<p class="callout-heading">The Agile Manifesto principles 1 and 2</p>
			<p class="callout">1) Our highest priority is to satisfy the customer through early and continuous delivery of valuable software.</p>
			<p class="callout">2) Working software is the primary measure of progress.</p>
			<p class="callout">Please refer to <a href="https://agilemanifesto.org/principles.html">https://agilemanifesto.org/principles.html</a> for more details.</p>
			<p>While the first principle is valuable, it turns out the seventh principle is often more important, at least in terms of the benefits of CI. Let's take a moment to understand why.</p>
			<p>In the traditional waterfall model, software developers create all of the code that is necessary to implement all identified requirements before initiating any testing. A significant problem with that development strategy is that the source of the software bugs becomes increasingly challenging to locate and resolve in larger code sets. A much better strategy is to test small increments, or units, of new code every step of the way. The advantage of this is that the developers know more precisely what they changed in the code when an error or bug shows up in testing.</p>
			<p>Additionally, frequent code updates can help identify code merge conflicts, diverging code strategies, and duplication attempts. In other words, with CI and automated testing, developers are forced to address these issues as they arise and not wait until they become exceedingly complex, time-consuming, and expensive to fix.</p>
			<p>This section ends our discussion on CI. Next, we will learn how CD capabilities both augment and improve the velocity of the CI phase of software delivery.</p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor138"/>Defining CD</h1>
			<p>CD capabilities allow product teams to stand up new environments to test new code updates with minimal manual labor and rapidly. CD's primary goal is to turn new updates into routine and high-velocity tasks that a development team can perform on-demand.</p>
			<p>Just as CI has a sequential sequence of steps, so does the CD process, as depicted in <em class="italic">Figure 5.5</em>:</p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B17087_Figure_5.5.jpg" alt="Figure 5.5 – CD pipeline flow&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.5 – CD pipeline flow</p>
			<p>The CD pipeline view depicted in the preceding diagram provides a high-level view, and similarly to the CI pipeline, it can be decomposed into a lengthier list of related activities. These activities might include the following:</p>
			<ul>
				<li>Conducting static code analysis</li>
				<li>Conducting unit tests</li>
				<li>Conducting API tests</li>
				<li>Staging to test environment(s)</li>
				<li>Parallel testing (for instance, useability/accessibility, exploratory, UI, and performance testing)</li>
				<li>Staging to preproduction environment</li>
				<li>Executing application tests (for example, acceptance, exploratory, capacity, load, and stress testing)</li>
				<li>Executing software and network security tests</li>
				<li>Conducting UAT</li>
				<li>Deploying the application to production environments</li>
			</ul>
			<p>Again, this list is not meant to be an exhaustive summary of all the possible tests that a team might need to execute.</p>
			<p>One final comment is <a id="_idIndexMarker457"/>that there is no definitive place where CI as a process ends and CD begins. For example, some analysts put the merge code process – a source code integration activity – in the CI pipeline, while others view it as a CD pipeline activity. In reality, CI/CD pipelines represent a continuous flow across the entire <strong class="bold">systems development life cycle</strong> (<strong class="bold">SDLC</strong>). There's no reason to make the distinction <a id="_idIndexMarker458"/>except to communicate the type of work that occurs in each section of the pipeline.</p>
			<p>Once the development team determines which tests to run and which tools they require, they can automate the tests' execution by writing configuration instructions in machine-readable code.</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor139"/>Automating configuration tasks via code</h2>
			<p>Before the advent of CD methods and tools, development teams had to ask the operations <a id="_idIndexMarker459"/>staff to set up testing and preproduction staging environments. Then, the operations staff manually followed the instructions in the configuration documentation to set up networks, computing equipment, and software. Such manual processes are expensive and time-consuming.</p>
			<p>In a modern CD environment, developers can deploy software and systems configuration instructions as machine-readable code. Moreover, these configurations can be managed in a source control repository and made available for rapid deployment as a self-service offering. The infrastructure and software resources are provisioned on demand in a cloud environment and are made available within minutes after executing the machine-readable code.</p>
			<p>The term used in the software industry to describe automated deployment configurations is Infrastructure as Code (IaC). However, you might have also come across the term Configuration as Code (CaC), which individual IT practitioners and vendors use to connote the general implementation of configurations as source code. We'll discuss the semantic differences between IaC and CaC later in this chapter.</p>
			<p>For now, it's essential to understand that IaC and CaC implement configuration instructions <a id="_idIndexMarker460"/>as machine-readable code to stand up and configure environments and software on demand. Before we get into the details of IaC and CaC, first, let's understand why CM is so important and why some configuration items cannot be deployed as code.</p>
			<p>Both <a id="_idIndexMarker461"/>infrastructure and software configurations broadly fall under the discipline of <strong class="bold">software configuration management</strong>, as described in the following subsection.</p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor140"/>Protecting our software assets</h2>
			<p>Complex software releases involve deploying and configuring a lot of hardware, network and application security, software components, and other related information artifacts. CM ensures that we have a complete understanding of the state and artifacts that make up each unique software release. Without that information, it's challenging, if not impossible, to go back to fix bugs and defects, sustain the product, or enhance previous software releases.</p>
			<p>As the software evolves, each new release has a unique configuration. With each release, some of the components will have changed, while others will not. Although all the components might start with the same version control number, those version control IDs will vary across the software components and other information-oriented artifacts over time. Therefore, assigning a version control ID to the release is not enough. We need to know the specific versions of each of the information artifacts and software components that make up the release.</p>
			<p>Modern version control repositories, such as Git (a local repo) and GitHub (a web-based collaboration SCM platform and repository), use a tree structure as a metaphor for managing the configuration items associated with each software release.</p>
			<p>Getting back to the tree metaphor applied to SCM repositories, developers create (and evolve), integrate, and test components as separate <em class="italic">branches</em> of an application before merging them with the main <em class="italic">trunk</em> of code. The main code is the closest code set available for release at any given time. That's not to say the main code is releasable, but it is the furthest along in terms of integrated capabilities and testing.</p>
			<p>However, besides <a id="_idIndexMarker462"/>tracking our source code, we also need to track all the other information artifacts that go with each release, which is the purpose of SCM.</p>
			<p>Information and software artifacts under SCM encompass the entire SDLC processes, including the following:</p>
			<ul>
				<li><strong class="bold">Software requirements</strong>: This includes functional and nonfunctional requirements <a id="_idIndexMarker463"/>specified in specifications, use cases, epics, and user stories.</li>
				<li><strong class="bold">Environment</strong>: This <a id="_idIndexMarker464"/>includes network switches, firewalls, routers, servers, OSes, cybersecurity systems, databases, and other critical infrastructure elements.</li>
				<li><strong class="bold">Software builds</strong>: This includes instructions to compile, link, and otherwise convert <a id="_idIndexMarker465"/>source code files into a standalone software artifact(s) so they can run on a computer.</li>
				<li><strong class="bold">Software release plan</strong>: This provides instructions on how to release the product <a id="_idIndexMarker466"/>into production, its timelines, delivery <a id="_idIndexMarker467"/>dates, and production-oriented testing requirements such as <strong class="bold">UAT</strong>, QA testing, preproduction testing (for instance, stress, load, and performance), and field testing, as applicable.</li>
				<li><strong class="bold">Software reviews</strong>: This <a id="_idIndexMarker468"/>could include peer <a id="_idIndexMarker469"/>reviews, <strong class="bold">software quality assurance</strong> (<strong class="bold">SQA</strong>) reviews, or <strong class="bold">independent verification and validation</strong> (<strong class="bold">IV&amp;V</strong>) testing <a id="_idIndexMarker470"/>by third parties.</li>
				<li><strong class="bold">Version control</strong>: This <a id="_idIndexMarker471"/>includes information on the current and past versions of all software system components and related artifacts, which are usually managed within a source control repository.</li>
				<li><strong class="bold">Configuration items</strong>: This includes all the software components and artifacts, as identified <a id="_idIndexMarker472"/>by their independent names and version control IDs and belonging to a specific release.</li>
				<li><strong class="bold">Testing information</strong>: This includes test cases, test scripts, test scenarios, and test results <a id="_idIndexMarker473"/>associated with each software release.</li>
				<li><strong class="bold">User support</strong>: The support team's information provides user assistance guidance <a id="_idIndexMarker474"/>and helps troubleshoot user problems during the product's implementation and use.</li>
				<li><strong class="bold">Documentation</strong>: This <a id="_idIndexMarker475"/>can include training aids, systems administration documentation, and architecture and design documentation.</li>
				<li><strong class="bold">Issue tracking</strong>: This is <a id="_idIndexMarker476"/>used to document information related to bugs and defects. A bug results from a coding error, while a defect is an identified deviation from the requirements.</li>
				<li><strong class="bold">Task management</strong>: This is <a id="_idIndexMarker477"/>used to maintain information on activities across the development and delivery life cycle.</li>
			</ul>
			<p>Most of the <a id="_idIndexMarker478"/>preceding information-based components that go with a release are not part of the application's source code. Therefore, the product team must implement processes and systems that will record and maintain this information for each product release. We need to manage all this information by release to ensure operability and sustainability in the production environments, fix bugs or defects upon discovery, and enhance the product over its life cycle.</p>
			<p>Now that <a id="_idIndexMarker479"/>we understand the more significant software configuration issues, let's examine the differences between managing and executing CaC and IaC. We'll start with a description of CaC.</p>
			<h3>CaC</h3>
			<p>CaC is a broad term that implies implementing configuration files in a source code repository, bu<a id="_idTextAnchor141"/>t the <a id="_idIndexMarker480"/>term is more generally applied to application configuration information. The purpose of CaC is to facilitate the versioned migration of application configurations between different environments.</p>
			<p>CaC configurations are implemented as machine-readable code via configuration files that focus on the settings and parameters that are required to install software applications, servers, and OSes. Developers specify configuration settings in CaC with parameters that can be changed to affect the target information system's remote hardware, software, or firmware components. These configuration settings and parameters affect the security levels and functionality of the system.</p>
			<p>A virtually unending list of potential configurations must be implemented and maintained across IT products, especially for those in which security-related configuration settings can be defined. The US <strong class="bold">National Institute of Standards and Technology</strong> (<strong class="bold">NIST</strong>) <em class="italic">Special Publication 800-53 (Rev. 4)</em> documents the following examples of configurable items:</p>
			<p>Mainframe computers, servers (for example, database, email, authentication, web, proxy, file, domain name), workstations, I/O devices (for example, scanners, copiers, and printers), network components (for example, firewalls, routers, gateways, voice and data switches, wireless access points, network appliances, sensors), OSes, middleware, and applications (<a href="https://nvd.nist.gov/800-53/Rev4/control/CM-6">https://nvd.nist.gov/800-53/Rev4/control/CM-6</a>).</p>
			<p>The NIST publication also lists standard security-related parameters, as noted in the passage that follows:</p>
			<p class="author-quote">Security-related parameters impact the security state of information systems, including the parameters required to satisfy other security control requirements.</p>
			<p class="author-quote">  (i) registry settings,</p>
			<p class="author-quote">  (ii) account, file, directory permission settings,</p>
			<p class="author-quote">  (iii) settings for functions, ports, protocols, services, and remote connections.</p>
			<p>Established settings become part of the system's configuration baseline parameters. In a manual process, the software developers create security configuration checklists, lockdown <a id="_idIndexMarker481"/>and hardening guides, security reference guides, and security technical implementation guides. The operations staff follow these guides to configure the systems and applications properly. The value of CaC is that it automates and streamlines the process of establishing the configuration settings and parameters.</p>
			<p>In contrast with CaC, IaC is about configuring IT infrastructures, including servers, networks, load balancing, and security, as described in the following subsection.</p>
			<h3>IaC</h3>
			<p>As its name implies, IaC allows developers to use a programming or scripting language to generate <a id="_idIndexMarker482"/>a repeatable set of code or scripted instructions to provision IT infrastructures. With IaC capabilities, developers do not need to manually provision or change the configurations of infrastructure components, such as servers, OSes, database connections, storage, networks, VMs, load balancers, and network topologies.</p>
			<p>Without IaC capabilities, the developers must manually set up and configure new system environments every time they want to develop, test, or deploy a software application. From the perspective of Lean, and therefore our customers, these activities are unnecessarily repetitive and non-value-added. That's not to say they are not necessary. However, automating such processes improve cycle times and eliminate waste in the form of human, or other errors.</p>
			<p>The issue of human error is especially concerning as the accumulation of configuration errors leads to <strong class="bold">environmental drift</strong> where each new environment configuration becomes uniquely different to previous configurations. Developers call the new configurations <em class="italic">snowflakes</em> because they share a similar characteristic in that each is unique from the others.</p>
			<h4>Managing environmental drift</h4>
			<p>The problem with environmental drift is that each new change in the system's configuration can affect previously deployed assets. Recall that the function of operations <a id="_idIndexMarker483"/>is to maintain stable, secure, and available environments and applications. However, let's suppose a developer fails to communicate a new configuration change to the infrastructure or applications. In that case, those modifications cause the production environments to fail or expose them to security-related risks.</p>
			<p>The same situation is valid for engineering and test environments. Changes in configurations make it increasingly difficult to isolate and fix errors that result from the configuration changes, as opposed to bugs or defects in the code. Again, each new configuration change can introduce waste in the form of defects that are time-consuming and expensive to resolve.</p>
			<p>There are innumerable issues that contribute to environmental drift. However, most often, these configuration changes result from improper documentation, communication, or the implementation of new or modified parameters when setting up servers, configuring networks, or other computing resources.</p>
			<p>The preceding examples show human error as the root cause. Computers are better at executing rote instructions with accurately described configurations defined via code. But those types of errors can be discovered during the testing process, thereby avoiding any negative consequences and limiting any risks when new releases move into production. </p>
			<h4>Avoiding configuration errors</h4>
			<p>As with the CaC concepts, development teams employ IaC write scripts or code to describe <a id="_idIndexMarker484"/>the configuration settings and parameters. Each configuration file represents a single and authoritative source of defining and setting up an environment or updating an environment over time.</p>
			<p>The code or scripts are saved as standalone configuration files and checked into the development team's version control and SCM systems. The advantage of managing IaC files in an SCM system is that the executable routines become freely available to all developers and operations staff via a self-service model.</p>
			<p>In other words, both CaC and IaC help to improve the velocity of software deliveries while simultaneously reducing errors.</p>
			<h4>Improving velocities while reducing errors</h4>
			<p>With CaC and IaC, IT staff can download and execute configuration files on demand and set up a new environment in minutes without introducing manual errors. The self-service model <a id="_idIndexMarker485"/>means that developers don't need to involve the operations team to stand up and provision new testing environments. Moreover, the operations team can ensure the code and configurations for new releases are appropriately tested before releasing new updates to the production environments.</p>
			<p>IaC enables high velocities for CD to match the same types of velocities developers have available via their CI tools. With a CI/CD pipeline, developers can change both the code and configurations on the fly and stand up test environments to quickly determine that everything is working correctly. Moreover, high-performing shops can deploy new functionality multiple times per day with lead times of less than 1 hour (Forsgren et al., Accelerate, 2018).</p>
			<p>IaC is the critical enabler for specifying CI/CD and DevOps pipeline configurations and flows. Manual configuration processes are simply too slow and too inefficient. As you will discover in the chapters on VSM, CI/CD and DevOps pipelines implement Lean production concepts across all IT value streams when they are adequately implemented.</p>
			<p>However, before we get to those chapters, we need to understand how CI/CD and DevOps pipelines support work and information flow across all IT value streams.</p>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor142"/>Enabling CI/CD and DevOps pipeline flows</h1>
			<p>This book makes a clear distinction between CI/CD and DevOps toolchains and pipelines:</p>
			<ul>
				<li>A toolchain is a combination of tools that collectively perform a specific set of IT tasks or functions. This term may or may not imply an integration or automation strategy and is somewhat vague.</li>
				<li>DevOps pipelines and CI/CD pipelines include a series of integrated tools to streamline and automate IT tasks or functions across an IT value stream. Pipelines are more equivalent to the Lean and VSM concepts presented later in this book. For now, let's understand CI/CD and DevOps pipelines as improving the speed and reliability of software value delivery.</li>
			</ul>
			<p>The term <strong class="bold">toolchain</strong> specifies a <a id="_idIndexMarker486"/>scope of tools that support IT value stream activities. Again, by itself, the term toolchain does not necessarily imply an integration or automation strategy. Although not ideal, developers can manually set up the following tool in line with the previous tools' outputs.</p>
			<p>A better strategy is to improve efficiencies by integrating and automating the toolchains to coordinate and <a id="_idIndexMarker487"/>streamline work and information flows. In this context, the word <strong class="bold">pipeline</strong> connotes a flow. In the case of Lean-oriented production philosophies, we want streamlined and efficient flows of both work and information across our IT value streams.</p>
			<p>CI/CD and DevOps <a id="_idIndexMarker488"/>toolchains are integrated and automated to support the efficient and streamlined flow of work and information across all IT value streams. CI/CD and DevOps toolchains are called <em class="italic">pipelines</em> when the tools are integrated and automated to support streamlined and efficient work and information flows.</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor143"/>Improving pipeline flows</h2>
			<p>The distinction <a id="_idIndexMarker489"/>between toolchains versus pipelines is an important one. For example, an Agile-based software development team can procure a set of tools <a id="_idIndexMarker490"/>that collectively make up a toolchain. However, when chartered under a project-based operating model, it is unlikely that the team has the time or budget to implement an integrated or automated toolchain.</p>
			<p>In such scenarios, Agile teams never achieve the same production efficiencies that are available to the product-oriented teams that implement a complete pipeline. Since the product team lives across its product's life cycle, they can justify and amortize the CI/CD and DevOps pipeline toolchain investments.</p>
			<p>However, there is a workaround for Agile teams. For example, the Agile team can access an integrated and automated toolchain as a cloud-based service via a commercial or internal DevOps platform provider. You will learn more about those options in <em class="italic">Section 3</em> of this book. For the remainder of this chapter, we will go beyond the CI/CD activities to look at the full scope of DevOps-related activities.</p>
			<h2 id="_idParaDest-120"><a id="_idTextAnchor144"/>Understanding the full scope of DevOps</h2>
			<p>CI/CD activities only take us as far as traditional SDLC processes, that is, from concept to deployment. But the IT organization must also maintain and support its software applications. For the remainder of this chapter, you will learn that DevOps goes beyond software development and delivery to ensure the proper life cycle support of deployed software products.</p>
			<p>While DevOps <a id="_idIndexMarker491"/>still includes the base CI/CD activities you've <a id="_idIndexMarker492"/>learned so far in this chapter, the overall scope of work in DevOps expands to encompass all software life cycle stages. These include the following:</p>
			<ul>
				<li>Build automation and CI</li>
				<li>Test automation</li>
				<li>CD and provisioning</li>
				<li>Deployment automation</li>
				<li>Operating, monitoring, supporting, and providing feedback</li>
				<li>Release coordination and automation</li>
			</ul>
			<p>Let's discuss what these activities are beyond the CI/CD phase of software development and delivery. In the following subsection, we will begin to define the boundaries between CI/CD pipelines and DevOps pipelines.</p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor145"/>Defining CI/CD and DevOps pipeline boundaries</h2>
			<p>As you learned at the beginning of this chapter, DevOps began as a collaboration strategy to <a id="_idIndexMarker493"/>enable Agile systems administration. The primary goal was to improve information flows between IT development <a id="_idIndexMarker494"/>and operations teams as a risk management strategy. However, DevOps necessarily evolved to address the issues related to mismatched velocities. In other words, the velocity of operations-oriented services needs to match the velocities of the Agile-based development teams.</p>
			<p>In traditional IT vernacular, we use the term SDLC to refer to IT value stream activities and tools <a id="_idIndexMarker495"/>implemented by the development teams. In contrast, the operations team uses the term <strong class="bold">ITSM</strong> to describe all the activities and supporting tools involved in designing, creating, delivering, supporting, and managing IT services-related activities.</p>
			<p>It should <a id="_idIndexMarker496"/>be no surprise that the term <strong class="bold">DevOps pipeline</strong> encompasses <a id="_idIndexMarker497"/>both SDLC and ITSM activities and tools, ultimately <a id="_idIndexMarker498"/>forming an integrated DevOps pipeline. In the following subsection, we'll take a look at how the CI/CD model is expanded to become a DevOps pipeline.</p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor146"/>Expanding the CI/CD model</h2>
			<p>The CI/CD model spans the activities customarily performed by a software development team spanning an iterative SDLC. DevOps expands the CI/CD pipeline concepts to include the IT operations <a id="_idIndexMarker499"/>team's activities. In other words, DevOps seeks to merge the activities of development and operations, ideally at the product team level.</p>
			<p>In an article titled <em class="italic">8 CI/CD best practices to set you up for success</em>, Taz Brown created the following diagram to show the larger complexities of implementing and supporting Lean value streams across the IT function. This diagram breaks the value stream into three distinct flows, that is, <strong class="bold">Software Development</strong>, <strong class="bold">User Support</strong>, and <strong class="bold">Incident Management</strong>: </p>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/B17087_Figure_5.6.jpg" alt="Figure 5.6 – IT value streams&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.6 – IT value streams</p>
			<p><a href="https://opensource.com/article/20/5/cicd-best-practices">https://opensource.com/article/20/5/cicd-best-practices (Taz Brown, CC BY-SA 4.0)</a></p>
			<p>This diagram simplifies the view of the activities needed to build, deploy, and support a product. While this model is incomplete from a DevOps perspective, it does highlight the separations between development and support-related activities.</p>
			<p>The first line of the diagram, which is within the boundary line, depicts a standard CI/CD pipeline set of activities. Note that there is a decision point within the boundary to decide whether the development and operations teams are ready to deploy the software into the organization's production environments.</p>
			<p>Given its presence <a id="_idIndexMarker500"/>outside the boundary line, the <strong class="bold">Deploy Product</strong> node suggests a manual decision and process. However, that doesn't have to be the case. With a mature CI/CD pipeline capability, the software release is automated.</p>
			<p>The development <a id="_idIndexMarker501"/>and operations teams might still prefer some manual review process before release. However, even that requirement becomes unnecessary when releasing tiny increments of new functionality with increased velocity, using automated testing capabilities, and perhaps automating UAT across a small segment of users before deploying the releases to the larger community of users.</p>
			<p>Looking at lines two and three in <em class="italic">Figure 5.6</em>, we move into the operations team's traditional IT support and incident management functions. These activities fall under the ITSM processes.</p>
			<p>However, this model <a id="_idIndexMarker502"/>is still missing the operations-oriented <strong class="bold">IT operations management</strong> (<strong class="bold">ITOM</strong>) processes. ITOM spans IT operations' control and facilities management but also overlaps with technical management and application management. A very mature DevOps pipeline integrates and automates these activities at the product team level.</p>
			<p>We will dive into ITOM and ITSM in the last part of this chapter. But before we get to those topics, let's examine how the mismatched velocities between development and operations became the driver for evolving DevOps strategies and, later, DevOps toolchains and pipelines. That is the topic of our following subsection.</p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor147"/>Resolving issues of mismatched velocities</h2>
			<p>As noted in the previous subsection, the speed of development often exceeds the ability of operations <a id="_idIndexMarker503"/>to manage the risks associated with effectively deploying new releases frequently. However, all of these issues can go away with CD and continuous deployment capabilities.</p>
			<p>Many Agile-based IT developers employ innovative practices to deploy small increments of new functionality frequently using CI methods and tools. CI capabilities automate the frontend SDLC development processes to perform automated code integrations, builds, and integration tests every time developers check their code into the source code repository.</p>
			<p>CD initially evolved to support automation testing needs, which sits at the boundaries between development and operations. Development teams should thoroughly test all new software releases before deployment, including systems, acceptance, load, stress, performance, and other critical tests. It takes time, computing resources, and human effort to manually set up the test environments to support these testing requirements.</p>
			<p>Put simply, manual testing processes cannot approach the velocities of CI. CD automates the activities that are required to read application and infrastructure configurations in code, provision testing servers, install and configure the applications, and then run all of the necessary tests.</p>
			<p>Continuous deployment takes the provisioning process one step further to automate the deployment process in the production environments. Moreover, continuous deployment can automate infrastructure resource provisioning in near real time to meet changing production demands.</p>
			<p>The operations function can match the Agile-based development teams' velocities using CI methods and tools with CD and deployment capabilities. With its emphasis on the rapid delivery of high-quality products and services and just-in-time deliveries, Lean production processes offer a way to integrate the IT DevOps pipeline activities within a single IT value stream. </p>
			<p>When you get to <em class="italic">Section 2</em> of this book, which is on <em class="italic">implementing VSM</em>, you will learn how to implement Lean production concepts across the IT value stream to create Lean pipeline flows. But before we move on to that part of the book, we need to look at the full scope of DevOps activities.</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor148"/>Scoping DevOps pipeline activities</h2>
			<p>As it turns out, a fully evolved DevOps pipeline encompasses quite a few integrated activities <a id="_idIndexMarker504"/>beyond CI/CD pipeline flows. In this section, we will explore the higher-level activities and how they operate as a continuous iterative and incremental development and support process.</p>
			<p>As you read this section, bear in mind what you learned in <a href="B17087_01_Final_PD_epub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Delivering Customer-Centric Value</em>, about organizations having two types of value streams: development and operations. As a reminder, operations-oriented value streams deliver products and services to an organization's external customers, while development value streams create things used by the organization's operations-oriented value streams.</p>
			<p>The DevOps acronym is confusing as the term implies operations and development are part of the same value stream, and they are. But the semantic meanings of development and operations in the DevOps acronym are different from Lean's contextual meanings. The DevOps paradigm includes the iterative SDLC activities associated with CI/CD pipelines, plus the operations activities include the product's ITSM activities.</p>
			<p><em class="italic">Figure 5.7</em> is a standard display for a DevOps pipeline. Although a DevOps pipeline can be displayed as a linear-sequential flow, the more common approach is to show it as an infinity loop. The infinity loop implies that iterative and incremental DevOps delivery activities operate as a continuous flow:</p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B17087_Figure_5.7.jpg" alt="Figure 5.7 – Infinite DevOps pipeline flows&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.7 – Infinite DevOps pipeline flows</p>
			<p>Modern Agile and Lean-Agile practices both implement iterative development cycles that deliver frequent increments of customer value. DevOps simply expands the iterative and incremental development model to encompass the IT service's management activities.</p>
			<p>This DevOps <a id="_idIndexMarker505"/>model is overly simplistic, as its focus is only on conveying high-level pipeline processes. Just as we discovered with the CI/CD pipeline activities, the DevOps pipeline's ITSM portion includes many more activities than portrayed in the DevOps infinity loop diagram.</p>
			<p>You already know how to implement CI/CD activities as pipelines into the DevOps pipeline model. In the next section, we will take a look at the activities associated with ITSM and their flows.</p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor149"/>Integrating ITSM</h1>
			<p>In <a href="B17087_01_Final_PD_epub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Delivering Customer-Centric Value</em>, you learned that organizational value streams support development or operations and are often linked. You also learned that IT-based development-oriented value streams often create software products that <a id="_idIndexMarker506"/>support the operations-oriented value streams.</p>
			<p>For example, an insurance company's internal software development team might create web-based services to support the company's insurance products' promotion, sales, and delivery. Similarly, health care software providers have development teams supporting multiple value streams. These can include patient registrations, claims management, financial management, accounting, diagnostic and billing codes, patient health data, appointment scheduling, compliance, and reporting.</p>
			<p>However, as you already know, IT value streams go beyond the implementation of software development and delivery activities. Besides these capabilities, the IT organization or software product team must install ITOM capabilities and ITSM processes and platforms.</p>
			<p>ITSM focuses on how IT teams deliver services. In contrast, ITOM focuses on the activities and tools used for event management, performance monitoring, and the operations processes depicted in the <strong class="bold">OPS</strong> portion of the DevOps pipeline (please refer to <em class="italic">Figure 5.7</em>). Ideally, the IT organization installs ITOM and ITSM activities at the product team level as part of their DevOps pipeline flows.</p>
			<p>Conveniently, ITIL 4 already add<a id="_idTextAnchor150"/>resses ITOM and ITSM from a <strong class="bold">service value s</strong><strong class="bold">ystem</strong> (<strong class="bold">SVS</strong>) perspective. If the <a id="_idIndexMarker507"/>organization has implemented <a id="_idIndexMarker508"/>ITIL 4 practices or equivalent, the VSM team needs to assess the operations-oriented work within the DevOps pipeline.</p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor151"/>Delivering service value</h2>
			<p>In the previously displayed <em class="italic">Figure 5.7</em>, the DevOps pipeline's <strong class="bold">OPS</strong> portion includes <strong class="bold">Release</strong>, <strong class="bold">Deploy</strong>, <strong class="bold">Operate</strong>, and <strong class="bold">Monitor</strong> as its primary activities. Two of these activities, <strong class="bold">Release</strong> and <strong class="bold">Deploy</strong>, are transition activities that require support from both the development and operations side of the product teams. </p>
			<p>Ho<a id="_idTextAnchor152"/>wever, in this section, you will learn that these 4 operations-oriented processes decompose <a id="_idIndexMarker509"/>into at least 34 separate ITSM domains spanning 3 management practices. The word domain implies a specified area of activity or knowledge. In the context of ITSM, you can presume the term domain includes specific areas of knowledge and related sets of activities.</p>
			<p>The term service management broadly describes the practices and activities designed to improve a company's customer service processes. Service management includes activities spanning strategy, design, development, integration, operation, and service improvements. ITSM then includes the practices and activities to support customers using software and related infrastructure and security components produced or acquired by an IT organization<a id="_idTextAnchor153"/>.</p>
			<p>There are <a id="_idIndexMarker510"/>multiple ITSM frameworks that a product team can choose to use, such as ISO/IEC 20000-1, ITIL 4®, COBIT 5, FitSM, <strong class="bold">Microsoft Operations Framework</strong> (<strong class="bold">MOF</strong>), The Open Group IT4IT Reference Architecture, VeriSM™, SIAM®, and YaSM®. Given its leadership position, this chapter evaluates <a id="_idIndexMarker511"/>how ITIL 4® defines its <strong class="bold">service value chain</strong> as part of<a id="_idTextAnchor154"/> its best practices to deliver ITSM in the context of DevOps, Agile, and Lean approaches.</p>
			<p>ITIL 4® defines a <em class="italic">service value chain</em> as a set of "joined-up practices, activities, and actions used across the ITSM value stream." In other words, the ITIL 4® service value chain represents a flow. Of course, we know an ITSM value stream is only a component of the larger IT value stream work and information flows that are encompassed within a DevOps pipeline.</p>
			<p>Before we <a id="_idIndexMarker512"/>get into the service value chain activities and flows, first, let's quickly take a look at the four dimensions of ITSM that help deliver value.</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor155"/>Encompassing the four dimensions of ITSM </h2>
			<p>ITIL 4 describes <a id="_idIndexMarker513"/>four dimensions of service management as the foundations of an ITSM provider's capabilities. From a systems-thinking perspective, these four dimensions are elements that participate in value-based ITSM deliveries. They include the following:</p>
			<ul>
				<li><strong class="bold">Organizations and People</strong>: This is to build the right organizational structures and competencies.</li>
				<li><strong class="bold">Information and Technology</strong>: This is to build IT systems and infrastructures with the right technologies to support service deliveries.</li>
				<li><strong class="bold">Partners and Suppliers</strong>: This is to implement third-party service delivery contracts that are financially and technically appropriate.</li>
				<li><strong class="bold">Value Streams and Processes</strong>: This is to develop efficient and customer-centric service value delivery capabilities.</li>
			</ul>
			<p>All four dimensions of service management help the product team deliver service value. Software product teams must coordinate their service value chain responses to involve all four dimensions. If not, the service delivery function does not operate optimally and will fail to deliver value to its customers and product users.</p>
			<p>As a final note, external factors such as the following can impact a service delivery's four dimensions: economic, environmental, legal, political, social, and technological. All of these factors must be considered when deciding how to deploy the four dimensions of service management.</p>
			<p>Now that we understand the foundational elements of service management, let's explore the activity flows associated with ITSM.</p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor156"/>Defining ITSM delivery flows</h2>
			<p>As with any value stream, the ITIL 4® service value chain represents an activity flow; although it <a id="_idIndexMarker514"/>is described at a high level, as we'll see in the following subsection. The service value chain includes six primary activities to respond to the IT service demands from a value delivery perspective:</p>
			<ol>
				<li><strong class="bold">Plan</strong>: This is to define an as-is/to-do assessment of the service capabilities, requirements, and policies to develop a common vision of what services are required and how they will be delivered.<p>Value delivered: New service identification and provisioning plan.</p></li>
				<li><strong class="bold">Improve</strong>: This is to ensure the c<a id="_idTextAnchor157"/>ontinual improvement of all products, services, and practices across all four dimensions of service management.<p>Value delivered: Attainment of service-level targets.</p></li>
				<li><strong class="bold">Engage</strong>: This is to confirm our understanding of stakeholder needs, plus ensure timely engagement and positive outcomes with our stakeholders.<p>Value delivered: Managing and resolving a user's complaint.</p></li>
				<li><strong class="bold">Design and transition</strong>: This is to ensure that new releases of products and related services repeatedly meet stakeholder expectations for quality, cost, and time to market.<p>Value delivered: Enable upgrades to the next version of a business application.</p></li>
				<li><strong class="bold">Obtain/build</strong>: This is to ensure that service components are available when and where they are needed and that they meet agreed specifications.<p>Value delivered: Timely and accurate fulfillment of a user request.</p></li>
				<li><strong class="bold">Deliver and support</strong>: This is to ensure services are delivered and supported according to the agreed specifications or service-level agreement while meeting stakeholder expectations.<p>Value delivered: Successful resolution of all incident reports.</p></li>
			</ol>
			<p>These six activities express the general flow of work to define, create, and deliver customer-centric services. So, just as software development has a flow that defined ideation <a id="_idIndexMarker515"/>through delivery, ITSM provides a value stream flow to define and deliver IT services. In DevOps, we need to integrate the two flows, as shown in <em class="italic">Figure 5.7</em>. Now, let's define the total potential scope of work involved in delivering value-based ITSM.</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor158"/>Delivering ITSM value</h2>
			<p>Earlier in this section, we mentioned, though not explicitly attributed to ITIL 4®, the service value <a id="_idIndexMarker516"/>chain document's 34 separate ITSM domains spanning 3 management practices. Again, the term domains refers to both knowledge areas and related sets of activities. The scope of this book limits our abilities to dive deeper into describing each domain. However, ITIL 4® provides detailed guidance on planning, managing, and improving these management practices and domain-related activities. </p>
			<p>The three groups of management practices include the following:</p>
			<ul>
				<li><strong class="bold">General Management Practices</strong>: This group spans 14 service management domains <a id="_idIndexMarker517"/>from general <a id="_idIndexMarker518"/>business management that helps support work or accomplishing specific objectives. The domains include the following:</li>
			</ul>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/B17087_05_Table_01.jpg" alt=""/>
				</div>
			</div>
			<ul>
				<li><strong class="bold">Service Management Practices</strong>: This group spans 17 domains to ensure that <a id="_idIndexMarker519"/>services deliver agreed <a id="_idIndexMarker520"/>availability levels to meet customers' and users' needs. The domains include the following:<div id="_idContainer048" class="IMG---Figure"><img src="image/B17087_05_Table_02.jpg" alt=""/></div></li>
			</ul>
			<ul>
				<li><strong class="bold">Technical Management Practices</strong>: This <a id="_idIndexMarker521"/>group spans three domains to implement <a id="_idIndexMarker522"/>service management practices that expand or shift the focus from technology solutions to IT services. The domains include the following: </li>
			</ul>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B17087_05_Table_03.jpg" alt=""/>
				</div>
			</div>
			<p>It should be apparent that ITSM brings in a much broader host of practices and activities to implement, improve, and support in a DevOps-oriented value stream. However, it's not necessary to go into the details of ITSM in this book. This chapter's primary goal is to introduce the scope of work involved in building and streamlining DevOps as orchestrated pipeline flows.</p>
			<p>We are approaching the end of this chapter. By now, you should appreciate the complex challenges that lie ahead in developing a streamlined DevOps production flow. There are investments required in toolchains and innumerable activities to implement, integrate, automate, and orchestrate.</p>
			<p>This book <a id="_idIndexMarker523"/>does not attempt to solve your specific <a id="_idIndexMarker524"/>CI/CD and DevOps pipeline flow issues but instead gives you the tools to do so. Specifically, <em class="italic">Section 2</em> of this book introduces an eight-step VSM methodology, modern VSM tools, and their capabilities. In this context, VSM encompasses the methods and tools you can use to improve Lean production flows across your CI/CD and DevOps pipelines.</p>
			<p>Before we finish this chapter and this part of the book, there is one other topic we need to address, which is moving from a project-oriented development paradigm to a product-oriented development strategy.</p>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor159"/>Moving beyond projects and into products</h1>
			<p>The traditional <a id="_idIndexMarker525"/>waterfall model for software development is project-based. In the industry's early days, the project-oriented approach <a id="_idIndexMarker526"/>seemed to make sense due to the high costs, complexities, and risks involved in software development.</p>
			<p>Let's review the type of work that is best suited to traditional project management practices. For <a id="_idIndexMarker527"/>example, the characteristics of project-based work include the following:</p>
			<ul>
				<li>Projects have definable <em class="italic">deliverables</em> or outputs in the form of <em class="italic">products</em>, <em class="italic">services</em>, or <em class="italic">results</em>.</li>
				<li>Project-based deliverables are relatively unique, and, therefore, the work has significant risks.</li>
				<li>Project constraints are defined in project charters, approved by customers or executive sponsors, with specific boundaries on authorized <em class="italic">scope</em>, <em class="italic">schedule</em>, <em class="italic">costs</em>, and <em class="italic">quality</em>.</li>
				<li>Project-oriented work is highly tailored to support each product's unique requirements, and, therefore, the work is relatively non-repetitive from one project to another.</li>
				<li>Given that software product requirements are relatively unique, the work's full details and scope might only become apparent as the project progresses (this is also true for Agile-based work).</li>
				<li>Project <a id="_idIndexMarker528"/>teams employ formal change management practices to minimize scope creep, budget shortfalls, and schedule overruns.</li>
				<li>Schedules help enforce the temporary nature of projects with defined start and delivery dates and predefined activities, dependencies, and durations.</li>
				<li>Project-based work often crosses organizational boundaries, thereby involving multiple skills.</li>
			</ul>
			<p>Managing highly customized work under a dictated set of constraints seems like an odd dichotomy – and it is. That said, we should understand why customers place constraints on project-related work. Specifically, our paying customers establish project constraints to ensure the projected ROI is achievable in the timeline and at a cost at which the investments make economic sense.</p>
			<p>Nevertheless, developing software under project-based constraints creates a host of problems, three of which are critical. First, given each software product's unique nature, the development <a id="_idIndexMarker529"/>team cannot foresee all of the issues <a id="_idIndexMarker530"/>they might encounter. Second, customers and users often don't know what they want or need until they have a version of the software product in hand to evaluate. Third, customer needs evolve, and their priorities change over time.</p>
			<p>The bottom line is that no matter how much time and effort the project team applies to project planning, it will be outdated before it can be executed.</p>
			<p>Modern software methods and tools have evolved to support the unique needs of software development, such as being responsive to customer demands and changes in priorities. That type of responsiveness was not possible under the traditional waterfall project management model. With fully developed CI/CD and DevOps pipelines, the most mature software development teams can iteratively, incrementally, and rapidly deliver new functionality, perhaps multiple times per day. Therefore, the CI/CD and DevOps pipelines have the functional equivalence of modern manufacturing facilities.</p>
			<p>In <em class="italic">Section </em><em class="italic"><a id="_idIndexMarker531"/></em><em class="italic">2 </em>of this book, we'll explore how VSM helps <a id="_idIndexMarker532"/>improve Lean production flows across DevOps pipelines. But before we get to that, first, let's take a moment to understand why a product-based development and delivery model is superior to the traditional project-based waterfall model in software development.</p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor160"/>Funding product teams</h2>
			<p>There is no need to constrain software development activities to a specific scope, schedule, cost, or quality metric. Instead, just as a manufacturing plant operates for as long as they <a id="_idIndexMarker533"/>have new customers' orders, the modern software factory continues to operate for as long as their customers have evolving product needs.</p>
			<p>Physical products tend to wear out, forcing customers to replace them. In contrast, software products do not physically wear out. On the other hand, the requirements that drove the initial software development objectives tend to have a shelf life. In that context, customers eventually need to replace or update their software applications.</p>
			<p>For these reasons, it makes sense to move beyond the project model to implement a product-oriented development model. In a product-oriented development model, product teams replace project teams, and the teams stay together for as long as customers use the product.</p>
			<p>The composition of the teams could change over time to support evolving requirements. At the beginning of the product's life cycle, development has an outsized role in efforts and costs. Toward the end of life, the development resources could dwindle, and the resource emphasis moves toward operations-oriented support.</p>
			<p>A product-based funding model is different than a project-based funding model. Project-based funding builds on projected future returns on investments. The risk of project-based funding is twofold. First, there is a question of whether the product can be built within the authorized constraints. Second, there is a question of whether the market will exist in the future to support the investment.</p>
			<p>Product-based funding is less risky as it turns the project-based model upside down. Instead of asking whether the product will eventually pay back the investment, product-based funding models evaluate current costs and revenues to assess how much money to invest in development and operations support.</p>
			<p>There is still risk in the initial development cost investments. However, those risks move to the portfolio level, where corporate executives determine what investments they need to make <a id="_idIndexMarker534"/>to best position the company for future business. Those investments can develop new products or investments as enhancements to existing products to attract customers in new market niches. Portfolio-level investments are strategic, while ongoing adjustments to product team budgets are tactical decisions based on actual costs compared to actual revenues.</p>
			<p>This section completes our final chapter of <em class="italic">Section 1 </em>of this book. We'll close with a summary section and a set of 10 questions that will help you to analyze your comprehension of this chapter's content.</p>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor161"/>Summary</h1>
			<p>This chapter introduced the complexities of implementing CI/CD and DevOps pipeline flows. The information is a precursor to <em class="italic">Section 3 </em>of this book, where you learn how to employ the methods and tools of VSM to implement and improve Lean production flows across your IT value streams.</p>
			<p>Specifically, in this chapter, you learned the complexities of implementing mature CI/CD and DevOps pipelines. You learned that virtualization, primarily through container-based technologies, is crucial in order to support the efficient use of IT infrastructure resources and enable the rapid delivery of small increments of new software capabilities. Finally, you learned that CI/CD pipelines integrate and automate the traditional SDLC processes, but DevOps extends the CI/CD pipeline to include service management functions.</p>
			<p>With this knowledge, you are now well prepared to understand how to use VSM methods and tools to implement and improve DevOps activities as Lean production-oriented flows. VSM methods and tools form the next part of this book's subject – <em class="italic">Section 2</em>, <em class="italic">Implementing Value Stream Management (VSM) Methods and Tools</em> – to improve IT value streams.</p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor162"/>Questions</h1>
			<ol>
				<li value="1">What drove the development of DevOps concepts and later its methods and tools?</li>
				<li>What are the three critical capabilities and related tools that support the implementation of CI/CD pipelines?</li>
				<li>What is CI, and what is its purpose?</li>
				<li>What are the significant cultural differences between a software development team and an IT operations team?</li>
				<li>What is CD, and what is its goal?</li>
				<li>What is the difference between IaC and CaC?</li>
				<li>What is the primary difference when using the term "toolchains" compared to "pipelines"?</li>
				<li>Using two terms only, what is the best way to describe the IT value streams that make up a DevOps pipeline?</li>
				<li>How can you differentiate ITOM from ITSM?</li>
				<li>How are project-oriented teams funded differently from product-oriented teams?</li>
			</ol>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor163"/>Further reading</h1>
			<ul>
				<li>Dietrich, E. (June 2019) <em class="italic">DevOps Table Stakes: The Minimum Amount Required to Play the Game</em>. DZone/DevOps Zone. <a href="https://dzone.com/articles/devops-table-stakes-the-minimum-amount-required-to">https://dzone.com/articles/devops-table-stakes-the-minimum-amount-required-to</a>. Accessed 2nd February 2021.</li>
				<li><strong class="bold">US National Institute of Standards and Technology</strong> (<strong class="bold">NIST</strong>) Information Technology Laboratory. NATIONAL VULNERABILITY DATABASENIST. Special Publication 800-53 (Rev. 4). <em class="italic">Security and Privacy Controls for Federal Information Systems and Organizations</em>. <a href="https://nvd.nist.gov/800-53/Rev4/control/CM-6">https://nvd.nist.gov/800-53/Rev4/control/CM-6</a>. Accessed 2nd February 2021.</li>
				<li>Forsgren, N., Humble, J., Kim, G. (2018) <em class="italic">Accelerate. Building and Scaling High-Performance Technology Organizations</em>. IT Revolution. Portland, OR.</li>
			</ul>
		</div>
	</body></html>