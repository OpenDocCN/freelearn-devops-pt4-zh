<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer163">
			<h1 id="_idParaDest-214"><em class="italic"><a id="_idTextAnchor402"/>Chapter 13</em>: <a id="_idTextAnchor403"/>Managing Logs Using Datadog</h1>
			<p>The logs generated by the operating system, the various platform components, and the application services contain a lot of information regarding the state of the infrastructure as well as the workings of the applications running on it. Managing all logs at a central repository and analyzing that for operational insights and monitoring purposes is an important area in monitoring. It usually involves the collection, aggregation, and indexing of logs. In <a href="B16483_01_Final_VK_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to Monitoring</em>, this monitoring type was briefly discussed. In <a href="B16483_12_Final_VK_ePub.xhtml#_idTextAnchor385"><em class="italic">Chapter 12</em></a>, <em class="italic">Monitoring Containers</em>, you learned how logs from containers are published to Datadog for aggregation and indexing for facilitating searches. </p>
			<p>Some of the popular monitoring product offerings in this area are ELK Stack (<strong class="bold">Elasticsearch</strong>, <strong class="bold">Logstash</strong>, and <strong class="bold">Kibana</strong>), Splunk, and Sumo Logic. Now, Datadog also provides this feature and you have seen <strong class="bold">Log Explorer</strong>, a frontend to that feature, in the last chapter.</p>
			<p>In this chapter, we will explore Datadog's log aggregation and indexing feature in detail. Specifically, we will look at the following topics:</p>
			<ul>
				<li>Collecting logs</li>
				<li>Processing logs</li>
				<li>Archiving logs</li>
				<li>Searching logs</li>
			</ul>
			<h1 id="_idParaDest-215"><a id="_idTextAnchor404"/>Technical requirements</h1>
			<p>To try out the examples mentioned in this book, you need to have the following tools installed and resources available:</p>
			<ul>
				<li>A Datadog account with admin-level access</li>
				<li>A Datadog Agent running at host level or as a microservice depending on the example, pointing to the Datadog account</li>
			</ul>
			<h1 id="_idParaDest-216"><a id="_idTextAnchor405"/>Collecting logs</h1>
			<p>The first step in any <a id="_idIndexMarker744"/>log management application is to collect the logs in a common storage repository for analyzing them later and archiving them for the records. That effort involves shipping the log files from machines and services where they are available to the common storage repository. </p>
			<p>The following diagram provides the workflow of collecting and processing the logs and rendering the aggregated information to end users. The aggregated information could be published as metrics, which could be used for setting up monitors. That is the same as using metrics to set up monitors in a conventional monitoring application: </p>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="Images/Figure_13.1_B16483.jpg" alt="Figure 13.1 – Log management workflow&#13;&#10;" width="1650" height="626"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.1 – Log management workflow</p>
			<p>In a modern production infrastructure, the logs could be generated by a variety of sources, and typical sources include the following:</p>
			<ul>
				<li><strong class="bold">Public cloud services</strong>: Public cloud services <a id="_idIndexMarker745"/>such as AWS S3 and RDS are very popular, especially if the production infrastructure is built <a id="_idIndexMarker746"/>predominantly using public cloud services. The logs from those services can be shipped into Datadog using the related Datadog integrations available.</li>
				<li><strong class="bold">Microservices</strong>: The Datadog integrations available for <a id="_idIndexMarker747"/>Docker and Kubernetes can be <a id="_idIndexMarker748"/>used to ship these logs into Datadog. We have already seen examples of doing that in the last chapter.</li>
				<li><strong class="bold">Hosts</strong>: Traditionally, logs are <a id="_idIndexMarker749"/>available as files on a disk location on the host, bare-metal or virtual. A Datadog Agent <a id="_idIndexMarker750"/>can be configured to <a id="_idIndexMarker751"/>ship local log files to the Datadog backend. </li>
			</ul>
			<p>Now, let's see the details of how logs are collected and shipped by Datadog, in the most common use cases that are summarized <a id="_idTextAnchor406"/>above.</p>
			<h2 id="_idParaDest-217"><a id="_idTextAnchor407"/>Collecting logs from public cloud services</h2>
			<p>Datadog provides integrations and methods for <a id="_idIndexMarker752"/>collecting logs from <a id="_idIndexMarker753"/>services offered on major public cloud platforms such as AWS, Azure, and GCP. The cloud platform-specific methods available are as follows:</p>
			<ul>
				<li> <strong class="bold">Cloudformation</strong> and <strong class="bold">Kinesis Firehose</strong>-based options are <a id="_idIndexMarker754"/>available to roll <a id="_idIndexMarker755"/>out the automation to collect logs from AWS services. </li>
				<li>On Azure, the automation to <a id="_idIndexMarker756"/>collect logs is <a id="_idIndexMarker757"/>based on <strong class="bold">Azure Event Hub</strong> and <strong class="bold">Azure Function</strong>. </li>
				<li>Datadog integration is available on GCP to collect logs from the services offered on th<a id="_idTextAnchor408"/>at platform.</li>
			</ul>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor409"/>Shipping logs from containers</h2>
			<p>When containers are <a id="_idIndexMarker758"/>run on Docker, outside of a Kubernetes cluster, the logs from containers can be shipped to Datadog by configuring a Datadog Agent and the <a id="_idIndexMarker759"/>related Docker images. The main requirement is to deploy a Datadog Agent as a container on the same host. You have learned how that is done in the last chapter. The second part is to instrument the Docker image with a container label for the logs to be auto-discovered by a Datadog Agent. For example, the following label in the NGINX <strong class="source-inline">Dockerfile</strong> helps a Datadog Agent to collect logs from the NGINX container spun up from that image, and running as <strong class="source-inline">webapp</strong>:</p>
			<p class="source-code">LABEL "com.datadoghq.ad.logs"='[{"source": "nginx", "service": "webapp"}]'</p>
			<p>Instrumenting a Docker image might not always be possible as some images could be supplied by third parties, or making such changes might not be viable operationally. In such scenarios, the environment variable <strong class="source-inline">DD_LOGS_CONFIG_CONTAINER_COLLECT_ALL</strong> could be specified in the Datadog Agent runtime environment to collect logs from all the containers running on that host. If there are any logs to be excluded from aggregation, Datadog has options to filter those logs out before shipping those for processing, and we will look at that later in this section.</p>
			<p>The configuration required in a Kubernetes cluster to ship logs running in that environment to the Datadog backend is similar to how that is done on a Docker host:</p>
			<ul>
				<li>The Datadog Agent must be run as a container in the cluster. </li>
				<li>The application containers must be <a id="_idIndexMarker760"/>annotated for <strong class="bold">autodiscovery</strong>, or the environment variable <strong class="source-inline">DD_LOGS_CONFIG_CONTAINER_COLLECT_ALL</strong> must be set to <strong class="source-inline">true</strong> for shipping logs from all the containers running in the cluster. For example, to enable the NGINX container for autodiscovery, the following annotation must be included in the deployment description in Kubernetes:<p class="source-code"><strong class="bold">template:</strong></p><p class="source-code"><strong class="bold">    metadata:</strong></p><p class="source-code"><strong class="bold">      annotations:</strong></p><p class="source-code"><strong class="bold">        ad.datadoghq.com/nginx.logs: '[{"source":"nginx","service":"webapp"}]'</strong></p></li>
			</ul>
			<p>Next, let's <a id="_idIndexMarker761"/>look at what <a id="_idIndexMarker762"/>configuration is needed for collecting logs when the Datadog Agent is run a<a id="_idTextAnchor410"/>t the host level.</p>
			<h2 id="_idParaDest-219"><a id="_idTextAnchor411"/>Shipping logs from hosts</h2>
			<p>The logs from a host, where the <a id="_idIndexMarker763"/>Datadog Agent runs, could be shipped using the option available with Datadog integration for third-party applications. If no <a id="_idIndexMarker764"/>integration is available, the logs generated by an application could be shipped by following the custom method, as explained in this section:</p>
			<p>In the first case, in which the Datadog integration is available for the application, such as NGINX, the existing configuration file can be used to specify the log collection requirements as specified in the following example:</p>
			<p class="source-code"><strong class="bold"># in the config file conf.d/nginx.d/conf.yaml add the following configuration.</strong></p>
			<p class="source-code"><strong class="bold">logs:</strong></p>
			<p class="source-code"><strong class="bold">  - type: file</strong></p>
			<p class="source-code"><strong class="bold">    service: webapp</strong></p>
			<p class="source-code"><strong class="bold">    path: /var/log/nginx/access.log</strong></p>
			<p class="source-code"><strong class="bold">    source: nginx</strong></p>
			<p class="source-code"><strong class="bold">  - type: file</strong></p>
			<p class="source-code"><strong class="bold">    service: webapp</strong></p>
			<p class="source-code"><strong class="bold">    path: /var/log/nginx/error.log</strong></p>
			<p class="source-code"><strong class="bold">    source: nginx</strong></p>
			<p>The log files <strong class="source-inline">/var/log/nginx/access.log</strong> and <strong class="source-inline">/var/log/nginx/error.log</strong> are configured to be collected in the preceding sample case scenario. If the logs have to be collected directly from a service running on a port, the log type will be <strong class="source-inline">tcp</strong> or <strong class="source-inline">udp</strong>, and <strong class="source-inline">port</strong> must be specified in place of <strong class="source-inline">path</strong>.</p>
			<p>In the Datadog Agent configuration, the option to ship logs is not enabled by default and this has to be done in the <strong class="source-inline">datadog.yaml</strong> file also: <strong class="source-inline">logs_enabled: true</strong>.</p>
			<p>If no integration exists for an <a id="_idIndexMarker765"/>application, a custom configuration file needs to be set up for shipping logs generated by that application. The following are the steps for doing that:</p>
			<ul>
				<li>Under the <strong class="source-inline">conf.d</strong> directory, create a sub-directory with an appropriate name on the basis of this syntax – <strong class="source-inline">&lt;CUSTOM_APP&gt;.d/conf.d</strong>.</li>
				<li>In the new directory, create a <a id="_idIndexMarker766"/>configuration file, <strong class="source-inline">conf.yaml</strong>, and add the following entry for each log to be collected:<p class="source-code">logs:</p><p class="source-code">  - type: file</p><p class="source-code">    path: "&lt;/path/to/logfile&gt;"</p><p class="source-code">    service: "&lt;CUSTOM_APP&gt;"</p><p class="source-code">    source: "&lt;SOURCE_NAME&gt;"</p></li>
			</ul>
			<p>The Datadog Agent must be restarted in order for these configuration ch<a id="_idTextAnchor412"/>anges to take effect.</p>
			<h2 id="_idParaDest-220"><a id="_idTextAnchor413"/>Filtering logs</h2>
			<p>As you have seen already, it's pretty easy to <a id="_idIndexMarker767"/>configure the Datadog Agent or integration to collect logs from any environment. However, shipping all the information tracked in the logs to Datadog might not be a good idea for a variety of reasons, including the following:</p>
			<ul>
				<li>Security concerns</li>
				<li>Restrictions imposed by agreements with customers</li>
				<li>Regulatory requirements </li>
				<li>Compliance controls in place</li>
			</ul>
			<p>Therefore, it may be necessary to filter out information from the logs that are not useful for monitoring and that are not allowed to be shared with a wider audience. Also, such filtering can result in minimizing storage utilization and data transfer to the Datadog backend.</p>
			<p>Two predefined rules, <strong class="source-inline">include_at_match</strong> and <strong class="source-inline">exclude_at_match</strong>, can be used to filter out logs at the collection phase. These rules work with a regular expression – if a log entry is matched with the regular expression used with the rule, that log is included or excluded, depending on the type of rule. </p>
			<p>In the following example, log entries starting with the string <strong class="source-inline">k8s-log</strong> are ignored:</p>
			<p class="source-code">logs:</p>
			<p class="source-code">  - type: file</p>
			<p class="source-code">    path: "&lt;/PATH/TO/LOGFILE&gt;"</p>
			<p class="source-code">    service: "webapp"</p>
			<p class="source-code">    source: "nginx"</p>
			<p class="source-code">    log_processing_rules:</p>
			<p class="source-code">       - type: exclude_at_match</p>
			<p class="source-code">         name: exclude_k8s_nginx_log</p>
			<p class="source-code">         pattern: ^k8s-log</p>
			<p>Multiple <strong class="source-inline">include_at_match</strong> rules are <a id="_idIndexMarker768"/>allowed for the same log file and that would result in an <strong class="source-inline">AND</strong> condition. This means that both the rules must be satisfied in order for a log entry to be collected. </p>
			<p>To implement an <strong class="source-inline">OR</strong> rule, the conditions must be specified in the same expression using the <strong class="source-inline">|</strong> symbol, as in the following example:</p>
			<p class="source-code">pattern:  ^warning | ^err</p>
			<p>This will result in any line beginning with <strong class="source-inline">warning</strong> or <strong class="source-inline">err</strong> being collected by the Datadog Agent.</p>
			<p>As you have seen earlier, the filtering rules can be implemented in the Docker runtime using labels, and in Kuberne<a id="_idTextAnchor414"/>tes using annotations. </p>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor415"/>Scrubbing sensitive data from logs</h2>
			<p>A common issue with <a id="_idIndexMarker769"/>aggregating logs and presenting this to a wider audience is that sensitive information that might be recorded in the logs could become <a id="_idIndexMarker770"/>visible inadvertently, and this constitutes a major security and privacy problem. Restricting access to the monitoring application that aggregates logs, such as Datadog's <strong class="bold">Log Management interface</strong>, can impair its <a id="_idIndexMarker771"/>general usefulness. The better solution is to scrub sensitive information off the logs at the source before they are shipped to the Datadog backend for processing. </p>
			<p>While the filtering option that you have seen earlier can help with not collecting an entire log entry containing sensitive information, it might leave out an important piece of detail from the log for analysis or monitoring. So, redacting such information is always better as that would still leave sufficient operational details in the log.</p>
			<p>The rule type to use for scrubbing information is <strong class="source-inline">mask_sequences</strong>. It works with the <strong class="source-inline">replace_placeholder</strong> option, which determines how the sensitive information is replaced with a placeholder to indicate that information is masked in the log entry. </p>
			<p>The following example explains how these two options are used in tandem to achieve the desired result in redacting, which is highly effective:</p>
			<p class="source-code"> log_processing_rules:</p>
			<p class="source-code">      - type: mask_sequences</p>
			<p class="source-code">        pattern: "^(User:.*), SSN:(.*), (.*)$"</p>
			<p class="source-code">        replace_placeholder: "${1}, SSN:[redacted], ${3}"</p>
			<p>The preceding sample rule will replace the field containing SSN information in a log entry with the string <strong class="source-inline">SSN:[redacted]</strong>. This is done by splitting the log entry into three parts and reassembling it using the format mentioned in <strong class="source-inline">replace_placeholder</strong>. The content matched within each <strong class="source-inline">()</strong> construct in <strong class="source-inline">pattern</strong> would be available as a numbered variable in <strong class="source-inline">replace_placeholder</strong>. </p>
			<p>As mentioned earlier, these rules have to be added to the <strong class="source-inline">datadog.yaml</strong> file at the host level and similar rules can be implemented in Docker runtime using labels, and in a Kubernetes cluster using annotations. </p>
			<p>You have learned how <a id="_idIndexMarker772"/>Datadog collects logs from different environments and stores that centrally for processing to derive operational insights in the form of <a id="_idIndexMarker773"/>metrics and to generate search indexes. In the next section, we will see how the logs are processed by Datadog and discuss the resourc<a id="_idTextAnchor416"/>es involved in that process.</p>
			<h1 id="_idParaDest-222"><a id="_idTextAnchor417"/>Processing logs</h1>
			<p>Once the logs are <a id="_idIndexMarker774"/>collected by Datadog, they will be available on <strong class="bold">Log Explorer</strong> to view and search. The logs in the structured JSON format are processed by Datadog automatically. The unstructured logs can be processed further and analytical insights extracted. Datadog <a id="_idIndexMarker775"/>uses <strong class="bold">Pipelines</strong> and <strong class="bold">Processors</strong> to process the <a id="_idIndexMarker776"/>incoming logs.</p>
			<p>To view the pipelines available, open the <strong class="bold">Logs</strong> | <strong class="bold">Configuration</strong> menu option from the main menu. The pipelines are listed under the <strong class="bold">Pipelines</strong> tab, as shown in the following sample screenshot:</p>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="Images/Figure_13.2_B16483.jpg" alt="Figure 13.2 – Pipelines for log processing&#13;&#10;" width="1650" height="835"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.2 – Pipelines for log processing</p>
			<p>A pipeline can be fed with a subset of logs and it uses a set of processors that are executed sequentially for processing the logs. There are predefined pipelines available based on the Datadog integrations currently in use, and they are enabled if related logs are collected by Datadog. It is also possible to set up custom pipelines to meet specific indexing requirements. </p>
			<p>In the following screenshot, the processors used in the <strong class="bold">Apache httpd</strong> sample integration pipeline are listed:</p>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="Images/Figure_13.3_B16483.jpg" alt="Figure 13.3 – List of processors in a sample pipeline&#13;&#10;" width="1113" height="421"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.3 – List of processors in a sample pipeline</p>
			<p>The details of parsing rules and the <a id="_idIndexMarker777"/>structured information parsed out from the log entries can be looked up by clicking on the <strong class="bold">View</strong> icon associated with each processor. For example, in the <strong class="bold">Apache httpd</strong> pipeline, the <strong class="bold">Grok Parser: Parsing Apache httpd logs</strong> processor extracts the following structured information as a result of processing:</p>
			<p class="source-code">{</p>
			<p class="source-code">  "http": {</p>
			<p class="source-code">    "status_code": 200,</p>
			<p class="source-code">    "auth": "frank",</p>
			<p class="source-code">    "method": "GET",</p>
			<p class="source-code">    "url": "/apache_pb.gif",</p>
			<p class="source-code">    "version": "1.0"</p>
			<p class="source-code">  },</p>
			<p class="source-code">  "network": {</p>
			<p class="source-code">    "bytes_written": 2326,</p>
			<p class="source-code">    "client": {</p>
			<p class="source-code">      "ip": "127.0.0.1"</p>
			<p class="source-code">    }</p>
			<p class="source-code">  },</p>
			<p class="source-code">  "date_access": 1468407336000</p>
			<p class="source-code">}</p>
			<p>All the predefined pipelines <a id="_idIndexMarker778"/>can be looked up using the <strong class="bold">Browse Pipeline Library</strong> link on the <strong class="bold">Pipelines</strong> tab in the main <strong class="bold">Log</strong> dashboard, as shown in <em class="italic">Figure 13.2</em>.</p>
			<p>Log-based metrics can be generated based on a query. To set a new metric, navigate to <strong class="bold">Logs</strong> | <strong class="bold">Generate Metrics</strong> | <strong class="bold">New Metric</strong>. The main part is to provide a query that will define the new metric well. A sample screenshot of the <strong class="bold">Generate Metric</strong> window is provided as follows:</p>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<img src="Images/Figure_13.4_B16483.jpg" alt="Figure 13.4 – Generating new metrics from logs&#13;&#10;" width="1584" height="1025"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.4 – Generating new metrics from logs</p>
			<p>By default, Datadog tracks all <a id="_idIndexMarker779"/>processed logs in one index. Datadog provides the option to create multiple indexes for finer control over the indexed data. For example, different retention periods can be set on the indexes based on the relative importance of the subsets of logs being tracked by those indexes. </p>
			<p>In this section, you have learned how pipelines with processors are used to extract information from unstructured logs. In the next section, we will look at how logs collected by Datadog can be arch<a id="_idTextAnchor418"/>ived and retrieved as required.</p>
			<h1 id="_idParaDest-223"><a id="_idTextAnchor419"/>Archiving logs</h1>
			<p>Having logs at a central location is itself a <a id="_idIndexMarker780"/>significant advantage for a business as access to the collected logs is simplified and logs from multiple sources can easily be correlated and analyzed. For monitoring and reporting the aggregated information, it is good enough and there is no need to retain the old logs. However, for compliance purposes and future audits, businesses may need to retain logs for longer periods. As old, raw logs are not needed for active use, those logs could be archived away with the option to retrieve them on demand.</p>
			<p>Datadog provides archival options with public cloud storage services as the backend storage infrastructure. To set up an archive for a subset of logs collected by Datadog, the general steps are as follows:</p>
			<ul>
				<li><strong class="bold">Set up an integration with cloud service</strong>: This step requires setting up integration with a public cloud storage service: <strong class="bold">AWS S3</strong>, <strong class="bold">Azure Storage</strong>, or <strong class="bold">Google Cloud Storage</strong>.</li>
				<li><strong class="bold">Create a storage bucket</strong>: This storage bucket will store the logs.</li>
				<li><strong class="bold">Set permissions</strong>: Set permissions on the storage bucket so that Datadog can store and access the logs there.</li>
				<li><strong class="bold">Route logs to the bucket</strong>: In this step, create a new archive in Datadog and point it to the new storage bucket set up in the previous step. This option is available under the <strong class="bold">Logs</strong> | <strong class="bold">Archives</strong> tab.</li>
			</ul>
			<p>These steps differ considerably depending on the public cloud storage service used for storing the archives, and those details can be found in the official documentation available at <a href="https://docs.datadoghq.com/logs/archives">https://docs.datadoghq.com/logs/archives</a>.</p>
			<p>When the archived logs need to be loaded back into Datadog for any purpose, which is usually due to the need for some audit or root cause analysis (such events are rare in real life), the <strong class="bold">Rehydrate from Archives</strong> option could be used for that purpose. Navigate to <strong class="bold">Logs</strong> | <strong class="bold">Rehydrate from Archives</strong>.</p>
			<p>In the next section, we will explore how the logs collected by Datadog could be searched, an important tool that is<a id="_idTextAnchor420"/> popular with operations teams. </p>
			<h1 id="_idParaDest-224"><a id="_idTextAnchor421"/>Searching logs</h1>
			<p>To search the logs, navigate to <strong class="bold">Logs</strong> | <strong class="bold">Search</strong> and the search <a id="_idIndexMarker781"/>window should look like the sample interface in the following screenshot:</p>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="Images/Figure_13.5_B16483.jpg" alt="Figure 13.5 – Searching logs&#13;&#10;" width="1650" height="773"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.5 – Searching logs</p>
			<p>A search query is <a id="_idIndexMarker782"/>composed <a id="_idIndexMarker783"/>of <strong class="bold">keywords</strong> and <strong class="bold">operators</strong>. In Datadog terminology, a <strong class="bold">single term</strong> is a single keyword, such as <strong class="source-inline">error</strong>, and a <strong class="bold">sequence</strong> is a group of keywords in quotes, such as <strong class="source-inline">"found error"</strong>. To coin a complex search query, terms and sequences are combined using the following <strong class="source-inline">boolean</strong> operators:</p>
			<ul>
				<li><strong class="source-inline">AND</strong>: Both terms must be in the selected log entry. </li>
				<li><strong class="source-inline">OR</strong>: One of the terms must be in the selected log entry. </li>
				<li><strong class="source-inline">- (Exclude)</strong>: The term follows the character "-" and should be excluded in the selected log entry.</li>
			</ul>
			<p>Built-in keywords such as <strong class="bold">host</strong> or <strong class="bold">source</strong> can be used as a search term by using the autocomplete option in the search field. You just need to click in the search field to see all the terms available to use, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="Images/Figure_13.6_B16483.jpg" alt="Figure 13.6 – Built-in keywords for search purposes&#13;&#10;" width="1436" height="881"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.6 – Built-in keywords for search purposes</p>
			<p>Special characters need to be <a id="_idIndexMarker784"/>escaped in search terms, and this could be done by prefixing the character to be escaped with the <strong class="source-inline">\</strong> character. For a complete list of special characters that need to be escaped, look up the complete list available at <a href="https://docs.datadoghq.com/logs/search_syntax">https://docs.datadoghq.com/logs/search_syntax</a>.</p>
			<p>The wild character, <strong class="source-inline">*</strong>, is supported with its usual meaning. For example, <strong class="source-inline">service*</strong> would match all log entries that begin with <strong class="source-inline">service</strong>, and <strong class="source-inline">*service</strong> matches all log entries that end with <strong class="source-inline">service</strong>.</p>
			<p>A successful search can be saved for future use. The <strong class="bold">Save</strong> button in the top-left corner of the Search dashboard could be used to save the current search, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer162" class="IMG---Figure">
					<img src="Images/Figure_13.7_B16483.jpg" alt="Figure 13.7 – Saving a search query&#13;&#10;" width="1275" height="355"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.7 – Saving a search query</p>
			<p>The saved searches could be <a id="_idIndexMarker785"/>listed and be rerun using the <strong class="bold">Views</strong> link, as shown in the same screenshot in <em class="italic">Figure 13.7</em>.</p>
			<p>You have learned how to search the logs aggregated by Datadog using the <strong class="bold">Search</strong><em class="italic"> </em>interface on the <strong class="bold">Log Explorer</strong> dashboard with directions on how to use the keywords and operators. This section concludes the chapter, so let's now look at the best practices and the summary rel<a id="_idTextAnchor422"/>ated to log management in Datadog.</p>
			<h1 id="_idParaDest-225"><a id="_idTextAnchor423"/>Best practices</h1>
			<p>There are certain <a id="_idIndexMarker786"/>patterns of best practices in log management. Let's see how they can be rolled out using related Datadog features:</p>
			<ul>
				<li>Plan to collect as many logs as possible. It is better to stop collecting some of the logs later if they are found to be not useful.</li>
				<li>Where possible, especially with the application logs that you will have control over in terms of formatting, make the format of logs parsing friendly. </li>
				<li>Consider generating new logs for the purpose of generating metrics out of such logs. Such efforts have been found to be very useful in generating data for reporting.</li>
				<li>Make sure that sensitive information in logs is redacted before allowing Datadog to collect. </li>
				<li>Implement the <a id="_idIndexMarker787"/>redaction of sensitive information from the logs instead of filtering out log entries. However, filter out log entries that might not be useful, so the volume of logs handled by Datadog will be minimal.</li>
				<li>Create a library of searches and publish it for general use. It's hard to create complex queries, and sharing such queries <a id="_idTextAnchor424"/>would make the team more efficient. </li>
			</ul>
			<h1 id="_idParaDest-226"><a id="_idTextAnchor425"/>Summary</h1>
			<p>In this chapter, you have learned how Datadog collects logs, processes these, and provides access to various aggregated information as well as the raw logs. Datadog also facilitates archiving of the logs with support for public cloud storage services. Using <strong class="bold">Log Explorer</strong>, you can search the entire set of active logs and valid searches can be saved for future use.</p>
			<p>In the next chapter, the final chapter of this book, we will discuss a number of advanced Datadog features that we haven't touched on yet.</p>
		</div>
	</div></body></html>