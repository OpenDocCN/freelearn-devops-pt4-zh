- en: '*Chapter 13*: Managing Logs Using Datadog'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第13章*：使用Datadog管理日志'
- en: The logs generated by the operating system, the various platform components,
    and the application services contain a lot of information regarding the state
    of the infrastructure as well as the workings of the applications running on it.
    Managing all logs at a central repository and analyzing that for operational insights
    and monitoring purposes is an important area in monitoring. It usually involves
    the collection, aggregation, and indexing of logs. In [*Chapter 1*](B16483_01_Final_VK_ePub.xhtml#_idTextAnchor014),
    *Introduction to Monitoring*, this monitoring type was briefly discussed. In [*Chapter
    12*](B16483_12_Final_VK_ePub.xhtml#_idTextAnchor385), *Monitoring Containers*,
    you learned how logs from containers are published to Datadog for aggregation
    and indexing for facilitating searches.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统、各个平台组件和应用服务生成的日志包含了大量关于基础设施状态以及其上运行的应用程序工作情况的信息。在中央存储库管理所有日志，并对其进行分析以获得运营洞察和监控数据，这是监控中的一个重要领域。它通常涉及日志的收集、聚合和索引。在[*第1章*](B16483_01_Final_VK_ePub.xhtml#_idTextAnchor014)《监控概述》中，简要讨论了这种监控类型。在[*第12章*](B16483_12_Final_VK_ePub.xhtml#_idTextAnchor385)《监控容器》中，你学习了如何将容器中的日志发布到Datadog，以便聚合和索引，从而便于搜索。
- en: Some of the popular monitoring product offerings in this area are ELK Stack
    (**Elasticsearch**, **Logstash**, and **Kibana**), Splunk, and Sumo Logic. Now,
    Datadog also provides this feature and you have seen **Log Explorer**, a frontend
    to that feature, in the last chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个领域，一些流行的监控产品包括ELK Stack（**Elasticsearch**、**Logstash** 和 **Kibana**）、Splunk
    和 Sumo Logic。现在，Datadog也提供了这个功能，你在上一章中已经见过了其前端工具**日志浏览器**。
- en: 'In this chapter, we will explore Datadog''s log aggregation and indexing feature
    in detail. Specifically, we will look at the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将详细探讨Datadog的日志聚合和索引功能。具体来说，我们将关注以下主题：
- en: Collecting logs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集日志
- en: Processing logs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理日志
- en: Archiving logs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归档日志
- en: Searching logs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索日志
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To try out the examples mentioned in this book, you need to have the following
    tools installed and resources available:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 若要尝试本书中提到的示例，你需要安装以下工具并具备相关资源：
- en: A Datadog account with admin-level access
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有管理员权限的Datadog帐户
- en: A Datadog Agent running at host level or as a microservice depending on the
    example, pointing to the Datadog account
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在主机级别或根据示例作为微服务运行的Datadog代理，指向Datadog帐户
- en: Collecting logs
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集日志
- en: The first step in any log management application is to collect the logs in a
    common storage repository for analyzing them later and archiving them for the
    records. That effort involves shipping the log files from machines and services
    where they are available to the common storage repository.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 任何日志管理应用程序的第一步是将日志收集到一个公共存储库中，以便稍后进行分析和归档保存。这项工作包括将日志文件从可用的机器和服务发送到公共存储库。
- en: 'The following diagram provides the workflow of collecting and processing the
    logs and rendering the aggregated information to end users. The aggregated information
    could be published as metrics, which could be used for setting up monitors. That
    is the same as using metrics to set up monitors in a conventional monitoring application:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了收集和处理日志的工作流程，并将聚合后的信息呈现给最终用户。聚合后的信息可以作为指标发布，这些指标可用于设置监控。这与在传统监控应用程序中使用指标设置监控是相同的：
- en: '![Figure 13.1 – Log management workflow'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![图13.1 – 日志管理工作流程'
- en: '](img/Figure_13.1_B16483.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_13.1_B16483.jpg)'
- en: Figure 13.1 – Log management workflow
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1 – 日志管理工作流程
- en: 'In a modern production infrastructure, the logs could be generated by a variety
    of sources, and typical sources include the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代生产基础设施中，日志可能由多种来源生成，典型的来源包括以下几种：
- en: '**Public cloud services**: Public cloud services such as AWS S3 and RDS are
    very popular, especially if the production infrastructure is built predominantly
    using public cloud services. The logs from those services can be shipped into
    Datadog using the related Datadog integrations available.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公共云服务**：公共云服务如AWS S3和RDS非常流行，尤其是在生产基础设施主要使用公共云服务构建的情况下。这些服务的日志可以通过相关的Datadog集成功能发送到Datadog。'
- en: '**Microservices**: The Datadog integrations available for Docker and Kubernetes
    can be used to ship these logs into Datadog. We have already seen examples of
    doing that in the last chapter.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微服务**：Datadog为Docker和Kubernetes提供的集成功能可用于将这些日志发送到Datadog。我们在上一章中已经看过如何实现这一操作。'
- en: '**Hosts**: Traditionally, logs are available as files on a disk location on
    the host, bare-metal or virtual. A Datadog Agent can be configured to ship local
    log files to the Datadog backend.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主机**：传统上，日志作为文件存储在主机上的磁盘位置，可能是裸金属或虚拟主机。可以配置 Datadog 代理将本地日志文件传输到 Datadog
    后端。'
- en: Now, let's see the details of how logs are collected and shipped by Datadog,
    in the most common use cases that are summarized above.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下 Datadog 如何收集和传输日志，了解上面总结的最常见用例的详细信息。
- en: Collecting logs from public cloud services
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从公共云服务收集日志
- en: 'Datadog provides integrations and methods for collecting logs from services
    offered on major public cloud platforms such as AWS, Azure, and GCP. The cloud
    platform-specific methods available are as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Datadog 提供了与主要公共云平台（如 AWS、Azure 和 GCP）上提供的服务集成和日志收集方法。以下是可用的特定于云平台的方法：
- en: '**Cloudformation** and **Kinesis Firehose**-based options are available to
    roll out the automation to collect logs from AWS services.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cloudformation** 和 **Kinesis Firehose** 基于的选项可以用于自动化收集 AWS 服务的日志。'
- en: On Azure, the automation to collect logs is based on **Azure Event Hub** and
    **Azure Function**.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Azure 上，收集日志的自动化是基于 **Azure Event Hub** 和 **Azure Function** 的。
- en: Datadog integration is available on GCP to collect logs from the services offered
    on that platform.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 GCP 上提供 Datadog 集成，可收集该平台上提供的服务的日志。
- en: Shipping logs from containers
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从容器传输日志
- en: 'When containers are run on Docker, outside of a Kubernetes cluster, the logs
    from containers can be shipped to Datadog by configuring a Datadog Agent and the
    related Docker images. The main requirement is to deploy a Datadog Agent as a
    container on the same host. You have learned how that is done in the last chapter.
    The second part is to instrument the Docker image with a container label for the
    logs to be auto-discovered by a Datadog Agent. For example, the following label
    in the NGINX `Dockerfile` helps a Datadog Agent to collect logs from the NGINX
    container spun up from that image, and running as `webapp`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当容器在 Docker 上运行，而不在 Kubernetes 集群中时，可以通过配置 Datadog 代理和相关的 Docker 镜像，将容器的日志传输到
    Datadog。主要要求是在同一主机上部署一个 Datadog 代理作为容器。你在上一章已经了解了如何做到这一点。第二步是为 Docker 镜像添加容器标签，以便
    Datadog 代理可以自动发现这些日志。例如，NGINX `Dockerfile` 中的以下标签有助于 Datadog 代理收集从该镜像启动的 NGINX
    容器的日志，并以 `webapp` 作为容器运行：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Instrumenting a Docker image might not always be possible as some images could
    be supplied by third parties, or making such changes might not be viable operationally.
    In such scenarios, the environment variable `DD_LOGS_CONFIG_CONTAINER_COLLECT_ALL`
    could be specified in the Datadog Agent runtime environment to collect logs from
    all the containers running on that host. If there are any logs to be excluded
    from aggregation, Datadog has options to filter those logs out before shipping
    those for processing, and we will look at that later in this section.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对 Docker 镜像进行工具化可能并不总是可行，因为某些镜像可能由第三方提供，或者进行此类更改在操作上不可行。在这种情况下，可以在 Datadog 代理的运行环境中指定环境变量
    `DD_LOGS_CONFIG_CONTAINER_COLLECT_ALL`，以收集该主机上运行的所有容器的日志。如果有任何日志需要排除在聚合之外，Datadog
    提供了选项，可以在将这些日志发送进行处理之前将其过滤掉，稍后我们会在本节中讨论这个问题。
- en: 'The configuration required in a Kubernetes cluster to ship logs running in
    that environment to the Datadog backend is similar to how that is done on a Docker
    host:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 集群中配置，将在该环境中运行的日志传输到 Datadog 后端的方式，类似于在 Docker 主机上的配置方式：
- en: The Datadog Agent must be run as a container in the cluster.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Datadog 代理必须作为容器在集群中运行。
- en: 'The application containers must be annotated for `DD_LOGS_CONFIG_CONTAINER_COLLECT_ALL`
    must be set to `true` for shipping logs from all the containers running in the
    cluster. For example, to enable the NGINX container for autodiscovery, the following
    annotation must be included in the deployment description in Kubernetes:'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用容器必须被注解，`DD_LOGS_CONFIG_CONTAINER_COLLECT_ALL` 必须设置为 `true`，以便从集群中所有运行的容器中传送日志。例如，要启用
    NGINX 容器的自动发现，必须在 Kubernetes 的部署描述中包含以下注解：
- en: '[PRE1]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, let's look at what configuration is needed for collecting logs when the
    Datadog Agent is run at the host level.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看看当 Datadog 代理在主机级别运行时，收集日志需要哪些配置。
- en: Shipping logs from hosts
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从主机传输日志
- en: 'The logs from a host, where the Datadog Agent runs, could be shipped using
    the option available with Datadog integration for third-party applications. If
    no integration is available, the logs generated by an application could be shipped
    by following the custom method, as explained in this section:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过 Datadog 为第三方应用提供的集成功能，将来自主机（Datadog Agent 运行所在的主机）的日志传送。如果没有可用的集成，则可以按照本节所述的自定义方法传送应用程序生成的日志：
- en: 'In the first case, in which the Datadog integration is available for the application,
    such as NGINX, the existing configuration file can be used to specify the log
    collection requirements as specified in the following example:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种情况中，应用程序如 NGINX 可用 Datadog 集成时，可以使用现有的配置文件来指定日志收集要求，如下例所示：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The log files `/var/log/nginx/access.log` and `/var/log/nginx/error.log` are
    configured to be collected in the preceding sample case scenario. If the logs
    have to be collected directly from a service running on a port, the log type will
    be `tcp` or `udp`, and `port` must be specified in place of `path`.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述示例场景中，日志文件 `/var/log/nginx/access.log` 和 `/var/log/nginx/error.log` 已配置为进行收集。如果需要直接从运行在端口上的服务收集日志，则日志类型将为
    `tcp` 或 `udp`，并且必须用 `port` 替代 `path` 来指定端口。
- en: 'In the Datadog Agent configuration, the option to ship logs is not enabled
    by default and this has to be done in the `datadog.yaml` file also: `logs_enabled:
    true`.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '在 Datadog Agent 配置中，默认情况下并未启用日志传输选项，需要在 `datadog.yaml` 文件中设置：`logs_enabled:
    true`。'
- en: 'If no integration exists for an application, a custom configuration file needs
    to be set up for shipping logs generated by that application. The following are
    the steps for doing that:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果应用程序没有现成的集成，需为该应用程序设置自定义配置文件以传输该应用生成的日志。以下是具体步骤：
- en: Under the `conf.d` directory, create a sub-directory with an appropriate name
    on the basis of this syntax – `<CUSTOM_APP>.d/conf.d`.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `conf.d` 目录下，根据以下语法创建一个子目录，命名为 `<CUSTOM_APP>.d/conf.d`。
- en: 'In the new directory, create a configuration file, `conf.yaml`, and add the
    following entry for each log to be collected:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在新目录中，创建配置文件 `conf.yaml`，并为每个要收集的日志添加以下条目：
- en: '[PRE3]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The Datadog Agent must be restarted in order for these configuration changes
    to take effect.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 必须重新启动 Datadog Agent，以使这些配置更改生效。
- en: Filtering logs
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志过滤
- en: 'As you have seen already, it''s pretty easy to configure the Datadog Agent
    or integration to collect logs from any environment. However, shipping all the
    information tracked in the logs to Datadog might not be a good idea for a variety
    of reasons, including the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，配置 Datadog Agent 或集成以从任何环境收集日志是相当简单的。然而，将日志中跟踪的所有信息传送到 Datadog 可能并非明智之举，原因有很多，以下是一些原因：
- en: Security concerns
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全问题
- en: Restrictions imposed by agreements with customers
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由与客户的协议所施加的限制
- en: Regulatory requirements
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 法规要求
- en: Compliance controls in place
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合规控制措施
- en: Therefore, it may be necessary to filter out information from the logs that
    are not useful for monitoring and that are not allowed to be shared with a wider
    audience. Also, such filtering can result in minimizing storage utilization and
    data transfer to the Datadog backend.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可能需要过滤掉日志中不用于监控、且不允许与更广泛的受众共享的信息。此外，这种过滤有助于减少存储使用量和传输到 Datadog 后端的数据量。
- en: Two predefined rules, `include_at_match` and `exclude_at_match`, can be used
    to filter out logs at the collection phase. These rules work with a regular expression
    – if a log entry is matched with the regular expression used with the rule, that
    log is included or excluded, depending on the type of rule.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 两个预定义规则 `include_at_match` 和 `exclude_at_match` 可以在收集阶段用于过滤日志。这些规则与正则表达式一起工作——如果日志条目与规则中使用的正则表达式匹配，则根据规则类型，日志将被包含或排除。
- en: 'In the following example, log entries starting with the string `k8s-log` are
    ignored:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，所有以字符串 `k8s-log` 开头的日志条目将被忽略：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Multiple `include_at_match` rules are allowed for the same log file and that
    would result in an `AND` condition. This means that both the rules must be satisfied
    in order for a log entry to be collected.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 同一个日志文件允许多个 `include_at_match` 规则，这将导致 `AND` 条件。也就是说，必须同时满足两个规则，才能收集日志条目。
- en: 'To implement an `OR` rule, the conditions must be specified in the same expression
    using the `|` symbol, as in the following example:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现 `OR` 规则，必须在同一表达式中使用 `|` 符号指定条件，如下例所示：
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will result in any line beginning with `warning` or `err` being collected
    by the Datadog Agent.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致任何以`warning`或`err`开头的行被Datadog代理收集。
- en: As you have seen earlier, the filtering rules can be implemented in the Docker
    runtime using labels, and in Kubernetes using annotations.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如您之前所见，过滤规则可以通过Docker运行时中的标签实现，也可以通过Kubernetes中的注释来实现。
- en: Scrubbing sensitive data from logs
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从日志中清除敏感数据
- en: A common issue with aggregating logs and presenting this to a wider audience
    is that sensitive information that might be recorded in the logs could become
    visible inadvertently, and this constitutes a major security and privacy problem.
    Restricting access to the monitoring application that aggregates logs, such as
    Datadog's **Log Management interface**, can impair its general usefulness. The
    better solution is to scrub sensitive information off the logs at the source before
    they are shipped to the Datadog backend for processing.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合日志并将其展示给更广泛的受众时，常见的问题是日志中可能记录的敏感信息会不小心暴露，这会导致严重的安全和隐私问题。限制访问聚合日志的监控应用程序（如Datadog的**日志管理界面**）可能会影响其一般用途。更好的解决方案是在日志被传输到Datadog后端进行处理之前，从源头上清除日志中的敏感信息。
- en: While the filtering option that you have seen earlier can help with not collecting
    an entire log entry containing sensitive information, it might leave out an important
    piece of detail from the log for analysis or monitoring. So, redacting such information
    is always better as that would still leave sufficient operational details in the
    log.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您之前看到的过滤选项可以帮助避免收集包含敏感信息的完整日志条目，但它可能会遗漏日志中的某个重要细节，影响分析或监控。因此，对这些信息进行编辑是更好的选择，这样可以在日志中保留足够的操作细节。
- en: The rule type to use for scrubbing information is `mask_sequences`. It works
    with the `replace_placeholder` option, which determines how the sensitive information
    is replaced with a placeholder to indicate that information is masked in the log
    entry.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 用于清除信息的规则类型是`mask_sequences`。它与`replace_placeholder`选项一起工作，`replace_placeholder`决定了如何用占位符替换敏感信息，以表明日志条目中的信息已被遮蔽。
- en: 'The following example explains how these two options are used in tandem to
    achieve the desired result in redacting, which is highly effective:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例解释了如何同时使用这两个选项来实现所需的编辑效果，这种方法非常有效：
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The preceding sample rule will replace the field containing SSN information
    in a log entry with the string `SSN:[redacted]`. This is done by splitting the
    log entry into three parts and reassembling it using the format mentioned in `replace_placeholder`.
    The content matched within each `()` construct in `pattern` would be available
    as a numbered variable in `replace_placeholder`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例规则将把日志条目中包含SSN信息的字段替换为字符串`SSN:[redacted]`。这是通过将日志条目分为三部分，并使用`replace_placeholder`中提到的格式重新组装来完成的。`pattern`中的每个`()`构造中匹配的内容将作为编号变量在`replace_placeholder`中使用。
- en: As mentioned earlier, these rules have to be added to the `datadog.yaml` file
    at the host level and similar rules can be implemented in Docker runtime using
    labels, and in a Kubernetes cluster using annotations.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这些规则必须在主机级别的`datadog.yaml`文件中添加，类似的规则可以通过Docker运行时中的标签实现，也可以在Kubernetes集群中通过注释实现。
- en: You have learned how Datadog collects logs from different environments and stores
    that centrally for processing to derive operational insights in the form of metrics
    and to generate search indexes. In the next section, we will see how the logs
    are processed by Datadog and discuss the resources involved in that process.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经了解了Datadog如何从不同环境收集日志，并将其集中存储以进行处理，从而获得运营见解（如度量标准）并生成搜索索引。在接下来的章节中，我们将了解Datadog如何处理日志，并讨论该过程中涉及的资源。
- en: Processing logs
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理日志
- en: Once the logs are collected by Datadog, they will be available on **Log Explorer**
    to view and search. The logs in the structured JSON format are processed by Datadog
    automatically. The unstructured logs can be processed further and analytical insights
    extracted. Datadog uses **Pipelines** and **Processors** to process the incoming
    logs.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦日志被Datadog收集，它们将在**日志浏览器**中提供查看和搜索功能。Datadog会自动处理结构化的JSON格式日志。未结构化的日志可以进一步处理并提取分析见解。Datadog使用**管道（Pipelines）**和**处理器（Processors）**来处理传入的日志。
- en: 'To view the pipelines available, open the **Logs** | **Configuration** menu
    option from the main menu. The pipelines are listed under the **Pipelines** tab,
    as shown in the following sample screenshot:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看可用的管道，请从主菜单中打开**日志** | **配置**菜单选项。管道列在**管道**标签下，如以下示例截图所示：
- en: '![Figure 13.2 – Pipelines for log processing'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.2 – 日志处理管道'
- en: '](img/Figure_13.2_B16483.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_13.2_B16483.jpg)'
- en: Figure 13.2 – Pipelines for log processing
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2 – 日志处理管道
- en: A pipeline can be fed with a subset of logs and it uses a set of processors
    that are executed sequentially for processing the logs. There are predefined pipelines
    available based on the Datadog integrations currently in use, and they are enabled
    if related logs are collected by Datadog. It is also possible to set up custom
    pipelines to meet specific indexing requirements.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 管道可以接收日志的子集，并使用一组处理器按顺序执行以处理日志。根据当前使用的 Datadog 集成，提供了预定义的管道，并且如果相关日志被 Datadog
    收集，则这些管道会被启用。也可以设置自定义管道以满足特定的索引要求。
- en: 'In the following screenshot, the processors used in the **Apache httpd** sample
    integration pipeline are listed:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，列出了**Apache httpd**示例集成管道中使用的处理器：
- en: '![Figure 13.3 – List of processors in a sample pipeline'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.3 – 样本管道中的处理器列表'
- en: '](img/Figure_13.3_B16483.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_13.3_B16483.jpg)'
- en: Figure 13.3 – List of processors in a sample pipeline
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3 – 样本管道中的处理器列表
- en: 'The details of parsing rules and the structured information parsed out from
    the log entries can be looked up by clicking on the **View** icon associated with
    each processor. For example, in the **Apache httpd** pipeline, the **Grok Parser:
    Parsing Apache httpd logs** processor extracts the following structured information
    as a result of processing:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 解析规则的详细信息以及从日志条目中解析出的结构化信息可以通过点击每个处理器旁边的**查看**图标来查看。例如，在**Apache httpd**管道中，**Grok
    解析器：解析 Apache httpd 日志**处理器在处理后提取以下结构化信息：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: All the predefined pipelines can be looked up using the **Browse Pipeline Library**
    link on the **Pipelines** tab in the main **Log** dashboard, as shown in *Figure
    13.2*.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 所有预定义的管道可以通过在主**日志**仪表板中的**管道**标签下点击**浏览管道库**链接来查看，如*图 13.2*所示。
- en: 'Log-based metrics can be generated based on a query. To set a new metric, navigate
    to **Logs** | **Generate Metrics** | **New Metric**. The main part is to provide
    a query that will define the new metric well. A sample screenshot of the **Generate
    Metric** window is provided as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 可以基于查询生成基于日志的指标。要设置新指标，请导航至**日志** | **生成指标** | **新指标**。主要内容是提供一个查询，以便准确地定义新指标。以下是**生成指标**窗口的示例截图：
- en: '![Figure 13.4 – Generating new metrics from logs'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.4 – 从日志生成新指标'
- en: '](img/Figure_13.4_B16483.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_13.4_B16483.jpg)'
- en: Figure 13.4 – Generating new metrics from logs
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4 – 从日志生成新指标
- en: By default, Datadog tracks all processed logs in one index. Datadog provides
    the option to create multiple indexes for finer control over the indexed data.
    For example, different retention periods can be set on the indexes based on the
    relative importance of the subsets of logs being tracked by those indexes.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Datadog 将所有处理过的日志跟踪到一个索引中。Datadog 提供了创建多个索引的选项，以便对索引数据进行更精细的控制。例如，可以根据被跟踪日志子集的重要性，设置不同的保留期。
- en: In this section, you have learned how pipelines with processors are used to
    extract information from unstructured logs. In the next section, we will look
    at how logs collected by Datadog can be archived and retrieved as required.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您已经了解了如何使用带处理器的管道从非结构化日志中提取信息。在下一节中，我们将探讨如何归档和按需检索由 Datadog 收集的日志。
- en: Archiving logs
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 归档日志
- en: Having logs at a central location is itself a significant advantage for a business
    as access to the collected logs is simplified and logs from multiple sources can
    easily be correlated and analyzed. For monitoring and reporting the aggregated
    information, it is good enough and there is no need to retain the old logs. However,
    for compliance purposes and future audits, businesses may need to retain logs
    for longer periods. As old, raw logs are not needed for active use, those logs
    could be archived away with the option to retrieve them on demand.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 将日志集中存放在一个地方本身就为企业带来了显著的优势，因为集中存放的日志可以简化访问，并且来自多个来源的日志可以轻松地关联和分析。对于监控和报告汇总的信息，这样的方式已经足够，而且不需要保留旧日志。然而，出于合规性和未来审计的需要，企业可能需要保留日志更长时间。由于旧的、原始的日志不再需要用于当前使用，因此可以将这些日志归档，按需取回。
- en: 'Datadog provides archival options with public cloud storage services as the
    backend storage infrastructure. To set up an archive for a subset of logs collected
    by Datadog, the general steps are as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Datadog提供了以公共云存储服务作为后端存储基础设施的存档选项。要为Datadog收集的日志子集设置存档，通常的步骤如下：
- en: '**Set up an integration with cloud service**: This step requires setting up
    integration with a public cloud storage service: **AWS S3**, **Azure Storage**,
    or **Google Cloud Storage**.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与云服务设置集成**：此步骤需要与公共云存储服务进行集成：**AWS S3**、**Azure 存储**或**Google Cloud Storage**。'
- en: '**Create a storage bucket**: This storage bucket will store the logs.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建存储桶**：这个存储桶将用来存储日志。'
- en: '**Set permissions**: Set permissions on the storage bucket so that Datadog
    can store and access the logs there.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设置权限**：设置存储桶的权限，使Datadog能够在其中存储和访问日志。'
- en: '**Route logs to the bucket**: In this step, create a new archive in Datadog
    and point it to the new storage bucket set up in the previous step. This option
    is available under the **Logs** | **Archives** tab.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将日志路由到存储桶**：在此步骤中，创建一个新的存档并将其指向前一步骤中设置的新存储桶。此选项可以在**日志** | **存档**标签下找到。'
- en: These steps differ considerably depending on the public cloud storage service
    used for storing the archives, and those details can be found in the official
    documentation available at [https://docs.datadoghq.com/logs/archives](https://docs.datadoghq.com/logs/archives).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤在使用不同的公共云存储服务存储存档时有很大不同，相关的详细信息可以在[https://docs.datadoghq.com/logs/archives](https://docs.datadoghq.com/logs/archives)的官方文档中找到。
- en: When the archived logs need to be loaded back into Datadog for any purpose,
    which is usually due to the need for some audit or root cause analysis (such events
    are rare in real life), the **Rehydrate from Archives** option could be used for
    that purpose. Navigate to **Logs** | **Rehydrate from Archives**.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要将存档日志加载回Datadog时，通常是由于需要进行某些审计或根本原因分析（这种事件在实际中很少发生），可以使用**从存档恢复**选项。导航到**日志**
    | **从存档恢复**。
- en: In the next section, we will explore how the logs collected by Datadog could
    be searched, an important tool that is popular with operations teams.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将探索Datadog收集的日志如何进行搜索，这是一个受运维团队欢迎的重要工具。
- en: Searching logs
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 搜索日志
- en: 'To search the logs, navigate to **Logs** | **Search** and the search window
    should look like the sample interface in the following screenshot:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要搜索日志，请导航至**日志** | **搜索**，搜索窗口应如以下截图所示：
- en: '![Figure 13.5 – Searching logs'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '![图13.5 – 搜索日志'
- en: '](img/Figure_13.5_B16483.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_13.5_B16483.jpg)'
- en: Figure 13.5 – Searching logs
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.5 – 搜索日志
- en: 'A search query is composed of `error`, and a `"found error"`. To coin a complex
    search query, terms and sequences are combined using the following `boolean` operators:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索查询由`error`和`"found error"`组成。要构建复杂的搜索查询，可以使用以下`boolean`运算符将术语和序列结合起来：
- en: '`AND`: Both terms must be in the selected log entry.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AND`：两个术语必须都出现在所选的日志条目中。'
- en: '`OR`: One of the terms must be in the selected log entry.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OR`：其中一个术语必须出现在所选的日志条目中。'
- en: '`- (Exclude)`: The term follows the character "-" and should be excluded in
    the selected log entry.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`- (排除)`：术语后跟字符“-”，并且应排除在所选日志条目中。'
- en: 'Built-in keywords such as **host** or **source** can be used as a search term
    by using the autocomplete option in the search field. You just need to click in
    the search field to see all the terms available to use, as shown in the following
    screenshot:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用内建的关键词，如**host**或**source**，作为搜索词，通过使用搜索字段中的自动完成功能。只需点击搜索字段，即可查看所有可用的术语，如以下截图所示：
- en: '![Figure 13.6 – Built-in keywords for search purposes'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '![图13.6 – 搜索用途的内建关键词'
- en: '](img/Figure_13.6_B16483.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_13.6_B16483.jpg)'
- en: Figure 13.6 – Built-in keywords for search purposes
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.6 – 搜索用途的内建关键词
- en: Special characters need to be escaped in search terms, and this could be done
    by prefixing the character to be escaped with the `\` character. For a complete
    list of special characters that need to be escaped, look up the complete list
    available at [https://docs.datadoghq.com/logs/search_syntax](https://docs.datadoghq.com/logs/search_syntax).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 特殊字符在搜索词中需要进行转义，可以通过在要转义的字符前加上`\`字符来实现。有关需要转义的特殊字符的完整列表，请查阅[https://docs.datadoghq.com/logs/search_syntax](https://docs.datadoghq.com/logs/search_syntax)的完整列表。
- en: The wild character, `*`, is supported with its usual meaning. For example, `service*`
    would match all log entries that begin with `service`, and `*service` matches
    all log entries that end with `service`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 支持通配符`*`，并按其通常的含义使用。例如，`service*`将匹配所有以`service`开头的日志条目，`*service`将匹配所有以`service`结尾的日志条目。
- en: 'A successful search can be saved for future use. The **Save** button in the
    top-left corner of the Search dashboard could be used to save the current search,
    as shown in the following screenshot:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的搜索可以保存以备未来使用。**保存**按钮位于搜索仪表盘的左上角，可以用来保存当前搜索，如下图所示：
- en: '![Figure 13.7 – Saving a search query'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.7 – 保存搜索查询'
- en: '](img/Figure_13.7_B16483.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_13.7_B16483.jpg)'
- en: Figure 13.7 – Saving a search query
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.7 – 保存搜索查询
- en: The saved searches could be listed and be rerun using the **Views** link, as
    shown in the same screenshot in *Figure 13.7*.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 保存的搜索可以列出并通过**视图**链接重新运行，如同在*图 13.7*中所示的截图。
- en: You have learned how to search the logs aggregated by Datadog using the **Search**interface
    on the **Log Explorer** dashboard with directions on how to use the keywords and
    operators. This section concludes the chapter, so let's now look at the best practices
    and the summary related to log management in Datadog.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学习了如何通过**日志浏览器（Log Explorer）**仪表盘上的**搜索**界面搜索由 Datadog 汇总的日志，并了解如何使用关键词和运算符。此部分总结了本章的内容，接下来我们将探讨与
    Datadog 日志管理相关的最佳实践和总结。
- en: Best practices
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最佳实践
- en: 'There are certain patterns of best practices in log management. Let''s see
    how they can be rolled out using related Datadog features:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在日志管理中存在一些最佳实践模式。让我们来看一下如何利用相关的 Datadog 功能来实现这些模式：
- en: Plan to collect as many logs as possible. It is better to stop collecting some
    of the logs later if they are found to be not useful.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计划收集尽可能多的日志。如果后来发现某些日志无用，最好停止收集它们。
- en: Where possible, especially with the application logs that you will have control
    over in terms of formatting, make the format of logs parsing friendly.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能地，特别是在你能控制格式的应用程序日志中，使日志格式便于解析。
- en: Consider generating new logs for the purpose of generating metrics out of such
    logs. Such efforts have been found to be very useful in generating data for reporting.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑生成新的日志，用于从这些日志中生成指标。这样的努力在生成报告数据方面被证明是非常有用的。
- en: Make sure that sensitive information in logs is redacted before allowing Datadog
    to collect.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保在允许 Datadog 收集日志之前，敏感信息已经被删除。
- en: Implement the redaction of sensitive information from the logs instead of filtering
    out log entries. However, filter out log entries that might not be useful, so
    the volume of logs handled by Datadog will be minimal.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施日志中敏感信息的删除，而不是过滤掉日志条目。但是，过滤掉那些可能没有用的日志条目，以便 Datadog 处理的日志量尽可能最小。
- en: Create a library of searches and publish it for general use. It's hard to create
    complex queries, and sharing such queries would make the team more efficient.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个搜索库并发布供大家使用。创建复杂查询比较困难，分享这些查询能提高团队的效率。
- en: Summary
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you have learned how Datadog collects logs, processes these,
    and provides access to various aggregated information as well as the raw logs.
    Datadog also facilitates archiving of the logs with support for public cloud storage
    services. Using **Log Explorer**, you can search the entire set of active logs
    and valid searches can be saved for future use.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你已经学习了 Datadog 如何收集日志、处理这些日志，并提供对各种汇总信息以及原始日志的访问。Datadog 还支持将日志归档，并且兼容公共云存储服务。通过使用**日志浏览器（Log
    Explorer）**，你可以搜索整个活动日志集，且有效的搜索可以保存以供未来使用。
- en: In the next chapter, the final chapter of this book, we will discuss a number
    of advanced Datadog features that we haven't touched on yet.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，即本书的最后一章中，我们将讨论一些尚未涉及的高级 Datadog 功能。
