- en: '*Chapter 13*: Managing Logs Using Datadog'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The logs generated by the operating system, the various platform components,
    and the application services contain a lot of information regarding the state
    of the infrastructure as well as the workings of the applications running on it.
    Managing all logs at a central repository and analyzing that for operational insights
    and monitoring purposes is an important area in monitoring. It usually involves
    the collection, aggregation, and indexing of logs. In [*Chapter 1*](B16483_01_Final_VK_ePub.xhtml#_idTextAnchor014),
    *Introduction to Monitoring*, this monitoring type was briefly discussed. In [*Chapter
    12*](B16483_12_Final_VK_ePub.xhtml#_idTextAnchor385), *Monitoring Containers*,
    you learned how logs from containers are published to Datadog for aggregation
    and indexing for facilitating searches.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the popular monitoring product offerings in this area are ELK Stack
    (**Elasticsearch**, **Logstash**, and **Kibana**), Splunk, and Sumo Logic. Now,
    Datadog also provides this feature and you have seen **Log Explorer**, a frontend
    to that feature, in the last chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore Datadog''s log aggregation and indexing feature
    in detail. Specifically, we will look at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Collecting logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Archiving logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Searching logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To try out the examples mentioned in this book, you need to have the following
    tools installed and resources available:'
  prefs: []
  type: TYPE_NORMAL
- en: A Datadog account with admin-level access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Datadog Agent running at host level or as a microservice depending on the
    example, pointing to the Datadog account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step in any log management application is to collect the logs in a
    common storage repository for analyzing them later and archiving them for the
    records. That effort involves shipping the log files from machines and services
    where they are available to the common storage repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram provides the workflow of collecting and processing the
    logs and rendering the aggregated information to end users. The aggregated information
    could be published as metrics, which could be used for setting up monitors. That
    is the same as using metrics to set up monitors in a conventional monitoring application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.1 – Log management workflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_13.1_B16483.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.1 – Log management workflow
  prefs: []
  type: TYPE_NORMAL
- en: 'In a modern production infrastructure, the logs could be generated by a variety
    of sources, and typical sources include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Public cloud services**: Public cloud services such as AWS S3 and RDS are
    very popular, especially if the production infrastructure is built predominantly
    using public cloud services. The logs from those services can be shipped into
    Datadog using the related Datadog integrations available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Microservices**: The Datadog integrations available for Docker and Kubernetes
    can be used to ship these logs into Datadog. We have already seen examples of
    doing that in the last chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hosts**: Traditionally, logs are available as files on a disk location on
    the host, bare-metal or virtual. A Datadog Agent can be configured to ship local
    log files to the Datadog backend.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's see the details of how logs are collected and shipped by Datadog,
    in the most common use cases that are summarized above.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting logs from public cloud services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Datadog provides integrations and methods for collecting logs from services
    offered on major public cloud platforms such as AWS, Azure, and GCP. The cloud
    platform-specific methods available are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cloudformation** and **Kinesis Firehose**-based options are available to
    roll out the automation to collect logs from AWS services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On Azure, the automation to collect logs is based on **Azure Event Hub** and
    **Azure Function**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datadog integration is available on GCP to collect logs from the services offered
    on that platform.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shipping logs from containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When containers are run on Docker, outside of a Kubernetes cluster, the logs
    from containers can be shipped to Datadog by configuring a Datadog Agent and the
    related Docker images. The main requirement is to deploy a Datadog Agent as a
    container on the same host. You have learned how that is done in the last chapter.
    The second part is to instrument the Docker image with a container label for the
    logs to be auto-discovered by a Datadog Agent. For example, the following label
    in the NGINX `Dockerfile` helps a Datadog Agent to collect logs from the NGINX
    container spun up from that image, and running as `webapp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Instrumenting a Docker image might not always be possible as some images could
    be supplied by third parties, or making such changes might not be viable operationally.
    In such scenarios, the environment variable `DD_LOGS_CONFIG_CONTAINER_COLLECT_ALL`
    could be specified in the Datadog Agent runtime environment to collect logs from
    all the containers running on that host. If there are any logs to be excluded
    from aggregation, Datadog has options to filter those logs out before shipping
    those for processing, and we will look at that later in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The configuration required in a Kubernetes cluster to ship logs running in
    that environment to the Datadog backend is similar to how that is done on a Docker
    host:'
  prefs: []
  type: TYPE_NORMAL
- en: The Datadog Agent must be run as a container in the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The application containers must be annotated for `DD_LOGS_CONFIG_CONTAINER_COLLECT_ALL`
    must be set to `true` for shipping logs from all the containers running in the
    cluster. For example, to enable the NGINX container for autodiscovery, the following
    annotation must be included in the deployment description in Kubernetes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, let's look at what configuration is needed for collecting logs when the
    Datadog Agent is run at the host level.
  prefs: []
  type: TYPE_NORMAL
- en: Shipping logs from hosts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The logs from a host, where the Datadog Agent runs, could be shipped using
    the option available with Datadog integration for third-party applications. If
    no integration is available, the logs generated by an application could be shipped
    by following the custom method, as explained in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first case, in which the Datadog integration is available for the application,
    such as NGINX, the existing configuration file can be used to specify the log
    collection requirements as specified in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The log files `/var/log/nginx/access.log` and `/var/log/nginx/error.log` are
    configured to be collected in the preceding sample case scenario. If the logs
    have to be collected directly from a service running on a port, the log type will
    be `tcp` or `udp`, and `port` must be specified in place of `path`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Datadog Agent configuration, the option to ship logs is not enabled
    by default and this has to be done in the `datadog.yaml` file also: `logs_enabled:
    true`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If no integration exists for an application, a custom configuration file needs
    to be set up for shipping logs generated by that application. The following are
    the steps for doing that:'
  prefs: []
  type: TYPE_NORMAL
- en: Under the `conf.d` directory, create a sub-directory with an appropriate name
    on the basis of this syntax – `<CUSTOM_APP>.d/conf.d`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the new directory, create a configuration file, `conf.yaml`, and add the
    following entry for each log to be collected:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The Datadog Agent must be restarted in order for these configuration changes
    to take effect.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering logs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you have seen already, it''s pretty easy to configure the Datadog Agent
    or integration to collect logs from any environment. However, shipping all the
    information tracked in the logs to Datadog might not be a good idea for a variety
    of reasons, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Security concerns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restrictions imposed by agreements with customers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regulatory requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compliance controls in place
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, it may be necessary to filter out information from the logs that
    are not useful for monitoring and that are not allowed to be shared with a wider
    audience. Also, such filtering can result in minimizing storage utilization and
    data transfer to the Datadog backend.
  prefs: []
  type: TYPE_NORMAL
- en: Two predefined rules, `include_at_match` and `exclude_at_match`, can be used
    to filter out logs at the collection phase. These rules work with a regular expression
    – if a log entry is matched with the regular expression used with the rule, that
    log is included or excluded, depending on the type of rule.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, log entries starting with the string `k8s-log` are
    ignored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Multiple `include_at_match` rules are allowed for the same log file and that
    would result in an `AND` condition. This means that both the rules must be satisfied
    in order for a log entry to be collected.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement an `OR` rule, the conditions must be specified in the same expression
    using the `|` symbol, as in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This will result in any line beginning with `warning` or `err` being collected
    by the Datadog Agent.
  prefs: []
  type: TYPE_NORMAL
- en: As you have seen earlier, the filtering rules can be implemented in the Docker
    runtime using labels, and in Kubernetes using annotations.
  prefs: []
  type: TYPE_NORMAL
- en: Scrubbing sensitive data from logs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common issue with aggregating logs and presenting this to a wider audience
    is that sensitive information that might be recorded in the logs could become
    visible inadvertently, and this constitutes a major security and privacy problem.
    Restricting access to the monitoring application that aggregates logs, such as
    Datadog's **Log Management interface**, can impair its general usefulness. The
    better solution is to scrub sensitive information off the logs at the source before
    they are shipped to the Datadog backend for processing.
  prefs: []
  type: TYPE_NORMAL
- en: While the filtering option that you have seen earlier can help with not collecting
    an entire log entry containing sensitive information, it might leave out an important
    piece of detail from the log for analysis or monitoring. So, redacting such information
    is always better as that would still leave sufficient operational details in the
    log.
  prefs: []
  type: TYPE_NORMAL
- en: The rule type to use for scrubbing information is `mask_sequences`. It works
    with the `replace_placeholder` option, which determines how the sensitive information
    is replaced with a placeholder to indicate that information is masked in the log
    entry.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example explains how these two options are used in tandem to
    achieve the desired result in redacting, which is highly effective:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The preceding sample rule will replace the field containing SSN information
    in a log entry with the string `SSN:[redacted]`. This is done by splitting the
    log entry into three parts and reassembling it using the format mentioned in `replace_placeholder`.
    The content matched within each `()` construct in `pattern` would be available
    as a numbered variable in `replace_placeholder`.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, these rules have to be added to the `datadog.yaml` file
    at the host level and similar rules can be implemented in Docker runtime using
    labels, and in a Kubernetes cluster using annotations.
  prefs: []
  type: TYPE_NORMAL
- en: You have learned how Datadog collects logs from different environments and stores
    that centrally for processing to derive operational insights in the form of metrics
    and to generate search indexes. In the next section, we will see how the logs
    are processed by Datadog and discuss the resources involved in that process.
  prefs: []
  type: TYPE_NORMAL
- en: Processing logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the logs are collected by Datadog, they will be available on **Log Explorer**
    to view and search. The logs in the structured JSON format are processed by Datadog
    automatically. The unstructured logs can be processed further and analytical insights
    extracted. Datadog uses **Pipelines** and **Processors** to process the incoming
    logs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To view the pipelines available, open the **Logs** | **Configuration** menu
    option from the main menu. The pipelines are listed under the **Pipelines** tab,
    as shown in the following sample screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.2 – Pipelines for log processing'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_13.2_B16483.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.2 – Pipelines for log processing
  prefs: []
  type: TYPE_NORMAL
- en: A pipeline can be fed with a subset of logs and it uses a set of processors
    that are executed sequentially for processing the logs. There are predefined pipelines
    available based on the Datadog integrations currently in use, and they are enabled
    if related logs are collected by Datadog. It is also possible to set up custom
    pipelines to meet specific indexing requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, the processors used in the **Apache httpd** sample
    integration pipeline are listed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.3 – List of processors in a sample pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_13.3_B16483.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.3 – List of processors in a sample pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'The details of parsing rules and the structured information parsed out from
    the log entries can be looked up by clicking on the **View** icon associated with
    each processor. For example, in the **Apache httpd** pipeline, the **Grok Parser:
    Parsing Apache httpd logs** processor extracts the following structured information
    as a result of processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: All the predefined pipelines can be looked up using the **Browse Pipeline Library**
    link on the **Pipelines** tab in the main **Log** dashboard, as shown in *Figure
    13.2*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Log-based metrics can be generated based on a query. To set a new metric, navigate
    to **Logs** | **Generate Metrics** | **New Metric**. The main part is to provide
    a query that will define the new metric well. A sample screenshot of the **Generate
    Metric** window is provided as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.4 – Generating new metrics from logs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_13.4_B16483.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.4 – Generating new metrics from logs
  prefs: []
  type: TYPE_NORMAL
- en: By default, Datadog tracks all processed logs in one index. Datadog provides
    the option to create multiple indexes for finer control over the indexed data.
    For example, different retention periods can be set on the indexes based on the
    relative importance of the subsets of logs being tracked by those indexes.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you have learned how pipelines with processors are used to
    extract information from unstructured logs. In the next section, we will look
    at how logs collected by Datadog can be archived and retrieved as required.
  prefs: []
  type: TYPE_NORMAL
- en: Archiving logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having logs at a central location is itself a significant advantage for a business
    as access to the collected logs is simplified and logs from multiple sources can
    easily be correlated and analyzed. For monitoring and reporting the aggregated
    information, it is good enough and there is no need to retain the old logs. However,
    for compliance purposes and future audits, businesses may need to retain logs
    for longer periods. As old, raw logs are not needed for active use, those logs
    could be archived away with the option to retrieve them on demand.
  prefs: []
  type: TYPE_NORMAL
- en: 'Datadog provides archival options with public cloud storage services as the
    backend storage infrastructure. To set up an archive for a subset of logs collected
    by Datadog, the general steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Set up an integration with cloud service**: This step requires setting up
    integration with a public cloud storage service: **AWS S3**, **Azure Storage**,
    or **Google Cloud Storage**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create a storage bucket**: This storage bucket will store the logs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Set permissions**: Set permissions on the storage bucket so that Datadog
    can store and access the logs there.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Route logs to the bucket**: In this step, create a new archive in Datadog
    and point it to the new storage bucket set up in the previous step. This option
    is available under the **Logs** | **Archives** tab.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These steps differ considerably depending on the public cloud storage service
    used for storing the archives, and those details can be found in the official
    documentation available at [https://docs.datadoghq.com/logs/archives](https://docs.datadoghq.com/logs/archives).
  prefs: []
  type: TYPE_NORMAL
- en: When the archived logs need to be loaded back into Datadog for any purpose,
    which is usually due to the need for some audit or root cause analysis (such events
    are rare in real life), the **Rehydrate from Archives** option could be used for
    that purpose. Navigate to **Logs** | **Rehydrate from Archives**.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore how the logs collected by Datadog could
    be searched, an important tool that is popular with operations teams.
  prefs: []
  type: TYPE_NORMAL
- en: Searching logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To search the logs, navigate to **Logs** | **Search** and the search window
    should look like the sample interface in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.5 – Searching logs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_13.5_B16483.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.5 – Searching logs
  prefs: []
  type: TYPE_NORMAL
- en: 'A search query is composed of `error`, and a `"found error"`. To coin a complex
    search query, terms and sequences are combined using the following `boolean` operators:'
  prefs: []
  type: TYPE_NORMAL
- en: '`AND`: Both terms must be in the selected log entry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OR`: One of the terms must be in the selected log entry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`- (Exclude)`: The term follows the character "-" and should be excluded in
    the selected log entry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Built-in keywords such as **host** or **source** can be used as a search term
    by using the autocomplete option in the search field. You just need to click in
    the search field to see all the terms available to use, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.6 – Built-in keywords for search purposes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_13.6_B16483.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.6 – Built-in keywords for search purposes
  prefs: []
  type: TYPE_NORMAL
- en: Special characters need to be escaped in search terms, and this could be done
    by prefixing the character to be escaped with the `\` character. For a complete
    list of special characters that need to be escaped, look up the complete list
    available at [https://docs.datadoghq.com/logs/search_syntax](https://docs.datadoghq.com/logs/search_syntax).
  prefs: []
  type: TYPE_NORMAL
- en: The wild character, `*`, is supported with its usual meaning. For example, `service*`
    would match all log entries that begin with `service`, and `*service` matches
    all log entries that end with `service`.
  prefs: []
  type: TYPE_NORMAL
- en: 'A successful search can be saved for future use. The **Save** button in the
    top-left corner of the Search dashboard could be used to save the current search,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.7 – Saving a search query'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_13.7_B16483.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.7 – Saving a search query
  prefs: []
  type: TYPE_NORMAL
- en: The saved searches could be listed and be rerun using the **Views** link, as
    shown in the same screenshot in *Figure 13.7*.
  prefs: []
  type: TYPE_NORMAL
- en: You have learned how to search the logs aggregated by Datadog using the **Search**interface
    on the **Log Explorer** dashboard with directions on how to use the keywords and
    operators. This section concludes the chapter, so let's now look at the best practices
    and the summary related to log management in Datadog.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are certain patterns of best practices in log management. Let''s see
    how they can be rolled out using related Datadog features:'
  prefs: []
  type: TYPE_NORMAL
- en: Plan to collect as many logs as possible. It is better to stop collecting some
    of the logs later if they are found to be not useful.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where possible, especially with the application logs that you will have control
    over in terms of formatting, make the format of logs parsing friendly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider generating new logs for the purpose of generating metrics out of such
    logs. Such efforts have been found to be very useful in generating data for reporting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure that sensitive information in logs is redacted before allowing Datadog
    to collect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement the redaction of sensitive information from the logs instead of filtering
    out log entries. However, filter out log entries that might not be useful, so
    the volume of logs handled by Datadog will be minimal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a library of searches and publish it for general use. It's hard to create
    complex queries, and sharing such queries would make the team more efficient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have learned how Datadog collects logs, processes these,
    and provides access to various aggregated information as well as the raw logs.
    Datadog also facilitates archiving of the logs with support for public cloud storage
    services. Using **Log Explorer**, you can search the entire set of active logs
    and valid searches can be saved for future use.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, the final chapter of this book, we will discuss a number
    of advanced Datadog features that we haven't touched on yet.
  prefs: []
  type: TYPE_NORMAL
