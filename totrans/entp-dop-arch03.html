<html><head></head><body>
		<div id="_idContainer022">
			<h1 id="_idParaDest-27"><em class="italic"><a id="_idTextAnchor027"/>Chapter 2</em>: Managing DevOps from Architecture </h1>
			<p>In the previous chapter, we learned about the different DevOps components, which comprise automation, collaboration, integration, and configuration management components. In this chapter, we will learn in more detail how to design these components and how to manage the <strong class="bold">DevOps cycle</strong> from these components. We will learn that automation and integration start with standardizing building blocks, called <strong class="bold">artifacts</strong>. These artifacts<a id="_idIndexMarker079"/> are linked with a portfolio that is defined by the <strong class="bold">enterprise architecture</strong>. Before we get to launch <strong class="bold">DevOps projects</strong> using automation and integration, we need to understand the business strategy and demand for architecture.  </p>
			<p>After completing this chapter, you will be able to identify the different components of demand management and how this drives portfolio management. You will also learn what the various stages are in <strong class="bold">continuous integration</strong> (<strong class="bold">CI</strong>) and how automation can help enterprises in speeding up deployment processes. In the last section, you will see that collaboration in these processes is crucial, and we will look at how you can enable teams in working together, introducing (as an example) <strong class="bold">Kanban</strong> and other frameworks.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Assessing demand as input for the architecture</li>
				<li>Designing and managing automation</li>
				<li>Implementing and managing configuration management</li>
				<li>Designing and managing integration</li>
				<li>Designing and managing collaboration</li>
			</ul>
			<h1 id="_idParaDest-28"><a id="_idTextAnchor028"/>Assessing demand as input for the architecture</h1>
			<p>You can't just start with a <a id="_idIndexMarker080"/>DevOps project—a business will need to know what they want to achieve before they launch projects. For that matter, there's no difference between a traditional Waterfall project and DevOps in an <em class="italic">Agile</em> way of working—you need to know where you're going. That's a very simple explanation of something that is called <strong class="bold">demand management</strong>. In this section, we <a id="_idIndexMarker081"/>will learn about demand as input for an architecture and how this leads to projects. </p>
			<p>Demand management can be defined as a process wherein an enterprise collects and prioritizes ideas to improve business outcomes. To be able to do that, the enterprise needs to assess the demands from the <em class="italic">outside</em>, meaning the market—in other words: <em class="italic">What do customers want?</em> But it also needs to assess whether the current portfolio is still up to date and that ongoing projects will still deliver the desired outcome. <strong class="bold">Portfolio management</strong> is a constant<a id="_idIndexMarker082"/> evaluation of market demands and ongoing activities. The modern challenge is that this <a id="_idIndexMarker083"/>evaluation has to be done at a higher speed than, let's say, a decade ago. Demands are changing fast and that does impact portfolio management drastically. </p>
			<p>Portfolio management<a id="_idIndexMarker084"/> includes at least the following components and processes (Romano, L., Grimaldi, R., &amp; Colasuonno, F. S. (2016). <em class="italic">Demand management as a critical success factor in portfolio management</em>. Paper presented at PMI® Global Congress 2016—EMEA, Barcelona, Spain. Newtown Square, PA: Project Management Institute):</p>
			<ul>
				<li>Definition of <strong class="bold">enterprise vision</strong> and <strong class="bold">strategy</strong>: This is part of the enterprise architecture and defines the strategy for the enterprise. </li>
				<li><strong class="bold">Demand management</strong>: The processes to<a id="_idIndexMarker085"/> collect ideas and identify opportunities for future products and services that are in the portfolio.</li>
				<li><strong class="bold">Ongoing components</strong>: Validation of<a id="_idIndexMarker086"/> whether existing products and services are still relevant for the enterprise and its customers.</li>
				<li><strong class="bold">Components assessment</strong>: This <a id="_idIndexMarker087"/>concerns the business case. What will be the investment in developing new products and services or changing existing components, and what is the expected revenue after the release or change?</li>
				<li><strong class="bold">Budgeting</strong>: The <a id="_idIndexMarker088"/>calculation of the required amount of resources needed to start development or realize upgrades of existing components in the portfolio.</li>
				<li><strong class="bold">Prioritization and selection</strong>: Development and changes need to be prioritized against the strategy of the enterprise. What is a must-do and what is a nice-to-have, and in what timeframe?</li>
				<li><strong class="bold">Portfolio governance and communication</strong>: This relates to who's responsible for what in demand and portfolio management. A recommended approach is to have a <strong class="bold">Responsible, Accountable, Consulted, and Informed</strong> (<strong class="bold">RACI</strong>) matrix for <a id="_idIndexMarker089"/>portfolio management: Who's responsible for which component? Who is accountable? Who should be consulted? Who should be informed?</li>
				<li><strong class="bold">Portfolio implementation</strong>: Adding the new or changed components to the portfolio in a structured acceptance<a id="_idIndexMarker090"/> process. Are the components documented in an appropriate way? Are the components signed off by the responsible owner? Have all acceptance criteria been met? </li>
				<li><strong class="bold">Portfolio reporting and review</strong>: The reports should reflect whether the portfolio is aligned with the business strategy. Is the portfolio still relevant to the strategy and the desired business outcomes? Or, does the enterprise need to change components, or maybe even consider adapting the whole portfolio (for instance, by divesting parts of the portfolio)?</li>
				<li><strong class="bold">Benefits realization</strong>: What has been the real value of the portfolio and the benefits to the enterprise and its business?</li>
			</ul>
			<p>The portfolio drives the <a id="_idIndexMarker091"/>projects in an enterprise, and also DevOps. The key to DevOps is automation so that projects can keep up with the speed of business changes and demands to <strong class="bold">Information Technology</strong> (<strong class="bold">IT</strong>). Crucial in automation is the creation of standardized building blocks. From the portfolio, we need to<a id="_idIndexMarker092"/> define these building blocks, called artifacts. <strong class="bold">Configuration management</strong> is about managing these artifacts so that they fit to the portfolio. To manage this, we need version control. We will discuss this in the <em class="italic">Implementing and managing configuration management</em> section.</p>
			<p>We now have the <a id="_idIndexMarker093"/>cycle of creating a portfolio to manage configuration items and enable automation. This is shown in the following screenshot: </p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/B17492_02_001.jpg" alt="Figure 2.1 – The configuration management cycle&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1 – The configuration management cycle</p>
			<p>The <strong class="bold">code repository</strong> holds the standard <a id="_idIndexMarker094"/>building blocks that <a id="_idIndexMarker095"/>are under version control. This is a requirement for enabling automation. In the next section, we will learn more about automation.</p>
			<h1 id="_idParaDest-29"><a id="_idTextAnchor029"/>Designing and managing automation</h1>
			<p>In this section, we will discuss <strong class="bold">automation</strong> for DevOps. For starters, automation is not only about tools, although <a id="_idIndexMarker096"/>we will discuss tools at the end of this section. The<a id="_idIndexMarker097"/> first questions that architects will need to answer regard <em class="italic">what</em> they need to automate and <em class="italic">why</em>. It's not about the tools but about the process. </p>
			<p>First, we need to answer the following question: <em class="italic">Why would we need automation?</em> The answer to that question is because of <em class="italic">standardization</em>. The reason for businesses to adapt DevOps is because they want to speed up delivery processes. The only way to do that is by standardization of building blocks, workflows, processes, and technologies. By implementing and adhering consistently to standards, companies will limit varieties in the delivery chain and can then start automating it. The big trick in automation is cutting down the waiting time. </p>
			<p>Before companies turned to DevOps, IT delivery was driven by waiting time. Developers had to wait for a server. In a worst-case scenario, the server needed to be ordered first, then installed and configured before it could be released to the development team. Then, developers could do their work and deliver the software to testers, but then had to wait for the testers to come back with results. Finally, the product was ready for release, but now the team would have to wait for the final <em class="italic">go</em>/<em class="italic">no-go</em> decision. </p>
			<p>In DevOps and automated <strong class="bold">CI/CD pipelines</strong> (where <strong class="bold">CD</strong> is an acronym for <strong class="bold">Continuous Delivery</strong>), the waiting time is strongly reduced by standardization, if done well. Architects will have to keep in mind that a<a id="_idIndexMarker098"/> pipeline consists of two major components: <em class="italic">software</em> and <em class="italic">infrastructure</em>, even though infrastructure has become code too. Typically, we will work in a cloud where we're not deploying physical <a id="_idIndexMarker099"/>servers, but <strong class="bold">Infrastructure as Code</strong> (<strong class="bold">IaC</strong>). </p>
			<p>Everything has<a id="_idIndexMarker100"/> become code. Automation of infrastructure, configuration, and<a id="_idIndexMarker101"/> deployment of software is the core of DevOps. Only by using automation can a company speed up delivery to periods of weeks, days, or even hours. </p>
			<p>The following screenshot shows the automation process for the CI/CD pipeline. There are two major <a id="_idIndexMarker102"/>components in<a id="_idIndexMarker103"/> this pipeline: the <strong class="bold">deployment pipeline</strong> and the <strong class="bold">infrastructure pipeline</strong>:</p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B17492_02_002.jpg" alt="Figure 2.2 – Deployment pipeline and infrastructure pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2 – Deployment pipeline and infrastructure pipeline</p>
			<p>The software is developed, tested, and deployed in the deployment pipeline. That software needs to land on infrastructure—for example, on a <strong class="bold">virtual machine</strong> (<strong class="bold">VM</strong>) in a public cloud such as <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>) or Azure. At a certain point, we will have to merge the <a id="_idIndexMarker104"/>software code with the infrastructure and configuration packages for that infrastructure. That whole package is tested, validated, and eventually pushed to the production stage.</p>
			<p class="callout-heading">Note</p>
			<p class="callout"><em class="italic">Figure 2.2</em> shows how important testing is. There are a couple of touchpoints that include testing procedures. In the <em class="italic">Designing and managing integration</em> section, we will explain more about the various tests in CI, such as static and dynamic analysis. <a href="B17492_03_ePub_RK.xhtml#_idTextAnchor040"><em class="italic">Chapter 3</em></a>, <em class="italic">Architecting for DevOps Quality</em> also has a section about executing tests. </p>
			<p>So, the <a id="_idIndexMarker105"/>deployment pipeline is used to convert code to a deployable application<a id="_idIndexMarker106"/> package, deploying the package, validating the package, and releasing the package to production.</p>
			<p>In the infrastructure pipeline, we provision the environments on which the application package can be deployed. In <em class="italic">Figure 2.2</em>, VMs are mentioned as an example, but infrastructure can also consist of <strong class="bold">containers</strong> or <strong class="bold">serverless components</strong>. In fact, native applications will use containers and serverless environments over VMs. </p>
			<h2 id="_idParaDest-30"><a id="_idTextAnchor030"/>Understanding pipeline components</h2>
			<p>Let's take a better look at the diagram shown in <em class="italic">Figure 2.2</em> and comment on the different components. The pipeline starts with <strong class="bold">version control</strong> and <strong class="bold">configuration items</strong>. In the next section, <em class="italic">Implementing and managing configuration management</em>, we will <a id="_idIndexMarker107"/>discuss this in more detail. </p>
			<p>The actual deployment starts with the package containing all the files that we need to run the application. This package can be deployed to an environment. <em class="italic">To any environment?</em> Well, that's the reason to abstract the deployment pipeline from the infrastructure. Ideally, we would like to be able to run code on different platforms, so we need to code in such a way that it can run on infrastructure in—for instance—AWS, Azure, or on a private stack, or even using different operating systems such as Windows or Linux distributions. </p>
			<p>If we have the package, we can start deployment: this is an automated series of tasks to deploy our application on a test or staging environment. Here, we can run automated simple tests—often referred to as smoke tests—to verify that the code is actually running. This simple test will not be <a id="_idIndexMarker108"/>sufficient to validate all the required functions and to detect bugs: this is done in the test phase. We will learn more about testing in the <em class="italic">Designing and managing integration</em> section.</p>
			<p>If all tests have been successfully executed, we have a validated application package that is ready to be pushed to the production stage. </p>
			<p>The following aspects need to be considered in terms of automation:</p>
			<ul>
				<li><strong class="bold">Code development</strong>: Automation<a id="_idIndexMarker109"/> in DevOps starts as soon as developers begin developing the code. The code needs to be "automation ready," meaning that from the moment the code is checked in, the actual build is triggered and runs through automated tests and code validation. Next, the code is compiled and brought under version control. Ultimately, when all tests have been executed and the full package is validated, it will be pushed to production. Developers need to take this automation sequence into account when they develop the code. After the code is deployed into production, it needs to be monitored. So, as soon as the package is "prepared" for go-live, scripts will be merged to the package to enable it to be monitored, and for logs to be collected. </li>
				<li><strong class="bold">Continuous testing</strong>: In DevOps cycles, software will constantly be evaluated and improved. That's what<a id="_idIndexMarker110"/> DevOps is about, after all: increasing agility and CD. Code will therefore be changed regularly. Every time code is changed, it needs to be tested and validated. That's where continuous testing comes in. Automated testing is used to track and predict issues in code changes, run multiple tests, and ultimately release approved automated builds. </li>
				<li><strong class="bold">Monitoring</strong>: It's a misunderstanding<a id="_idIndexMarker111"/> that monitoring is only needed when applications are pushed to production. Monitoring is relevant during the whole CI/CD life cycle and is a crucial component in automation. Monitoring is required to track events, identify causes for why code is malfunctioning, prioritize events, and proactively suggest actionable improvements. </li>
			</ul>
			<p>In short, by applying automation, we can bring release timelines down from months or weeks to just a few days, or even hours. This is only possible if we automate as much as we can. But automation does more than just speed up delivery; it also reduces the risk of manual errors and will enable companies to achieve more consistency and higher reliability  for the products they release. DevOps is not only about agility and speed but maybe even more about quality, by creating products with more accuracy in a repeatable process that can be delivered at a high pace. </p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor031"/>Choosing the DevOps automation toolset</h2>
			<p>Now, so far, we haven't talked about <strong class="bold">tooling</strong>. The selection <a id="_idIndexMarker112"/>of the right tools is indeed very important. The challenge, however, is that there are so many tools<a id="_idIndexMarker113"/> available on the market. One of the tasks of an architect would be to guide the selection. The first decision that an architect will have to make is whether they want the automation to be "single-stack" or whether to opt for multiple tools for different automation areas. </p>
			<p>Just look at the <strong class="bold">periodic table</strong> of <strong class="bold">DevOps tools</strong> (by <em class="italic">Digital.ai</em>) in the following screenshot. For each of the DevOps domains, there's a huge choice of toolsets. There are tools for managing the code, creating packages, automating the merging of pipelines, testing code, bug fixing, provisioning of infrastructure, managing infrastructure, and monitoring: </p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B17492_02_003.jpg" alt="Figure 2.3 – The idea of having a periodic table of DevOps tools&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3 – The idea of having a periodic table of DevOps tools</p>
			<p>Organizations will need<a id="_idIndexMarker114"/> tools to manage the DevOps process from end to end. There are tools that promise this end-to-end capability, but a set of tools is typically required, from the code build and commit to testing, deployment, and operations. One important thing to keep in mind is the level of integration between the tools. This depends not only on the source code that developers use, but also on the target platform. If the target platform is Azure, it makes perfect sense to use <strong class="bold">Azure DevOps</strong> and <strong class="bold">Azure Resource Manager</strong> (<strong class="bold">ARM</strong>) to deploy infrastructure on that platform. If the main platform is AWS, then tools such as Chef and Puppet, in combination with <strong class="bold">AWS CodeDeploy</strong>, are probably a better choice. </p>
			<p>Then again, there's a major shift going on from using VMs to containers and serverless technology. A lot of DevOps projects use containers to deploy code, by using Docker and Kubernetes as the container <em class="italic">orchestration platform</em>. Containers are much more agnostic to the underlying infrastructure or cloud platforms than VMs, but also here, platforms all have their own Kubernetes engines. Think of <strong class="bold">Azure Kubernetes Services</strong> (<strong class="bold">AKS</strong>) on Azure and <strong class="bold">Elastic Kubernetes Services</strong> (<strong class="bold">EKS</strong>) on AWS. </p>
			<p>One more important thing to consider is the type of tool that will be chosen. Open source tooling is very popular, but it's good to <a id="_idIndexMarker115"/>consider whether these tools meet the enterprise requirements. That differs per domain. Azure DevOps Pipelines and AWS CodePipeline are perceived as enterprise tools, but tools for version control (Git, GitHub, GitLab, <strong class="bold">Subversion</strong> (<strong class="bold">SVN</strong>), Cloud Foundry) are mainly open source. There can be good reasons <a id="_idIndexMarker116"/>for an enterprise to go for open source, as outlined here:</p>
			<ul>
				<li><strong class="bold">Community driven</strong>: Large communities constantly improving the software. Enterprises can benefit from this.</li>
				<li><strong class="bold">Cost-effective</strong>: There are typically licenses required for open source software, but looking at the total costs, open source is often a very good deal.</li>
				<li><strong class="bold">No risk of lock-in</strong>: This is becoming more and more important for organizations. They don't want to be completely locked into the solutions of one software provider or the ecosystem of that provider. Open source allows an organization a great amount of freedom.</li>
			</ul>
			<p>The negative side of open source that is often cited is that this software would be less secure and less stable than<a id="_idIndexMarker117"/> enterprise software. However, because the software is constantly reviewed and improved by a community, we find that open source software is just as secure and stable as non-open source software. </p>
			<h1 id="_idParaDest-32"><a id="_idTextAnchor032"/>Implementing and managing configuration management</h1>
			<p>In the previous section, we<a id="_idIndexMarker118"/> learned that automation starts with version<a id="_idIndexMarker119"/> control and configuration items that form an application package in an artifact's repository. In this section, we will study how we can manage these artifacts. </p>
			<p>Automation can only be done when <strong class="bold">building blocks</strong> (artifacts) and processes are standardized. <strong class="bold">Standardization</strong> requires three components, outlined as follows:</p>
			<ul>
				<li><strong class="bold">Portfolio</strong> and <strong class="bold">portfolio management</strong>: A portfolio is the translation of the business strategy and the products that a <a id="_idIndexMarker120"/>business delivers to its customers. Those products consist of several artifacts: product components and processes. So, a portfolio is at the strategic level of an enterprise, whereas products and artifacts sit at a tactical level. A portfolio is defined by the enterprise architecture, products, and artifacts that are managed at a business-unit and project level. In short, products can't exist without a definition in the portfolio, but a portfolio is not something that we can automate in projects and pipelines. We can automate artifacts and even products, as long they are standardized and managed through version control. </li>
				<li><strong class="bold">Version control</strong>: In the first<a id="_idIndexMarker121"/> section of this chapter, we <a id="_idIndexMarker122"/>learned that artifacts<a id="_idIndexMarker123"/> are derived from a portfolio. Artifacts need to be put under version control. A centralized version control repository is absolutely the number-one priority in CI/CD. This repository is a <strong class="bold">single source of truth</strong> (<strong class="bold">SSOT</strong>) for all <a id="_idIndexMarker124"/>configuration items that will be used for the development and building of the deployment packages. Creating packages with configuration items is done using the<a id="_idIndexMarker125"/> file versions in the <strong class="bold">version control system</strong> (<strong class="bold">VCS</strong>). Version control is mandatory to keep all configuration items consistent. A build produces a versioned package that can be retrieved from the repository.</li>
				<li><strong class="bold">Configuration items</strong>:  The term <em class="italic">configuration item</em> comes from the <strong class="bold">IT Infrastructure Library</strong> (<strong class="bold">ITIL</strong>), which is an IT library<a id="_idIndexMarker126"/> for IT service management. Configuration items <a id="_idIndexMarker127"/>are assets that need to be managed in order to deliver an IT service—basically, everything that is needed to execute a build and deliver an IT service. It includes building blocks for the application code, but also images for the operating systems, configurations for network settings, security packages, licenses, database settings, and test scripts. All these items need to be managed. Configuration items are defined by a <strong class="bold">unique identifier</strong> (<strong class="bold">UUID</strong>), the type of an item (hardware, software, script, license, and so on), a clear description, and the<a id="_idIndexMarker128"/> relationship that the item has with other items. To keep configuration items consistent, they need to be verified and validated at regular intervals or even in "real time." A best practice<a id="_idIndexMarker129"/> is to automate this, as we have already seen in the previous section. Agents will constantly monitor all assets in an environment<a id="_idIndexMarker130"/> and update the status of configuration items in the repository, commonly referred to as the <strong class="bold">Configuration Management Database</strong> (<strong class="bold">CMDB</strong>). </li>
			</ul>
			<p>How is this linked to the enterprise architecture and the portfolio? Let's take security as an example. From the enterprise strategy, the security standards must be defined—for instance—to which industry framework<a id="_idIndexMarker131"/> the enterprise should be compliant with. That is translated into security policies that need to be set in the<a id="_idIndexMarker132"/> security packages. These packages containing security policies and rules are known as configuration items. In the third part of this book, about <strong class="bold">DevSecOps</strong>, we will learn more about this.</p>
			<p>Let's make this a little bit more tangible. We have an application that runs on a VM and is attached to a database. The different configuration items could then be listed as follows:</p>
			<ul>
				<li>Application code</li>
				<li>VM template</li>
				<li>Operating system image</li>
				<li>Security settings for the application</li>
				<li>Security settings for the VM</li>
				<li>Access rules </li>
				<li>Database image</li>
				<li>Licenses for the operating system</li>
				<li>Licenses for database</li>
				<li>Network settings (<strong class="bold">Internet Protocol</strong> (<strong class="bold">IP</strong>) address allocation; network ports; routing)</li>
			</ul>
			<p>This list is by no means exhaustive—these are just examples. All these items need to be managed to keep them consistent. The CMDB enables verification and validation of all items. By applying version control, we make sure that new builds only use verified configuration items—for example, a certain version of an operating system or a specific version of the application code. </p>
			<p>A key takeaway here is that<a id="_idIndexMarker133"/> every configuration item is linked to the business strategy, portfolio, and enterprise architecture. </p>
			<p>In this section, we've<a id="_idIndexMarker134"/> learned what configuration items are and how we manage these items from a single repository (CMDB) so that they remain consistent. In the next section, we will see why version control and consistency are so important. </p>
			<h1 id="_idParaDest-33"><a id="_idTextAnchor033"/>Designing and managing integration</h1>
			<p>In this section, we will learn more about <strong class="bold">CI</strong>. First, we will look at the development and deployment of <strong class="bold">application code</strong>. Next, we will learn about<a id="_idIndexMarker135"/> the integration of <strong class="bold">code pipelines</strong> for <a id="_idIndexMarker136"/>applications and the infrastructure. Somewhere, the application code and the IaC<a id="_idIndexMarker137"/> have to merge together with specific configuration packages. Only then will we have a fully integrated model.</p>
			<p>Let's look at a definition of integration first. This refers to an automated series of tasks to version, compile, package, and <a id="_idIndexMarker138"/>publish an application. This includes testing, whereby <strong class="bold">unit tests</strong> are used to validate <a id="_idIndexMarker139"/>that existing code performs well without interruptions. <strong class="bold">Integration tests</strong> run to ensure no integration issues occur. Additional <a id="_idIndexMarker140"/>checks, such as <strong class="bold">static code analysis</strong>, can be included as well to increase quality and feedback.</p>
			<p>The CI/CD pipeline—and with that, the automation—starts with a merge of <strong class="bold">source code</strong>. This code is transformed to a build, as a product of an executable package that integrates with other executable packages. Code is pulled from the repository, analyzed, and committed to a build server, where a series of automated tasks will be executed to push to code into production, including the merge with the infrastructure. </p>
			<p>As stated, testing and validation of the code is a crucial part of the build. Code is tested to verify that all risks are identified and mitigated so that the build meets enterprise compliance, defined in the enterprise architecture and, subsequently, the security policies. The next step is that the code is deployed to a development or test environment. As we have seen, this is preferably done in an automated way. The code is brought under version control and committed back to the repository. </p>
			<p>Although it requires <a id="_idIndexMarker141"/>a lot of manual tasks, it's important to maintain a wiki—a collection <a id="_idIndexMarker142"/>of documents that are maintained by the developers—as part of the version control, containing the release notes with the build number, and issues that have been encountered and mitigated after the tests. The outcomes of the tests should be listed in the release notes. </p>
			<p>To summarize, the CI process consists of the following steps:</p>
			<ol>
				<li><strong class="bold">Detecting new committed code</strong>: The source code in the application repository is forked, which means that the<a id="_idIndexMarker143"/> code is committed to the automated pipeline. This starts a sequence of build tasks. The next step is to validate the code. This is the <strong class="bold">pull process</strong>, whereby the <a id="_idIndexMarker144"/>code is checked out and placed on a <strong class="bold">build environment</strong>—in most cases, a <strong class="bold">development server</strong>. Static analysis is executed, which is a test <a id="_idIndexMarker145"/>without actuall<a id="_idTextAnchor034"/>y running the code. This step is also referred to as <strong class="bold">linting</strong>. </li>
				<li><strong class="bold">Source code quality analysis</strong>: The <a id="_idIndexMarker146"/>code is looked at to check it meets security and compliance requirements.. Part of this quality analysis is also a scan of the software used: is it licensed and is it compliant with the enterprise standards and software portfolio?  </li>
				<li><strong class="bold">Build</strong>: This is the step <a id="_idIndexMarker147"/>where code is compiled and packages are assembled. The actual build begins with pulling the executable packages from the artifact's repository. This repository holds all the standards and policies that an enterprise uses to deploy code. The source code is merged with these packages. As an example, these packages contain the images for the operating systems, but also the security policies, such as hardening templates to protect systems from attacks. Packages for infrastructure are often stored in a repository that holds the images: the trusted image registry. These images are used to assemble an executable package to final deployment.  </li>
				<li><strong class="bold">Unit tests</strong>: Unit tests ensure that the<a id="_idIndexMarker148"/> executable package runs without issues.</li>
				<li><strong class="bold">Integration tests</strong>: In the code<a id="_idIndexMarker149"/> validation, we executed a <strong class="bold">static analysis</strong>; now, a dynamic analysis is performed. The assembled package is tested to verify that it runs without errors, and test reports are generated. </li>
				<li><strong class="bold">Creating a runtime executable file</strong>: We're<a id="_idIndexMarker150"/> getting close to the final steps, which is the release of the code to the next phase. Before we can do that, runtime executables are generated so that the code can be started in a specific runtime order or routine. These runtime executables are often stored in a separate runtime library.    </li>
				<li><strong class="bold">Build notification</strong>: The last step<a id="_idIndexMarker151"/> before deployment is notification that the<a id="_idIndexMarker152"/> build is ready, meaning the code and the executable packages have passed all tests. The package is ready for<a id="_idIndexMarker153"/> deployment. If the integration is stopped at any time during the automation sequence, developers will be notified so that they can fix any issues and then recommit the code. Every step, as described, is then repeated.</li>
			</ol>
			<p>The integration process is shown in the following screenshot: </p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B17492_02_004.jpg" alt="Figure 2.4 – The CI process&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4 – The CI process</p>
			<p><em class="italic">Figure 2.4</em> shows that the output of the CI path is directed toward something called a <strong class="bold">promotion path</strong>. Applications are<a id="_idIndexMarker154"/> seldom pushed directly to production <a id="_idIndexMarker155"/>environments. The application release is usually "staged" in a few steps <a id="_idIndexMarker156"/>before it actually lands on production. The next section explains this promotion path. </p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor035"/>Understanding the promotion path</h2>
			<p>Although eventually code will be<a id="_idIndexMarker157"/> pushed to production, it's good practice to run the deployment to a development , staging, or pre-production environment. This is called a <a id="_idIndexMarker158"/>promotion path, which involves <strong class="bold">development</strong>, <strong class="bold">test</strong>, <strong class="bold">acceptance </strong>and <strong class="bold">production</strong> (<strong class="bold">DATP</strong>). In some cases, an acceptance environment is part of the promotion path. Acceptance environments sit between the test and production systems. </p>
			<p>This is shown in the following diagram: </p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B17492_02_005.jpg" alt="Figure 2.5 – A promotion path for IT systems&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5 – A promotion path for IT systems</p>
			<p>Acceptance environments are strongly recommended for business-critical applications. In fact, enterprises use <a id="_idIndexMarker159"/>acceptance environments often as the <strong class="bold">disaster recovery</strong> (<strong class="bold">DR</strong>) system when production fails. Production is then switched to acceptance, with a minimal loss of data. Acceptance environments should be identical to production systems in terms of configuration.  </p>
			<p>A promotion path<a id="_idIndexMarker160"/> containing DTAP is recommended as a minimal setup. The executable package is the outcome of the integration pipeline and is now pushed to development, pre-production, and testing or staging. Here, the packages are tested to verify that they will run without issues in production. The application needs to be robust, stable, secured, and compliant at a minimum. Also, cloud-native features such as scalability—and maybe even self-healing—are tested, with load and performance tests. Ultimately, the final product will go live against the specifications that were originally collected from the business. We are then back at the beginning of the cycle, which started with demand management. </p>
			<p>Why would we run such an intensive process to release code? Because there are several benefits a business could gain from this. The five main benefits are listed here:</p>
			<ul>
				<li>Code changes are more controlled and can be smaller since this is a continuous process where improvements can be made in small iterations of that code.</li>
				<li>Because of automation, code release can be done much faster.</li>
				<li>Because of automation, the costs for development will be significantly lower—this will be reflected in the overall costs to the enterprise.</li>
				<li>Automated tests will lead to more reliable tests and test results. </li>
				<li>Management and updates are easy.</li>
			</ul>
			<p>In this section, we learned about CI: what the requirements are, how we initiate integration from automated pipelines, and why enterprises should implement CI in projects. There's one more thing we need to discuss as part of the full DevOps setup, and that is collaboration. DevOps is not only about technology and tools; it's a <em class="italic">mindset</em>. We will discuss collaboration in the last section of this chapter. </p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor036"/>Designing and managing collaboration</h1>
			<p>To put it very simply, DevOps only <a id="_idIndexMarker161"/>succeeds if teams work together. Teams can collaborate if they use the same processes and, indeed, the same toolsets. In DevOps, <strong class="bold">collaboration</strong> ties processes and technology together to enable teams to join forces. </p>
			<p>In <a href="B17492_01_ePub_RK.xhtml#_idTextAnchor013"><em class="italic">Chapter 1</em></a>, <em class="italic">Defining the Reference Architecture for Enterprise DevOps</em>, we saw that a lot of enterprises have outsourced major parts of their IT. This makes collaboration hard, every now and then. DevOps requires that teams <a id="_idIndexMarker162"/>carrying out operations and that are part of a certain sourcing partner or vendor work together with developers that come from a different company. It's up to the enterprise to set the scene, engagement rules, and co-working principles. The ownership of that can only be at an enterprise level. </p>
			<p>In enterprises, it's very rare that only one team is completely responsible for an application. Often, there are more teams involved, and—even—more than one supplier. DevOps, however, assumes that there's one DevOps team responsible end to end. The main goal of this is to reduce overhead and manage waiting time in handovers between teams. This requires constant communication between team members since these members will have different skills and tasks. </p>
			<p>Don't be fooled by the <strong class="bold">T-shaped</strong> dogma. Yes—a cloud engineer will probably know how to attach storage to a VM, and maybe they also know how to build network configurations in Azure or AWS, yet configuring databases or a firewall are specific tasks that typically require a database or a network engineer. A DevOps team will have to onboard these <strong class="bold">subject-matter experts</strong> (<strong class="bold">SMEs</strong>), but they<a id="_idIndexMarker163"/> work together in one team and not in separate silos. </p>
			<p>The next thing is that an enterprise needs to enable collaboration. This is the task of a project leader or, in an agile way of working, a <em class="italic">Scrum</em> master. One of their key tasks is to "remove barriers" and help team members to achieve a common goal. Setting the objectives and priorities for the team are step one.</p>
			<p>How do we define the objectives and the priorities? This is the task of an architect. Their responsibility is to lay out a clear roadmap, pointing out how to reach each objective, and in what order. That roadmap is derived from the <a id="_idIndexMarker164"/>end-state architecture, also referred to as "<strong class="bold">soll</strong>", or <strong class="bold">target architecture</strong>. This <a id="_idIndexMarker165"/>architecture defines how the environment should eventually look, containing the application architecture, the required infrastructure components, and the <strong class="bold">application programming interfaces</strong> (<strong class="bold">APIs</strong>).  </p>
			<p>An example<a id="_idIndexMarker166"/> of a roadmap is shown<a id="_idIndexMarker167"/> in the following diagram. The example has been taken from <em class="italic">ProductPlan</em> and shows perfectly how a simple roadmap helps create visibility <a id="_idIndexMarker168"/>and clarity in tasks that have been foreseen: </p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B17492_02_006.jpg" alt="Figure 2.6 – Example of an architecture roadmap by ProductPlan&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.6 – Example of an architecture roadmap by ProductPlan</p>
			<p>Finally, the roadmap shows the roles and responsibilities of team members: who needs to do what, and how much time will it consume? A good way to create clear overviews of roadmaps, timelines, and <a id="_idIndexMarker169"/>tasks is to use <strong class="bold">Kanban boards</strong>. Kanban, originally a Japanese invention that was first introduced at Toyota, shows when certain components are needed in the production process. Both Scrum and Kanban are tools to implement Agile as a development method for applications. </p>
			<p>To a lot of enterprises, this still sounds very new, but the truth is that they probably have been working with a similar planning methodology for years, since that's what Kanban does in essence. It helps teams to plan complex projects in achievable chunks and, with that, improve the quality of the end product. </p>
			<p>In the next chapter, <em class="italic">Architecting for DevOps Quality</em>, we will learn more about implementing quality measures.  </p>
			<h1 id="_idParaDest-36"><a id="_idTextAnchor037"/>Summary</h1>
			<p>This chapter started with an overview of demand management as input for an architecture. We learned that assessing business demand is the key driver for portfolios. In turn, a portfolio defines the artifacts and building blocks that we use to develop products, services, and, as such, applications. Since demand is changing fast, enterprises will need to speed up deployment processes. This can be achieved by automating as much as possible. Automation is done through pipelines, and in this chapter, we've learned what the different components are in architecting both application deployment and infrastructure pipelines. </p>
			<p>In the last section, we discussed collaboration that is crucial in CI/CD, using DevOps. Teams will be formed by engineers with different skill sets, and they even may be hired from different companies, something we see frequently at large enterprises that have outsourced their IT. Therefore, enterprises will need to encourage strong collaboration, based on a clear roadmap with clear objectives and planning that states exactly who is responsible for which task. For this, Kanban boards are a good approach. </p>
			<p>In the next chapter, we will dive deeper into the quality measures of DevOps projects and learn about <strong class="bold">Definition of Ready</strong> (<strong class="bold">DoR</strong>) and <strong class="bold">Definition of Done</strong> (<strong class="bold">DoD</strong>), as well as looking at how to execute tests for quality assurance.</p>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor038"/>Questions</h1>
			<ol>
				<li value="1">Demand management is a process of collecting ideas and identifying opportunities for future products and services that are in a portfolio. Rate the following statement true or false: the business case is not relevant for demand management.</li>
				<li>One of the first tests is a test without actually running the code. What do we call this test?</li>
				<li>The promotion path contains various stages. What are these stages, and in what order are they set?</li>
			</ol>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor039"/>Further reading</h1>
			<ul>
				<li><em class="italic">Demand management as a critical success factor in portfolio management</em>. Paper presented at PMI® Global Congress 2016 by Romano, L., Grimaldi, R., &amp; Colasuonno, F. S. (2016):<p>https://www.pmi.org/learning/library/demand-management-success-factor-<a href="http://portfolio-10189">portfolio-10189</a></p></li>
				<li><em class="italic">Kanban and Scrum – Making the Most of Both</em>, Henrik Kniberg and Mattias Skarin:  <p><a href="https://www.infoq.com/minibooks/kanban-scrum-minibook/">https://www.infoq.com/minibooks/kanban-scrum-minibook/</a></p></li>
			</ul>
		</div>
	</body></html>