- en: '*Chapter 9*: Integrating AIOps in DevOps'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've looked at automating development from a DevOps perspective and
    have automated operations. The next step is **artificial intelligence** (**AI**)-enabled
    DevOps. DevOps engineers manage multiple libraries and various pipelines. To speed
    up digital transformation, it's crucial that issues are detected and remediated
    fast. AI can also be of great added value in these DevOps processes. In this chapter,
    you will learn how to implement AI-enabled DevOps and enable rapid innovation.
  prefs: []
  type: TYPE_NORMAL
- en: After completing this chapter, you will have a good understanding of the various
    steps that need to be taken to implement and integrate AI-driven pipelines for
    development and deployment. You will be introduced to some major tools and will
    learn the requirements to implement these as part of the innovation of digitally
    transforming enterprises.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing AI-enabled DevOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling rapid innovation for digital transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring pipelines with AIOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assessing the enterprise readiness of AI-enabled DevOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing AI-enabled DevOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we studied the AIOps platform, concluding that it will
    help operators in getting rid of tedious, repetitive tasks, detecting and solving
    issues faster, and enabling more stable systems. Stability and resilience are
    still the key aspects operators strive for with IT systems, yet new features and
    changes to the systems are being developed and launched at an increasing speed.
    If AI can help operations, it can also help development. This section will explain
    why AI-enabled DevOps will help in creating better systems at a higher velocity.
  prefs: []
  type: TYPE_NORMAL
- en: 'AI can help developers monitor and detect issues in their builds faster than
    if this were done only manually or even in an automated process, without the power
    of AI. With AI, it''s possible to continuously monitor code changes, compare these
    to other code building blocks, and swiftly detect issues. But AI will also enable
    predictive mitigations: it will learn how certain code changes may impact systems.
    Given the fact that new features and thus new code are developed at an ever-increasing
    pace in systems that become more complex, AI-enabled DevOps is a solution to ensure
    the stability of these systems. AI will help in managing various code libraries,
    keeping track of configurations and deployment scripts, and avoiding unexpected
    application behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: How does it work? Well, in the same way as AIOps, which we discussed in [*Chapter
    8*](B17492_08_ePub_RK.xhtml#_idTextAnchor095), *Architecting AIOps.* AI-enabled
    DevOps will learn from patterns in the DevOps cycle. To do that, it needs data.
    It will use the code data that is stored in the code repository and the process
    data that is used to build the CI/CD pipeline, and then run the various test and
    deployment procedures. Additionally, it will learn from historical data; that
    is, issues and events that have occurred and the way these have been solved. Through
    analytics and machine learning, AI will learn how to optimize code builds, testing,
    and deployments.
  prefs: []
  type: TYPE_NORMAL
- en: 'DevOps has certainly evolved over the last decade, but developers and operators
    still face some tedious tasks in coding, testing, and deploying new features to
    systems. A lot of the work is still very manual and requires several steps in
    testing and code reviews. Since code is becoming increasingly complex and systems
    are becoming more entangled in various platforms, issues may not always be detected
    in time or at all. To save time, code review is sometimes done through sampling:
    some code is randomly picked, and that specific piece of code is reviewed, leaving
    no guarantee that the rest of the code is OK. So, when is code OK? Often, the
    focus is on removing empty lines or obsolete spaces, but these things can easily
    be solved with formatting tools. Obviously, bugs need to be removed, but code
    also needs to be optimized to perform well. All this needs to be done while production
    is kept running and stable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'AI, **machine learning** (**ML**), and deep learning can help overcome these
    issues. Then, there''s AI-enabled DevOps. This requires the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: Access and control over the source repository
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data lake and data marts for modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI-integrated pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The basic process contains the steps shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Concepts of an AI-integrated pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17492_09_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.1 – Concepts of an AI-integrated pipeline
  prefs: []
  type: TYPE_NORMAL
- en: We will elaborate on these steps in the *Monitoring pipelines with AIOps* section,
    where we will discuss various tools and technologies for monitoring processes
    in DevOps pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: AI is a new domain in DevOps. It's an innovation that can speed up the digital
    transformation of enterprises. Before we dive into the details of injecting AI
    into DevOps pipelines, it's good to get a better understanding of the innovation
    cycle and the rationale for including AI and ML. We will discuss this in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling rapid innovation in digital transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The majority of modern enterprises are well underway in transforming their business
    to make it more digital native. We talked about this extensively in the first
    two chapters of this book. Customers continuously demand new features, and they
    want these features to be delivered almost instantly. To control this process,
    enterprises need to develop an innovation strategy catering for rapid innovation.
    An innovation strategy can be depicted as a pyramid, where AI-driven innovation
    is at the very peak of this pyramid.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Pyramid of AI-enabled innovation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17492_09_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.2 – Pyramid of AI-enabled innovation
  prefs: []
  type: TYPE_NORMAL
- en: Enterprises do not get to the top of the pyramid in one go; they usually start
    at the bottom, where innovation is driven by cost savings. From there, they need
    to develop the next steps, resulting in rapid innovation using AI and ML.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first steps typically involve ways to find a budget, which they do by implementing
    technology that can lead to drastic cost savings, for instance, by moving to another
    platform. That choice of platform is crucial: enterprises will want to make sure
    that they make a sustainable choice for the future and have planned such innovations
    to drive digital transformation. Such a platform can be a public cloud, offering
    not just hosting services, but allowing us to use cloud-native technology and
    integrate that with DevOps, AI, and ML.'
  prefs: []
  type: TYPE_NORMAL
- en: An even more important step is to collect and leverage the use of data. AI and
    ML need data, and it needs to be aggregated for analytics purposes and for training
    data models. Data is likely the most valuable asset in any enterprise, so this
    step will take a lot of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Aggregating data doesn''t mean that every single piece of data needs to be
    in one data lake. There will be a need for different datasets, but architects
    will have to think of efficient ways to create these datasets without the usual
    silos in an enterprise. And then there''s data security: who is authorized to
    see what data and for what reason? How do you prevent data from getting somewhere
    it shouldn''t be? Data **identity and access management** (**IAM**) and **data
    loss prevention** (**DLP**) are important topics. For this, an enterprise will
    need to have a consistent system for data classification.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can bring AI and ML to the table. Data models need to be trained, deployed,
    and integrated with the CI/CD pipelines to enhance coding and application development.
    The pipelines will have to be integrated with the data analytics models and AI
    services from the platform of choice. The end of this journey might be an AI-controlled
    CI/CD pipeline. That's the topic of the next section, where we will have a look
    at some AI-driven tools in the major public clouds; that is, Google Cloud Platform,
    AWS, and Azure.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring pipelines with AIOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will study AI-driven technology that will help developers
    in monitoring and improving their CI/CD pipelines. Let''s recap on the principle
    of a pipeline first. A pipeline should be seen as a workflow: it guides code through
    a process where it''s tested and eventually deployed to a platform. Following
    this process, code will be pushed to different levels in the promotion path: development,
    testing, acceptance, and production. This process can be automated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the start of this process, and thus the pipeline, there is a repository
    where the various components of systems are stored. Since everything is code,
    the repository will hold code for applications, infrastructure components, configuration
    templates, and scripts to launch APIs. While building a system through a pipeline,
    DevOps software will make sure that the appropriate components are pulled from
    the repository and compiled into packages that can be deployed. A common way to
    do this is by using containers, such as Docker images that have been orchestrated
    via Kubernetes. Containers are also very suitable for injecting AI and ML into
    pipelines. The following diagram shows the basic functionality of Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Functionality of Kubernetes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17492_09_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.3 – Functionality of Kubernetes
  prefs: []
  type: TYPE_NORMAL
- en: From the DevOps CI/CD pipeline, Kubernetes can be instructed on how components
    must be pushed to the target platform using containers. To enable this, we can
    use the Kubernetes `kubectl` command-line tool. It runs commands against Kubernetes
    clusters and tells these clusters how to deploy application code while monitoring
    the cluster's resources and hosts.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Kubeflow by Google
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: According to their own documentation, Kubeflow is the ML toolkit for Kubernetes.
    It allows us to implement and monitor complex workflows and application development
    processes in pipelines that are running on Kubernetes and using ML on any cloud,
    such as AWS, Azure, and Google Cloud, just to name a few.
  prefs: []
  type: TYPE_NORMAL
- en: In-depth monitoring and analytics for all the components in the pipelines are
    highly valued features of Kubeflow. Originally, TensorFlow and PyTorch were used
    to train the data models. In the words of Kubeflow itself, it takes care of all
    the *boring stuff* by using ML, so that developers can concentrate on the new
    features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The high-level architecture of Kubeflow is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Conceptual architecture of Kubeflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17492_09_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.4 – Conceptual architecture of Kubeflow
  prefs: []
  type: TYPE_NORMAL
- en: ML is used to load vast amounts of data, verify that data, process it, and,
    by doing that, train models so that they learn from this data. However, developers
    want to be able to do this at scale. An example could be image recognition. You
    can train ML models to recognize images, a feature that is becoming increasingly
    popular in diagnostic imaging at hospitals. Using AI and ML clinical images from;
    for example, CT scans can already be valued, denoised, and have possible focus
    areas highlighted to support doctors in getting a more precise diagnosis faster.
    To run this type of model at scale, using containers is a viable option. This
    is what Kubeflow does.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: On [https://www.kubeflow.org/docs/examples/](https://www.kubeflow.org/docs/examples/),
    you can find tutorials and examples of Kubeflow use cases, including a sample
    for image recognition and a tutorial on how to use a Jupyter Notebook on a Kubeflow
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of Kubeflow shows that AI-enabled DevOps requires a number
    of components and that different tools need to be integrated. Some of these tools
    are platform native, such as CodeGuru by AWS and MLOps in Azure. We will briefly
    evaluate these in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing CodeGuru by AWS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are more developments underway, proving that this is a growing market.
    As an example, AWS announced the introduction of ML in DevOps in May 2021, using
    CodeGuru.
  prefs: []
  type: TYPE_NORMAL
- en: CodeGuru Reviewer is a tool that uses ML to detect issues and bugs in code,
    but also vulnerabilities in security policies that have been applied to code.
    It also provides recommendations to improve the code, either by solving bugs or
    suggesting enhancements to be made to the code.
  prefs: []
  type: TYPE_NORMAL
- en: A second component of CodeGuru is CodeGuru Profiler. Once the code review has
    been completed, Profiler validates the runtime of the code, identifying and removing
    inefficiencies in the code and, with that, improving the performance of the application.
    AWS claims that it also helps in decreasing compute costs, since Profiler also
    checks the code against the resources that it calls during the runtime process.
    By using ML, it can do this proactively, since it learns how code can be optimized,
    from comparing new code to existing code patterns. According to the AWS documentation,
    CodeGuru has already reviewed over 200 million lines of code since its launch.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the architecture of CodeGuru using CodeGuru Reviewer
    and CodeGuru Profiler:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – CodeGuru and CodeGuru Profiler by AWS'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17492_09_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.5 – CodeGuru and CodeGuru Profiler by AWS
  prefs: []
  type: TYPE_NORMAL
- en: The next step in this domain is Amazon DevOps Guru. DevOps Guru works more on
    the infrastructure level of DevOps. It runs pre-trained ML models that analyze
    system logs and metrics against operational baselines to detect anomalies in infrastructure
    components. Because it uses deep learning, the model is trained and enhanced continuously.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing MLOps in Azure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All major cloud providers have AI-driven solutions. The last one we will discuss
    briefly is MLOps in Azure. The basic principle is the same as with CodeGuru: MLOps
    will pull code from the repository, which is usually integrated with Azure DevOps.
    The following diagram shows the MLOps architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Architecture of MLOps in Microsoft Azure'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17492_09_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.6 – Architecture of MLOps in Microsoft Azure
  prefs: []
  type: TYPE_NORMAL
- en: MLOps performs various tests on the committed code. Although the outcomes will
    be comparable to other AI-enabled tools, MLOps works a bit differently, using
    an Azure ML pipeline where the models are trained. However, MLOps also works on
    the foundation of containerization with **Azure Kubernetes Services** (**AKS**)
    and **Azure Container Instances** (**ACI**).
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, using AI-enabled DevOps will result in you being able to do the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify missing code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detect badly written code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detect unnecessary code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detect expected and/or required but missing dependencies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform configuration checks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make recommendations for improvements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trigger automated actions for improvements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI-enabled DevOps is a fast-growing market, so besides the native services of
    major cloud providers such as CodeGuru and Kubeflow, which were launched by Google,
    there are a lot of emerging tools from startup companies that have been launched
    over the past few years. Examples include Pipeline.ai, Enterprise AI by DataRobot,
    Hydrosphere.io, and Xpanse AI. The first three focus more on ML-driven pipelines,
    while Xpanse AI is an environment for creating data models using AI.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we're discussing DevOps pipelines that have been enhanced with
    AI and ML. To get started with data modeling and data analytics, you will also
    need tools to enable them. Popular tools in this field include Databricks, MapR,
    Cloudera, and RapidMiner.
  prefs: []
  type: TYPE_NORMAL
- en: To conclude, it makes sense for enterprises to invest in AI-driven DevOps, enabling
    rapid innovation and speeding up digital transformation. So, the first question
    that needs to be answered is, when is the enterprise ready for this paradigm shift?
    We will briefly discuss this in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Assessing the enterprise readiness of AI-enabled DevOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've learned that digital transformation is a process. It doesn't come
    in one go; the enterprise needs to be prepared for this. It includes adopting
    cloud platforms and cloud-native technology. Enterprises will have legacy systems
    and likely a lot of data sitting in different silos, leaving the enterprise with
    the challenge that this data is used in an optimized way. It's a misperception
    to think that AI-enabled tools and data science can solve this issue from the
    beginning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The enterprise will need to have a complete overview of all its assets, but
    also its skills and capabilities. First, data specialists will need to assess
    the locations, formats, and usability of data sources. The data scientists then
    will have to design data models. They can''t do this in isolation: they will have
    to collaborate with DevOps engineers and the application owners to agree on things
    such as version control, model training, and testing.'
  prefs: []
  type: TYPE_NORMAL
- en: The agreed-upon data models can then be integrated into DevOps pipelines. In
    the previous section, we learned that tools will have to be selected and integrated
    with the CI/CD tooling, for example, by using containers and container orchestration
    platforms such as Kubernetes. Engineers specialized in ML can help in implementing,
    tracking, and training these data models by leveraging the benefits of AI.
  prefs: []
  type: TYPE_NORMAL
- en: That's not all. Processes – we called these engagement processes – such as incident,
    problem, and change management need to be aligned with the new development and
    deployment setup. Service managers will need to understand the new metrics that
    come with ML- and AI-enabled platforms. What needs to be done in the case of alerts
    or recommendations coming from the platforms? What is monitored and for what reason?
    Can tasks be automated even further, saving time and thus costs?
  prefs: []
  type: TYPE_NORMAL
- en: However, the most important question is, what recommendations need to be actioned
    to improve development and speed up releases? AI can help, but it needs time to
    *learn* the enterprise. Trained and skilled staff are required to help AI become
    familiar with the enterprise. It isn't magic.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the enterprise needs to have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Full visibility of all the assets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Full visibility of all the data sources and how these are related to business
    processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engagement processes that are implemented throughout the enterprise for consistency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trained and skilled staff, such as DevOps engineers, data scientists, and AI/ML
    engineers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An innovation roadmap with realistic timelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The aforementioned roadmap may lead to a distant horizon where operations are
    completely automated by means of AI and ML and no manual intervention is required
    anymore. That's where NoOps comes in. In [*Chapter 10*](B17492_10_ePub_RK.xhtml#_idTextAnchor122),
    *Making the Final Step to NoOps*, we will introduce this concept.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to integrate AI and ML into our DevOps pipelines.
    We discussed the basic requirements and steps for implementing AI-enabled DevOps,
    starting with access to source repositories, creating data lakes, initiating and
    training data models, and follow-up recommendations and actions. We also learned
    that AI-enabled DevOps is a stage in digital transformation, but that enterprises
    need to set out a roadmap that eventually allows them to integrate AI and ML into
    their development and deployment processes. AI-driven development and operations
    are at the peak of innovation in digital transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we introduced some tools that will help us in implementing AI-enabled
    DevOps. We learned that it's a fast-growing market where major cloud providers
    try to integrate their native DevOps tools with AI and ML. Examples include Kubeflow
    by Google, CodeGuru by AWS, and MLOps by Microsoft Azure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we discussed the readiness assessment for enterprises that want to
    implement AI-enabled DevOps. It''s crucial to develop a comprehensive roadmap,
    including the different steps and a realistic timeline. The end of that roadmap
    might be a fully automated pipeline orchestration without any manual intervention:
    NoOps. This is the topic of the next chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We introduced the innovation pyramid for digital transformation. What is the
    base platform of this pyramid?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Integrating AI into DevOps pipelines is typically done through containerization.
    We discussed a container orchestration tool that allows us to agnostically deploy
    containers to various platforms. What is this tool called?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name three possible outcomes/results of AI-enabled DevOps, specifically for
    improving code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Pragmatic Enterprise Architecture*, by James V. Luisi, 2014'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Documentation on Kubeflow: [https://www.kubeflow.org/](https://www.kubeflow.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Blog about the introduction of CodeGuru, by AWS: [https://www.allthingsdistributed.com/2021/05/devops-powered-by-machine-learning.html](https://www.allthingsdistributed.com/2021/05/devops-powered-by-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Blog on MLOps in Microsoft Azure, by Lee Stott: [https://techcommunity.microsoft.com/t5/educator-developer-blog/machine-learning-devops-mlops-with-azure-ml/ba-p/742150](https://techcommunity.microsoft.com/t5/educator-developer-blog/machine-learning-devops-mlops-with-azure-ml/ba-p/742150)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
