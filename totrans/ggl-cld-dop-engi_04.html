<html><head></head><body>
		<div id="_idContainer035">
			<h1 id="_idParaDest-65"><em class="italic"><a id="_idTextAnchor064"/>Chapter 3</em>: Understanding Monitoring and Alerting to Target Reliability</h1>
			<p><strong class="bold">Reliability</strong> is the most critical feature of a service or a system. <strong class="bold">Site Reliability Engineering</strong> (<strong class="bold">SRE</strong>) prescribes specific technical tools or practices that help measure characteristics to define and track reliability, such as <strong class="bold">SLAs</strong>, <strong class="bold">SLOs</strong>, <strong class="bold">SLIs</strong>, and <strong class="bold">Error</strong> <strong class="bold">Budgets</strong>. <a href="B15587_02_Final_ASB_ePub.xhtml#_idTextAnchor038"><em class="italic">Chapter 2</em></a>, <em class="italic">SRE Technical Practices â€“ Deep Dive</em>, took a deep dive into these SRE technical practices across multiple topics, including a blueprint for a well-defined SLA, the need for SLOs to achieve SLAs, the guidelines for setting SLOs, the need for SLIs to achieve SLOs, the different types of SLIs based on user journey categorization, different sources to measure SLIs, the importance of error budgets, and how to set error budgets to make a service reliable.</p>
			<p>SLAs are external promises made to the customer, while SLOs are internal promises that need to be met so that SLAs are not violated. This raises a raft of important questions:</p>
			<ul>
				<li>How to observe SLAs for a service so that user or customer expectations are met</li>
				<li>How to observe SLOs for SLAs so that the service is reliable, and SLAs are met</li>
				<li>How to observe SLIs for SLOs so that the service is reliable, and SLAs are met</li>
			</ul>
			<p>The preceding questions are critical because it not only has an impact on a user's expectations regarding the service, but also leads to an imbalance between development velocity to deliver new features versus system reliability. This ultimately impacts the promised SLAs, leading to financial or loyalty repercussions. So, in simple terms, the main goal is to identify how to track SRE technical practices to target system reliability.</p>
			<p>To track SRE technical practices, three fundamental concepts are required: <strong class="bold">Monitoring</strong>, <strong class="bold">Alerting</strong>, and <strong class="bold">Time Series</strong>. Monitoring is the process of monitoring key indicators that represent system reliability. Alerting is the process of alerting or reporting when the key indicators monitored fall below an acceptable threshold or condition. Monitoring and alerting are configured as a function of time. This means that the data needs to be collected at successive, equally spaced points in time representing a sequence of discrete-time data. This sequence of discrete-time data is also known as a time series.</p>
			<p>In this chapter, we're going to explore the following topics and their role in relation to target system reliability:</p>
			<ul>
				<li><strong class="bold">Monitoring</strong>: Feedback loop, monitoring types, and golden signals?</li>
				<li><strong class="bold">Alerting</strong>: Key attributes and approaches for an alerting strategy?</li>
				<li><strong class="bold">Time series</strong>: Structure, cardinality, and metric types?</li>
			</ul>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor065"/>Understanding monitoring</h1>
			<p><strong class="bold">Monitoring</strong> is defined by <a id="_idIndexMarker318"/>Google SRE as the action of collecting, processing, aggregating, and displaying real-time quantitative data about a system, such as query counts and types, error counts and types, processing times, and server lifetimes.</p>
			<p>In simple terms, the essence of monitoring is to verify whether a service or an application is behaving as expected. Customers expect a service to be reliable and delivering the service to the customer is just the first step. But ensuring that the service is reliable should be the desired goal. To achieve this goal, it is important to explore key data, otherwise also known as metrics. Examples of some metrics can be tied to uptime, resource usage, network utilization, and application performance.</p>
			<p>Monitoring is the means of exploring metric data and providing a holistic view of a system's health, which is a reflection of its reliability. Apart from metric data, monitoring can include data from text-based logging, event-based logging, and distributed tracing. The next topic details how monitoring acts as a continuous feedback loop that is critical to continuously improving system reliability by providing constant feedback.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor066"/>Monitoring as a feedback loop</h2>
			<p>The ultimate goal is to <a id="_idIndexMarker319"/>build a reliable system. For a system to be reliable, the system needs to be continuously observed, in terms of understanding the system's internal states based on its external outputs. This process is <a id="_idIndexMarker320"/>known as <strong class="bold">o</strong><strong class="bold">bservability</strong>. </p>
			<p>Observability helps to identify performance bottlenecks or investigate why a request failed. But for a system to be observable, it is important to collect and track several sources of outputs related to the health of the application. These outputs give insights into the application's health and identify any outstanding problems. This is referred to as monitoring. So, monitoring provides inputs that help a system to be observable. In simple terms, <em class="italic">monitoring indicates when something is wrong, while observability helps to show why something went wrong</em>.</p>
			<p>Once an application is deployed, there are four primary areas across which the application needs to be inspected or otherwise monitored:</p>
			<ul>
				<li>Verify an application's performance against the application objectives and identify any deviations in performance by raising relevant questions.</li>
				<li>Analyze data collected by the application over a period of time.</li>
				<li>Alert key personnel when key issues are identified through insights or data analysis.</li>
				<li>Debug the captured information to understand the root cause of an identified problem.</li>
			</ul>
			<p>These areas or categories <a id="_idIndexMarker321"/>provide a <strong class="bold">continuous feedback loop</strong> as part of monitoring the system. This feedback helps to continuously improve the system by identifying issues, analyzing the <a id="_idIndexMarker322"/>root cause, and resolving the same. Each of the four categories mentioned is elaborated on in this chapter to provide further insights.</p>
			<p>One of the key aspects of monitoring is to raise relevant questions pertaining to the health of the system. This is covered as the next topic.</p>
			<h3>Raising relevant questions</h3>
			<p>It is important to raise relevant questions to <a id="_idIndexMarker323"/>monitor a system's health post-deployment. These questions provide feedback on how the system is performing. Here are some questions to define where monitoring the service/system can effectively provide a feedback loop:</p>
			<ul>
				<li>Is the size of the database growing faster than anticipated?</li>
				<li>Has the system slowed down after taking the latest software update of a specific system component?</li>
				<li>Can the use of new techniques aid <a id="_idIndexMarker324"/>system performance (such as the use of <strong class="bold">Memcached</strong> to improve caching performance)?</li>
				<li>What changes are required to ensure that the service/system can accept traffic from a new geographic location?</li>
				<li>Are traffic patterns <a id="_idIndexMarker325"/>pointing to a potential hack of the service/system?</li>
			</ul>
			<p>The key to answering the preceding questions is to analyze the data at hand. The next topic introduces the possibilities as a result of data analysis.</p>
			<h3>Long-term trend analysis</h3>
			<p>Data analysis always leads to a <a id="_idIndexMarker326"/>set of trends or patterns. Monitoring these trends can lead to one or more of the following possibilities:</p>
			<ul>
				<li>Point to an existing issue.</li>
				<li>Uncover a potential issue.</li>
				<li>Improve system performance to handle a sudden period of increased traffic.</li>
				<li>Influence experimentation of new system features to proactively avoid issues.</li>
			</ul>
			<p>Data analysis is key to ensuring that a system is performing as expected and helps in identifying any outstanding potential issues. Data analysis can be done manually by humans or can be programmed to be done by a system. The intent to identify the root cause once an incident has occurred is referred to as debugging and is introduced as the next key topic in this discussion regarding feedback loops.</p>
			<h3>Debugging</h3>
			<p>Debugging allows ad hoc retrospective analysis to be conducted from the information gathered through analyzing data. It helps to answer questions such as what are the other events that happened around the same time when an event occurred.</p>
			<p>Any software service or system is <a id="_idIndexMarker327"/>bound to have unforeseen events or circumstances. These events are triggered due to an outage or loss of data or monitoring failure or the need for toil to perform a manual intervention. The active events are then responded to by either automated systems or humans. However, the response is based on the analysis of signal data that comes through the monitoring systems. These signals evaluate the impact and escalate the situation as needed and help to formulate an initial response.</p>
			<p>Debugging is also <a id="_idIndexMarker328"/>key to an effective post-mortem technique that includes updating documentation as needed, performing root cause analysis, communicating the details of the events across teams to foster knowledge sharing, and coming up with a list of preventive actions.</p>
			<p>The next topic in the discussion on feedback loops focuses on alerting. Alerting is essential for notifying either before an event occurs or as soon as possible after an event occurs.</p>
			<h3>Alerting</h3>
			<p>Alerting is a key follow-up to <a id="_idIndexMarker329"/>data analysis and informs the problem at hand. Real-time, or near-real-time, alerting is critical in mitigating the problem and potentially also identifying the root cause.</p>
			<p>Alerting rules can be <a id="_idIndexMarker330"/>complex in reflecting a sophisticated business scenario and notifications can be sent when these rules are violated. Common means of notification include the following:</p>
			<ul>
				<li>An email (that indicates something happened)</li>
				<li>A page (that calls for immediate attention)</li>
				<li>A ticket (that triggers the need to address an issue sooner rather than later)</li>
			</ul>
			<p>In <a href="B15587_02_Final_ASB_ePub.xhtml#_idTextAnchor038"><em class="italic">Chapter 2</em></a>, <em class="italic">SRE Technical Practices â€“ Deep Dive</em>, we discussed the implications of setting reliability targets. For example, if the reliability target is set to 4 9's of reliability (that is, 99.99%) then this translates to 4.38 minutes of downtime in a 30-day period. This is not enough time for a human to be notified and then intervene. However, a system can be notified and can potentially take steps to remediate the issue at hand. This is accomplished through alerting.</p>
			<p>These topics attempt to elaborate on why monitoring can be used as a feedback loop. This is critical in SRE because the goal of SRE is to maintain a balance between releasing new features and system reliability. Monitoring helps to identify issues in a timely manner (as they occur), provide an alert (when they do occur), and provide data to debug. This is key to <a id="_idIndexMarker331"/>understanding how an error budget is tracked over a period. More issues will lead to a faster burn of the error budget and thereby it becomes <a id="_idIndexMarker332"/>more important to stabilize at that point rather than release new features. However, if monitoring provides information that indicates that the current system is stable, then there will be a significant error budget remaining to prioritize new features over system stability.</p>
			<p>Given that we have established monitoring as an essential element in providing continuous feedback to achieve continuous improvement, it is also equally important to understand the common misconceptions to avoid. This will be covered as the next topic.</p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor067"/>Monitoring misconceptions to avoid</h2>
			<p>There are several common <a id="_idIndexMarker333"/>misconceptions when it comes to setting up monitoring for a service or system. The following is a list of such misconceptions that should be avoided:</p>
			<ul>
				<li>Monitoring should be regarded as a specialized skill that requires a technical understanding of the components involved and requires a functional understanding of the application or even the domain. This skill needs to be cultivated within the team that's responsible for maintaining the monitoring systems.</li>
				<li>There is no all-in-one tool to monitor a service or system. In many situations, monitoring is achieved using a combination of tools or services. For example, the latency of an API call can be monitored by <a id="_idIndexMarker334"/>tools such as <em class="italic">Grafana</em>, but the detailed breakdown of the API calls across specific methods, including the time taken for a database <a id="_idIndexMarker335"/>query, can be monitored using a tool such as <em class="italic">Dynatrace</em>.</li>
				<li>Monitoring shouldn't be limited to one viewpoint and instead cover multiple viewpoints. The things that matter to the end consumer might be different to what matters to the business, and may also differ from the viewpoint of the service provider.</li>
				<li>Monitoring is never limited to a single service. It could be extended to a set of related or unrelated services. For example, it is required to monitor a caching service as a service related to a web server. Similarly, it is important to monitor directly unrelated services, such as the machine or cluster hosting the web server.</li>
				<li>Monitoring doesn't always have to be complex. There may be complex business conditions that need to be checked with a combination of rules, but in many cases, monitoring <a id="_idIndexMarker336"/>should follow the <strong class="bold">Keep it Simple Stupid</strong> (<strong class="bold">KISS</strong>) principle.</li>
				<li>Establishing monitoring for a <a id="_idIndexMarker337"/>distributed system should focus on individual services that make up the system and should not solely focus on the holistic operation. For example, the latency of a request can be longer than expected. The focus should be on the elements or underlying services that cumulatively contribute to the request latency (which includes method calls and query responses).</li>
				<li>The phrase <em class="italic">single pane of glass</em> is often associated with effective monitoring, where a pane of glass metaphorically refers to a management console that collects data from multiple sources representing all possible services. But merely displaying information from multiple sources doesn't provide a holistic view of the relationships between the data or an idea of what could possibly go wrong. Instead, a single pane of glass should deliver a logical grouping of multiple services into a single workspace by establishing the correlation between monitoring signals.</li>
			</ul>
			<p>Monitoring should not only focus on the symptoms, but also on their causes. Let's look at some examples:</p>
			<div>
				<div id="_idContainer030" class="IMG---Figure">
					<img src="image/B15587_03_Table-01.jpg" alt=""/>
				</div>
			</div>
			<p>Essentially, the focus of <a id="_idIndexMarker338"/>monitoring should not be on collecting or displaying data, but instead <em class="italic">on establishing a relationship between what's broken and a potential reason for why it's broken</em>. There are multiple sources from which monitoring data can be collected or captured. This will be discussed as the next topic.</p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor068"/>Monitoring sources</h2>
			<p>Monitoring data is essential in <a id="_idIndexMarker339"/>monitoring systems. There are two common <a id="_idIndexMarker340"/>sources of monitoring data. The first source is <strong class="bold">metrics</strong>:</p>
			<ul>
				<li>Metrics represent numerical measurements of resource usage or behavior that can be observed and collected across the system over many data points at regular time intervals. Typical time intervals for collecting metrics could be once per second, once per minute, and so on.</li>
				<li>Metrics can be gathered from low-level metrics provided by the operating system. In most cases, the low-level metrics are readily available as they are specific to a resource, such as database instances, virtual machines, and disks.</li>
				<li>Metrics can also be gathered from higher-level types of data tied to a specific component or application. In such cases, custom metrics should be created and exposed through the process of instrumentation.</li>
				<li>Metrics are used as <a id="_idIndexMarker341"/>input to display less granular real-time data in <a id="_idIndexMarker342"/>dashboards or trigger alerts for real-time notification.</li>
			</ul>
			<p>The next source is <strong class="bold">logs</strong>:</p>
			<ul>
				<li>Logs represent granular <a id="_idIndexMarker343"/>information of data and are typically <a id="_idIndexMarker344"/>written in large volumes.</li>
				<li>Logs are not real time. Logs always have an inherent delay between when an event occurs and when it is visible in logs.</li>
				<li>Logs are used to find the root cause of an issue as the key data required for analysis is usually not present as a metric.</li>
				<li>Logs can be used to generate detailed non-time-sensitive reports using log processing systems.</li>
				<li>Logs can be used to create metrics by running queries against a stream of logs using batch processing systems.<p class="callout-heading">Logging versus monitoring</p><p class="callout"><strong class="bold">Logging</strong> provides insights into the <a id="_idIndexMarker345"/>execution of an application. Logging can capture event records and the minutest details, along with actionable errors that could be converted into alerts. Logging essentially describes what could have happened and provides data to investigate an issue. </p><p class="callout"><strong class="bold">Monitoring</strong>, on the <a id="_idIndexMarker346"/>other hand, provides capabilities to detect issues as they happen, and alert as needed. In fact, monitoring requires logging as an essential source of information. Also, the inverse is true, that logging requires monitoring. This is because an application with fantastic logging but no monitoring is not going to help the end user.</p></li>
			</ul>
			<p>To summarize, both metrics and logs are popular choices as monitoring sources. They are used in different situations and, in most cases, a combination of both sources is always recommended. Metrics are a <a id="_idIndexMarker347"/>good source if there are internal or external components that provide information about events and performance. Logs are best suited to <a id="_idIndexMarker348"/>track various <a id="_idIndexMarker349"/>events that an application goes through. Metrics can also be <a id="_idIndexMarker350"/>created from logs. The next topic discusses a few of the recommended monitoring strategies.</p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor069"/>Monitoring strategies</h2>
			<p>The following are some <a id="_idIndexMarker351"/>recommended strategies while choosing a monitoring system:</p>
			<ul>
				<li><strong class="bold">Data should not be stale</strong>: The speed of data retrieval becomes critical when querying vast amounts of data. If retrieval is slow, data will become stale and may be misinterpreted, with actions potentially being taken on incorrect data.</li>
				<li><strong class="bold">Dashboards should be configurable and include robust features</strong>: A monitoring system should include interfaces that have the capabilities to display time series data in different formats, such as <em class="italic">heatmaps</em>, <em class="italic">histograms</em>, <em class="italic">counters</em>, or a <em class="italic">distribution</em>. Options to aggregate information using multiple options should be present as a configurable feature.</li>
				<li><strong class="bold">Alerts should be classified and suppressed if needed</strong>: Monitoring systems should have the ability to set different severity levels. In addition, once an alert has been notified, it is extremely useful if there is the ability to suppress the same alert for a period that will avoid unnecessary noise that could possibly distract the on-call engineer.</li>
			</ul>
			<p>These recommended strategies are implemented across two types of monitoring. This classification will be discussed as the next topic.</p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor070"/>Monitoring types</h2>
			<p>Monitoring can be <a id="_idIndexMarker352"/>classified into the two most common types:</p>
			<ul>
				<li>Black box monitoring</li>
				<li>White box monitoring </li>
			</ul>
			<h3>Black box monitoring</h3>
			<p><strong class="bold">Black box monitoring</strong> refers to <a id="_idIndexMarker353"/>monitoring the system based on testing externally visible behavior based on the user's perspective. This kind of monitoring does not involve any access to the technical details or build or configuration of the system. Monitoring is strictly based on testing visible behavior that is a reflection of <a id="_idIndexMarker354"/>how an end consumer would access the system. It is metaphorically referred to as a black box since the internals of the system are not opaque and there is no control or visibility in terms of what's happening inside the system. It is also referred to as server or hardware monitoring.</p>
			<p>Black box monitoring is best used for paging incidents after the incident has occurred or is ongoing. Black box monitoring is a representation of active problems and is system-oriented, with a specific focus on system load and disk/memory/CPU utilization. Additional examples include the monitoring of network switches and network devices, such as load balancers and hypervisor level resource usage.</p>
			<h3>White box monitoring</h3>
			<p><strong class="bold">White box monitoring</strong> is commonly <a id="_idIndexMarker355"/>referred to as application monitoring and is based on the metrics collected and exposed by the internals of the system. For example, white box monitoring can give insights into an application or endpoint performance by capturing the total number of HTTP requests or the total <a id="_idIndexMarker356"/>number of errors or the average request latency per request. In contrast, black box monitoring can only capture if the endpoint returned a successful response. White box monitoring is both symptom-and cause-oriented and this depends on how informative the internals of the system are. White box monitoring can also provide insights into future problems, as information retrieved from one internal can be the reason for an issue in another internal. White box monitoring collects information from three critical components â€“ metrics, logs, and traces, described as follows:</p>
			<ul>
				<li><strong class="bold">Metrics</strong>: These are readily <a id="_idIndexMarker357"/>available or custom created metrics that represent the state of the system in a measurable way and typically take the form of counters, gauges, and distribution.<p>Metrics <a id="_idIndexMarker358"/>must be <strong class="bold">SMART</strong>:</p><p>a) <strong class="bold">S</strong>: <strong class="bold">Specific</strong> (such as automation results should be at least 99% versus high quality)</p><p>b) <strong class="bold">M</strong>: <strong class="bold">Measurable</strong> (such as results should be returned within 200 ms versus fast enough)</p><p>c) <strong class="bold">A</strong>: <strong class="bold">Achievable</strong> (such as a service is 99.99% available versus 100% available)</p><p>d) <strong class="bold">R</strong>: <strong class="bold">Relevant</strong> (such as observing latency versus throughput for browsing video titles)</p><p>e) <strong class="bold">T</strong>: <strong class="bold">Time Bound</strong> (such as service is 99.99% available over 30 days versus over time)</p></li>
				<li><strong class="bold">Logs</strong>: These represent a <a id="_idIndexMarker359"/>single thread of work at a single point in time. Logs reflect the application's state and are user-created at the time of application development. Logs can be structured or semi-structured, which typically includes a timestamp and a message code. Log entries are written using client libraries such as log4j and sl4j. Log processing is a reliable source of producing statistics and can also be processed in real time to produce log-based metrics.</li>
				<li><strong class="bold">Traces</strong>: These are made up of <a id="_idIndexMarker360"/>spans. A span is the primary building block of a distributed trace that represents a specific event or a user action in a distributed system. A span represents the <a id="_idIndexMarker361"/>path of a request through one server. However, there could be multiple spans at the same time, where one span can <a id="_idIndexMarker362"/>reference another span. This allows multiple spans to be assembled into a common trace, which is essentially a visualization of requests as it traverses through a distributed system.<p class="callout-heading">Black box monitoring versus white box monitoring â€“ which is more critical?</p><p class="callout">Both types of monitoring are equally critical, and each is recommended based on the situation and the type of audience. Black box monitoring provides information that the Ops team typically looks at, such as disk usage, memory utilization, and CPU utilization, whereas white box monitoring provides more details on the internals of the system, which could reflect the reason for a metric produced by black box monitoring. For example, a black box metric such as high CPU utilization will indicate that there is a problem, but a white box metric such as active database connections or information on long-running queries can indicate a potential problem that is bound to happen.</p></li>
			</ul>
			<p>To summarize, the reliability of a <a id="_idIndexMarker363"/>system can be tracked by monitoring specific metrics. However, there could potentially be multiple metrics that could be <a id="_idIndexMarker364"/>tracked and sometimes can also lead to confusion while prioritizing these metrics. The next topic lists the most important metrics to track as recommended by Google for a user-facing system. These metrics are known as the golden signals.</p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor071"/>The golden signals</h2>
			<p>System reliability is <a id="_idIndexMarker365"/>tracked by SLOs. SLOs require SLIs or specific metrics to monitor. The types of metrics to monitor depend on the user journey tied to the service. It's strongly recommended that every service/system should measure a definite and a finite set of SLIs. So, if there is a situation where it is possible to define multiple metrics for a service, then it is recommended to prioritize the metrics to measure and monitor.</p>
			<p>Google proposes the use of four golden signals. Golden signals refer to the most important metrics that should be measured for a user-facing system:</p>
			<ul>
				<li><strong class="bold">Latency</strong>: This is an <a id="_idIndexMarker366"/>indicator of the time taken to serve a request and reflects user experience. Latency can point to emerging issues. Example metrics include <em class="italic">Page load</em>/<em class="italic">transaction</em>/<em class="italic">query duration</em>, <em class="italic">Time until first response</em>, and <em class="italic">Time until complete data duration</em>.</li>
				<li><strong class="bold">Traffic</strong>: This is an indicator of <a id="_idIndexMarker367"/>current system demand and is also the basis for calculating infrastructure spend. Traffic is historically used for capacity planning. Example metrics include <em class="italic"># write</em>/<em class="italic">read ops</em>, <em class="italic"># transactions</em>/<em class="italic">retrievals</em>/<em class="italic">HTTP requests per second</em>, and <em class="italic"># active requests</em>/<em class="italic">connections</em>.</li>
				<li><strong class="bold">Errors</strong>: This is an indicator of the rate of <a id="_idIndexMarker368"/>requests that are failing. It essentially represents the rate of errors at an individual service and for the entire system. Errors represent the rate of requests that fail explicitly or implicitly or by policy. Example metrics include <em class="italic"># 400</em>/<em class="italic">500</em> <em class="italic">HTTP Codes</em> and <em class="italic"># exceptions</em>/<em class="italic">stack traces</em>/<em class="italic">dropped connections</em>.</li>
				<li><strong class="bold">Saturation</strong>: This is an indicator of the <a id="_idIndexMarker369"/>overall capacity of the service. It essentially represents how full the service is and reflects degrading performance. Saturation can also indicate SLOs, resulting in the need to alert. Example metrics include <em class="italic">Disk</em>/<em class="italic">Memory quota</em>, <em class="italic"># memory</em>/<em class="italic">thread pool</em>/<em class="italic">cache</em>/<em class="italic">disk</em>/<em class="italic">CPU utilization,</em> and the <em class="italic"># of available connections</em>/<em class="italic">users on the system</em>. </li>
			</ul>
			<p>This completes the section on <a id="_idIndexMarker370"/>monitoring, with the insights into desirable features of a monitoring system that could essentially help in creating a feedback loop, potential monitoring sources, types of monitoring, and Google's recommended golden signals, which represent the four key metrics that should be measured for a user-facing system. The next section will provide an overview of alerting and how information from the monitoring system can be used as input.</p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor072"/>Alerting</h1>
			<p>SLIs are quantitative <a id="_idIndexMarker371"/>measurements at a given point in time and SLOs use SLIs to reflect the reliability of the system. SLIs are captured or represented in the form of metrics. Monitoring systems monitor these metrics against a specific set of policies. These policies represent the target SLOs over a period and are referred to as alerting rules.</p>
			<p><strong class="bold">Alerting</strong> is the process of processing the alerting rules, which track the SLOs and notify or perform certain actions when the rules are violated. In other words, alerting allows the conversion of SLOs into actionable alerts on significant events. Alerts can then be sent to an external application or a ticketing system or a person.</p>
			<p>Common scenarios for triggering alerts include (and are not limited to) the following:</p>
			<ul>
				<li>The service or system is down.</li>
				<li>SLOs or SLAs are not met.</li>
				<li>Immediate human intervention is required to change something.</li>
			</ul>
			<p>As discussed previously, SLOs represent an achievable target, and error budgets represent the acceptable level of unreliability or unavailability. SRE strongly recommends the use of <a id="_idIndexMarker372"/>alerts to track the burn rate of error budgets. If the error budget burn rate is too fast, setting up alerts before the entire budget is exhausted can work as a warning signal, allowing teams to shift their focus on system reliability rather than push risky features.</p>
			<p>The core concept behind alerting is to track events. The events are processed through a time series. <strong class="bold">Time series</strong> is defined as a <a id="_idIndexMarker373"/>series of event data points broken into successive equally spaced windows of time. It is possible to configure the duration of each window and the math applied to the member data points inside each window. Sometimes, it is important to summarize events to prevent false positives and this can be done through time series. Eventually, error rates can be continuously calculated, monitored against set targets, and alerts can be triggered at the right time.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor073"/>Alerting strategy â€“ key attributes</h2>
			<p>The key to configuring alert(s) for a <a id="_idIndexMarker374"/>service or a system is to design an effective alerting strategy. To measure the accuracy or effectiveness of a particular alerting strategy, the following key attributes should be considered during the design.</p>
			<h3>Precision</h3>
			<p>From an effectiveness standpoint, alerts <a id="_idIndexMarker375"/>should be bucketized into <a id="_idIndexMarker376"/>relevant alerts and irrelevant alerts. <strong class="bold">Precision</strong> is defined as the proportion of events detected that are significant.</p>
			<p>The following is a mathematical formula for calculating precision:</p>
			<p><img src="image/Formula_03_001.png" alt=""/></p>
			<p>In other words, if precision needs to be 100%, then the count of irrelevant alerts should be 0. Precision is a measure of <a id="_idIndexMarker377"/>exactness and is often adversely <a id="_idIndexMarker378"/>affected by false positives or false alerts. This is a situation that could occur during a low-traffic period.</p>
			<h3>Recall</h3>
			<p>An alert needs to <a id="_idIndexMarker379"/>capture every <a id="_idIndexMarker380"/>significant event. This means that there should not be any missed alerts. <strong class="bold">Recall</strong> is defined as the proportion of significant events detected.</p>
			<p>The following is a mathematical formula for calculating recall:</p>
			<p><img src="image/Formula_03_002.png" alt=""/></p>
			<p>In other words, if recall needs to be 100%, then every significant event should result in an alert and there should not be any missed alerts. Recall is a measure of completeness and is often adversely affected by missed alerts.</p>
			<h3>Detection time</h3>
			<p><strong class="bold">Detection time</strong> is defined as the <a id="_idIndexMarker381"/>time taken by a system to notice an alert condition. It is also referred to as the time to detect. Long detection times can <a id="_idIndexMarker382"/>negatively impact the error budget. So, it is critical to notify or raise an alert as soon as an issue occurs. However, raising alerts too fast will result in false positives, which will eventually lead to poor precision.</p>
			<h3>Reset time</h3>
			<p><strong class="bold">Reset time</strong> is defined as the <a id="_idIndexMarker383"/>length or duration of time the alerts are fired after an issue has been resolved. Longer reset times will have an adverse impact <a id="_idIndexMarker384"/>because alerts will be fired on repaired systems, leading to confusion.</p>
			<p>This concludes an introduction to key attributes that are critical in defining an effective alerting strategy. The next topic elaborates on a potential approach to define an effective alerting strategy.</p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor074"/>Alerting strategy â€“ potential approaches</h2>
			<p>SRE recommends six <a id="_idIndexMarker385"/>different approaches to configure alerts on significant events. Each of these approaches addresses a different problem. These approaches also offer a certain level of balance across key attributes, such as precision, recall, detection time, and reset time. Some of these approaches could solve multiple problems at the same time.</p>
			<h3>Approach # 1 â€“ Target error rate &gt;= Error budget (with a shorter alert window)</h3>
			<p>In this approach, a shorter alert window or smaller window length is chosen (for example, 10 minutes). Smaller windows tend to yield faster alert detection and shorter reset times but also tend to decrease precision because they tend toward false positives or false alerts.</p>
			<p>As per this approach, an alert should be triggered if the target error rate equals or exceeds the error budget within the defined shorter alert window.</p>
			<p>Consider an example with the following input parameters:</p>
			<ul>
				<li>The expected SLO is 99.9% over 30 days, resulting in 0.1% as the error budget.</li>
				<li>Alert window to examine: 10 minutes.</li>
			</ul>
			<p>Accordingly, a potential alert definition would be the following:</p>
			<p><em class="italic">If the SLO is 99.9% over 30 days, alert if the error rate over the last 10 minutes is &gt;= 0.1%.</em></p>
			<p>In approach # 1, a shorter time window results in alerts being fired more frequently, but tends to decrease precision.</p>
			<h3>Approach # 2 â€“ Target error rate &gt;= Error budget (with a longer alert window)</h3>
			<p>In this approach, a longer alert window or larger window length is chosen (for example, 36 hours). Larger windows tend to yield better precision, but will have longer reset and detection times. This means that the portion of the error budget spent before the issue is detected is also high.</p>
			<p>As per this approach, an alert should be triggered if the target error rate equals or exceeds the error budget within the defined larger time window. </p>
			<p>Consider an example with the following input parameters:</p>
			<ul>
				<li><strong class="bold">The expected SLO is 99.9% over 30 days</strong>: Resulting in 0.1% as the error budget</li>
				<li><strong class="bold">Alert window to examine</strong>: 36 hours</li>
			</ul>
			<p>Accordingly, a potential alert definition would be the following:</p>
			<p><em class="italic">If the SLO is 99.9% over 30 days, alert if the error rate over the last 36 hours is &gt;= 0.1%</em>.</p>
			<p>In approach # 2, a longer time window results in higher precision, but the alert will be fired less frequently and may result in a higher detection time.</p>
			<h3>Approach # 3 â€“ Adding a duration for better precision</h3>
			<p>In this approach, a duration <a id="_idIndexMarker386"/>parameter can be added to the alert criteria, so that the alert won't fire unless the value remains above the threshold for that duration. The choice of threshold also becomes significant as some of the alerts can go undetected if the threshold is too high. For example, if the duration window is 10 minutes and the signal data was up for 9 minutes but returned to normal before the 10th minute, the error budget will be consumed, but alerts will go undetected or will not get fired. With the right selection of duration parameter and threshold, this approach enables the error to be spotted quickly, but treats the error as an anomaly until the duration is reached:</p>
			<ul>
				<li>The advantage is that the alert fired after a defined duration will generally correspond to a significant event, and so will increase precision.</li>
				<li>The disadvantage is that the error will continue to happen for a larger window and, as a result, will lead to a deteriorating recall.</li>
			</ul>
			<p>A longer alert window or larger window length is recommended (for example, 36 hours). Larger windows tend to yield better precision, but will have longer reset and detection times.</p>
			<p>As per this approach, an alert should be triggered if the target error rate equals or exceeds the error budget within the defined larger time window. </p>
			<p>Consider an example with the following input parameters:</p>
			<ul>
				<li><strong class="bold">The expected SLO is 99.9% over 30 days</strong>: Resulting in 0.1% as the error budget</li>
				<li><strong class="bold">Alert window to examine</strong>: 36 hours</li>
			</ul>
			<p>Accordingly, a potential alert definition would be as follows: </p>
			<p><em class="italic">If the SLO is 99.9% over 30 days, alert if the error rate over the last 36 hours is &gt;= 0.1%</em>.</p>
			<p>In approach # 3, a longer duration window also means that the portion of the error budget spent before the issue is detected is high, but is highly likely to indicate to a significant event.</p>
			<h3>Approach # 4 â€“ Alert regarding the burn rate</h3>
			<p>In this approach, alerts <a id="_idIndexMarker387"/>should be defined based on burn rate. <strong class="bold">Burn rate</strong> is defined as <a id="_idIndexMarker388"/>how fast, relative to the SLO, the service consumes the error budget.</p>
			<p>For example, with a 0.1% error budget over 30 days, if the error rate is constant at 0.1% across the 30 days, then the budget is spent equally and 0 budget remains at the end of the 30th day. In this case, the burn rate is calculated as 1. But if the error rate is 0.2%, then the time to exhaustion will be 15 days, and the burn rate will be 2.</p>
			<p>When alerting based on burn rate, the following are two possible alerting policies:</p>
			<ul>
				<li><strong class="bold">Fast burn alert</strong>: An alert is fired <a id="_idIndexMarker389"/>because of a sudden large change in consumption of the error budget. If not notified, then the error budget will exhaust quicker than normal. For a fast burn alert, the lookback period should be shorter (say 1 to 2 hours), but the threshold for the rate of consumption for the alert should be much higher (say 10x times) than the ideal baseline for the lookback period.</li>
				<li><strong class="bold">Slow burn alert</strong>: An alert is not fired <a id="_idIndexMarker390"/>until a condition is continuously violated for a long period. This kind of alert only consumes a small percentage of the error budget when it occurs and is significantly less urgent than a fast burn alert. For a slow burn alert, the lookback period should be longer (say 24 hours) but the threshold is slightly higher (say 2x times) than the baseline for the lookback period.</li>
			</ul>
			<p>So, as per approach # 4, alerts will be fired if the burn rate is greater than the desired burn rate at any point in time. This approach provides better precision over a shorter time window with good detection times.</p>
			<h3>Approach # 5 â€“ Multiple burn rate alerts</h3>
			<p>The same alerting <a id="_idIndexMarker391"/>strategy doesn't need to always work for a service. This might depend on factors such as the amount of traffic, the variable error budget, or peak and slow periods. For example, during a holiday shopping season, it is common that the SLO for a service will be higher than normal. This means a lower error budget during peak season and this will revert to a slightly less strict error budget during off-peak seasons. </p>
			<p>In this approach, instead of defining a single condition in an alerting policy, multiple conditions can be defined to get better precession, recall, detection time, and reset time. Each condition can have a different level of severity and a different notification channel to notify the alert based on the nature of severity.</p>
			<p>For example, an alerting policy can be defined with the following multiple conditions:</p>
			<ul>
				<li><em class="italic">Trigger an alert if 10% of the budget is consumed over a time duration of 3 days and notify by creating a ticket</em>.</li>
				<li><em class="italic">Trigger an alert if 5% of the budget is consumed over a time duration of 6 hours and notify through a page</em>.</li>
				<li><em class="italic">Trigger an alert if 2% of the budget is consumed over a time duration of 1 hour and notify through a page</em>.</li>
			</ul>
			<p>In approach # 5, multiple conditions can be defined for a single alerting policy. Each condition could result in a different action that could potentially represent the severity level for the alert.</p>
			<h3>Approach # 6 â€“ Multiple burn rate alerts across multiple windows</h3>
			<p>This is an extension of approach # 5, where the major difference is to use multiple windows to check whether the error rate exceeds the error budget rather than a single window for a single condition. This will ensure that the alert raised is always significant and is actively burning the error budget when it gets notified.</p>
			<p>This helps to create an alerting framework that is flexible and shows the direct impact based on the severity of the incident. A flexible window emphasizes or confirms whether the alert condition is active in the last specified duration. This helps to immediately troubleshoot when alerted. The flip side is that such conditions also involve multiple variables that might add to maintenance in the long term.</p>
			<p>For example, an alerting policy can be defined with the following multiple conditions:</p>
			<ul>
				<li><em class="italic">Trigger an alert if 10% of the budget is consumed over a time duration of 3 days and is currently being consumed over the last 6 hours. Notify by creating a ticket</em>.</li>
				<li><em class="italic">Trigger an alert if 5% of the budget is consumed over a time duration of 6 hours and is currently being consumed over the last 30 minutes. Notify through a page</em>.</li>
				<li><em class="italic">Trigger an alert if 2% of the budget is consumed over a time duration of 1 hour and is currently being consumed over the last 5 minutes. Notify through a page</em>.</li>
			</ul>
			<p>Approach # 6 is potentially an <a id="_idIndexMarker392"/>extension of approach # 5, where multiple conditions can be defined for an alerting policy where each condition, if breached, can result in a different action or notification. The difference is that approach # 6 emphasizes specifying an alert window that could confirm that the fired alert is potentially active.</p>
			<p>These approaches are well suited for many situations that could require an alerting strategy. However, there could be a specific situation where the traffic received by the service is less or low. The next topic discusses approaches on how to deal with such situations.</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor075"/>Handling service with low traffic</h2>
			<p>If a service receives a <a id="_idIndexMarker393"/>smaller amount of traffic, a single or a smaller number of failed requests might result in a higher error rate, indicating a significant burn of the error budget. SRE recommends a few options for handling a low-traffic service:</p>
			<ul>
				<li><strong class="bold">Generating artificial traffic</strong>: This option provides more signals or requests to work with. However, a significant amount of engineering effort is required to ensure that the artificial traffic behavior matches real user behavior closely.</li>
				<li><strong class="bold">Combining services</strong>: This option recommends bucketizing similar low-request services into a single group representing a single function. This will result in higher precision and fewer false positives. However, careful consideration needs to be given when combining the services into a single group. This is because the failure of an individual service might not always result in the failure of the overall function and, as a result, will not result in an alert for a significant event.</li>
				<li><strong class="bold">Modifying clients</strong>: This option is used to deal with ephemeral failures, especially if it is impractical to generate artificial traffic or combine services into a group. The impact of a single failed request can be reduced by modifying the client and implement exponential backoff. Additionally, fallback paths should be set up to capture the request for eventual execution post-backoff.</li>
				<li><strong class="bold">Lowering the SLO or increasing the window</strong>: This option is the simplest way to handle low-traffic services. This will reduce the impact of a single failure on the error budget. However, lowering the <a id="_idIndexMarker394"/>SLO is also a way to lower the expectations on how the service should behave.</li>
			</ul>
			<p>Given that we learned about topics specific to creating an effective alerting strategy, the next logical step is to learn about steps required to establish an SLO altering policy.</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor076"/>Steps to establish an SLO alerting policy</h2>
			<p>The following is the <a id="_idIndexMarker395"/>sequence of steps to establish an SLO alert policy:</p>
			<ol>
				<li value="1"><strong class="bold">Select the SLO to monitor</strong>: Choose the suitable SLO for the service. It is recommended to monitor only one SLO at a time.</li>
				<li><strong class="bold">Construct an appropriate condition</strong>: It is possible to have multiple conditions for an alerting policy where the condition is different for a slow burn when compared to a fast burn.</li>
				<li><strong class="bold">Identify the notification channel</strong>: Multiple notification channels can be selected at the same time for an alerting policy.</li>
				<li><strong class="bold">Include documentation</strong>: It is recommended to include documentation about the alert that might help to resolve the potential underlying issue.</li>
				<li><strong class="bold">Create an alerting policy</strong>: The policy <a id="_idIndexMarker396"/>should be created in the monitoring system of choice, either through a configuration file or CLI, or through the console (based on the features supported by the monitoring system).</li>
			</ol>
			<p>This concludes establishing the blueprint for the SLO alerting policy. The next topic introduces the desirable characteristics of an alerting system.</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor077"/>Alerting system â€“ desirable characteristics</h2>
			<p>The alerting system should <a id="_idIndexMarker397"/>control the number of alerts that the on-call engineers receive. The following is a shortlist of desirable characteristics that an alerting system should possess:</p>
			<ul>
				<li>Inhibit certain alerts when others are active.</li>
				<li>Remove or silence duplicate alerts from multiple sources that have the same label sets.</li>
				<li>Fan-in or fan-out alerts based on their label sets when multiple alerts with similar label sets fire.</li>
			</ul>
			<p>This completes the section on alerting with insights into constructs that are required to define an effective alerting policy that includes possible approaches and key attributes, such as precision, recall, detection time, and reset time. The next section will provide an overview of time series, their structure, cardinality, and metric types.</p>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor078"/>Time series</h1>
			<p><strong class="bold">Time series</strong> data is the data that <a id="_idIndexMarker398"/>collectively represents how a system's behavior changes over time. Essentially, applications relay a form of data that measures how things change over time. Time is not only regarded as a variable being captured; time is the primary focal point. Real-world examples of time series data include the following:</p>
			<ul>
				<li>Self-driving cars that continuously collect data to capture the ever-changing driving conditions or environment</li>
				<li>Smart homes that capture events such as a change in temperature or motion<p class="callout-heading">Metric versus events</p><p class="callout"><strong class="bold">Metrics</strong> are time series <a id="_idIndexMarker399"/>measurements gathered at regular intervals. <strong class="bold">Events</strong> are time series measurements <a id="_idIndexMarker400"/>gathered at irregular time intervals.</p></li>
			</ul>
			<p>The following are some characteristics that qualify data as time series data:</p>
			<ul>
				<li>Data that arrives is always recorded as a new entry.</li>
				<li>Data arrives in time order.</li>
				<li>Time is the primary axis.<p class="callout-heading">Adding a time field to the dataset is not the same as time series data</p><p class="callout">Data related to a sensor is being collected in a non-time series database. If the sensor collects a new set of data, then writing this data will overwrite the previously stored data in the database with updated time. The database will eventually return the latest reading, but will not be able to track the change of data over a period of time. So, a non-time series database tracks changes to the system as <strong class="bold">UPDATES</strong>, but a time series database tracks changes to the system as <strong class="bold">INSERTS</strong>.</p></li>
			</ul>
			<p>The next topic discusses the structure of time series.</p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor079"/>Time series structure</h2>
			<p>Monitoring data is <a id="_idIndexMarker401"/>stored in time series. Each individual time series data has three pieces of information (refer to <em class="italic">Figure 3.1</em>):</p>
			<ul>
				<li><strong class="bold">Points</strong>: Refers to a series of (timestamp, value) pairs. The value is the measurement, and the timestamp is the time at which the measurement was taken.</li>
				<li><strong class="bold">Metric</strong>: Refers to the name of the metric type that indicates how to interpret the data points. This also includes a combination of values for the metric labels.</li>
				<li><strong class="bold">Monitored resource</strong>: Refers to the monitored resource that is the source of the time series data, and one combination of values for the resource's label.</li>
			</ul>
			<p>The following screenshot shows the structure of time series data:</p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/B15587_03_01.jpg" alt="Figure 3.1 â€“ Structure of time series data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 â€“ Structure of time series data</p>
			<p>Let's look at an example:</p>
			<p>The following screenshot shows an illustration of how time series data is represented for metric types with sample values: <strong class="source-inline">storage.googleapis.com/api/request_count</strong>:</p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/B15587_03_02.jpg" alt="Figure 3.2 â€“ Illustration of a GCP Cloud storage metric with sample values&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 â€“ Illustration of a GCP Cloud storage metric with sample values</p>
			<p>The preceding screenshot represents the <a id="_idIndexMarker402"/>various events that were captured as part of the metrics as they happened over time. For example, the following information can be extracted from <em class="italic">Figure 3.2</em>: </p>
			<ul>
				<li>There are a total of eight registered events in bucket <strong class="source-inline">1234</strong> between <strong class="source-inline">2:00 pm</strong> and <strong class="source-inline">2:10 pm</strong>.</li>
				<li>This includes six successful events (three reads and three writes with different timestamps) and three unsuccessful write attempts.</li>
			</ul>
			<p>The next topic introduces the concept of cardinality with respect to time series data.</p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor080"/>Time series cardinality</h2>
			<p>Each time series is associated <a id="_idIndexMarker403"/>with a specific pair of metric and monitored resource types, but each pair can have many time series. The possible number of time series is determined by the cardinality of the pair: the number of labels and the number of values each label can take on.</p>
			<p>For example, a time series metric is represented as a combination of two labels: <strong class="bold">zone</strong> and <strong class="bold">color</strong>. There are two zones (east and west) and three colors for each zone (red, green, and blue). So, the cardinality for this metric is six. The following is the potential time series data:</p>
			<p class="source-code">Request_by_zone_and_color{zone="east",color="red"}</p>
			<p class="source-code">Request_by_zone_and_color{zone="east",color="green"}</p>
			<p class="source-code">Request_by_zone_and_color{zone="east",color="blue"}</p>
			<p class="source-code">Request_by_zone_and_color{zone="west",color="red"}</p>
			<p class="source-code">Request_by_zone_and_color{zone="west",color="green"}</p>
			<p class="source-code">Request_by_zone_and_color{zone="west",color="blue"}</p>
			<p>With metric cardinality, there are a <a id="_idIndexMarker404"/>couple key pointers to bear in mind:</p>
			<ul>
				<li>Metric cardinality is a critical factor with respect to performance. If the cardinality is too high, this means that there is a lot of time series data, resulting in higher query response times.</li>
				<li>For custom metrics, the maximum number of labels that can be defined in a metric type is 10.</li>
			</ul>
			<p>The next topic introduces the common metric types with respect to time series data.</p>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor081"/>Time series data â€“ metric types</h2>
			<p>Each time series includes a metric type to represent its data points. The metric type defines how to interpret the values relative to one another. The three most common types of metric are as follows:</p>
			<ul>
				<li><strong class="bold">Counter</strong>: A counter is a <a id="_idIndexMarker405"/>cumulative metric that represents a <a id="_idIndexMarker406"/>monotonically increasing function whose value can only increase (but can never decrease) or be reset to zero on restart; for example, a counter to represent the number of requests served, tasks completed, or errors observed up to a particular point in time.</li>
				<li><strong class="bold">Gauge</strong>: A gauge metric <a id="_idIndexMarker407"/>represents a single numerical value that can arbitrarily go up and down. It is useful for monitoring things with <a id="_idIndexMarker408"/>upper bounds.<p>Examples of a gauge include the size of a collection or map or the number of threads in a running state. Gauges are also typically used for measured values such as temperatures or current memory usage, but also <em class="italic">counts</em> that can go up and down, such as the number of concurrent requests.</p></li>
				<li><strong class="bold">Distribution</strong>: A distribution metric is <a id="_idIndexMarker409"/>used to track the distribution of events across configurable buckets; for example, measure the <a id="_idIndexMarker410"/>payload sizes of requests hitting the server.</li>
			</ul>
			<p>This completes the section on time series, where key concepts related to time series structure, cardinality, and possible metric types were summarized. This also brings us to the end of the chapter.</p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor082"/>Summary</h1>
			<p>In this chapter, we discussed the concepts related to monitoring, alerting, and time series that are critical in tracking SRE technical practices, such as the SLO and error budgets. We also discussed the differences between black box monitoring and white box monitoring. In addition, we examined the four golden signals as recommended by Google to be the desired SLI metrics for a user-facing system.</p>
			<p>In the next chapter, we will focus on the constructs required to build an SRE team and apply cultural practices such as handling facets of incident management, being on-call, avoiding psychological safety, promoting communication and collaboration, and knowledge sharing.</p>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor083"/>Points to remember</h1>
			<p>The following are some important points to remember:</p>
			<ul>
				<li>Black box monitoring is based on testing externally visible behavior.</li>
				<li>White box monitoring is based on the metrics collected and exposed by the internals of the system.</li>
				<li>Metrics must be specific, measurable, achievable, relevant, and time-bound.</li>
				<li>The four golden signals recommended for a user-facing system are latency, traffic, errors, and saturation.</li>
				<li>Latency can point to emerging issues, and traffic is historically used for capacity planning.</li>
				<li>Errors represent the rate of requests that fail explicitly or implicitly or by policy.</li>
				<li>Saturation represents how full the service is and reflects degrading performance.</li>
				<li>Precision is defined as the proportion of events detected that are significant.</li>
				<li>Recall is defined as the proportion of significant events detected.</li>
				<li>Detection time is defined as the time taken by a system to notice an alert condition.</li>
				<li>Reset time is defined as the length or duration of time the alerts are fired after an issue has been resolved.</li>
				<li>An individual time series data has three critical pieces of information â€“ points, metrics, and monitored resources.</li>
			</ul>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor084"/>Further reading</h1>
			<p>For more information on GCP's approach to DevOps, read the following articles:</p>
			<ul>
				<li><strong class="bold">SRE</strong>: <a href="https://landing.google.com/sre/">https://landing.google.com/sre/</a></li>
				<li><strong class="bold">SRE Fundamentals</strong>: <a href="https://cloud.google.com/blog/products/gcp/sre-fundamentals-slis-slas-and-slos">https://cloud.google.com/blog/products/gcp/sre-fundamentals-slis-slas-and-slos</a></li>
				<li><strong class="bold">SRE Youtube Playlist</strong>: <a href="https://www.youtube.com/watch?v=uTEL8Ff1Zvk&amp;list=PLIivdWyY5sqJrKl7D2u-gmis8h9K66qoj">https://www.youtube.com/watch?v=uTEL8Ff1Zvk&amp;list=PLIivdWyY5sqJrKl7D2u-gmis8h9K66qoj</a></li>
				<li><strong class="bold">Metrics, time series, and resources</strong>: <a href="https://cloud.google.com/monitoring/api/v3/metrics">https://cloud.google.com/monitoring/api/v3/metrics</a></li>
			</ul>
			<h1 id="_idParaDest-86"><a id="_idTextAnchor085"/>Practice test</h1>
			<p>Answer the following questions:</p>
			<ol>
				<li value="1">Select the monitoring option that works based on the metrics exposed by the internals of the system.<p>a) Alert-based monitoring</p><p>b) White box monitoring</p><p>c) Log-based monitoring</p><p>d) Black box monitoring</p></li>
				<li>Select the monitoring source that doesn't provide information in near-real time.<p>a) Logs</p><p>b) Metrics</p><p>c) Both</p><p>d) None of the above</p></li>
				<li>From the perspective of a fast burn alerting policy, select the appropriate threshold in relative comparison to the baseline for the defined lookback interval.<p>a) The threshold = the baseline.</p><p>b) The threshold is &lt; the baseline.</p><p>c) The threshold is significantly higher than the baseline.</p><p>d) The threshold is slightly higher than the baseline.</p></li>
				<li>Select the appropriate options for sending alerts<p>a) Email</p><p>b) Page</p><p>c) Text</p><p>d) All of the above</p></li>
				<li>Select the monitoring that is best suited to paging incidents.<p>a) Alert-based monitoring</p><p>b) White box monitoring</p><p>c) Log-based monitoring</p><p>d) Black box monitoring</p></li>
				<li>Which of the following is not part of Google's recommended golden signals?<p>a) Traffic</p><p>b) Throughput</p><p>c) Saturation</p><p>d) Errors</p></li>
				<li>Which of the following alerting policies recommends a longer lookback window?<p>a) Fast burn</p><p>b) Slow burn</p><p>c) Both</p><p>d) None</p></li>
				<li>Which of the following represents the action of collecting, processing, aggregating, and displaying real-time quantitative data relating to a system, such as query counts and types, error counts and types, processing times, and server lifetimes?<p>a) Alerting</p><p>b) Monitoring</p><p>c) Debugging</p><p>d) Troubleshooting</p></li>
				<li>Which of the following represents time series measurements gathered over irregular time intervals?<p>a) Metrics</p><p>b) Events</p><p>c) Logs</p><p>d) Trace</p></li>
				<li>Which of the following is not a suitable source of white box monitoring?<p>a) Metrics</p><p>b) Load balancer</p><p>c) Logs</p><p>d) Traces</p></li>
			</ol>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor086"/>Answers</h1>
			<ol>
				<li value="1">(b) White box monitoring.</li>
				<li>(a) Logs.</li>
				<li>(c) The threshold is significantly higher than the baseline. The recommended level is 10x.</li>
				<li>(d) All of the above, including email, pages, and texts.</li>
				<li>(d) Black box monitoring.</li>
				<li>(b) Throughput. The four golden signals are latency, errors, traffic, and saturation</li>
				<li>(b) Slow burn alert policy</li>
				<li>(b) Monitoring</li>
				<li>(b) Events. Events are time series measurements gathered at irregular time intervals. Metrics are time series measurements gathered at regular time intervals.</li>
				<li>(b) Load balancer. It is best suited as a source for black box monitoring. White box monitoring collects information from three critical components: metrics, logs, and traces.</li>
			</ol>
		</div>
	</body></html>