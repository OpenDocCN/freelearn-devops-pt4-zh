- en: '*Chapter 3*: Understanding Monitoring and Alerting to Target Reliability'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Reliability** is the most critical feature of a service or a system. **Site
    Reliability Engineering** (**SRE**) prescribes specific technical tools or practices
    that help measure characteristics to define and track reliability, such as **SLAs**,
    **SLOs**, **SLIs**, and **Error** **Budgets**. [*Chapter 2*](B15587_02_Final_ASB_ePub.xhtml#_idTextAnchor038),
    *SRE Technical Practices – Deep Dive*, took a deep dive into these SRE technical
    practices across multiple topics, including a blueprint for a well-defined SLA,
    the need for SLOs to achieve SLAs, the guidelines for setting SLOs, the need for
    SLIs to achieve SLOs, the different types of SLIs based on user journey categorization,
    different sources to measure SLIs, the importance of error budgets, and how to
    set error budgets to make a service reliable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'SLAs are external promises made to the customer, while SLOs are internal promises
    that need to be met so that SLAs are not violated. This raises a raft of important
    questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How to observe SLAs for a service so that user or customer expectations are
    met
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to observe SLOs for SLAs so that the service is reliable, and SLAs are met
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to observe SLIs for SLOs so that the service is reliable, and SLAs are met
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding questions are critical because it not only has an impact on a
    user's expectations regarding the service, but also leads to an imbalance between
    development velocity to deliver new features versus system reliability. This ultimately
    impacts the promised SLAs, leading to financial or loyalty repercussions. So,
    in simple terms, the main goal is to identify how to track SRE technical practices
    to target system reliability.
  prefs: []
  type: TYPE_NORMAL
- en: 'To track SRE technical practices, three fundamental concepts are required:
    **Monitoring**, **Alerting**, and **Time Series**. Monitoring is the process of
    monitoring key indicators that represent system reliability. Alerting is the process
    of alerting or reporting when the key indicators monitored fall below an acceptable
    threshold or condition. Monitoring and alerting are configured as a function of
    time. This means that the data needs to be collected at successive, equally spaced
    points in time representing a sequence of discrete-time data. This sequence of
    discrete-time data is also known as a time series.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to explore the following topics and their role
    in relation to target system reliability:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Monitoring**: Feedback loop, monitoring types, and golden signals?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alerting**: Key attributes and approaches for an alerting strategy?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time series**: Structure, cardinality, and metric types?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Monitoring** is defined by Google SRE as the action of collecting, processing,
    aggregating, and displaying real-time quantitative data about a system, such as
    query counts and types, error counts and types, processing times, and server lifetimes.'
  prefs: []
  type: TYPE_NORMAL
- en: In simple terms, the essence of monitoring is to verify whether a service or
    an application is behaving as expected. Customers expect a service to be reliable
    and delivering the service to the customer is just the first step. But ensuring
    that the service is reliable should be the desired goal. To achieve this goal,
    it is important to explore key data, otherwise also known as metrics. Examples
    of some metrics can be tied to uptime, resource usage, network utilization, and
    application performance.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring is the means of exploring metric data and providing a holistic view
    of a system's health, which is a reflection of its reliability. Apart from metric
    data, monitoring can include data from text-based logging, event-based logging,
    and distributed tracing. The next topic details how monitoring acts as a continuous
    feedback loop that is critical to continuously improving system reliability by
    providing constant feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring as a feedback loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ultimate goal is to build a reliable system. For a system to be reliable,
    the system needs to be continuously observed, in terms of understanding the system's
    internal states based on its external outputs. This process is known as **o****bservability**.
  prefs: []
  type: TYPE_NORMAL
- en: Observability helps to identify performance bottlenecks or investigate why a
    request failed. But for a system to be observable, it is important to collect
    and track several sources of outputs related to the health of the application.
    These outputs give insights into the application's health and identify any outstanding
    problems. This is referred to as monitoring. So, monitoring provides inputs that
    help a system to be observable. In simple terms, *monitoring indicates when something
    is wrong, while observability helps to show why something went wrong*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once an application is deployed, there are four primary areas across which
    the application needs to be inspected or otherwise monitored:'
  prefs: []
  type: TYPE_NORMAL
- en: Verify an application's performance against the application objectives and identify
    any deviations in performance by raising relevant questions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze data collected by the application over a period of time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alert key personnel when key issues are identified through insights or data
    analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debug the captured information to understand the root cause of an identified
    problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These areas or categories provide a **continuous feedback loop** as part of
    monitoring the system. This feedback helps to continuously improve the system
    by identifying issues, analyzing the root cause, and resolving the same. Each
    of the four categories mentioned is elaborated on in this chapter to provide further
    insights.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key aspects of monitoring is to raise relevant questions pertaining
    to the health of the system. This is covered as the next topic.
  prefs: []
  type: TYPE_NORMAL
- en: Raising relevant questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is important to raise relevant questions to monitor a system''s health post-deployment.
    These questions provide feedback on how the system is performing. Here are some
    questions to define where monitoring the service/system can effectively provide
    a feedback loop:'
  prefs: []
  type: TYPE_NORMAL
- en: Is the size of the database growing faster than anticipated?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Has the system slowed down after taking the latest software update of a specific
    system component?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can the use of new techniques aid system performance (such as the use of **Memcached**
    to improve caching performance)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What changes are required to ensure that the service/system can accept traffic
    from a new geographic location?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are traffic patterns pointing to a potential hack of the service/system?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key to answering the preceding questions is to analyze the data at hand.
    The next topic introduces the possibilities as a result of data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Long-term trend analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data analysis always leads to a set of trends or patterns. Monitoring these
    trends can lead to one or more of the following possibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: Point to an existing issue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uncover a potential issue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve system performance to handle a sudden period of increased traffic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Influence experimentation of new system features to proactively avoid issues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data analysis is key to ensuring that a system is performing as expected and
    helps in identifying any outstanding potential issues. Data analysis can be done
    manually by humans or can be programmed to be done by a system. The intent to
    identify the root cause once an incident has occurred is referred to as debugging
    and is introduced as the next key topic in this discussion regarding feedback
    loops.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Debugging allows ad hoc retrospective analysis to be conducted from the information
    gathered through analyzing data. It helps to answer questions such as what are
    the other events that happened around the same time when an event occurred.
  prefs: []
  type: TYPE_NORMAL
- en: Any software service or system is bound to have unforeseen events or circumstances.
    These events are triggered due to an outage or loss of data or monitoring failure
    or the need for toil to perform a manual intervention. The active events are then
    responded to by either automated systems or humans. However, the response is based
    on the analysis of signal data that comes through the monitoring systems. These
    signals evaluate the impact and escalate the situation as needed and help to formulate
    an initial response.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging is also key to an effective post-mortem technique that includes updating
    documentation as needed, performing root cause analysis, communicating the details
    of the events across teams to foster knowledge sharing, and coming up with a list
    of preventive actions.
  prefs: []
  type: TYPE_NORMAL
- en: The next topic in the discussion on feedback loops focuses on alerting. Alerting
    is essential for notifying either before an event occurs or as soon as possible
    after an event occurs.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Alerting is a key follow-up to data analysis and informs the problem at hand.
    Real-time, or near-real-time, alerting is critical in mitigating the problem and
    potentially also identifying the root cause.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alerting rules can be complex in reflecting a sophisticated business scenario
    and notifications can be sent when these rules are violated. Common means of notification
    include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An email (that indicates something happened)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A page (that calls for immediate attention)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A ticket (that triggers the need to address an issue sooner rather than later)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [*Chapter 2*](B15587_02_Final_ASB_ePub.xhtml#_idTextAnchor038), *SRE Technical
    Practices – Deep Dive*, we discussed the implications of setting reliability targets.
    For example, if the reliability target is set to 4 9's of reliability (that is,
    99.99%) then this translates to 4.38 minutes of downtime in a 30-day period. This
    is not enough time for a human to be notified and then intervene. However, a system
    can be notified and can potentially take steps to remediate the issue at hand.
    This is accomplished through alerting.
  prefs: []
  type: TYPE_NORMAL
- en: These topics attempt to elaborate on why monitoring can be used as a feedback
    loop. This is critical in SRE because the goal of SRE is to maintain a balance
    between releasing new features and system reliability. Monitoring helps to identify
    issues in a timely manner (as they occur), provide an alert (when they do occur),
    and provide data to debug. This is key to understanding how an error budget is
    tracked over a period. More issues will lead to a faster burn of the error budget
    and thereby it becomes more important to stabilize at that point rather than release
    new features. However, if monitoring provides information that indicates that
    the current system is stable, then there will be a significant error budget remaining
    to prioritize new features over system stability.
  prefs: []
  type: TYPE_NORMAL
- en: Given that we have established monitoring as an essential element in providing
    continuous feedback to achieve continuous improvement, it is also equally important
    to understand the common misconceptions to avoid. This will be covered as the
    next topic.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring misconceptions to avoid
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several common misconceptions when it comes to setting up monitoring
    for a service or system. The following is a list of such misconceptions that should
    be avoided:'
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring should be regarded as a specialized skill that requires a technical
    understanding of the components involved and requires a functional understanding
    of the application or even the domain. This skill needs to be cultivated within
    the team that's responsible for maintaining the monitoring systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no all-in-one tool to monitor a service or system. In many situations,
    monitoring is achieved using a combination of tools or services. For example,
    the latency of an API call can be monitored by tools such as *Grafana*, but the
    detailed breakdown of the API calls across specific methods, including the time
    taken for a database query, can be monitored using a tool such as *Dynatrace*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring shouldn't be limited to one viewpoint and instead cover multiple
    viewpoints. The things that matter to the end consumer might be different to what
    matters to the business, and may also differ from the viewpoint of the service
    provider.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring is never limited to a single service. It could be extended to a set
    of related or unrelated services. For example, it is required to monitor a caching
    service as a service related to a web server. Similarly, it is important to monitor
    directly unrelated services, such as the machine or cluster hosting the web server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring doesn't always have to be complex. There may be complex business
    conditions that need to be checked with a combination of rules, but in many cases,
    monitoring should follow the **Keep it Simple Stupid** (**KISS**) principle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establishing monitoring for a distributed system should focus on individual
    services that make up the system and should not solely focus on the holistic operation.
    For example, the latency of a request can be longer than expected. The focus should
    be on the elements or underlying services that cumulatively contribute to the
    request latency (which includes method calls and query responses).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The phrase *single pane of glass* is often associated with effective monitoring,
    where a pane of glass metaphorically refers to a management console that collects
    data from multiple sources representing all possible services. But merely displaying
    information from multiple sources doesn't provide a holistic view of the relationships
    between the data or an idea of what could possibly go wrong. Instead, a single
    pane of glass should deliver a logical grouping of multiple services into a single
    workspace by establishing the correlation between monitoring signals.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Monitoring should not only focus on the symptoms, but also on their causes.
    Let''s look at some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15587_03_Table-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Essentially, the focus of monitoring should not be on collecting or displaying
    data, but instead *on establishing a relationship between what's broken and a
    potential reason for why it's broken*. There are multiple sources from which monitoring
    data can be collected or captured. This will be discussed as the next topic.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Monitoring data is essential in monitoring systems. There are two common sources
    of monitoring data. The first source is **metrics**:'
  prefs: []
  type: TYPE_NORMAL
- en: Metrics represent numerical measurements of resource usage or behavior that
    can be observed and collected across the system over many data points at regular
    time intervals. Typical time intervals for collecting metrics could be once per
    second, once per minute, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics can be gathered from low-level metrics provided by the operating system.
    In most cases, the low-level metrics are readily available as they are specific
    to a resource, such as database instances, virtual machines, and disks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics can also be gathered from higher-level types of data tied to a specific
    component or application. In such cases, custom metrics should be created and
    exposed through the process of instrumentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics are used as input to display less granular real-time data in dashboards
    or trigger alerts for real-time notification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The next source is **logs**:'
  prefs: []
  type: TYPE_NORMAL
- en: Logs represent granular information of data and are typically written in large
    volumes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logs are not real time. Logs always have an inherent delay between when an event
    occurs and when it is visible in logs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logs are used to find the root cause of an issue as the key data required for
    analysis is usually not present as a metric.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logs can be used to generate detailed non-time-sensitive reports using log processing
    systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logs can be used to create metrics by running queries against a stream of logs
    using batch processing systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging versus monitoring
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Logging** provides insights into the execution of an application. Logging
    can capture event records and the minutest details, along with actionable errors
    that could be converted into alerts. Logging essentially describes what could
    have happened and provides data to investigate an issue.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Monitoring**, on the other hand, provides capabilities to detect issues as
    they happen, and alert as needed. In fact, monitoring requires logging as an essential
    source of information. Also, the inverse is true, that logging requires monitoring.
    This is because an application with fantastic logging but no monitoring is not
    going to help the end user.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To summarize, both metrics and logs are popular choices as monitoring sources.
    They are used in different situations and, in most cases, a combination of both
    sources is always recommended. Metrics are a good source if there are internal
    or external components that provide information about events and performance.
    Logs are best suited to track various events that an application goes through.
    Metrics can also be created from logs. The next topic discusses a few of the recommended
    monitoring strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are some recommended strategies while choosing a monitoring system:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data should not be stale**: The speed of data retrieval becomes critical
    when querying vast amounts of data. If retrieval is slow, data will become stale
    and may be misinterpreted, with actions potentially being taken on incorrect data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dashboards should be configurable and include robust features**: A monitoring
    system should include interfaces that have the capabilities to display time series
    data in different formats, such as *heatmaps*, *histograms*, *counters*, or a
    *distribution*. Options to aggregate information using multiple options should
    be present as a configurable feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alerts should be classified and suppressed if needed**: Monitoring systems
    should have the ability to set different severity levels. In addition, once an
    alert has been notified, it is extremely useful if there is the ability to suppress
    the same alert for a period that will avoid unnecessary noise that could possibly
    distract the on-call engineer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These recommended strategies are implemented across two types of monitoring.
    This classification will be discussed as the next topic.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Monitoring can be classified into the two most common types:'
  prefs: []
  type: TYPE_NORMAL
- en: Black box monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: White box monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Black box monitoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Black box monitoring** refers to monitoring the system based on testing externally
    visible behavior based on the user''s perspective. This kind of monitoring does
    not involve any access to the technical details or build or configuration of the
    system. Monitoring is strictly based on testing visible behavior that is a reflection
    of how an end consumer would access the system. It is metaphorically referred
    to as a black box since the internals of the system are not opaque and there is
    no control or visibility in terms of what''s happening inside the system. It is
    also referred to as server or hardware monitoring.'
  prefs: []
  type: TYPE_NORMAL
- en: Black box monitoring is best used for paging incidents after the incident has
    occurred or is ongoing. Black box monitoring is a representation of active problems
    and is system-oriented, with a specific focus on system load and disk/memory/CPU
    utilization. Additional examples include the monitoring of network switches and
    network devices, such as load balancers and hypervisor level resource usage.
  prefs: []
  type: TYPE_NORMAL
- en: White box monitoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**White box monitoring** is commonly referred to as application monitoring
    and is based on the metrics collected and exposed by the internals of the system.
    For example, white box monitoring can give insights into an application or endpoint
    performance by capturing the total number of HTTP requests or the total number
    of errors or the average request latency per request. In contrast, black box monitoring
    can only capture if the endpoint returned a successful response. White box monitoring
    is both symptom-and cause-oriented and this depends on how informative the internals
    of the system are. White box monitoring can also provide insights into future
    problems, as information retrieved from one internal can be the reason for an
    issue in another internal. White box monitoring collects information from three
    critical components – metrics, logs, and traces, described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Metrics**: These are readily available or custom created metrics that represent
    the state of the system in a measurable way and typically take the form of counters,
    gauges, and distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Metrics must be **SMART**:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) **S**: **Specific** (such as automation results should be at least 99% versus
    high quality)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) **M**: **Measurable** (such as results should be returned within 200 ms
    versus fast enough)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) **A**: **Achievable** (such as a service is 99.99% available versus 100%
    available)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) **R**: **Relevant** (such as observing latency versus throughput for browsing
    video titles)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'e) **T**: **Time Bound** (such as service is 99.99% available over 30 days
    versus over time)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Logs**: These represent a single thread of work at a single point in time.
    Logs reflect the application''s state and are user-created at the time of application
    development. Logs can be structured or semi-structured, which typically includes
    a timestamp and a message code. Log entries are written using client libraries
    such as log4j and sl4j. Log processing is a reliable source of producing statistics
    and can also be processed in real time to produce log-based metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Traces**: These are made up of spans. A span is the primary building block
    of a distributed trace that represents a specific event or a user action in a
    distributed system. A span represents the path of a request through one server.
    However, there could be multiple spans at the same time, where one span can reference
    another span. This allows multiple spans to be assembled into a common trace,
    which is essentially a visualization of requests as it traverses through a distributed
    system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Black box monitoring versus white box monitoring – which is more critical?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Both types of monitoring are equally critical, and each is recommended based
    on the situation and the type of audience. Black box monitoring provides information
    that the Ops team typically looks at, such as disk usage, memory utilization,
    and CPU utilization, whereas white box monitoring provides more details on the
    internals of the system, which could reflect the reason for a metric produced
    by black box monitoring. For example, a black box metric such as high CPU utilization
    will indicate that there is a problem, but a white box metric such as active database
    connections or information on long-running queries can indicate a potential problem
    that is bound to happen.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To summarize, the reliability of a system can be tracked by monitoring specific
    metrics. However, there could potentially be multiple metrics that could be tracked
    and sometimes can also lead to confusion while prioritizing these metrics. The
    next topic lists the most important metrics to track as recommended by Google
    for a user-facing system. These metrics are known as the golden signals.
  prefs: []
  type: TYPE_NORMAL
- en: The golden signals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: System reliability is tracked by SLOs. SLOs require SLIs or specific metrics
    to monitor. The types of metrics to monitor depend on the user journey tied to
    the service. It's strongly recommended that every service/system should measure
    a definite and a finite set of SLIs. So, if there is a situation where it is possible
    to define multiple metrics for a service, then it is recommended to prioritize
    the metrics to measure and monitor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Google proposes the use of four golden signals. Golden signals refer to the
    most important metrics that should be measured for a user-facing system:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Latency**: This is an indicator of the time taken to serve a request and
    reflects user experience. Latency can point to emerging issues. Example metrics
    include *Page load*/*transaction*/*query duration*, *Time until first response*,
    and *Time until complete data duration*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Traffic**: This is an indicator of current system demand and is also the
    basis for calculating infrastructure spend. Traffic is historically used for capacity
    planning. Example metrics include *# write*/*read ops*, *# transactions*/*retrievals*/*HTTP
    requests per second*, and *# active requests*/*connections*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Errors**: This is an indicator of the rate of requests that are failing.
    It essentially represents the rate of errors at an individual service and for
    the entire system. Errors represent the rate of requests that fail explicitly
    or implicitly or by policy. Example metrics include *# 400*/*500* *HTTP Codes*
    and *# exceptions*/*stack traces*/*dropped connections*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Saturation**: This is an indicator of the overall capacity of the service.
    It essentially represents how full the service is and reflects degrading performance.
    Saturation can also indicate SLOs, resulting in the need to alert. Example metrics
    include *Disk*/*Memory quota*, *# memory*/*thread pool*/*cache*/*disk*/*CPU utilization,*
    and the *# of available connections*/*users on the system*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This completes the section on monitoring, with the insights into desirable features
    of a monitoring system that could essentially help in creating a feedback loop,
    potential monitoring sources, types of monitoring, and Google's recommended golden
    signals, which represent the four key metrics that should be measured for a user-facing
    system. The next section will provide an overview of alerting and how information
    from the monitoring system can be used as input.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SLIs are quantitative measurements at a given point in time and SLOs use SLIs
    to reflect the reliability of the system. SLIs are captured or represented in
    the form of metrics. Monitoring systems monitor these metrics against a specific
    set of policies. These policies represent the target SLOs over a period and are
    referred to as alerting rules.
  prefs: []
  type: TYPE_NORMAL
- en: '**Alerting** is the process of processing the alerting rules, which track the
    SLOs and notify or perform certain actions when the rules are violated. In other
    words, alerting allows the conversion of SLOs into actionable alerts on significant
    events. Alerts can then be sent to an external application or a ticketing system
    or a person.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Common scenarios for triggering alerts include (and are not limited to) the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: The service or system is down.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SLOs or SLAs are not met.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Immediate human intervention is required to change something.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As discussed previously, SLOs represent an achievable target, and error budgets
    represent the acceptable level of unreliability or unavailability. SRE strongly
    recommends the use of alerts to track the burn rate of error budgets. If the error
    budget burn rate is too fast, setting up alerts before the entire budget is exhausted
    can work as a warning signal, allowing teams to shift their focus on system reliability
    rather than push risky features.
  prefs: []
  type: TYPE_NORMAL
- en: The core concept behind alerting is to track events. The events are processed
    through a time series. **Time series** is defined as a series of event data points
    broken into successive equally spaced windows of time. It is possible to configure
    the duration of each window and the math applied to the member data points inside
    each window. Sometimes, it is important to summarize events to prevent false positives
    and this can be done through time series. Eventually, error rates can be continuously
    calculated, monitored against set targets, and alerts can be triggered at the
    right time.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting strategy – key attributes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The key to configuring alert(s) for a service or a system is to design an effective
    alerting strategy. To measure the accuracy or effectiveness of a particular alerting
    strategy, the following key attributes should be considered during the design.
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From an effectiveness standpoint, alerts should be bucketized into relevant
    alerts and irrelevant alerts. **Precision** is defined as the proportion of events
    detected that are significant.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a mathematical formula for calculating precision:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_001.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, if precision needs to be 100%, then the count of irrelevant
    alerts should be 0\. Precision is a measure of exactness and is often adversely
    affected by false positives or false alerts. This is a situation that could occur
    during a low-traffic period.
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An alert needs to capture every significant event. This means that there should
    not be any missed alerts. **Recall** is defined as the proportion of significant
    events detected.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a mathematical formula for calculating recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_002.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, if recall needs to be 100%, then every significant event should
    result in an alert and there should not be any missed alerts. Recall is a measure
    of completeness and is often adversely affected by missed alerts.
  prefs: []
  type: TYPE_NORMAL
- en: Detection time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Detection time** is defined as the time taken by a system to notice an alert
    condition. It is also referred to as the time to detect. Long detection times
    can negatively impact the error budget. So, it is critical to notify or raise
    an alert as soon as an issue occurs. However, raising alerts too fast will result
    in false positives, which will eventually lead to poor precision.'
  prefs: []
  type: TYPE_NORMAL
- en: Reset time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Reset time** is defined as the length or duration of time the alerts are
    fired after an issue has been resolved. Longer reset times will have an adverse
    impact because alerts will be fired on repaired systems, leading to confusion.'
  prefs: []
  type: TYPE_NORMAL
- en: This concludes an introduction to key attributes that are critical in defining
    an effective alerting strategy. The next topic elaborates on a potential approach
    to define an effective alerting strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting strategy – potential approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SRE recommends six different approaches to configure alerts on significant events.
    Each of these approaches addresses a different problem. These approaches also
    offer a certain level of balance across key attributes, such as precision, recall,
    detection time, and reset time. Some of these approaches could solve multiple
    problems at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Approach # 1 – Target error rate >= Error budget (with a shorter alert window)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this approach, a shorter alert window or smaller window length is chosen
    (for example, 10 minutes). Smaller windows tend to yield faster alert detection
    and shorter reset times but also tend to decrease precision because they tend
    toward false positives or false alerts.
  prefs: []
  type: TYPE_NORMAL
- en: As per this approach, an alert should be triggered if the target error rate
    equals or exceeds the error budget within the defined shorter alert window.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider an example with the following input parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The expected SLO is 99.9% over 30 days, resulting in 0.1% as the error budget.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alert window to examine: 10 minutes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Accordingly, a potential alert definition would be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*If the SLO is 99.9% over 30 days, alert if the error rate over the last 10
    minutes is >= 0.1%.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In approach # 1, a shorter time window results in alerts being fired more frequently,
    but tends to decrease precision.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Approach # 2 – Target error rate >= Error budget (with a longer alert window)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this approach, a longer alert window or larger window length is chosen (for
    example, 36 hours). Larger windows tend to yield better precision, but will have
    longer reset and detection times. This means that the portion of the error budget
    spent before the issue is detected is also high.
  prefs: []
  type: TYPE_NORMAL
- en: As per this approach, an alert should be triggered if the target error rate
    equals or exceeds the error budget within the defined larger time window.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider an example with the following input parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The expected SLO is 99.9% over 30 days**: Resulting in 0.1% as the error
    budget'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alert window to examine**: 36 hours'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Accordingly, a potential alert definition would be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*If the SLO is 99.9% over 30 days, alert if the error rate over the last 36
    hours is >= 0.1%*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In approach # 2, a longer time window results in higher precision, but the
    alert will be fired less frequently and may result in a higher detection time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Approach # 3 – Adding a duration for better precision'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this approach, a duration parameter can be added to the alert criteria,
    so that the alert won''t fire unless the value remains above the threshold for
    that duration. The choice of threshold also becomes significant as some of the
    alerts can go undetected if the threshold is too high. For example, if the duration
    window is 10 minutes and the signal data was up for 9 minutes but returned to
    normal before the 10th minute, the error budget will be consumed, but alerts will
    go undetected or will not get fired. With the right selection of duration parameter
    and threshold, this approach enables the error to be spotted quickly, but treats
    the error as an anomaly until the duration is reached:'
  prefs: []
  type: TYPE_NORMAL
- en: The advantage is that the alert fired after a defined duration will generally
    correspond to a significant event, and so will increase precision.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The disadvantage is that the error will continue to happen for a larger window
    and, as a result, will lead to a deteriorating recall.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A longer alert window or larger window length is recommended (for example, 36
    hours). Larger windows tend to yield better precision, but will have longer reset
    and detection times.
  prefs: []
  type: TYPE_NORMAL
- en: As per this approach, an alert should be triggered if the target error rate
    equals or exceeds the error budget within the defined larger time window.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider an example with the following input parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The expected SLO is 99.9% over 30 days**: Resulting in 0.1% as the error
    budget'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alert window to examine**: 36 hours'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Accordingly, a potential alert definition would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*If the SLO is 99.9% over 30 days, alert if the error rate over the last 36
    hours is >= 0.1%*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In approach # 3, a longer duration window also means that the portion of the
    error budget spent before the issue is detected is high, but is highly likely
    to indicate to a significant event.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Approach # 4 – Alert regarding the burn rate'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this approach, alerts should be defined based on burn rate. **Burn rate**
    is defined as how fast, relative to the SLO, the service consumes the error budget.
  prefs: []
  type: TYPE_NORMAL
- en: For example, with a 0.1% error budget over 30 days, if the error rate is constant
    at 0.1% across the 30 days, then the budget is spent equally and 0 budget remains
    at the end of the 30th day. In this case, the burn rate is calculated as 1\. But
    if the error rate is 0.2%, then the time to exhaustion will be 15 days, and the
    burn rate will be 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'When alerting based on burn rate, the following are two possible alerting policies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fast burn alert**: An alert is fired because of a sudden large change in
    consumption of the error budget. If not notified, then the error budget will exhaust
    quicker than normal. For a fast burn alert, the lookback period should be shorter
    (say 1 to 2 hours), but the threshold for the rate of consumption for the alert
    should be much higher (say 10x times) than the ideal baseline for the lookback
    period.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Slow burn alert**: An alert is not fired until a condition is continuously
    violated for a long period. This kind of alert only consumes a small percentage
    of the error budget when it occurs and is significantly less urgent than a fast
    burn alert. For a slow burn alert, the lookback period should be longer (say 24
    hours) but the threshold is slightly higher (say 2x times) than the baseline for
    the lookback period.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, as per approach # 4, alerts will be fired if the burn rate is greater than
    the desired burn rate at any point in time. This approach provides better precision
    over a shorter time window with good detection times.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Approach # 5 – Multiple burn rate alerts'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The same alerting strategy doesn't need to always work for a service. This might
    depend on factors such as the amount of traffic, the variable error budget, or
    peak and slow periods. For example, during a holiday shopping season, it is common
    that the SLO for a service will be higher than normal. This means a lower error
    budget during peak season and this will revert to a slightly less strict error
    budget during off-peak seasons.
  prefs: []
  type: TYPE_NORMAL
- en: In this approach, instead of defining a single condition in an alerting policy,
    multiple conditions can be defined to get better precession, recall, detection
    time, and reset time. Each condition can have a different level of severity and
    a different notification channel to notify the alert based on the nature of severity.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, an alerting policy can be defined with the following multiple
    conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Trigger an alert if 10% of the budget is consumed over a time duration of
    3 days and notify by creating a ticket*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Trigger an alert if 5% of the budget is consumed over a time duration of 6
    hours and notify through a page*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Trigger an alert if 2% of the budget is consumed over a time duration of 1
    hour and notify through a page*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In approach # 5, multiple conditions can be defined for a single alerting policy.
    Each condition could result in a different action that could potentially represent
    the severity level for the alert.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Approach # 6 – Multiple burn rate alerts across multiple windows'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is an extension of approach # 5, where the major difference is to use
    multiple windows to check whether the error rate exceeds the error budget rather
    than a single window for a single condition. This will ensure that the alert raised
    is always significant and is actively burning the error budget when it gets notified.'
  prefs: []
  type: TYPE_NORMAL
- en: This helps to create an alerting framework that is flexible and shows the direct
    impact based on the severity of the incident. A flexible window emphasizes or
    confirms whether the alert condition is active in the last specified duration.
    This helps to immediately troubleshoot when alerted. The flip side is that such
    conditions also involve multiple variables that might add to maintenance in the
    long term.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, an alerting policy can be defined with the following multiple
    conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Trigger an alert if 10% of the budget is consumed over a time duration of
    3 days and is currently being consumed over the last 6 hours. Notify by creating
    a ticket*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Trigger an alert if 5% of the budget is consumed over a time duration of 6
    hours and is currently being consumed over the last 30 minutes. Notify through
    a page*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Trigger an alert if 2% of the budget is consumed over a time duration of 1
    hour and is currently being consumed over the last 5 minutes. Notify through a
    page*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Approach # 6 is potentially an extension of approach # 5, where multiple conditions
    can be defined for an alerting policy where each condition, if breached, can result
    in a different action or notification. The difference is that approach # 6 emphasizes
    specifying an alert window that could confirm that the fired alert is potentially
    active.'
  prefs: []
  type: TYPE_NORMAL
- en: These approaches are well suited for many situations that could require an alerting
    strategy. However, there could be a specific situation where the traffic received
    by the service is less or low. The next topic discusses approaches on how to deal
    with such situations.
  prefs: []
  type: TYPE_NORMAL
- en: Handling service with low traffic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If a service receives a smaller amount of traffic, a single or a smaller number
    of failed requests might result in a higher error rate, indicating a significant
    burn of the error budget. SRE recommends a few options for handling a low-traffic
    service:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generating artificial traffic**: This option provides more signals or requests
    to work with. However, a significant amount of engineering effort is required
    to ensure that the artificial traffic behavior matches real user behavior closely.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Combining services**: This option recommends bucketizing similar low-request
    services into a single group representing a single function. This will result
    in higher precision and fewer false positives. However, careful consideration
    needs to be given when combining the services into a single group. This is because
    the failure of an individual service might not always result in the failure of
    the overall function and, as a result, will not result in an alert for a significant
    event.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modifying clients**: This option is used to deal with ephemeral failures,
    especially if it is impractical to generate artificial traffic or combine services
    into a group. The impact of a single failed request can be reduced by modifying
    the client and implement exponential backoff. Additionally, fallback paths should
    be set up to capture the request for eventual execution post-backoff.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lowering the SLO or increasing the window**: This option is the simplest
    way to handle low-traffic services. This will reduce the impact of a single failure
    on the error budget. However, lowering the SLO is also a way to lower the expectations
    on how the service should behave.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given that we learned about topics specific to creating an effective alerting
    strategy, the next logical step is to learn about steps required to establish
    an SLO altering policy.
  prefs: []
  type: TYPE_NORMAL
- en: Steps to establish an SLO alerting policy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following is the sequence of steps to establish an SLO alert policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Select the SLO to monitor**: Choose the suitable SLO for the service. It
    is recommended to monitor only one SLO at a time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Construct an appropriate condition**: It is possible to have multiple conditions
    for an alerting policy where the condition is different for a slow burn when compared
    to a fast burn.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Identify the notification channel**: Multiple notification channels can be
    selected at the same time for an alerting policy.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Include documentation**: It is recommended to include documentation about
    the alert that might help to resolve the potential underlying issue.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create an alerting policy**: The policy should be created in the monitoring
    system of choice, either through a configuration file or CLI, or through the console
    (based on the features supported by the monitoring system).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This concludes establishing the blueprint for the SLO alerting policy. The next
    topic introduces the desirable characteristics of an alerting system.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting system – desirable characteristics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The alerting system should control the number of alerts that the on-call engineers
    receive. The following is a shortlist of desirable characteristics that an alerting
    system should possess:'
  prefs: []
  type: TYPE_NORMAL
- en: Inhibit certain alerts when others are active.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove or silence duplicate alerts from multiple sources that have the same
    label sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan-in or fan-out alerts based on their label sets when multiple alerts with
    similar label sets fire.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This completes the section on alerting with insights into constructs that are
    required to define an effective alerting policy that includes possible approaches
    and key attributes, such as precision, recall, detection time, and reset time.
    The next section will provide an overview of time series, their structure, cardinality,
    and metric types.
  prefs: []
  type: TYPE_NORMAL
- en: Time series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Time series** data is the data that collectively represents how a system''s
    behavior changes over time. Essentially, applications relay a form of data that
    measures how things change over time. Time is not only regarded as a variable
    being captured; time is the primary focal point. Real-world examples of time series
    data include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Self-driving cars that continuously collect data to capture the ever-changing
    driving conditions or environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smart homes that capture events such as a change in temperature or motion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metric versus events
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Metrics** are time series measurements gathered at regular intervals. **Events**
    are time series measurements gathered at irregular time intervals.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following are some characteristics that qualify data as time series data:'
  prefs: []
  type: TYPE_NORMAL
- en: Data that arrives is always recorded as a new entry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data arrives in time order.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time is the primary axis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding a time field to the dataset is not the same as time series data
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Data related to a sensor is being collected in a non-time series database. If
    the sensor collects a new set of data, then writing this data will overwrite the
    previously stored data in the database with updated time. The database will eventually
    return the latest reading, but will not be able to track the change of data over
    a period of time. So, a non-time series database tracks changes to the system
    as **UPDATES**, but a time series database tracks changes to the system as **INSERTS**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The next topic discusses the structure of time series.
  prefs: []
  type: TYPE_NORMAL
- en: Time series structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Monitoring data is stored in time series. Each individual time series data
    has three pieces of information (refer to *Figure 3.1*):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Points**: Refers to a series of (timestamp, value) pairs. The value is the
    measurement, and the timestamp is the time at which the measurement was taken.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metric**: Refers to the name of the metric type that indicates how to interpret
    the data points. This also includes a combination of values for the metric labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitored resource**: Refers to the monitored resource that is the source
    of the time series data, and one combination of values for the resource''s label.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot shows the structure of time series data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Structure of time series data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15587_03_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 – Structure of time series data
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows an illustration of how time series data is represented
    for metric types with sample values: `storage.googleapis.com/api/request_count`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Illustration of a GCP Cloud storage metric with sample values'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15587_03_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 – Illustration of a GCP Cloud storage metric with sample values
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding screenshot represents the various events that were captured as
    part of the metrics as they happened over time. For example, the following information
    can be extracted from *Figure 3.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: There are a total of eight registered events in bucket `1234` between `2:00
    pm` and `2:10 pm`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This includes six successful events (three reads and three writes with different
    timestamps) and three unsuccessful write attempts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next topic introduces the concept of cardinality with respect to time series
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Time series cardinality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Each time series is associated with a specific pair of metric and monitored
    resource types, but each pair can have many time series. The possible number of
    time series is determined by the cardinality of the pair: the number of labels
    and the number of values each label can take on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a time series metric is represented as a combination of two labels:
    **zone** and **color**. There are two zones (east and west) and three colors for
    each zone (red, green, and blue). So, the cardinality for this metric is six.
    The following is the potential time series data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'With metric cardinality, there are a couple key pointers to bear in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: Metric cardinality is a critical factor with respect to performance. If the
    cardinality is too high, this means that there is a lot of time series data, resulting
    in higher query response times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For custom metrics, the maximum number of labels that can be defined in a metric
    type is 10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next topic introduces the common metric types with respect to time series
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Time series data – metric types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Each time series includes a metric type to represent its data points. The metric
    type defines how to interpret the values relative to one another. The three most
    common types of metric are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Counter**: A counter is a cumulative metric that represents a monotonically
    increasing function whose value can only increase (but can never decrease) or
    be reset to zero on restart; for example, a counter to represent the number of
    requests served, tasks completed, or errors observed up to a particular point
    in time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gauge**: A gauge metric represents a single numerical value that can arbitrarily
    go up and down. It is useful for monitoring things with upper bounds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of a gauge include the size of a collection or map or the number of
    threads in a running state. Gauges are also typically used for measured values
    such as temperatures or current memory usage, but also *counts* that can go up
    and down, such as the number of concurrent requests.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Distribution**: A distribution metric is used to track the distribution of
    events across configurable buckets; for example, measure the payload sizes of
    requests hitting the server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This completes the section on time series, where key concepts related to time
    series structure, cardinality, and possible metric types were summarized. This
    also brings us to the end of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the concepts related to monitoring, alerting,
    and time series that are critical in tracking SRE technical practices, such as
    the SLO and error budgets. We also discussed the differences between black box
    monitoring and white box monitoring. In addition, we examined the four golden
    signals as recommended by Google to be the desired SLI metrics for a user-facing
    system.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will focus on the constructs required to build an SRE
    team and apply cultural practices such as handling facets of incident management,
    being on-call, avoiding psychological safety, promoting communication and collaboration,
    and knowledge sharing.
  prefs: []
  type: TYPE_NORMAL
- en: Points to remember
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some important points to remember:'
  prefs: []
  type: TYPE_NORMAL
- en: Black box monitoring is based on testing externally visible behavior.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: White box monitoring is based on the metrics collected and exposed by the internals
    of the system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics must be specific, measurable, achievable, relevant, and time-bound.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The four golden signals recommended for a user-facing system are latency, traffic,
    errors, and saturation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latency can point to emerging issues, and traffic is historically used for capacity
    planning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Errors represent the rate of requests that fail explicitly or implicitly or
    by policy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saturation represents how full the service is and reflects degrading performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision is defined as the proportion of events detected that are significant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall is defined as the proportion of significant events detected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detection time is defined as the time taken by a system to notice an alert condition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reset time is defined as the length or duration of time the alerts are fired
    after an issue has been resolved.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An individual time series data has three critical pieces of information – points,
    metrics, and monitored resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information on GCP''s approach to DevOps, read the following articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SRE**: [https://landing.google.com/sre/](https://landing.google.com/sre/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SRE Fundamentals**: [https://cloud.google.com/blog/products/gcp/sre-fundamentals-slis-slas-and-slos](https://cloud.google.com/blog/products/gcp/sre-fundamentals-slis-slas-and-slos)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SRE Youtube Playlist**: [https://www.youtube.com/watch?v=uTEL8Ff1Zvk&list=PLIivdWyY5sqJrKl7D2u-gmis8h9K66qoj](https://www.youtube.com/watch?v=uTEL8Ff1Zvk&list=PLIivdWyY5sqJrKl7D2u-gmis8h9K66qoj)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metrics, time series, and resources**: [https://cloud.google.com/monitoring/api/v3/metrics](https://cloud.google.com/monitoring/api/v3/metrics)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practice test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Select the monitoring option that works based on the metrics exposed by the
    internals of the system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Alert-based monitoring
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) White box monitoring
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Log-based monitoring
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Black box monitoring
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Select the monitoring source that doesn't provide information in near-real time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Logs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Metrics
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Both
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) None of the above
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From the perspective of a fast burn alerting policy, select the appropriate
    threshold in relative comparison to the baseline for the defined lookback interval.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) The threshold = the baseline.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The threshold is < the baseline.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) The threshold is significantly higher than the baseline.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) The threshold is slightly higher than the baseline.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Select the appropriate options for sending alerts
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Email
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Page
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Text
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) All of the above
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Select the monitoring that is best suited to paging incidents.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Alert-based monitoring
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) White box monitoring
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Log-based monitoring
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Black box monitoring
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which of the following is not part of Google's recommended golden signals?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Traffic
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Throughput
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Saturation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Errors
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which of the following alerting policies recommends a longer lookback window?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Fast burn
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Slow burn
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Both
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) None
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which of the following represents the action of collecting, processing, aggregating,
    and displaying real-time quantitative data relating to a system, such as query
    counts and types, error counts and types, processing times, and server lifetimes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Alerting
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Monitoring
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Debugging
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Troubleshooting
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which of the following represents time series measurements gathered over irregular
    time intervals?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Metrics
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Events
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Logs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Trace
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which of the following is not a suitable source of white box monitoring?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Metrics
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Load balancer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Logs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Traces
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: (b) White box monitoring.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (a) Logs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (c) The threshold is significantly higher than the baseline. The recommended
    level is 10x.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (d) All of the above, including email, pages, and texts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (d) Black box monitoring.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (b) Throughput. The four golden signals are latency, errors, traffic, and saturation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (b) Slow burn alert policy
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (b) Monitoring
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (b) Events. Events are time series measurements gathered at irregular time intervals.
    Metrics are time series measurements gathered at regular time intervals.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(b) Load balancer. It is best suited as a source for black box monitoring.
    White box monitoring collects information from three critical components: metrics,
    logs, and traces.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
