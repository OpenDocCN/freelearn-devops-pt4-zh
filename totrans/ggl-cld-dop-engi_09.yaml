- en: '*Chapter 7*: Understanding Kubernetes Essentials to Deploy Containerized Applications'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last two chapters ([*Chapter 5*](B15587_05_Final_ASB_ePub.xhtml#_idTextAnchor110),
    *Managing Source Code Using Cloud Source Repositories*, and [*Chapter 6*](B15587_06_Final_ASB_ePub.xhtml#_idTextAnchor131),
    *Building Code Using Cloud Build, and Pushing to Container Registry*) focused
    on Google Cloud services to manage source code via cloud source repositories,
    build code via Cloud Build, and create image artifacts using Container Registry.
    Given that the focus of this book is to deploy containerized applications, the
    next three chapters (from [*Chapter 7*](B15587_07_Final_ASB_ePub.xhtml#_idTextAnchor154),
    *Understanding Kubernetes Essentials to Deploy Containerized Applications*, through
    to [*Chapter 9*](B15587_09_Final_ASB_ePub.xhtml#_idTextAnchor201), *Securing the
    Cluster Using GKE Security Constructs*) are centered around essential concepts
    related to deploying containerized applications through Kubernetes, easy cluster
    management through **Google Kubernetes Engine** (**GKE**), and a rundown of key
    security features in GKE that are essential for hardening the Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes, or K8s, is an open source container orchestration system that can
    run containerized applications. Kubernetes originated as an internal cluster management
    tool from Google that it donated to **Cloud Native Computing Foundation** (**CNCF**)
    as an open source project in 2014\. This specific chapter will focus on Kubernetes
    essentials that are required for containerized deployments. This includes understanding
    the cluster anatomy, and getting acquainted with Kubernetes objects, specifically
    related to workloads, deployment strategies, and constraints around scheduling
    applications. Google open sourced Kubernetes and donated it to CNCF. This specific
    chapter doesn't deep dive into setting up a Kubernetes cluster. It takes a significant
    effort and manual intervention to run the open source version of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Google offers a managed version of Kubernetes called Google Kubernetes Engine,
    otherwise known as GKE. Essentially, the fundamentals of Kubernetes apply to GKE
    as well. However, GKE makes it easy to set up a Kubernetes cluster and includes
    additional capabilities that facilitate cluster management. The next chapter focuses
    on GKE core features and includes steps to create a cluster. However, this chapter
    essentially focuses and elaborates on the fundamentals of Kubernetes, which is
    also the core of GKE and helps to make the transition to GKE much easier.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter introduces Kubernetes as the container orchestration of choice
    and provides details on the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubernetes**: A quick introduction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes Cluster Anatomy**: Deep dives into the constructs that form a
    Kubernetes cluster along with the components that form the master control plane'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes Objects**: Provides a high-level overview of critical Kubernetes
    objects used to deploy workloads'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scheduling and interacting with Pods**: Details the constraints evaluated
    and interactions involved while scheduling applications on Kubernetes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes deployment strategies**: Details potential deployment strategies,
    from the option that essentially recreates applications, via deployment options
    that ensure zero downtime, to the option that enables the shifting of a specific
    amount of traffic to the new application'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are four main technical requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A valid **Google Cloud Platform** (**GCP**) account to go hands-on with GCP
    services: [https://cloud.google.com/free](https://cloud.google.com/free).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Install Google Cloud SDK: [https://cloud.google.com/sdk/docs/quickstart](https://cloud.google.com/sdk/docs/quickstart).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Install Git: [https://git-scm.com/book/en/v2/Getting-Started-Installing-Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Install Docker: [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes – a quick introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **container** is a unit of software that packages code and its dependencies,
    such as libraries and configuration files. When compared to running applications
    on physical or virtual machines, a container enables applications to run faster
    and reliably across computing environments. Containers make it easier to build
    applications that use microservice design patterns. They are critical to the concept
    of continuous development, integration, and deployment as incremental changes
    can be made against a container image and can be quickly deployed to a compute
    environment of choice (that supports process isolation).
  prefs: []
  type: TYPE_NORMAL
- en: Given that containers are lean and easy to deploy, an organization might end
    up deploying its applications as several containers. This poses challenges as
    some of the applications might need to interact with one another. Additionally,
    the life cycle of the application should also be monitored and managed. For example,
    if an application goes down due to resource constraints, then another instance
    of the application should be made available. Similarly, if there is a sudden spike
    in traffic, the application should horizontally scale up and when traffic returns
    to normal, the application should subsequently scale down.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling actions (up or down) should be provisioned automatically rather than
    manually. This creates a need for container orchestration and will be discussed
    as the next topic.
  prefs: []
  type: TYPE_NORMAL
- en: Container orchestration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Container orchestration** is about managing the life cycle of containers,
    specifically in large dynamic environments. Container orchestration can control
    and automate tasks such as provisioning, deployment, maintaining redundancy, ensuring
    availability, and handling changing traffic by scaling up or down as needed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, container orchestration can also handle the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: Move containers from one host node to the other in case the host node dies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set eviction policies if a container is consuming more resources than expected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provision access to persistent storage volumes in case a container restarts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secure interactions between containers by storing keys/secrets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor the health of the containers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes** traces its lineage from Borg – an internal Google project that
    is essentially a cluster manager that runs large-scale containerized workloads
    to support core Google services such as Google Search. Kubernetes was the next-generation
    cluster manager after Borg. The most popular concepts in Kubernetes came from
    Borg, such as Pods, services, labels, and ip-per-pod.'
  prefs: []
  type: TYPE_NORMAL
- en: Alternative container orchestration options
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm, Apache Mesos, OpenShift, and suchlike are a few alternatives for
    container orchestration outside Kubernetes. Docker Swarm is easy to get started
    and set up the cluster, but has limited features specific to scaling. Mesos is
    a cluster manager that is best suited to large systems and designed with maximum
    redundancy. It is complex in nature (in terms of features and configuration) and
    recommended for workloads such as Hadoop and Kafka, but is not suitable for mid-
    or small-scale systems.
  prefs: []
  type: TYPE_NORMAL
- en: The upcoming section summarizes the main features of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are some of the key features in Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Declarative configuration**: Kubernetes administers the infrastructure declaratively,
    in other words, Kubernetes monitors the current state and takes the required action
    to ensure that the current state matches the desired state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automation**: Kubernetes'' implementation of declarative configuration inherently
    supports automation. In addition, Kubernetes allows a wide range of user preferences
    and configurations. As a result, Kubernetes can automatically scale in and scale
    out containerized applications based on a myriad of conditions, with resource
    utilization or resource limits being a few of them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stateful and stateless**: Kubernetes supports both stateful and stateless
    applications. In the case of stateful applications, a user''s state can be stored
    persistently. In addition, both batch jobs and daemon tasks are also supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Container management**: Kubernetes supports the features of infrastructure
    as service, such as logging, monitoring, and load balancing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next section outlines the structure of a Kubernetes deployment and deep
    dives into its key components and their capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes cluster anatomy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Kubernetes cluster is a collection of machines with compute power. These machines
    could be actual physical computers or could even be **virtual machines** (**VMs**).
  prefs: []
  type: TYPE_NORMAL
- en: In reference to cloud deployments, a Kubernetes cluster will be a collection
    of VMs. Each VM is termed a node. The nodes in a cluster are categorized as either
    *master* or *worker* nodes. Worker nodes run applications that are deployed in
    containers. The master node runs the control plane components that are responsible
    for coordinating tasks across the worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '*Throughout this chapter, and for ease of reference, the node running the control
    plane components will be referred to as the master, and worker nodes will be referred
    to as nodes.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The master has the following responsibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: It tracks information across all nodes in a cluster, such as applications or
    containers that a node is running.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It schedules applications on nodes by identifying nodes based on requirements
    (such as resource, affinity, or Anti-Affinity constraints).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It ensures that a desired number of instances are always running as per the
    deployment specifications and orchestrates all operations within the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 7.1* shows an illustration of a Kubernetes cluster that includes both
    master and nodes. These are comprised of machines with compute power:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Outline of a Kubernetes cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15587_07_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.1 – Outline of a Kubernetes cluster
  prefs: []
  type: TYPE_NORMAL
- en: The master performs its responsibilities using a set of key components that
    form the **Kubernetes Control Plane** and will be detailed in the upcoming topic.
  prefs: []
  type: TYPE_NORMAL
- en: Master components – Kubernetes control plane
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Kubernetes control plane consists of components that make decisions with
    regard to operations within the cluster and respond to cluster-specific events.
    Events can include, but are not limited to, scaling up the number of instances
    with respect to the application if the average CPU utilization exceeds a specific
    configured threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the key components of the Kubernetes control plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '**kube-apiserver**: A frontend to Kubernetes for cluster interactions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**etcd**: Distributed key-value stores for cluster-specific information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kube-scheduler**: Responsible for distributing workloads across nodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kube-controller-manager**: Tracks whether a node or application is down'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cloud-controller-manager**: Embeds cloud-specific control logic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 7.2* shows an illustration of the components that run on the master
    and form the Kubernetes control plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Kubernetes control plane on the master node'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15587_07_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.2 – Kubernetes control plane on the master node
  prefs: []
  type: TYPE_NORMAL
- en: The control plane components can be run on any machine in the cluster, but it
    is recommended to run on the same machine and avoid any user-specific containers.
    It's also possible to have multiple control planes when building a highly available
    cluster. Each of the key components is introduced in the upcoming sub-section.
  prefs: []
  type: TYPE_NORMAL
- en: kube-apiserver
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`HTTP`, `gRPC`, and `kubectl`). All other components of the control plane can
    be viewed as clients to `kube-apiserver`. Additionally, `kube-apiserver` is also
    responsible for authentication, authorization, and managing admission control.'
  prefs: []
  type: TYPE_NORMAL
- en: Authentication, authorization, and admission control
  prefs: []
  type: TYPE_NORMAL
- en: From a cluster standpoint, authentication is about who can interact with the
    cluster (this could be a user or service account); authorization is about what
    specific operations are permitted and admission control represents a set of plugins
    that could limit requests to create, delete, modify, or connect to a proxy. ResourceQuota
    is an example of an admission controller where a namespace can be restricted to
    only use up to a certain capacity of memory and CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Any query or change to the cluster is handled by `kube-apiserver`. It is also
    designed to horizontally scale by deploying instances to handle incoming requests
    to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: etcd
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`etcd` is a distributed key-value store used by Kubernetes to store information
    that is required to manage the cluster, such as cluster configuration data. This
    includes nodes, Pods, configs, secrets, accounts, roles, and bindings. When a
    `get` request is made to the cluster, the information is retrieved from `etcd`.
    Any `create`, `update`, or `delete` request made to the cluster is complete only
    if the change is reflected in `etcd`.'
  prefs: []
  type: TYPE_NORMAL
- en: kube-scheduler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`kube-scheduler` considers multiple factors, such as the resource requirements
    of an application, node availability, affinity, and Anti-Affinity specifications.
    Affinity and Anti-Affinity specifications represent policy definitions that allow
    certain applications to be deployed against specific nodes or prevent deployment
    against specific nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: kube-controller-manager
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`kube-apiserver` and ensures that the current state of the cluster matches
    the desired state. `kube-controller-manager` is responsible for the actual running
    of the cluster, and accomplishes this by using several controller functions. As
    an example, a node controller monitors and responds when a node is offline. Other
    examples include the replication controller, namespace controller, and endpoints
    controller.'
  prefs: []
  type: TYPE_NORMAL
- en: cloud-controller-manager
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**cloud-controller-manager** includes controller functions that allow Kubernetes
    to be integrated with services from a cloud provider. The controller functions
    are responsible for handling constructs such as networking, load balancers, and
    storage volumes that are specific to the cloud provider.'
  prefs: []
  type: TYPE_NORMAL
- en: The master receives a request to perform a specific operation and the components
    in the control plane schedule, plan, and manage the operations to be performed
    on the nodes. Kubernetes doesn't natively consist of out-of-the-box integration
    (say with Google or AWS). The operations on the nodes are carried out by a set
    of components that form the node control plane and will be detailed in the upcoming
    sub-section.
  prefs: []
  type: TYPE_NORMAL
- en: Node components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Nodes receive instructions from the master, specifically, the `kube-apiserver`.
    Nodes are responsible for running applications deployed in containers and establish
    communication between services across the cluster. The nodes perform these responsibilities
    by using a set of key components. These components are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-apiserver` and runs containers as per the Pod specification provided'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kube-proxy**: A network proxy that enables communication between services'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**container runtime**: Software that is responsible for running containers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 7.3* shows an illustration of components that form the node control
    plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Components of the node control plane'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15587_07_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.3 – Components of the node control plane
  prefs: []
  type: TYPE_NORMAL
- en: Node components run on each worker node in the cluster and provide the Kubernetes
    runtime environment. Each of the key components for a worker node is introduced
    in the upcoming topic.
  prefs: []
  type: TYPE_NORMAL
- en: kubelet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`kube-apiserver` connects with the node through `kubelet`, the node''s agent.
    `kubelet` listens for instructions and deploys or deletes containers when told
    to. `kubelet` doesn''t manage containers that are not created by Kubernetes.'
  prefs: []
  type: TYPE_NORMAL
- en: kube-proxy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`kube-proxy` runs on each node in the cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: container runtime engine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`containerd`, and CRI-O.'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes deprecating Docker as a container runtime engine
  prefs: []
  type: TYPE_NORMAL
- en: Based on the release notes for Kubernetes v1.20, `dockershim` will be deprecated
    and cannot be used from v1.22\. `dockershim` is a module in `kubelet` and a temporary
    solution proposed by the Kubernetes community to use Docker as a container runtime.
    Due to the maintenance burden, `dockershim` will be deprecated and the Kubernetes
    community will only maintain the Kubernetes `containerd` and CRI-O are examples
    of a CRI-compliant runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'This completes a deep dive into Kubernetes cluster anatomy that specifically
    consists of components from the master control plane and components from the node
    control plane. Communication within the master control plane is driven by the
    `kube-api` server, which sends instructions to the `kubelet` on the respective
    nodes. `kubelet` executes the instructions sent by the `kube-api` server. *Figure
    7.4* shows an illustration of the entire cluster anatomy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Kubernetes cluster anatomy'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15587_07_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.4 – Kubernetes cluster anatomy
  prefs: []
  type: TYPE_NORMAL
- en: It is important to understand that any interaction against a Kubernetes object,
    such as create, modify, or delete, can only be performed through the Kubernetes
    API. These operations on the object can also be performed through the CLI using
    the `kubectl` command. The next topic details the usage of the `kubectl` command.
  prefs: []
  type: TYPE_NORMAL
- en: Using kubectl
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`kubectl` is typically used by administrators. `kubectl` enables an action
    to be performed, such as get or delete, against a specific object type with a
    specific object name along with supported request parameters. `kubectl` communicates
    with `kube-apiserver` on the master and converts commands issued by the CLI into
    API calls. `kubectl` can be used to create Kubernetes objects, view existing objects,
    delete objects, and view/export configurations. The syntax structure of `kubectl`
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step involved in using the `kubectl` command is to configure the
    credentials of the cluster, such as the cluster name and its location. `kubectl`
    stores this configuration in a file called `config` and stores the file in a hidden
    folder called `.kube` in the home directory. The current configuration can be
    retrieved by using the `view` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The actions on the cluster are executed using Kubernetes objects. Each object
    has a specific purpose and functionality. There are many such objects in Kubernetes.
    The upcoming section introduces the concept of Kubernetes objects and details
    the most frequently used objects.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Kubernetes object is a persistent entity and represents a record of intent.
    An object can be defined using the **YAML** configuration. It will have two main
    fields – spec and status. The object spec represents the specification, and the
    object state represents the desired state. Once the object is created, the Kubernetes
    system will ensure that the object exists as per the specified declarative configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes supports multiple object types. Each object type is meant for a
    specific purpose. The following are some critical Kubernetes objects that will
    be used throughout this chapter. This is not an exhaustive list:'
  prefs: []
  type: TYPE_NORMAL
- en: Pods – The smallest atomic unit in Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment – Provides declarative updates for Pods and ReplicaSets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StatefulSet – Manages stateful applications and guarantees ordering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DaemonSet – Runs a copy of the Pod on each node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Job – Creates one or more Pods and will continue to retry execution until a
    specified number of them terminate successfully
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CronJob – A job that occurs on a schedule represented by a cron expression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Services – Exposes applications running one or more Pods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment, ReplicaSet, StatefulSet, DaemonSet, Jobs, and CronJobs are specifically
    categorized as **Workload Resources**. All these workload resources run one or
    more Pods. This chapter details the abovementioned Kubernetes objects in the upcoming
    sub-sections. Please note that the information provided is not exhaustive from
    the aspect of an object's functionality, but provides an in-depth review of the
    object's purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Pod
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Pod** is a Kubernetes object and is the smallest deployable compute unit
    in a Kubernetes cluster. Application code exists in container images. Container
    images are run using containers and containers run inside a Pod. A Pod resides
    inside a node.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A Pod can contain one or more containers. A Pod provides a specification on
    how to run the containers. The containers in a Pod share filesystem, namespace,
    and network resources. A Pod also has a set of ports or port ranges assigned.
    All containers in the Pod have the same IP address, but with different ports.
    The containers within the Pod can communicate by using the port number on localhost.
    The following is a declarative specification for a Pod that runs an `nginx` container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is an equivalent imperative command to create a similar Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: What is CrashLoopBackOff?
  prefs: []
  type: TYPE_NORMAL
- en: There are certain situations where a Pod attempts to start, crashes, starts
    again, and then crashes again; essentially, a condition reported by a Pod where
    a container in a Pod has failed to start after repeated attempts.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the Kubernetes platform, Pods are the atomic units and run one or more containers.
    A Pod consists of multiple containers if they form a single cohesive unit of Service.
    The sidecar pattern is a common implementation of a Pod with multiple containers.
    These are popularly used in ETL-specific use cases. For example, logs of the `hello-world`
    container need to be analyzed in real time. `logs-analyzer` is a specialized application
    that is meant to analyze logs. If each of these containers is in their respective
    Pods as `pod-hello-world` and `pod-logs-analyzer`, the `logs-analyzer` container
    can get the logs of the `hello-world` container through a `GET` request. Refer
    to *Figure 7.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Communication between containers in different Pods'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15587_07_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.5 – Communication between containers in different Pods
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there will be minimal network latency since both the containers are
    in separate Pods. If both containers are part of the same Pod, `pod-hello-world-etl`
    forming a sidecar pattern, then the Pod will consist of two containers – `logs-analyzer`
    acting as the sidecar container that will analyze logs from another container,
    `hello-world`. Then, these containers can communicate on localhost because they
    are on the same network interface, providing real-time communication. Refer to
    *Figure 7.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Communication between containers in a sidecar pattern'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15587_07_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.6 – Communication between containers in a sidecar pattern
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a single Pod with multiple containers allows the application to run as
    a single unit and reduces network latency as the containers communicate on the
    same network interface. The following is a declarative specification of a Pod
    that runs multiple containers with the specific example as illustrated in *Figure
    7.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Job and CronJob
  prefs: []
  type: TYPE_NORMAL
- en: Job and CronJob are workload resources. A job represents a task to execute a
    Pod. A job is completed if the task is executed successfully or, in other words,
    a Pod runs successfully to completion for a specified number of times. If a job
    is deleted, then Pods tied to the jobs are also deleted. If a job is suspended,
    then active Pods are deleted. Multiple jobs can be run in parallel. CronJob is
    a workload resource and is essentially a job that is set with a schedule through
    a cron expression.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.7* brings together the examples related to a single container Pod,
    `my-nginx`, and the multiple container Pod, `pod-hello-world-etl`, and illustrates
    how these Pods can be potentially connected within a node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Pod connectivity within a node'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15587_07_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.7 – Pod connectivity within a node
  prefs: []
  type: TYPE_NORMAL
- en: Pods are ephemeral in nature, and so is the storage associated with Pods. Hence,
    Pods are better suited for stateless applications. However, Pods can also be used
    for stateful applications, but in such cases, Pods should be attached to persistent
    storage or volumes. Pods are also meant to run a single instance of the application.
    Multiple instances of Pods should be used to scale horizontally. This is referred
    to as replication. So, Pods cannot scale by themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Liveness, readiness, and start up probes
  prefs: []
  type: TYPE_NORMAL
- en: A liveness probe is used to check whether the application is running as expected
    and if not, the container is restarted. A readiness probe is used to check whether
    an application is up but also ready to accept traffic. A start up probe indicates
    when a container application has started. If a start up probe is configured, then
    this will disable the liveness and readiness checks until the start up probe succeeds.
    For more detailed information, refer to [https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/).
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes uses specific workload resources to create and manage multiple Pods.
    The most common ones are Deployment, StatefulSet, and DaemonSet, and these will
    be detailed in the upcoming sub-sections.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **Deployment** is a Kubernetes object that provides declarative updates for
    Pods and ReplicaSets. A Deployment is part of the Kubernetes API group called
    apps.
  prefs: []
  type: TYPE_NORMAL
- en: API groups
  prefs: []
  type: TYPE_NORMAL
- en: API groups are a way to extend the Kubernetes API. All supported API requests
    or future requests are placed in a specific group for easy categorization and
    this includes versioning. The most common group is the core group, also known
    as the legacy group. The core group is specified with `apiVersion` as `v1`. Pods
    fall under the core group. A Deployment falls under the apps group and is referred
    to with `apiVersion` as `apps/v1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Deployment provides a declarative way to manage a set of Pods that are replicas.
    The deployment specification consists of a Pod template, Pod specification, and
    the desired number of Pod replicas. The cluster will have controllers that constantly
    monitor and work to maintain the desired state, and create, modify, or remove
    the replica Pods accordingly. Deployment controllers identify Pod replicas based
    on the matching label selector. The following is a declarative specification of
    a deployment that wraps three replicas of **nginx** Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a set of equivalent imperative commands to create a similar
    deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Deployments support autoscaling using the concept of **HorizontalPodAutoscaler**
    (**HPA**), based on metrics such as CPU utilization. The following is the command
    to implement HPA. HPA will be discussed in detail as part of [*Chapter 8*](B15587_08_Final_ASB_TD_ePub.xhtml#_idTextAnchor182),
    *Understanding GKE Essentials to Deploy Containerized Applications*. This focuses
    on GKE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: A deployment can be updated using the rolling update strategy. For example,
    if the image version is updated, then a new ReplicaSet is created. A rolling update
    will ensure that the deployment will move the Pods from the old ReplicaSet to
    the new ReplicaSet in a phased-out manner to ensure 0% downtime. If an error occurs
    while performing a rolling update, the new ReplicaSet will never reach *Ready*
    status and the old ReplicaSet will not terminate, thereby enabling 0% downtime.
    Deployments and Pods are connected by labels. Each Pod is given a label. The deployment
    has a label selector. So, any updates to the deployments are rolled out to the
    Pods with matching labels.
  prefs: []
  type: TYPE_NORMAL
- en: A Deployment is well-suited for a stateless application, where a request will
    be served in a similar manner by either of the replica Pods. However, there is
    another Deployment resource that is stateful in nature and is called StatefulSets.
    This will be covered as the next topic.
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`sample`, then the Pod''s name will be `sample-0`. If there are three replicas,
    then additional Pods called `sample-1` and `sample-2` will be created. This is
    completely different from deployment, where all Pods share the same volume.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a declarative specification of a StatefulSet with three replicas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If the StatefulSet is scaled down, then the last Pod in the StatefulSet will
    be removed (in other words, in reverse order). In the preceding example, if the
    replica count is reduced to two from three, then the `sample-2` Pod will be deleted.
    The StatefulSet supports rolling updates if there is any change. An old version
    of the Pod for a specific replica will be replaced when the new version of the
    Pod on that specific replica is back up. For example, `sample-0` will be replaced
    with a new version of `sample-0`. The next topic provides an overview of DaemonSets.
  prefs: []
  type: TYPE_NORMAL
- en: DaemonSets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`kube-proxy` is a DaemonSet because a copy of it runs on each node in the cluster
    as part of the node control plane. Additional examples include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Running a log collector daemon on every node or a certain subset of nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a cluster storage daemon on every node or a certain subset of nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a node monitoring daemon on every node or a certain subset of nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To elaborate further, in the case of a log collection daemon, logs are exported
    from each node using a log collector such as `fluentd`. This can be done by a
    `fluentd` Pod and should be run on every node in the cluster. The following is
    a declarative specification of a log collection DaemonSet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Like Deployments, the DaemonSet also supports rolling updates. So, if the DaemonSet
    is updated, then a new Pod is created and when the new Pod is up, the current
    Pod will be deleted.
  prefs: []
  type: TYPE_NORMAL
- en: The next topic discusses a Kubernetes object called Service. This is essential
    for establishing communication with an application from within the cluster and
    from outside the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As previously mentioned, Pods are ephemeral in nature. Pods' IP addresses are
    not long-lived and can keep changing. This poses a challenge if an API request
    needs to be sent to the Pod's container using its IP address.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes provides a stable abstraction point for a set of Pods called a **Service**.
    Every Service has a fixed IP address that doesn't change, and this gets registered
    with the cluster's built-in DNS. A Service identifies associated Pods using label
    selectors.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, when a Service object is created, Kubernetes creates another object
    called **EndPoint**. The EndPoint object will maintain the list of all IPs for
    the Pods that match the label selector and is constantly updated as Pods are deleted
    and created. The Service object gets the current set of active Pods from the EndPoint
    object.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.8* illustrates the interaction between the Service object, endpoint
    object, and the associated Pods based on the matching label selector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Service object interaction based on a matching label selector'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15587_07_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.8 – Service object interaction based on a matching label selector
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a declarative specification that exposes a Service for a set
    of Pods. This allows the Pod to be accessed using the Service, since the Service
    is not ephemeral in nature and will have a fixed IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is an equivalent imperative command that can expose Pods as a
    Service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'There are four types of Service, and each Service type exposes Pods differently:'
  prefs: []
  type: TYPE_NORMAL
- en: ClusterIP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NodePort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LoadBalancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ExternalName
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding specification represents a Service of the ClusterIP type as that's
    the default Service type. This will be introduced as the next topic.
  prefs: []
  type: TYPE_NORMAL
- en: ClusterIP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**ClusterIP** is the default Service type. Each Service gets an IP that can
    only be accessed by other services within the cluster. This is essentially an
    internal IP, and hence the application inside the Pods cannot be accessed by public
    traffic or by an external Service that resides outside the cluster. The default
    Service type, if not specified, is ClusterIP. The preceding declarative specification
    is an example of a ClusterIP Service.'
  prefs: []
  type: TYPE_NORMAL
- en: NodePort
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**NodePort** is a Service type where the Service gets an internal IP that can
    be accessed by other services within the cluster. In addition, the NodePort Service
    gets a cluster-wide port. This port can be accessed by a Service that resides
    outside the cluster only if the request is sent to Node''s IP address along with
    the cluster-wide port. Any traffic sent to the cluster-wide port will be redirected
    to the Pods associated with the Service. The following is a declarative specification
    that exposes a node port Service for a set of Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: LoadBalancer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**LoadBalancer** is a Service type where the Service gets an internal IP that
    can be accessed by other services within the cluster. In addition, the Service
    also gets an external IP address that allows the application to receive traffic
    from a Service that resides outside the cluster. This is facilitated by the public
    cloud load balancer attached to the Service. The following is a declarative specification
    that exposes a load balancer Service for a set of Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: ExternalName
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`ExternalName` type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Hence, a request from an internal client will go to `my-service.default.svc.cluster.local`,
    and then the request gets redirected to `hello.com`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This completes an overview of the most common Service types in Kubernetes.
    One of the factors to consider while using services is to map services to Pods,
    otherwise known as Service resolution. Kubernetes has an add-on feature called
    `kube-dns` is a DNS server that is essentially a directory mapping of IP addresses
    against easy-to-remember names along with a record type:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `kube-dns` server watches the API server for the creation of a new Service.
    When a new server is created, the `kube-dns` server creates a set of DNS records.
    Kubernetes is configured to use the `kube-dns` server''s IP to resolve DNS names
    for Pods. Pods can resolve their Service IP by querying the `kube-dns` server
    using the Service name, the Pod''s namespace, and the default cluster domain:'
  prefs: []
  type: TYPE_NORMAL
- en: If the Pod and Service are on the same namespace, then the Pod can resolve the
    Service IP by querying the `kube-dns` server using the Service name directly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the Pod and Service are not on the same namespace, then the Pod can resolve
    the Service IP by querying the `kube-dns` server using the Service and the Service
    namespace.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Pod in any other namespace can resolve the IP address of the Service by using
    the fully qualified domain name, `foo.bar.svc.cluster.local`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-dns` maintains the following types of DNS record for Pods and services:'
  prefs: []
  type: TYPE_NORMAL
- en: Every Service defined in the cluster is assigned a DNS A record.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every named Pod in the cluster is assigned a DNS SRV record.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following table represents `kube-dns` records where the hostname is `foo`
    and the namespace is `bar`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15587_07_Table_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This concludes a high-level overview of specific Kubernetes objects, and this
    should provide a good basis for discussing GKE in the next chapter. There are
    several other objects, such as job, CronJob, volumes, and persistent volumes,
    but a deep dive on those will be beyond the scope of the book.
  prefs: []
  type: TYPE_NORMAL
- en: The next topic details several concepts with respect to scheduling and interacting
    with Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling and interacting with Pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Pod is the smallest unit of deployment in a Kubernetes cluster that runs containerized
    applications. The `kube-scheduler` master control plane component is responsible
    for finding a suitable node for the Pod and includes interactions with other components
    of the control plane. In addition, `kube-scheduler` needs to consider multiple
    configuration options, such as NodeSelector, NodeAffinity, and PodAffinity, to
    find the right node for the Pod. This section details the interactions that happen
    during a Pod creation and details the factors that need to be considered while
    scheduling Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing master plane interactions on Pod creation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A Pod is a workload that needs to be deployed in a Kubernetes cluster. A Pod
    needs to run on a node and will host an application. A Pod can be in various phases.
    The following is a summary of valid Pod phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pending**: A Pod is accepted by the Kubernetes cluster, but is waiting to
    be scheduled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Running**: A Pod is tied to a node and the container in the Pod is running.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Succeeded** or **Completed**: All containers in a Pod have terminated successfully
    and will not be restarted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Failed**: All containers in the Pod have terminated and at least one container
    exited with a non-zero status or failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unknown**: The state of the Pod cannot be obtained due to a communication
    error between the node where the Pod should be running.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Right from the time a request is received to create a Pod to the time the Pod
    is created, there is a series of interactions between the components of the master
    plane that will create the Pod on the worker node. The sequence of interactions
    is listed as follows. This reflects a scenario where a Pod is being created. The
    sequence of steps for other interactions, such as list or delete, or even other
    workloads, such as job or deployment, follow the same pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-apiserver` receives a request to create a Pod. The request can come from
    a `kubectl` command or a direct API interaction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kube-apiserver` authenticates and authorizes the incoming request.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upon successful validation, `kube-apiserver` creates a Pod object but will not
    assign the newly created Pod object to any node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kube-apiserver` will update the information about the newly created Pod object
    against the `etcd` database and sends a response to the original request for Pod
    creation that a Pod has been created.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kube-scheduler` continuously monitors and realizes that there is a new Pod
    object but with no node assigned.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kube-controller` identifies the right node to put the Pod and communicates
    this back to `kube-apiserver`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kube-apiserver` updates the node for the Pod object against the `etcd` database.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kube-apiserver` passes instructions to `kubelet` on the node (worker) to physically
    create the Pod object.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubelet` creates the Pod on the node and instructs the container runtime engine
    to deploy the application image.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubelet` updates the status back to `kube-apiserver` and `kube-apiserver`
    updates the `etcd` database.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This summarizes the interactions between master plane components when a request
    is sent to the Kubernetes cluster through `kubectl` or the Kubernetes client.
    The next sub-section focuses on critical factors that should be considered while
    scheduling Pods against the node.
  prefs: []
  type: TYPE_NORMAL
- en: Critical factors to consider while scheduling Pods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are multiple factors that `kube-scheduler` considers when scheduling a
    Pod against a node. One such common factor is resource requests and maximum limits.
    A Pod optionally allows the specification of CPU/memory requests and sets the
    respective maximum limits on a container basis. These requests and limits at container
    level are summed up for a Pod and are used by `kube-scheduler` to determine the
    appropriate node for the Pod. `kube-scheduler` schedules a Pod on the node where
    the Pod's requests and limits are within the node's available capacity.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Pod provides additional properties that exercise more control in forcing
    `kube-scheduler` to schedule Pods only if certain conditions are met. A node also
    provides properties that are considered during scheduling. The following are such
    properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NodeSelector**: Schedules a Pod against the node with matching label values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NodeAffinity**: Schedules a Pod against the node with matching flexible conditions;
    also considers Anti-Affinity conditions to avoid scheduling a Pod against specific
    node(s)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inter-pod affinity and Anti-Affinity**: Schedules a Pod on nodes with Pods
    having matching attributes; also considers Anti-Affinity conditions that avoid
    scheduling Pods against specific node(s) that have Pods with specific attributes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NodeName**: Schedules a Pod against a very specific node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Taints and Tolerations**: Avoids scheduling Pods on nodes that are tainted,
    but can make an exception if tolerations are defined on the Pod'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The upcoming sub-sections will detail the aforementioned attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Node Selector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`kube-scheduler` to schedule a Pod only against a node with a matching label
    and corresponding value for the label.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider a cluster where nodes in the cluster belong to different
    CPU platforms. The nodes are labeled with a label selector and an appropriate
    value indicating the CPU platform of the node. If there is a need to run a Pod
    on a node with a specific CPU platform, then the Pod attribute, `nodeSelector`,
    can be used. `kube-scheduler` will find a node that matches the `nodeSelector`
    specification on the Pod against the matching label on the node. If no such node
    is found, then the Pod will not be scheduled.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.9* shows the use of `nodeSelector` in a Pod and its matching relevance
    to a node specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Specifying a nodeSelector on a Pod that matches a node'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15587_07_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.9 – Specifying a nodeSelector on a Pod that matches a node
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, `kube-scheduler` will schedule the Pod on a node where
    the node label is `cpuPlatform` and the corresponding value is `Skylake`.
  prefs: []
  type: TYPE_NORMAL
- en: Node Affinity and Anti-Affinity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`nodeSelector`, where only an exact value for a label match can be specified.
    These preferences are only considered during scheduling and are ignored during
    execution. This means that once a Pod is scheduled on a node, the Pod continues
    to run on the node even though the node labels have changed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, the Node Affinity and Anti-Affinity preferences can be set against
    two properties that serve as a hard or soft constraint. These two properties are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**requiredDuringSchedulingIgnoredDuringExecution**: This is a hard limit where
    a Pod is scheduled only if the criterion is met.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**preferredDuringSchedulingIgnoredDuringExecution**: This is a soft limit where
    the scheduler tries to deploy the Pod on the node that matches the specified criterion.
    The Pod is still deployed on a node even if a match is not found.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is a Pod specification involving the use of `nodeAffinity`. The
    Pod specification indicates that the Pod should not be scheduled on nodes with
    a specific CPU platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, `kube-scheduler` will not schedule the Pod on nodes
    where the CPU platform is either `Skylake` or `Broadwell`.
  prefs: []
  type: TYPE_NORMAL
- en: Inter-pod affinity and Anti-Affinity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is an extension of Node Affinity with the same fundamentals. This specification
    allows the scheduling of Pods to nodes based on the labels on the Pods that are
    already running on the nodes. Similarly, Anti-Affinity will ensure that a Pod
    is not scheduled on a node if there are other Pods of specific labels running
    on a node.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rules for Pod affinity and Anti-Affinity can be illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pod-affinity**: Pod P should be scheduled on Node N only if Node N has other
    Pods running with matching rule A.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pod-anti-affinity**: Pod P should not be scheduled on Node N if Node N has
    other Pods running with matching rule B.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 7.10* shows a Pod specification with Pod affinity and Anti-Affinity
    definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Pod definition with inter-pod and anti-pod affinity'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15587_07_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.10 – Pod definition with inter-pod and anti-pod affinity
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, `kube-scheduler` will schedule Pods on nodes where
    other Pods that are already running on the node have matching labels that reflect
    `app` as either `webserver` or `elasticserver`. On the other hand, `kube-scheduler`
    will not attempt to schedule Pods on nodes where other Pods that are already running
    on the nodes have matching labels that reflect `app` as a database. In short,
    this Pod specification tries to schedule Pods on nodes that don't run database
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Node name
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**nodeName** is an attribute that can be specified in a Pod definition file
    and is also the simplest way to specify constraints regarding node selection.
    The biggest limitation of this kind of specification is that it is an all-or-nothing
    proposition.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if the node is available for scheduling, then the Pod can be scheduled
    on that specific node. However, if the node is not available, then the Pod doesn''t
    have a choice of any other node. A Pod cannot be scheduled until the node can
    take further workloads. In addition, nodes can be ephemeral, specifically, when
    the nodes are VMs. So, specifying a node name might not be a good design to start
    with, and hence this method is the least preferred. The following is a Pod specification
    with the `nodeName` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, `kube-scheduler` will attempt to schedule Pods on
    nodes where the node name is `node01`.
  prefs: []
  type: TYPE_NORMAL
- en: Taints and tolerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Node Affinity and Pod affinity are properties of a Pod for finding a set of
    nodes. Taint is a property of a node that can repel one or more Pods. A node is
    tainted with a specific effect, based on a defined combination of a key, operator,
    and optionally, a value attribute. Possible indications that a node may be tainted
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NoSchedule**: Indicates that no more Pods can be scheduled on this node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NoExecute**: Indicates that no more Pods can run on this node and existing
    ones should be terminated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PreferNoSchedule**: Indicates a soft limit that no more Pods can be scheduled
    on this node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tolerations is a feature that allows Pods to be scheduled on nodes with matching
    taints. So, tolerations are a way to counter the impact of a taint.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the CLI command to taint a node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a Pod specification that defines toleration against the tainted
    node, which makes the Pod still eligible to be scheduled against the node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a two-part example for tainting a node:'
  prefs: []
  type: TYPE_NORMAL
- en: The CLI command taints `node01` by specifying not to schedule Pods with a matching
    label key-value pair as `sky=blue`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, the Pod specification defines a toleration for `node01`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, the Pod can be potentially scheduled on `node01` by `kube-scheduler`. This
    completes the deep dive into critical factors that need to be considered while
    scheduling Pods on nodes.
  prefs: []
  type: TYPE_NORMAL
- en: In a Kubernetes deployment, application changes in terms of new features or
    bug fixes are reflected by deploying updated container images. There are several
    strategies to enforce a change in deployment or apply a new deployment. These
    will be discussed in the upcoming section.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes deployment strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If a change is required to horizontally scale an application by increasing the
    number of replicas or if a change is required to the application by updating the
    container image, then a change is required to the deployment specification in
    Kubernetes. This will lead to automatic updates, either resulting in deploying
    additional Pods to scale horizontally or deploying a new Pod with the updated
    image and replace the current running Pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'Changes to deployment can either happen by applying an updated deployment spec
    or by editing an existing deployment or specifically updating the image on the
    deployment. All of these can be done through the `kubectl` commands. However,
    the strategy used to perform the deployment makes an immense difference in terms
    of how end users of the application are impacted. There are four specific deployment
    strategies. Each of these strategies offers a different use case. These are mentioned
    as follows and will be illustrated in detail in the upcoming sub-sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Recreate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rolling update
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blue/Green
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Canary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first deployment strategy that will be detailed will be the *Recreate* strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Recreate strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Recreate** strategy is a basic strategy and is also the most straightforward
    compared to other strategies. Essentially, the current running Pods are all destroyed
    or brought down first and then the desired number of Pods are brought up against
    a new ReplicaSet.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example snippet that illustrates a Recreate update:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the preceding example snippet, Kubernetes will first bring down all
    four running Pods on the current ReplicaSet. Following that, Kubernetes will create
    a new ReplicaSet and will start four new Pods. Refer to *Figure 7.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – Illustrating the ''Recreate'' strategy in Kubernetes deployment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15587_07_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.11 – Illustrating the 'Recreate' strategy in Kubernetes deployment
  prefs: []
  type: TYPE_NORMAL
- en: The Recreate strategy results in downtime as the application will remain unavailable
    for a brief period. This will result in disruptions and is therefore not a suggested
    strategy for applications that have a live user base. However, this strategy is
    used in scenarios where old and new versions of the application should or can
    never serve user traffic at the exact same time.
  prefs: []
  type: TYPE_NORMAL
- en: This completes the Recreate strategy. The clear downside is unavoidable downtime.
    This downside can be handled by another deployment strategy, called the rolling
    update strategy, and will be covered in the next sub-section.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling update strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Rolling update** strategy enables the incremental deployment of applications
    with zero downtime. The current running Pod instances are gradually updated with
    a new Pod instance until all of them are replaced. The application stays available
    at all times. The rolling update strategy is the default deployment strategy in
    Kubernetes. However, this strategy doesn't exercise control in terms of specifying
    or having control over the amount of traffic directed to the new Pod instances
    versus old Pod instances.
  prefs: []
  type: TYPE_NORMAL
- en: The deployment also gets updated over time, and the process is time-consuming
    and gradual. There are specific fields that control the rolling update strategy,
    and these are detailed as follows, starting with **Max unavailable**.
  prefs: []
  type: TYPE_NORMAL
- en: Max unavailable
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`.spec.strategy.rollingUpdate.maxUnavailable` is an optional field and refers
    to the maximum number of Pods that can be unavailable during the deployment process.
    This can be specified as an absolute number, or as a percentage of the desired
    Pods. If the field is not explicitly specified, then the default value is 25%.
    In addition, the default value is always considered if the value is explicitly
    specified as `0`.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider an example. If the desired set of Pods is 5, and `maxUnavailable`
    is 2, this means that at any point in time, the total number of minimum Pods running
    across the old and new versions should be 3.
  prefs: []
  type: TYPE_NORMAL
- en: The next sub-section will cover **Max surge**. This indicates the maximum number
    of Pods that can exist at any time across current and new replica sets.
  prefs: []
  type: TYPE_NORMAL
- en: Max surge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`.spec.strategy.rollingUpdate.maxSurge` is an optional field and refers to
    the maximum number of Pods that can be created in addition to the desired number
    of Pods during the deployment process. This can be specified as an absolute number
    or a percentage of the desired Pods. If the field is not explicitly specified,
    then the default value is 25%. In addition, the default value is always considered
    if the value is explicitly specified as `0`.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider an example. If the desired set of Pods is 5 and the maximum surge
    is 3, this means that the deployment process can get started by rolling out three
    new Pods and ensure that the total number of running Pods doesn't exceed 8 (desired
    Pods + maximum surge). If the maximum surge is specified in terms of a percentage
    and the value is set to 20%, then the total number of running Pods across old
    and new deployment will not exceed 6 (desired Pods + 10% of desired Pods).
  prefs: []
  type: TYPE_NORMAL
- en: The next sub-section will cover **Min Ready**. This indicates the minimum time
    that the container should run for, indicating the Pod to be ready.
  prefs: []
  type: TYPE_NORMAL
- en: Min ready
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`.spec.minReadySeconds` is an optional field and refers to the minimum number
    of seconds that a newly created Pod should be in the ready state where containers
    are running without any failures or crashes. The default value, if not specified,
    is `0` and indicates that the Pod is ready as soon as it is created. However,
    if a value of `10` seconds is specified, for example, then the Pod needs to be
    in the ready state for 10 seconds without any containers failing in order to consider
    the Pod as available.'
  prefs: []
  type: TYPE_NORMAL
- en: The next sub-section will cover **Progress Deadline**; the minimum wait time
    before concluding that a deployment is not progressing.
  prefs: []
  type: TYPE_NORMAL
- en: Progress deadline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`.spec.progressDeadlineSeconds` is an optional field and refers to the waiting
    period before a deployment reports that it has failed to progress. The default
    value, if not explicitly specified, is 600 (in seconds). If explicitly specified,
    then this value needs to be greater than `.spec.minReadySeconds`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'During a rolling update strategy, a new ReplicaSet is always created. New Pods
    are created in the new ReplicaSet and currently running Pods are gradually removed
    from the old ReplicaSet. The following is an example snippet that illustrates
    a rolling update strategy and includes the key fields that impact the way the
    rolling update will be performed internally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the preceding example snippet, the following are the specific values
    that will be used to illustrate the example:'
  prefs: []
  type: TYPE_NORMAL
- en: Desired number of Pods = 8 Pods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum surge = 4\. At any point, the total number of running Pods across old
    and new running Pods cannot exceed 12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum unavailable = 50% of desired = 4\. At any point, there should be a minimum
    of 4 running Pods across old and new replica sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes will create a new ReplicaSet and will launch 4 new Pods. Kubernetes
    will then wait for 5 seconds once the Pods have been created to consider whether
    the Pods are available. So, at this moment, the total number of Pods across old
    and new replica sets is 12, which is the maximum value allowed. This is illustrated
    in *Figure 7.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – Rolling update; creating Pods up to the maximum surge in a
    new ReplicaSet'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15587_07_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.12 – Rolling update; creating Pods up to the maximum surge in a new
    ReplicaSet
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, given that the minimum number of running Pods is four in this example,
    across the old and new replica sets, Kubernetes can potentially kill all eight
    of the old replica sets since it will still leave four in the new ReplicaSet.
    So, the core values are still not violated. This is illustrated in *Figure 7.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13 – Rolling update; removing Pods up to the maximum unavailable
    in the current ReplicaSet'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15587_07_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.13 – Rolling update; removing Pods up to the maximum unavailable in
    the current ReplicaSet
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, Kubernetes will launch four more new Pods in the new ReplicaSet and will
    reach the desired number of Pods as well. This is illustrated in *Figure 7.14*.
    This completes the rolling update, where the specified limits were met throughout
    the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14 – Rolling update; creating new Pods up to the desired number
    in a new ReplicaSet'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15587_07_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.14 – Rolling update; creating new Pods up to the desired number in
    a new ReplicaSet
  prefs: []
  type: TYPE_NORMAL
- en: The rolling update strategy ensures zero downtime, but the downside is that
    there is no control in terms of the time taken for the deployment to complete,
    or no control in terms of the traffic going across old and new versions. The next
    strategy solves this specific downside.
  prefs: []
  type: TYPE_NORMAL
- en: Blue/Green strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the case of the **Blue/Green** strategy, there will be two versions of the
    deployment running. That means that there are two replica sets, one ReplicaSet
    per deployment. However, each ReplicaSet will have a different set of labels that
    differentiate the Pods. Traffic to the Pods is sent through a Service. The Service
    will initially have labels that send traffic to the first deployment or ReplicaSet.
    The second deployment will also be running, but traffic will not be served. When
    the Service is patched, and the labels are updated on the Service, matching the
    labels of Pods on the second deployment, then traffic will be diverted to the
    second deployment without any downtime.
  prefs: []
  type: TYPE_NORMAL
- en: The following is an example of two running deployments in the Kubernetes cluster.
    In this example, the name of the deployment is `demo-app`. Both deployments are
    running the same application, but different versions of the application image.
    The difference in the deployment is also reflected by the Pod label selector,
    where the current version of the deployment has a label selector with the version
    as *blue*, whereas the new deployment has a label selector with the version as
    *green*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example is illustrated in *Figure 7.15*. The Service initially points to
    the deployment where the version is blue. This is because the label selector on
    the Service has the version as *blue* and matches the label selectors on the Pods
    in the current deployment. Hence, the incoming traffic to the Service is only
    handled by Pods in the *blue* deployment. The Pods in the *green* deployment are
    running, but they are not serving any incoming traffic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15 – Blue/Green deployment; traffic served only by the blue version'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15587_07_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.15 – Blue/Green deployment; traffic served only by the blue version
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.16* shows a snippet that reflects an update to the Service spec,
    where the Service label selector is updated to reflect the new version as green
    from blue:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16 – Updating the Service specification to switch to a new deployment
    version'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15587_07_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.16 – Updating the Service specification to switch to a new deployment
    version
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.17* reflects how the traffic is served after the Service label selector
    is updated. In this case, incoming traffic will now be served by the Pods in the
    green deployment, as the Pod label selectors match those of the Service. The Pods
    in the blue deployment will no longer serve incoming traffic, although the Pods
    can continue to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17 – Blue/Green deployment; traffic served only by the green version'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15587_07_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.17 – Blue/Green deployment; traffic served only by the green version
  prefs: []
  type: TYPE_NORMAL
- en: Rolling out the deployment is as simple as updating the labels on the Service
    to point back to Pods matching the blue deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Blue/Green deployment is alternatively known as red/black deployment, or A/B
    deployment. Although Blue/Green deployment provides control over the specific
    deployment against which traffic can be sent or rolled back, the downside is that
    double the number of applications are always running, thereby increasing infrastructure
    costs significantly. Also, it's an all-or-nothing scenario where a bug or issue
    in the application related to the updated deployment impacts all users of the
    application. This downside is solved by using the next deployment strategy, canary
    deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Canary deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Canary deployment** provides more control in terms of how much traffic can
    be sent to the new deployment. This ensures that a change in the application only
    impacts a subset of the users. If the change is not as desired, then it also impacts
    only a small percentage of total active users, thereby controlling customers''
    perceptions. Canary deployment is increasingly used in a continuous deployment
    process as it''s a slow change where new features can be constantly added to active
    users, but in a controlled fashion.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.18* illustrates a canary deployment where only 10% of the traffic
    is sent to the new deployment (version=green), whereas the remaining 90% is going
    to the current deployment (version=blue):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.18 – Canary deployment; traffic sent to both versions based on the
    weighted percentage'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15587_07_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.18 – Canary deployment; traffic sent to both versions based on the
    weighted percentage
  prefs: []
  type: TYPE_NORMAL
- en: Canary deployment is a true and reliable reflection of the continuous deployment
    model since a change to the application can flow through the CI/CD pipeline and
    can be deployed to production. In addition, deployment can also be targeted at
    just a specific set of users or a user base. This ensures that the new features
    are tested by live users (like beta users), but also ensures that a break in the
    new feature doesn't negatively impact the entire user base. Canary deployment
    is popularly implemented in the real world by using a resource such as Istio.
    Istio can split and route traffic between two versions based on predefined weights.
    As the new version becomes more stable, the traffic can gradually be shifted to
    the new deployment by changing the weighted percentage.
  prefs: []
  type: TYPE_NORMAL
- en: This completes a detailed illustration of the possible deployment strategies
    in Kubernetes. This also concludes a chapter that primarily focused on understanding
    the essential Kubernetes constructs for containerized deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed Kubernetes workloads in detail and considered
    Kubernetes as an option for deploying containerized applications. We learned about
    Kubernetes cluster anatomy, with a specific focus on understanding the key components
    that form the master control plane and the node control plane. In addition, we
    focused on learning key Kubernetes objects that are critical to deploying applications
    in the cluster, along with possible deployment strategies. Finally, we deep dived
    into how the master plane components interact while performing an action against
    an object such as Pod and discussed various factors involved in scheduling Pods
    onto Kubernetes nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter focuses on the managed version of Kubernetes, called GKE, or
    GKE. The fundamental constructs of Kubernetes studied in this chapter, such as
    cluster anatomy or Kubernetes objects, are essentially the same for GKE. However,
    GKE makes cluster creation a lot easier and, in addition, GKE provides additional
    features for cluster management. Topics specifc to GKE, such as node pools, cluster
    configuration choices and autoscaling will also be detailed.
  prefs: []
  type: TYPE_NORMAL
- en: Points to remember
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some important points to remember:'
  prefs: []
  type: TYPE_NORMAL
- en: A node in a Kubernetes cluster is categorized as a *master* or *worker* node.
    The master node runs the control plane components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key components of the Kubernetes control plane are `kube-apiserver`, `etcd`,
    `kube-scheduler`, `kube-controller-manager`, and `cloud-controller-manager`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is recommended to run the control plane components on the same node and avoid
    any user-specific containers on that node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A highly available cluster can have multiple control planes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-apiserver` handles any queries or changes to the cluster and can be horizontally
    scaled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`etcd` is a distributed key-value store used by Kubernetes to store cluster
    configuration data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-scheduler` chooses a suitable node where an application can be deployed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-controller-manager` runs several controller functions to ensure that
    the current state of the cluster matches the desired state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cloud-controller-manager` includes controller functions that allow Kubernetes
    to integrate with services from a cloud provider.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key components of the worker node include `kubelet` (a Kubernetes agent that
    listens to instructions from `kube-apiserver` and runs containers as per the Pod
    specification), `kube-proxy` (a network proxy to enable communication between
    services), and container runtime (software responsible for running containers).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment, ReplicaSet, StatefulSet, DaemonSet, Jobs, and CronJobs are categorized
    as workload resources, and each run one or more Pods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Pod is the smallest deployable unit in a Kubernetes cluster and can contain
    one or more containers that share filesystem, namespace, and network resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment provides a declarative way to manage a set of Pods that are replicas.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StatefulSets manage stateful applications and can scale a set of Pods, but each
    replica is unique and has its own state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DaemonSets ensure that a copy of the Pod runs on every node or a certain subset
    of nodes in the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The EndPoint object will maintain a list of all IPs for the Pods that match
    the label selector and is constantly updated as Pods are deleted and created.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ExternalName is a Service type where the Service uses DNS names instead of label
    selectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Critical factors to consider while scheduling Pods are NodeSelector, NodeAffinity,
    inter-pod affinity and Anti-Affinity, taints and tolerations, and NodeName.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Possible Kubernetes deployment strategies are Recreate, Rolling update, Blue/Green,
    and Canary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information on GCP''s approach to DevOps, read the following article:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubernetes**: [https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practice test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A user changes the image of a container running in a Pod against a deployment
    in a Kubernetes cluster. A user updates the deployment specification. Select the
    option that describes the accurate behavior:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) The container image of the Pod tied to the deployment will get instantly
    updated and the running Pods will use the new container image.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) A new ReplicaSet will be created with the new image running inside a new
    Pod and will run in parallel with the older ReplicaSet with the older image.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) The current running Pod will stop instantly, and a new Pod will be created
    with the new image. There will be some downtime.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) A new ReplicaSet will be created with the new image running inside a new
    Pod and will gradually replace Pods from the old ReplicaSet.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Select the smallest unit of deployment in Kubernetes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Deployment
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Container
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Pod
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) ReplicaSet
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Select the object and the basis on which a Service object directs traffic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) The Service object sends traffic to Deployments based on metadata.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The Service object sends traffic to Pods based on label selectors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) The Service object sends traffic to containers based on label selectors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) The Service object sends traffic to Pods based on using the same name for
    the Pod and Service.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A Pod is in a ready state, but performing some actions internally when started,
    and is thereby unable to serve incoming traffic. Traffic from a Service is failing.
    Select the option that could be a potential solution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Configure a start up probe.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Configure a liveness probe.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Configure a readiness probe.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) None of the above.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'There is a need to deploy multiple applications in a GKE cluster that could
    scale independently based on demand. Some of these applications are memory-intensive,
    some are I/O-intensive, and some are CPU-intensive. Select the option that represents
    the most appropriate cluster design:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Select the majority category that applications fall under and create a cluster
    with either a CPU-intensive machine type, or memory-intensive or I/O-intensive.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Create a cluster where the nodes have the maximum possible CPU and memory.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Create a cluster with multiple node pools. Each node pool can be used to
    run a specific type of application with specific CPU, RAM, or I/O requirements.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) (b) and (c).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which specific deployment option allows the testing of a new version of the
    application in production with a small percentage of actual traffic?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Percentage deployment
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Rolling update
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Canary deployment
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Blue/Green deployment
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Select the appropriate Service type where a Service gets an internal IP address:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) ClusterIP
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) NodePort
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) LoadBalancer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) All of the above
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which of the following deployment options enables running the last successful
    deployment on standby so that it could be used if the latest deployment has an
    issue? (Select all applicable options)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Rolling update
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) A/B deployment
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Canary deployment
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Red/black deployment
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which of the following controllers allows multiple development teams to use
    the same cluster, but with specific controls on the consumption of CPU and memory?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Authorization controller
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) `kube-controller-manager`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) ResourceQuota admission controller
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) `cloud-controller-manager`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What is the function of a DaemonSet?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) It runs a specific Pod on every node in the cluster.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) It runs multiple copies of the specific Pod on every node in the cluster.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) It runs a specific Pod on every node in the cluster or a subset of selected
    nodes in the cluster.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) It runs multiple copies of the Pod on every node in the cluster or a subset
    of selected nodes in the cluster.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'There is a specific requirement where Container C1 should be terminated if
    the memory or CPU currently utilized is three times more than the specified request
    limits. Select all possible options that match the specified requirements and
    should be added to the Pod spec:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a) Requests: CPU=1000m, Memory=500Mi'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Limits: CPU=3000m, Memory=1250Mi'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) Limits: CPU=3000m, Memory=1500Mi'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Requests: CPU=1000m, Memory=500Mi'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) Requests: CPU=750m, Memory=1000Mi'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Limits: CPU=2250m, Memory=3000Mi'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) Limits: CPU=1200m, Memory=500Mi'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Requests: CPU=3600m, Memory=1500Mi'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A StatefulSet called `log-collector` consists of three replicas. Assume the
    Pods are labeled as `log-collector-0`, `log-collector-1`, and `log-collector-2`.
    The replica count is now scaled down to two replicas. Which of the following Pods
    will be deleted?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) The first Pod that was created in sequence will be deleted (`log-collector-0`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) A random Pod will be deleted.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) The last Pod that was created will be deleted (`log-collector-2`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) It's not possible to scale down a StatefulSet.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Select the option that depicts the reason for a CrashLoopBackOff error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Containers are terminated when an update is made to the Pod.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) A container in a Pod failed to start successfully following repeated attempts.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Containers are terminated when an update is made to the deployment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) None of the above.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Select the option that depicts the state where all containers in a Pod have
    terminated successfully and will not be restarted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Unknown
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Pending
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Completed
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Failed
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Select the appropriate Service type where a Service gets a cluster-wide port:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) ClusterIP
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) NodePort
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) LoadBalancer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) All of the above
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: (d) – A new ReplicaSet will be created with the new image running inside a new
    Pod and will gradually replace Pods from the old ReplicaSet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (c) – Pod.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (b) – The Service object sends traffic to Pods based on label selectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (c) – Configure a readiness probe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (c) – Create a cluster with multiple node pools.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (c) – Canary deployment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (d) – All of the above.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (b) and (d) – A/B and Red/Black is the same as Blue/Green.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (b) – ResourceQuota is an example of an admission controller where a namespace
    can be restricted to only use up to a certain capacity of memory and CPU. Each
    development team can work exclusively in its own namespace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (c) – It runs a specific Pod on every node in the cluster or a subset of selected
    nodes in the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (b) and (c).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (c) – The last Pod that was created will be deleted (`log-collector-2`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (b) – A container in a Pod failed to start successfully following repeated attempts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (c) – Completed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (b) – NodePort.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
