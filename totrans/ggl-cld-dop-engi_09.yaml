- en: '*Chapter 7*: Understanding Kubernetes Essentials to Deploy Containerized Applications'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第七章*：理解 Kubernetes 基础以部署容器化应用'
- en: The last two chapters ([*Chapter 5*](B15587_05_Final_ASB_ePub.xhtml#_idTextAnchor110),
    *Managing Source Code Using Cloud Source Repositories*, and [*Chapter 6*](B15587_06_Final_ASB_ePub.xhtml#_idTextAnchor131),
    *Building Code Using Cloud Build, and Pushing to Container Registry*) focused
    on Google Cloud services to manage source code via cloud source repositories,
    build code via Cloud Build, and create image artifacts using Container Registry.
    Given that the focus of this book is to deploy containerized applications, the
    next three chapters (from [*Chapter 7*](B15587_07_Final_ASB_ePub.xhtml#_idTextAnchor154),
    *Understanding Kubernetes Essentials to Deploy Containerized Applications*, through
    to [*Chapter 9*](B15587_09_Final_ASB_ePub.xhtml#_idTextAnchor201), *Securing the
    Cluster Using GKE Security Constructs*) are centered around essential concepts
    related to deploying containerized applications through Kubernetes, easy cluster
    management through **Google Kubernetes Engine** (**GKE**), and a rundown of key
    security features in GKE that are essential for hardening the Kubernetes cluster.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的两章（[*第5章*](B15587_05_Final_ASB_ePub.xhtml#_idTextAnchor110)，*使用云源代码库管理源代码*，以及
    [*第6章*](B15587_06_Final_ASB_ePub.xhtml#_idTextAnchor131)，*使用 Cloud Build 构建代码并推送到容器注册表*）集中讨论了通过
    Google Cloud 服务来管理源代码、使用 Cloud Build 构建代码以及通过容器注册表创建镜像文件。鉴于本书的重点是部署容器化应用，接下来的三章（从
    [*第7章*](B15587_07_Final_ASB_ePub.xhtml#_idTextAnchor154)，*理解 Kubernetes 基础以部署容器化应用*，一直到
    [*第9章*](B15587_09_Final_ASB_ePub.xhtml#_idTextAnchor201)，*使用 GKE 安全构造保护集群*）围绕通过
    Kubernetes 部署容器化应用的核心概念、通过 **Google Kubernetes Engine**（**GKE**）轻松管理集群，以及 GKE
    中一些关键的安全功能，旨在加强 Kubernetes 集群的安全性。
- en: Kubernetes, or K8s, is an open source container orchestration system that can
    run containerized applications. Kubernetes originated as an internal cluster management
    tool from Google that it donated to **Cloud Native Computing Foundation** (**CNCF**)
    as an open source project in 2014\. This specific chapter will focus on Kubernetes
    essentials that are required for containerized deployments. This includes understanding
    the cluster anatomy, and getting acquainted with Kubernetes objects, specifically
    related to workloads, deployment strategies, and constraints around scheduling
    applications. Google open sourced Kubernetes and donated it to CNCF. This specific
    chapter doesn't deep dive into setting up a Kubernetes cluster. It takes a significant
    effort and manual intervention to run the open source version of Kubernetes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes，或 K8s，是一个开源的容器编排系统，可以运行容器化的应用程序。Kubernetes 起源于 Google 内部的集群管理工具，后来于
    2014 年捐赠给 **Cloud Native Computing Foundation**（**CNCF**）作为开源项目。本章将重点介绍容器化部署所需的
    Kubernetes 基础知识，包括理解集群结构，并了解 Kubernetes 对象，特别是与工作负载、部署策略和应用程序调度约束相关的内容。Google
    将 Kubernetes 开源并捐赠给 CNCF。本章并未深入探讨如何设置 Kubernetes 集群，搭建开源版本的 Kubernetes 需要付出较大努力并进行手动干预。
- en: Google offers a managed version of Kubernetes called Google Kubernetes Engine,
    otherwise known as GKE. Essentially, the fundamentals of Kubernetes apply to GKE
    as well. However, GKE makes it easy to set up a Kubernetes cluster and includes
    additional capabilities that facilitate cluster management. The next chapter focuses
    on GKE core features and includes steps to create a cluster. However, this chapter
    essentially focuses and elaborates on the fundamentals of Kubernetes, which is
    also the core of GKE and helps to make the transition to GKE much easier.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Google 提供了一个托管版本的 Kubernetes，称为 Google Kubernetes Engine，简称 GKE。本质上，Kubernetes
    的基础知识同样适用于 GKE。然而，GKE 使得设置 Kubernetes 集群变得更容易，并且包括了一些促进集群管理的附加功能。下一章将重点介绍 GKE
    的核心功能，并包括创建集群的步骤。然而，本章主要关注并详细阐述了 Kubernetes 的基础知识，这也是 GKE 的核心内容，有助于让过渡到 GKE 更加顺利。
- en: 'This chapter introduces Kubernetes as the container orchestration of choice
    and provides details on the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了 Kubernetes 作为容器编排工具的优势，并提供了以下主题的详细信息：
- en: '**Kubernetes**: A quick introduction'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes**：简要介绍'
- en: '**Kubernetes Cluster Anatomy**: Deep dives into the constructs that form a
    Kubernetes cluster along with the components that form the master control plane'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes 集群解剖**：深入探讨构成 Kubernetes 集群的结构以及构成主控平面的组件'
- en: '**Kubernetes Objects**: Provides a high-level overview of critical Kubernetes
    objects used to deploy workloads'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes 对象**：提供一个关于用于部署工作负载的关键 Kubernetes 对象的高级概述。'
- en: '**Scheduling and interacting with Pods**: Details the constraints evaluated
    and interactions involved while scheduling applications on Kubernetes'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调度和与 Pods 交互**：详细说明了在 Kubernetes 上调度应用程序时评估的约束和涉及的交互。'
- en: '**Kubernetes deployment strategies**: Details potential deployment strategies,
    from the option that essentially recreates applications, via deployment options
    that ensure zero downtime, to the option that enables the shifting of a specific
    amount of traffic to the new application'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes 部署策略**：详细说明了潜在的部署策略，从本质上重新创建应用程序的选项，到确保零停机时间的部署选项，再到将特定流量转移到新应用程序的选项。'
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'There are four main technical requirements:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有四个主要的技术要求：
- en: 'A valid **Google Cloud Platform** (**GCP**) account to go hands-on with GCP
    services: [https://cloud.google.com/free](https://cloud.google.com/free).'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个有效的**Google Cloud Platform**（**GCP**）账户，以便动手操作 GCP 服务: [https://cloud.google.com/free](https://cloud.google.com/free)。'
- en: 'Install Google Cloud SDK: [https://cloud.google.com/sdk/docs/quickstart](https://cloud.google.com/sdk/docs/quickstart).'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '安装 Google Cloud SDK: [https://cloud.google.com/sdk/docs/quickstart](https://cloud.google.com/sdk/docs/quickstart)。'
- en: 'Install Git: [https://git-scm.com/book/en/v2/Getting-Started-Installing-Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '安装 Git: [https://git-scm.com/book/en/v2/Getting-Started-Installing-Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)。'
- en: 'Install Docker: [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '安装 Docker: [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)。'
- en: Kubernetes – a quick introduction
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes – 简介
- en: A **container** is a unit of software that packages code and its dependencies,
    such as libraries and configuration files. When compared to running applications
    on physical or virtual machines, a container enables applications to run faster
    and reliably across computing environments. Containers make it easier to build
    applications that use microservice design patterns. They are critical to the concept
    of continuous development, integration, and deployment as incremental changes
    can be made against a container image and can be quickly deployed to a compute
    environment of choice (that supports process isolation).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**容器**是一个软件单元，封装了代码及其依赖项，如库和配置文件。与在物理或虚拟机上运行应用程序相比，容器使得应用程序能够更快、更可靠地在不同计算环境中运行。容器使得构建使用微服务设计模式的应用程序变得更容易。它们对持续开发、集成和部署概念至关重要，因为可以对容器镜像进行增量更改，并迅速部署到支持进程隔离的计算环境中。'
- en: Given that containers are lean and easy to deploy, an organization might end
    up deploying its applications as several containers. This poses challenges as
    some of the applications might need to interact with one another. Additionally,
    the life cycle of the application should also be monitored and managed. For example,
    if an application goes down due to resource constraints, then another instance
    of the application should be made available. Similarly, if there is a sudden spike
    in traffic, the application should horizontally scale up and when traffic returns
    to normal, the application should subsequently scale down.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于容器轻量且易于部署，组织可能会将其应用程序部署为多个容器。这带来了挑战，因为一些应用程序可能需要彼此交互。此外，应用程序的生命周期也应该得到监控和管理。例如，如果一个应用程序因资源限制而崩溃，则应提供该应用程序的另一个实例。同样，如果流量突然激增，应用程序应横向扩展，而当流量恢复正常时，应用程序应随之缩减。
- en: Scaling actions (up or down) should be provisioned automatically rather than
    manually. This creates a need for container orchestration and will be discussed
    as the next topic.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展操作（向上或向下）应自动进行，而不是手动进行。这就需要容器编排，接下来将讨论这一主题。
- en: Container orchestration
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器编排
- en: '**Container orchestration** is about managing the life cycle of containers,
    specifically in large dynamic environments. Container orchestration can control
    and automate tasks such as provisioning, deployment, maintaining redundancy, ensuring
    availability, and handling changing traffic by scaling up or down as needed.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**容器编排**涉及管理容器的生命周期，特别是在大型动态环境中。容器编排可以控制和自动化诸如资源配置、部署、维持冗余、确保可用性以及通过根据需要上下扩展来处理变化流量等任务。'
- en: 'In addition, container orchestration can also handle the following scenarios:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，容器编排还可以处理以下场景：
- en: Move containers from one host node to the other in case the host node dies.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在主机节点故障时，将容器从一个主机节点迁移到另一个主机节点。
- en: Set eviction policies if a container is consuming more resources than expected.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果容器消耗的资源超过预期，设置驱逐策略。
- en: Provision access to persistent storage volumes in case a container restarts.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供持久存储卷的访问权限，以防容器重启。
- en: Secure interactions between containers by storing keys/secrets.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过存储密钥/秘密来确保容器之间的安全交互。
- en: Monitor the health of the containers.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控容器的健康状态。
- en: '**Kubernetes** traces its lineage from Borg – an internal Google project that
    is essentially a cluster manager that runs large-scale containerized workloads
    to support core Google services such as Google Search. Kubernetes was the next-generation
    cluster manager after Borg. The most popular concepts in Kubernetes came from
    Borg, such as Pods, services, labels, and ip-per-pod.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kubernetes** 源自 Borg——一个内部的 Google 项目，实际上是一个集群管理器，用于运行大规模的容器化工作负载，以支持 Google
    搜索等核心 Google 服务。Kubernetes 是 Borg 的下一代集群管理器。Kubernetes 中最流行的概念来自 Borg，例如 Pods、服务、标签和每个
    pod 的 IP 地址。'
- en: Alternative container orchestration options
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 替代容器编排选项
- en: Docker Swarm, Apache Mesos, OpenShift, and suchlike are a few alternatives for
    container orchestration outside Kubernetes. Docker Swarm is easy to get started
    and set up the cluster, but has limited features specific to scaling. Mesos is
    a cluster manager that is best suited to large systems and designed with maximum
    redundancy. It is complex in nature (in terms of features and configuration) and
    recommended for workloads such as Hadoop and Kafka, but is not suitable for mid-
    or small-scale systems.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm、Apache Mesos、OpenShift 等是 Kubernetes 之外的一些容器编排替代选项。Docker Swarm
    易于入门并设置集群，但在扩展方面功能有限。Mesos 是一个集群管理器，最适合大规模系统，并以最大冗余设计。它在功能和配置上较为复杂，适用于 Hadoop
    和 Kafka 等工作负载，但不适用于中小型系统。
- en: The upcoming section summarizes the main features of Kubernetes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分总结了 Kubernetes 的主要特点。
- en: Kubernetes features
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 特性
- en: 'The following are some of the key features in Kubernetes:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 Kubernetes 中的一些关键特点：
- en: '**Declarative configuration**: Kubernetes administers the infrastructure declaratively,
    in other words, Kubernetes monitors the current state and takes the required action
    to ensure that the current state matches the desired state.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**声明式配置**：Kubernetes 以声明性方式管理基础设施，换句话说，Kubernetes 监控当前状态并采取必要的措施，确保当前状态与期望状态一致。'
- en: '**Automation**: Kubernetes'' implementation of declarative configuration inherently
    supports automation. In addition, Kubernetes allows a wide range of user preferences
    and configurations. As a result, Kubernetes can automatically scale in and scale
    out containerized applications based on a myriad of conditions, with resource
    utilization or resource limits being a few of them.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化**：Kubernetes 的声明式配置实现本身就支持自动化。此外，Kubernetes 允许广泛的用户偏好和配置。因此，Kubernetes
    可以根据多种条件自动扩展或缩减容器化应用程序，资源利用率或资源限制只是其中的一些条件。'
- en: '**Stateful and stateless**: Kubernetes supports both stateful and stateless
    applications. In the case of stateful applications, a user''s state can be stored
    persistently. In addition, both batch jobs and daemon tasks are also supported.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有状态与无状态**：Kubernetes 支持有状态和无状态应用程序。在有状态应用程序的情况下，用户的状态可以持久存储。此外，Kubernetes
    还支持批处理任务和守护进程任务。'
- en: '**Container management**: Kubernetes supports the features of infrastructure
    as service, such as logging, monitoring, and load balancing.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器管理**：Kubernetes 支持基础设施即服务的功能，如日志记录、监控和负载均衡。'
- en: The next section outlines the structure of a Kubernetes deployment and deep
    dives into its key components and their capabilities.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分概述了 Kubernetes 部署的结构，并深入探讨其关键组件及其功能。
- en: Kubernetes cluster anatomy
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 集群结构
- en: A Kubernetes cluster is a collection of machines with compute power. These machines
    could be actual physical computers or could even be **virtual machines** (**VMs**).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 集群是具有计算能力的机器集合。这些机器可以是实际的物理计算机，甚至可以是 **虚拟机** (**VMs**)。
- en: In reference to cloud deployments, a Kubernetes cluster will be a collection
    of VMs. Each VM is termed a node. The nodes in a cluster are categorized as either
    *master* or *worker* nodes. Worker nodes run applications that are deployed in
    containers. The master node runs the control plane components that are responsible
    for coordinating tasks across the worker nodes.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 关于云部署，Kubernetes 集群将是虚拟机的集合。每个虚拟机被称为一个节点。集群中的节点被分类为*主节点*或*工作节点*。工作节点运行在容器中部署的应用程序。主节点运行控制平面组件，负责协调工作节点之间的任务。
- en: '*Throughout this chapter, and for ease of reference, the node running the control
    plane components will be referred to as the master, and worker nodes will be referred
    to as nodes.*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*为了方便参考，本章中运行控制平面组件的节点将称为主节点，而工作节点将称为节点。*'
- en: 'The master has the following responsibilities:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 主节点有以下职责：
- en: It tracks information across all nodes in a cluster, such as applications or
    containers that a node is running.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它跟踪集群中所有节点的信息，例如节点正在运行的应用程序或容器。
- en: It schedules applications on nodes by identifying nodes based on requirements
    (such as resource, affinity, or Anti-Affinity constraints).
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过根据要求（如资源、亲和性或反亲和性约束）识别节点来调度应用程序到节点上。
- en: It ensures that a desired number of instances are always running as per the
    deployment specifications and orchestrates all operations within the cluster.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它确保始终按照部署规范运行所需数量的实例，并协调集群中的所有操作。
- en: '*Figure 7.1* shows an illustration of a Kubernetes cluster that includes both
    master and nodes. These are comprised of machines with compute power:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.1* 显示了一个 Kubernetes 集群的示意图，该集群包括主节点和节点，这些节点由具备计算能力的机器组成：'
- en: '![Figure 7.1 – Outline of a Kubernetes cluster'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.1 – Kubernetes 集群概述'
- en: '](img/B15587_07_01.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15587_07_01.jpg)'
- en: Figure 7.1 – Outline of a Kubernetes cluster
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – Kubernetes 集群概述
- en: The master performs its responsibilities using a set of key components that
    form the **Kubernetes Control Plane** and will be detailed in the upcoming topic.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 主节点通过一组关键组件执行其职责，这些组件构成了**Kubernetes 控制平面**，将在接下来的主题中详细介绍。
- en: Master components – Kubernetes control plane
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主节点组件 – Kubernetes 控制平面
- en: The Kubernetes control plane consists of components that make decisions with
    regard to operations within the cluster and respond to cluster-specific events.
    Events can include, but are not limited to, scaling up the number of instances
    with respect to the application if the average CPU utilization exceeds a specific
    configured threshold.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 控制平面由一组组件组成，这些组件负责在集群内进行操作决策并响应集群特定的事件。事件可能包括但不限于，当平均 CPU 利用率超过特定配置的阈值时，按应用程序的需求扩展实例数量。
- en: 'The following are the key components of the Kubernetes control plane:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 Kubernetes 控制平面的关键组件：
- en: '**kube-apiserver**: A frontend to Kubernetes for cluster interactions'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kube-apiserver**：Kubernetes 的前端，用于集群交互'
- en: '**etcd**: Distributed key-value stores for cluster-specific information'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**etcd**：用于存储集群特定信息的分布式键值存储'
- en: '**kube-scheduler**: Responsible for distributing workloads across nodes'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kube-scheduler**：负责在节点之间分配工作负载'
- en: '**kube-controller-manager**: Tracks whether a node or application is down'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kube-controller-manager**：跟踪节点或应用程序是否出现故障'
- en: '**cloud-controller-manager**: Embeds cloud-specific control logic'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cloud-controller-manager**：嵌入云特定的控制逻辑'
- en: '*Figure 7.2* shows an illustration of the components that run on the master
    and form the Kubernetes control plane:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.2* 显示了在主节点上运行并组成 Kubernetes 控制平面的组件示意图：'
- en: '![Figure 7.2 – Kubernetes control plane on the master node'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.2 – 主节点上的 Kubernetes 控制平面'
- en: '](img/B15587_07_02.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15587_07_02.jpg)'
- en: Figure 7.2 – Kubernetes control plane on the master node
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 主节点上的 Kubernetes 控制平面
- en: The control plane components can be run on any machine in the cluster, but it
    is recommended to run on the same machine and avoid any user-specific containers.
    It's also possible to have multiple control planes when building a highly available
    cluster. Each of the key components is introduced in the upcoming sub-section.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面组件可以在集群中的任何机器上运行，但建议运行在相同的机器上并避免使用任何特定于用户的容器。构建高可用集群时，也可以拥有多个控制平面。每个关键组件将在接下来的子章节中介绍。
- en: kube-apiserver
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: kube-apiserver
- en: '`HTTP`, `gRPC`, and `kubectl`). All other components of the control plane can
    be viewed as clients to `kube-apiserver`. Additionally, `kube-apiserver` is also
    responsible for authentication, authorization, and managing admission control.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`HTTP`、`gRPC` 和 `kubectl`）。控制平面的所有其他组件都可以视为 `kube-apiserver` 的客户端。此外，`kube-apiserver`
    还负责认证、授权和管理准入控制。'
- en: Authentication, authorization, and admission control
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 认证、授权和准入控制
- en: From a cluster standpoint, authentication is about who can interact with the
    cluster (this could be a user or service account); authorization is about what
    specific operations are permitted and admission control represents a set of plugins
    that could limit requests to create, delete, modify, or connect to a proxy. ResourceQuota
    is an example of an admission controller where a namespace can be restricted to
    only use up to a certain capacity of memory and CPU.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从集群的角度来看，认证是关于谁可以与集群交互（这可以是用户或服务账户）；授权是关于哪些特定操作被允许，准入控制表示一组插件，这些插件可以限制创建、删除、修改或连接代理的请求。ResourceQuota
    是一个准入控制器的例子，其中一个命名空间可以被限制只使用一定容量的内存和 CPU。
- en: Any query or change to the cluster is handled by `kube-apiserver`. It is also
    designed to horizontally scale by deploying instances to handle incoming requests
    to the cluster.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 任何对集群的查询或更改都由 `kube-apiserver` 处理。它还通过部署多个实例来水平扩展，以处理对集群的传入请求。
- en: etcd
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: etcd
- en: '`etcd` is a distributed key-value store used by Kubernetes to store information
    that is required to manage the cluster, such as cluster configuration data. This
    includes nodes, Pods, configs, secrets, accounts, roles, and bindings. When a
    `get` request is made to the cluster, the information is retrieved from `etcd`.
    Any `create`, `update`, or `delete` request made to the cluster is complete only
    if the change is reflected in `etcd`.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcd` 是一个分布式键值存储，Kubernetes 使用它来存储管理集群所需的信息，例如集群配置数据。这些数据包括节点、Pods、配置、密钥、账户、角色和绑定。当对集群发出
    `get` 请求时，信息将从 `etcd` 中检索。对集群发出的任何 `create`、`update` 或 `delete` 请求，只有当更改反映在 `etcd`
    中时，才算完成。'
- en: kube-scheduler
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: kube-scheduler
- en: '`kube-scheduler` considers multiple factors, such as the resource requirements
    of an application, node availability, affinity, and Anti-Affinity specifications.
    Affinity and Anti-Affinity specifications represent policy definitions that allow
    certain applications to be deployed against specific nodes or prevent deployment
    against specific nodes.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`kube-scheduler` 考虑多个因素，例如应用程序的资源需求、节点可用性、亲和性和反亲和性规范。亲和性和反亲和性规范是政策定义，允许将某些应用程序部署到特定节点，或防止将应用程序部署到特定节点。'
- en: kube-controller-manager
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: kube-controller-manager
- en: '`kube-apiserver` and ensures that the current state of the cluster matches
    the desired state. `kube-controller-manager` is responsible for the actual running
    of the cluster, and accomplishes this by using several controller functions. As
    an example, a node controller monitors and responds when a node is offline. Other
    examples include the replication controller, namespace controller, and endpoints
    controller.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`kube-apiserver` 确保集群的当前状态与期望状态匹配。`kube-controller-manager` 负责集群的实际运行，并通过使用多个控制器功能来完成这项工作。例如，节点控制器在节点离线时监视并响应。其他示例包括复制控制器、命名空间控制器和端点控制器。'
- en: cloud-controller-manager
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: cloud-controller-manager
- en: '**cloud-controller-manager** includes controller functions that allow Kubernetes
    to be integrated with services from a cloud provider. The controller functions
    are responsible for handling constructs such as networking, load balancers, and
    storage volumes that are specific to the cloud provider.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**cloud-controller-manager** 包含控制器功能，使 Kubernetes 能够与云提供商的服务进行集成。控制器功能负责处理与云提供商特定的构造，如网络、负载均衡器和存储卷。'
- en: The master receives a request to perform a specific operation and the components
    in the control plane schedule, plan, and manage the operations to be performed
    on the nodes. Kubernetes doesn't natively consist of out-of-the-box integration
    (say with Google or AWS). The operations on the nodes are carried out by a set
    of components that form the node control plane and will be detailed in the upcoming
    sub-section.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 主控接收到执行特定操作的请求后，控制平面中的组件调度、计划并管理将在节点上执行的操作。Kubernetes 本身并没有开箱即用的集成（比如与 Google
    或 AWS 的集成）。节点上的操作由一组组成节点控制平面的组件执行，接下来的子章节将详细介绍这些组件。
- en: Node components
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 节点组件
- en: 'Nodes receive instructions from the master, specifically, the `kube-apiserver`.
    Nodes are responsible for running applications deployed in containers and establish
    communication between services across the cluster. The nodes perform these responsibilities
    by using a set of key components. These components are as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 节点从主节点接收指令，特别是从`kube-apiserver`。节点负责运行部署在容器中的应用程序，并在集群内建立服务间的通信。节点通过使用一组关键组件来执行这些职责。这些组件如下：
- en: '`kube-apiserver` and runs containers as per the Pod specification provided'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-apiserver`并根据Pod规范运行容器'
- en: '**kube-proxy**: A network proxy that enables communication between services'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kube-proxy**：一个网络代理，允许服务之间的通信'
- en: '**container runtime**: Software that is responsible for running containers'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器运行时**：负责运行容器的软件'
- en: '*Figure 7.3* shows an illustration of components that form the node control
    plane:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.3*展示了构成节点控制平面的组件示意图：'
- en: '![Figure 7.3 – Components of the node control plane'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.3 – 节点控制平面组件](img/B15587_07_03.jpg)'
- en: '](img/B15587_07_03.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15587_07_03.jpg)'
- en: Figure 7.3 – Components of the node control plane
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 节点控制平面组件
- en: Node components run on each worker node in the cluster and provide the Kubernetes
    runtime environment. Each of the key components for a worker node is introduced
    in the upcoming topic.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 节点组件在集群中的每个工作节点上运行，并提供Kubernetes运行时环境。每个工作节点的关键组件将在接下来的主题中介绍。
- en: kubelet
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: kubelet
- en: '`kube-apiserver` connects with the node through `kubelet`, the node''s agent.
    `kubelet` listens for instructions and deploys or deletes containers when told
    to. `kubelet` doesn''t manage containers that are not created by Kubernetes.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`kube-apiserver`通过`kubelet`（节点的代理）与节点连接。`kubelet`监听指令，并在接收到指令时部署或删除容器。`kubelet`不管理那些不是由Kubernetes创建的容器。'
- en: kube-proxy
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: kube-proxy
- en: '`kube-proxy` runs on each node in the cluster.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`kube-proxy`在集群中的每个节点上运行。'
- en: container runtime engine
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 容器运行时引擎
- en: '`containerd`, and CRI-O.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`containerd`和CRI-O。'
- en: Kubernetes deprecating Docker as a container runtime engine
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes弃用Docker作为容器运行时引擎
- en: Based on the release notes for Kubernetes v1.20, `dockershim` will be deprecated
    and cannot be used from v1.22\. `dockershim` is a module in `kubelet` and a temporary
    solution proposed by the Kubernetes community to use Docker as a container runtime.
    Due to the maintenance burden, `dockershim` will be deprecated and the Kubernetes
    community will only maintain the Kubernetes `containerd` and CRI-O are examples
    of a CRI-compliant runtime.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Kubernetes v1.20的发布说明，`dockershim`将被弃用，并且从v1.22开始无法使用。`dockershim`是`kubelet`中的一个模块，是Kubernetes社区提出的临时解决方案，用于将Docker作为容器运行时。由于维护负担，`dockershim`将被弃用，Kubernetes社区将只维护Kubernetes的`containerd`和CRI-O，它们是符合CRI标准的运行时示例。
- en: 'This completes a deep dive into Kubernetes cluster anatomy that specifically
    consists of components from the master control plane and components from the node
    control plane. Communication within the master control plane is driven by the
    `kube-api` server, which sends instructions to the `kubelet` on the respective
    nodes. `kubelet` executes the instructions sent by the `kube-api` server. *Figure
    7.4* shows an illustration of the entire cluster anatomy:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了对Kubernetes集群结构的深入探讨，具体包括来自主控制平面和节点控制平面的组件。主控制平面内部的通信由`kube-api`服务器驱动，`kube-api`服务器将指令发送到相应节点的`kubelet`。`kubelet`执行`kube-api`服务器发送的指令。*图
    7.4*展示了整个集群结构的示意图：
- en: '![Figure 7.4 – Kubernetes cluster anatomy'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.4 – Kubernetes 集群结构](img/B15587_07_04.jpg)'
- en: '](img/B15587_07_04.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15587_07_04.jpg)'
- en: Figure 7.4 – Kubernetes cluster anatomy
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – Kubernetes 集群结构
- en: It is important to understand that any interaction against a Kubernetes object,
    such as create, modify, or delete, can only be performed through the Kubernetes
    API. These operations on the object can also be performed through the CLI using
    the `kubectl` command. The next topic details the usage of the `kubectl` command.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 需要理解的是，任何针对Kubernetes对象的操作，如创建、修改或删除，只能通过Kubernetes API来执行。这些操作也可以通过CLI使用`kubectl`命令执行。接下来的主题将详细介绍如何使用`kubectl`命令。
- en: Using kubectl
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 kubectl
- en: '`kubectl` is typically used by administrators. `kubectl` enables an action
    to be performed, such as get or delete, against a specific object type with a
    specific object name along with supported request parameters. `kubectl` communicates
    with `kube-apiserver` on the master and converts commands issued by the CLI into
    API calls. `kubectl` can be used to create Kubernetes objects, view existing objects,
    delete objects, and view/export configurations. The syntax structure of `kubectl`
    is as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl` 通常由管理员使用。`kubectl` 使得可以对特定对象类型及其对象名称执行操作，如获取或删除，并支持请求参数。`kubectl`
    与主节点上的 `kube-apiserver` 通信，并将 CLI 发出的命令转换为 API 调用。`kubectl` 可用于创建 Kubernetes 对象、查看现有对象、删除对象以及查看/导出配置。`kubectl`
    的语法结构如下：'
- en: '[PRE0]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The first step involved in using the `kubectl` command is to configure the
    credentials of the cluster, such as the cluster name and its location. `kubectl`
    stores this configuration in a file called `config` and stores the file in a hidden
    folder called `.kube` in the home directory. The current configuration can be
    retrieved by using the `view` command:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `kubectl` 命令的第一步是配置集群的凭证，例如集群名称和位置。`kubectl` 将此配置存储在一个名为 `config` 的文件中，并将文件保存在主目录下的隐藏文件夹
    `.kube` 中。可以使用 `view` 命令检索当前配置：
- en: '[PRE1]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The actions on the cluster are executed using Kubernetes objects. Each object
    has a specific purpose and functionality. There are many such objects in Kubernetes.
    The upcoming section introduces the concept of Kubernetes objects and details
    the most frequently used objects.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 集群上的操作是通过 Kubernetes 对象执行的。每个对象都有特定的用途和功能。Kubernetes 中有许多这样的对象。接下来的部分介绍了 Kubernetes
    对象的概念，并详细介绍了最常用的对象。
- en: Kubernetes objects
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 对象
- en: A Kubernetes object is a persistent entity and represents a record of intent.
    An object can be defined using the **YAML** configuration. It will have two main
    fields – spec and status. The object spec represents the specification, and the
    object state represents the desired state. Once the object is created, the Kubernetes
    system will ensure that the object exists as per the specified declarative configuration.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 对象是一个持久化实体，表示意图的记录。对象可以使用 **YAML** 配置定义。它将具有两个主要字段——spec 和 status。spec
    表示规范，status 表示期望的状态。对象创建后，Kubernetes 系统将确保该对象按指定的声明式配置存在。
- en: 'Kubernetes supports multiple object types. Each object type is meant for a
    specific purpose. The following are some critical Kubernetes objects that will
    be used throughout this chapter. This is not an exhaustive list:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 支持多种对象类型。每种对象类型都有其特定的用途。以下是本章将使用的一些关键 Kubernetes 对象，这并不是一个详尽无遗的列表：
- en: Pods – The smallest atomic unit in Kubernetes
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pods – Kubernetes 中最小的原子单位
- en: Deployment – Provides declarative updates for Pods and ReplicaSets
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署（Deployment）– 为 Pods 和 ReplicaSets 提供声明式更新
- en: StatefulSet – Manages stateful applications and guarantees ordering
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有状态集（StatefulSet）– 管理有状态的应用，并保证顺序
- en: DaemonSet – Runs a copy of the Pod on each node
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 守护进程集（DaemonSet）– 在每个节点上运行 Pod 的副本
- en: Job – Creates one or more Pods and will continue to retry execution until a
    specified number of them terminate successfully
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务（Job）– 创建一个或多个 Pod，并会继续重试执行，直到指定数量的 Pod 成功终止
- en: CronJob – A job that occurs on a schedule represented by a cron expression
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CronJob – 一个基于 cron 表达式的定时任务
- en: Services – Exposes applications running one or more Pods
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务（Services）– 暴露运行一个或多个 Pod 的应用
- en: Deployment, ReplicaSet, StatefulSet, DaemonSet, Jobs, and CronJobs are specifically
    categorized as **Workload Resources**. All these workload resources run one or
    more Pods. This chapter details the abovementioned Kubernetes objects in the upcoming
    sub-sections. Please note that the information provided is not exhaustive from
    the aspect of an object's functionality, but provides an in-depth review of the
    object's purpose.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 部署（Deployment）、副本集（ReplicaSet）、有状态集（StatefulSet）、守护进程集（DaemonSet）、任务（Jobs）和定时任务（CronJobs）被专门分类为
    **工作负载资源（Workload Resources）**。所有这些工作负载资源都运行一个或多个 Pod。本章将详细介绍上述 Kubernetes 对象。请注意，提供的信息从对象功能的角度并不全面，而是提供了对象目的的深入回顾。
- en: Pod
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pod
- en: '**Pod** is a Kubernetes object and is the smallest deployable compute unit
    in a Kubernetes cluster. Application code exists in container images. Container
    images are run using containers and containers run inside a Pod. A Pod resides
    inside a node.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pod** 是一个 Kubernetes 对象，是 Kubernetes 集群中最小的可部署计算单元。应用程序代码存在于容器镜像中，容器镜像通过容器运行，容器在
    Pod 内运行。Pod 存在于节点内部。'
- en: 'A Pod can contain one or more containers. A Pod provides a specification on
    how to run the containers. The containers in a Pod share filesystem, namespace,
    and network resources. A Pod also has a set of ports or port ranges assigned.
    All containers in the Pod have the same IP address, but with different ports.
    The containers within the Pod can communicate by using the port number on localhost.
    The following is a declarative specification for a Pod that runs an `nginx` container:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Pod 可以包含一个或多个容器。Pod 提供了一个规范，说明如何运行这些容器。Pod 中的容器共享文件系统、命名空间和网络资源。Pod 还分配了一组端口或端口范围。Pod
    中的所有容器具有相同的 IP 地址，但端口不同。Pod 内的容器可以通过本地主机上的端口号进行通信。以下是一个声明式的 Pod 规范，用于运行一个 `nginx`
    容器：
- en: '[PRE2]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following is an equivalent imperative command to create a similar Pod:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个等效的命令式命令，用于创建类似的 Pod：
- en: '[PRE3]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: What is CrashLoopBackOff?
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是 CrashLoopBackOff？
- en: There are certain situations where a Pod attempts to start, crashes, starts
    again, and then crashes again; essentially, a condition reported by a Pod where
    a container in a Pod has failed to start after repeated attempts.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，Pod 尝试启动、崩溃、再次启动，然后再次崩溃；本质上，这是一个 Pod 报告的情况，表示 Pod 中的容器在多次尝试后未能启动。
- en: 'On the Kubernetes platform, Pods are the atomic units and run one or more containers.
    A Pod consists of multiple containers if they form a single cohesive unit of Service.
    The sidecar pattern is a common implementation of a Pod with multiple containers.
    These are popularly used in ETL-specific use cases. For example, logs of the `hello-world`
    container need to be analyzed in real time. `logs-analyzer` is a specialized application
    that is meant to analyze logs. If each of these containers is in their respective
    Pods as `pod-hello-world` and `pod-logs-analyzer`, the `logs-analyzer` container
    can get the logs of the `hello-world` container through a `GET` request. Refer
    to *Figure 7.5*:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 平台上，Pod 是最小的单位，并运行一个或多个容器。如果多个容器形成一个服务的整体单元，则 Pod 包含多个容器。Sidecar
    模式是一个常见的实现方式，Pod 中有多个容器。此模式广泛应用于 ETL 特定的用例。例如，`hello-world` 容器的日志需要实时分析。`logs-analyzer`
    是一个专门用于分析日志的应用程序。如果每个容器分别运行在其各自的 Pod 中，名为 `pod-hello-world` 和 `pod-logs-analyzer`，那么
    `logs-analyzer` 容器可以通过 `GET` 请求获取 `hello-world` 容器的日志。参考 *图 7.5*：
- en: '![Figure 7.5 – Communication between containers in different Pods'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.5 – 不同 Pods 中容器之间的通信](img/B15587_07_05.jpg)'
- en: '](img/B15587_07_05.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15587_07_05.jpg)'
- en: Figure 7.5 – Communication between containers in different Pods
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – 不同 Pods 中容器之间的通信
- en: 'However, there will be minimal network latency since both the containers are
    in separate Pods. If both containers are part of the same Pod, `pod-hello-world-etl`
    forming a sidecar pattern, then the Pod will consist of two containers – `logs-analyzer`
    acting as the sidecar container that will analyze logs from another container,
    `hello-world`. Then, these containers can communicate on localhost because they
    are on the same network interface, providing real-time communication. Refer to
    *Figure 7.6*:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于这两个容器位于不同的 Pod 中，网络延迟会最小化。如果这两个容器都属于同一个 Pod，形成一个 Sidecar 模式，即 `pod-hello-world-etl`，那么该
    Pod 将包含两个容器——`logs-analyzer` 作为侧车容器，将分析来自另一个容器 `hello-world` 的日志。然后，这些容器可以在本地主机上进行通信，因为它们共享相同的网络接口，实现实时通信。参考
    *图 7.6*：
- en: '![Figure 7.6 – Communication between containers in a sidecar pattern'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.6 – Sidecar 模式中容器之间的通信](img/B15587_07_06.jpg)'
- en: '](img/B15587_07_06.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15587_07_06.jpg)'
- en: Figure 7.6 – Communication between containers in a sidecar pattern
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – Sidecar 模式中容器之间的通信
- en: 'Using a single Pod with multiple containers allows the application to run as
    a single unit and reduces network latency as the containers communicate on the
    same network interface. The following is a declarative specification of a Pod
    that runs multiple containers with the specific example as illustrated in *Figure
    7.6*:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单个包含多个容器的 Pod，可以让应用程序作为一个整体运行，并减少网络延迟，因为容器之间在相同的网络接口上进行通信。以下是一个声明式的 Pod 规范，运行多个容器，并按照
    *图 7.6* 中所示的特定示例：
- en: '[PRE4]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Job and CronJob
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Job 和 CronJob
- en: Job and CronJob are workload resources. A job represents a task to execute a
    Pod. A job is completed if the task is executed successfully or, in other words,
    a Pod runs successfully to completion for a specified number of times. If a job
    is deleted, then Pods tied to the jobs are also deleted. If a job is suspended,
    then active Pods are deleted. Multiple jobs can be run in parallel. CronJob is
    a workload resource and is essentially a job that is set with a schedule through
    a cron expression.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Job 和 CronJob 是工作负载资源。Job 代表执行 Pod 的任务。如果任务执行成功，Job 就完成了，换句话说，Pod 会成功运行并完成指定次数。如果删除一个
    Job，则与该 Job 相关的 Pod 也会被删除。如果 Job 被挂起，则活动 Pod 会被删除。多个 Job 可以并行运行。CronJob 是一种工作负载资源，实际上是一个通过
    cron 表达式设置了调度的 Job。
- en: '*Figure 7.7* brings together the examples related to a single container Pod,
    `my-nginx`, and the multiple container Pod, `pod-hello-world-etl`, and illustrates
    how these Pods can be potentially connected within a node:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.7* 汇总了与单容器 Pod `my-nginx` 和多容器 Pod `pod-hello-world-etl` 相关的示例，并说明了这些
    Pod 如何在节点内潜在地连接：'
- en: '![Figure 7.7 – Pod connectivity within a node'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.7 – 节点内的 Pod 连接性'
- en: '](img/B15587_07_07.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15587_07_07.jpg)'
- en: Figure 7.7 – Pod connectivity within a node
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 节点内的 Pod 连接性
- en: Pods are ephemeral in nature, and so is the storage associated with Pods. Hence,
    Pods are better suited for stateless applications. However, Pods can also be used
    for stateful applications, but in such cases, Pods should be attached to persistent
    storage or volumes. Pods are also meant to run a single instance of the application.
    Multiple instances of Pods should be used to scale horizontally. This is referred
    to as replication. So, Pods cannot scale by themselves.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Pods 本质上是短暂的，Pod 关联的存储也是如此。因此，Pods 更适合无状态应用程序。然而，Pods 也可以用于有状态应用程序，但在这种情况下，Pods
    应该附加到持久存储或卷上。Pods 也旨在运行单个应用程序实例。要进行水平扩展，应使用多个 Pod 实例。这被称为复制。因此，Pods 不能单独进行扩展。
- en: Liveness, readiness, and start up probes
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 活跃性、就绪性和启动探针
- en: A liveness probe is used to check whether the application is running as expected
    and if not, the container is restarted. A readiness probe is used to check whether
    an application is up but also ready to accept traffic. A start up probe indicates
    when a container application has started. If a start up probe is configured, then
    this will disable the liveness and readiness checks until the start up probe succeeds.
    For more detailed information, refer to [https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 活跃性探针用于检查应用程序是否按预期运行，如果没有，容器将被重启。就绪性探针用于检查应用程序是否已启动并准备好接收流量。启动探针指示容器应用程序何时启动。如果配置了启动探针，则在启动探针成功之前，将禁用活跃性和就绪性检查。有关更详细的信息，请参阅[https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)。
- en: Kubernetes uses specific workload resources to create and manage multiple Pods.
    The most common ones are Deployment, StatefulSet, and DaemonSet, and these will
    be detailed in the upcoming sub-sections.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 使用特定的工作负载资源来创建和管理多个 Pods。最常见的资源是 Deployment、StatefulSet 和 DaemonSet，接下来的子章节将详细介绍这些资源。
- en: Deployment
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署
- en: A **Deployment** is a Kubernetes object that provides declarative updates for
    Pods and ReplicaSets. A Deployment is part of the Kubernetes API group called
    apps.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**Deployment** 是一个 Kubernetes 对象，它提供了 Pod 和 ReplicaSet 的声明式更新。Deployment 是
    Kubernetes API 组中的一部分，属于应用程序组。'
- en: API groups
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: API 组
- en: API groups are a way to extend the Kubernetes API. All supported API requests
    or future requests are placed in a specific group for easy categorization and
    this includes versioning. The most common group is the core group, also known
    as the legacy group. The core group is specified with `apiVersion` as `v1`. Pods
    fall under the core group. A Deployment falls under the apps group and is referred
    to with `apiVersion` as `apps/v1`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: API 组是一种扩展 Kubernetes API 的方式。所有支持的 API 请求或未来的请求都被放置在特定的组中以便于分类，并包括版本控制。最常见的组是核心组，也称为传统组。核心组通过
    `apiVersion` 指定为 `v1`。Pods 属于核心组。Deployment 属于应用程序组，并通过 `apiVersion` 指定为 `apps/v1`。
- en: 'A Deployment provides a declarative way to manage a set of Pods that are replicas.
    The deployment specification consists of a Pod template, Pod specification, and
    the desired number of Pod replicas. The cluster will have controllers that constantly
    monitor and work to maintain the desired state, and create, modify, or remove
    the replica Pods accordingly. Deployment controllers identify Pod replicas based
    on the matching label selector. The following is a declarative specification of
    a deployment that wraps three replicas of **nginx** Pods:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 部署提供了一种声明性方式来管理一组 Pod 副本。部署规范包括 Pod 模板、Pod 规范和所需的 Pod 副本数。集群将有控制器不断监视并工作以保持所需状态，并根据需要创建、修改或删除副本
    Pods。部署控制器根据匹配的标签选择器来识别 Pod 副本。以下是一个声明性规范，封装了三个 **nginx** Pod 副本的部署：
- en: '[PRE5]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following is a set of equivalent imperative commands to create a similar
    deployment:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一组等效的命令，用于创建类似的部署：
- en: '[PRE6]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Deployments support autoscaling using the concept of **HorizontalPodAutoscaler**
    (**HPA**), based on metrics such as CPU utilization. The following is the command
    to implement HPA. HPA will be discussed in detail as part of [*Chapter 8*](B15587_08_Final_ASB_TD_ePub.xhtml#_idTextAnchor182),
    *Understanding GKE Essentials to Deploy Containerized Applications*. This focuses
    on GKE:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 部署支持使用 **HorizontalPodAutoscaler**（**HPA**）概念进行自动扩缩，基于 CPU 使用率等指标。以下是实现 HPA
    的命令。HPA 将在 [*第 8 章*](B15587_08_Final_ASB_TD_ePub.xhtml#_idTextAnchor182) 中详细讨论，*理解
    GKE 基础知识以部署容器化应用程序*，该部分重点介绍 GKE：
- en: '[PRE7]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: A deployment can be updated using the rolling update strategy. For example,
    if the image version is updated, then a new ReplicaSet is created. A rolling update
    will ensure that the deployment will move the Pods from the old ReplicaSet to
    the new ReplicaSet in a phased-out manner to ensure 0% downtime. If an error occurs
    while performing a rolling update, the new ReplicaSet will never reach *Ready*
    status and the old ReplicaSet will not terminate, thereby enabling 0% downtime.
    Deployments and Pods are connected by labels. Each Pod is given a label. The deployment
    has a label selector. So, any updates to the deployments are rolled out to the
    Pods with matching labels.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 部署可以使用滚动更新策略进行更新。例如，如果镜像版本被更新，则会创建一个新的 ReplicaSet。滚动更新将确保部署将 Pods 从旧的 ReplicaSet
    逐步迁移到新的 ReplicaSet，从而确保 0% 的停机时间。如果在执行滚动更新时发生错误，新的 ReplicaSet 将永远无法达到 *Ready*
    状态，而旧的 ReplicaSet 不会终止，从而实现 0% 的停机时间。部署和 Pods 通过标签连接。每个 Pod 都会被分配一个标签。部署具有标签选择器。因此，任何部署的更新都会按标签匹配的
    Pods 进行推广。
- en: A Deployment is well-suited for a stateless application, where a request will
    be served in a similar manner by either of the replica Pods. However, there is
    another Deployment resource that is stateful in nature and is called StatefulSets.
    This will be covered as the next topic.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 部署非常适合无状态应用程序，其中请求可以由任一副本 Pod 以相似方式处理。然而，还有一种资源是有状态的，称为 StatefulSets。下一个主题将介绍这一内容。
- en: StatefulSets
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StatefulSets
- en: '`sample`, then the Pod''s name will be `sample-0`. If there are three replicas,
    then additional Pods called `sample-1` and `sample-2` will be created. This is
    completely different from deployment, where all Pods share the same volume.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`sample`，则 Pod 的名称将是 `sample-0`。如果有三个副本，则会创建名为 `sample-1` 和 `sample-2` 的额外
    Pods。这与部署完全不同，在部署中，所有 Pods 共享相同的卷。'
- en: 'The following is a declarative specification of a StatefulSet with three replicas:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是具有三个副本的 StatefulSet 的声明性规范：
- en: '[PRE8]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If the StatefulSet is scaled down, then the last Pod in the StatefulSet will
    be removed (in other words, in reverse order). In the preceding example, if the
    replica count is reduced to two from three, then the `sample-2` Pod will be deleted.
    The StatefulSet supports rolling updates if there is any change. An old version
    of the Pod for a specific replica will be replaced when the new version of the
    Pod on that specific replica is back up. For example, `sample-0` will be replaced
    with a new version of `sample-0`. The next topic provides an overview of DaemonSets.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 StatefulSet 被缩减，那么 StatefulSet 中的最后一个 Pod 将被删除（换句话说，按照逆序）。在前面的例子中，如果副本数从三减少到两，则
    `sample-2` Pod 将被删除。如果有任何变更，StatefulSet 支持滚动更新。特定副本的旧版本 Pod 会在该副本的新版本 Pod 启动时被替换。例如，`sample-0`
    将被新的 `sample-0` 版本替换。下一个主题将概述 DaemonSets。
- en: DaemonSets
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DaemonSets
- en: '`kube-proxy` is a DaemonSet because a copy of it runs on each node in the cluster
    as part of the node control plane. Additional examples include the following:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`kube-proxy` 是一个 DaemonSet，因为它的副本会在集群中的每个节点上运行，作为节点控制平面的一部分。其他示例包括以下内容：'
- en: Running a log collector daemon on every node or a certain subset of nodes
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个节点或某些子集节点上运行日志收集守护进程
- en: Running a cluster storage daemon on every node or a certain subset of nodes
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个节点或某些子集节点上运行集群存储守护进程
- en: Running a node monitoring daemon on every node or a certain subset of nodes
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个节点或某些子集节点上运行节点监控守护进程
- en: 'To elaborate further, in the case of a log collection daemon, logs are exported
    from each node using a log collector such as `fluentd`. This can be done by a
    `fluentd` Pod and should be run on every node in the cluster. The following is
    a declarative specification of a log collection DaemonSet:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步解释，在日志收集守护进程的情况下，日志通过像 `fluentd` 这样的日志收集器从每个节点导出。这可以通过一个 `fluentd` Pod 来完成，并且应该在集群中的每个节点上运行。以下是一个声明式的日志收集
    DaemonSet 配置：
- en: '[PRE9]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Like Deployments, the DaemonSet also supports rolling updates. So, if the DaemonSet
    is updated, then a new Pod is created and when the new Pod is up, the current
    Pod will be deleted.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Deployments 类似，DaemonSet 也支持滚动更新。因此，当 DaemonSet 被更新时，会创建一个新的 Pod，并且当新 Pod
    启动后，当前的 Pod 将被删除。
- en: The next topic discusses a Kubernetes object called Service. This is essential
    for establishing communication with an application from within the cluster and
    from outside the cluster.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的主题将讨论一个名为 Service 的 Kubernetes 对象。它对于在集群内部和集群外部建立与应用程序的通信至关重要。
- en: Service
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务
- en: As previously mentioned, Pods are ephemeral in nature. Pods' IP addresses are
    not long-lived and can keep changing. This poses a challenge if an API request
    needs to be sent to the Pod's container using its IP address.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Pods 本质上是短暂的。Pods 的 IP 地址不是长期有效的，可能会不断变化。如果需要通过 IP 地址向 Pod 的容器发送 API 请求，这会带来挑战。
- en: Kubernetes provides a stable abstraction point for a set of Pods called a **Service**.
    Every Service has a fixed IP address that doesn't change, and this gets registered
    with the cluster's built-in DNS. A Service identifies associated Pods using label
    selectors.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 为一组 Pods 提供了一个稳定的抽象点，称为 **Service**。每个 Service 都有一个固定的 IP 地址，不会改变，并且会注册到集群的内建
    DNS 中。Service 使用标签选择器来识别关联的 Pods。
- en: In addition, when a Service object is created, Kubernetes creates another object
    called **EndPoint**. The EndPoint object will maintain the list of all IPs for
    the Pods that match the label selector and is constantly updated as Pods are deleted
    and created. The Service object gets the current set of active Pods from the EndPoint
    object.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当创建 Service 对象时，Kubernetes 会创建另一个名为 **EndPoint** 的对象。EndPoint 对象会维护所有符合标签选择器的
    Pods 的 IP 地址列表，并在 Pods 被删除和创建时不断更新。Service 对象会从 EndPoint 对象获取当前活动的 Pods 列表。
- en: '*Figure 7.8* illustrates the interaction between the Service object, endpoint
    object, and the associated Pods based on the matching label selector:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.8* 展示了基于匹配标签选择器，Service 对象、endpoint 对象与关联 Pods 之间的交互：'
- en: '![Figure 7.8 – Service object interaction based on a matching label selector'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.8 – 基于匹配标签选择器的 Service 对象交互'
- en: '](img/B15587_07_08.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15587_07_08.jpg)'
- en: Figure 7.8 – Service object interaction based on a matching label selector
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 基于匹配标签选择器的 Service 对象交互
- en: 'The following is a declarative specification that exposes a Service for a set
    of Pods. This allows the Pod to be accessed using the Service, since the Service
    is not ephemeral in nature and will have a fixed IP address:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个声明式配置，它将一组 Pods 暴露为一个 Service。这样，Pod 就可以通过 Service 进行访问，因为 Service 不是短暂的，并且会有一个固定的
    IP 地址：
- en: '[PRE10]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following is an equivalent imperative command that can expose Pods as a
    Service:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个等效的命令，它可以将 Pods 暴露为 Service：
- en: '[PRE11]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'There are four types of Service, and each Service type exposes Pods differently:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 有四种类型的 Service，每种类型的 Service 暴露 Pods 的方式不同：
- en: ClusterIP
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ClusterIP
- en: NodePort
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NodePort
- en: LoadBalancer
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LoadBalancer
- en: ExternalName
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ExternalName
- en: The preceding specification represents a Service of the ClusterIP type as that's
    the default Service type. This will be introduced as the next topic.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 上述配置表示一个 ClusterIP 类型的 Service，因为这是默认的 Service 类型。接下来将介绍这个内容。
- en: ClusterIP
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ClusterIP
- en: '**ClusterIP** is the default Service type. Each Service gets an IP that can
    only be accessed by other services within the cluster. This is essentially an
    internal IP, and hence the application inside the Pods cannot be accessed by public
    traffic or by an external Service that resides outside the cluster. The default
    Service type, if not specified, is ClusterIP. The preceding declarative specification
    is an example of a ClusterIP Service.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**ClusterIP** 是默认的 Service 类型。每个 Service 会获得一个只能在集群内的其他服务访问的 IP。这本质上是一个内部 IP，因此
    Pods 中的应用程序无法被公共流量或外部集群中的 Service 访问。如果没有指定，默认的 Service 类型为 ClusterIP。前面的声明性规范是一个
    ClusterIP Service 的示例。'
- en: NodePort
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NodePort
- en: '**NodePort** is a Service type where the Service gets an internal IP that can
    be accessed by other services within the cluster. In addition, the NodePort Service
    gets a cluster-wide port. This port can be accessed by a Service that resides
    outside the cluster only if the request is sent to Node''s IP address along with
    the cluster-wide port. Any traffic sent to the cluster-wide port will be redirected
    to the Pods associated with the Service. The following is a declarative specification
    that exposes a node port Service for a set of Pods:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**NodePort** 是一种 Service 类型，Service 获得一个内部 IP，可以被集群内的其他服务访问。此外，NodePort Service
    会获得一个全局端口。只有当请求发送到节点的 IP 地址并携带全局端口时，外部集群中的 Service 才能访问这个端口。任何发送到全局端口的流量都会被重定向到与
    Service 关联的 Pods。以下是一个声明性规范，公开了一个节点端口服务，用于一组 Pods：'
- en: '[PRE12]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: LoadBalancer
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LoadBalancer
- en: '**LoadBalancer** is a Service type where the Service gets an internal IP that
    can be accessed by other services within the cluster. In addition, the Service
    also gets an external IP address that allows the application to receive traffic
    from a Service that resides outside the cluster. This is facilitated by the public
    cloud load balancer attached to the Service. The following is a declarative specification
    that exposes a load balancer Service for a set of Pods:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**LoadBalancer** 是一种 Service 类型，Service 获得一个内部 IP，可以被集群内的其他服务访问。此外，Service
    还会获得一个外部 IP 地址，允许应用程序接收来自外部集群的 Service 的流量。这是通过附加在 Service 上的公共云负载均衡器来实现的。以下是一个声明性规范，公开了一个负载均衡器服务，用于一组
    Pods：'
- en: '[PRE13]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ExternalName
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ExternalName
- en: '`ExternalName` type:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExternalName` 类型：'
- en: '[PRE14]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Hence, a request from an internal client will go to `my-service.default.svc.cluster.local`,
    and then the request gets redirected to `hello.com`.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，来自内部客户端的请求将会发送到 `my-service.default.svc.cluster.local`，然后请求被重定向到 `hello.com`。
- en: 'This completes an overview of the most common Service types in Kubernetes.
    One of the factors to consider while using services is to map services to Pods,
    otherwise known as Service resolution. Kubernetes has an add-on feature called
    `kube-dns` is a DNS server that is essentially a directory mapping of IP addresses
    against easy-to-remember names along with a record type:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了对 Kubernetes 中最常见的 Service 类型的概述。使用服务时需要考虑的一个因素是将服务映射到 Pods，也称为服务解析。Kubernetes
    有一个附加功能，叫做 `kube-dns`，它是一个 DNS 服务器，本质上是一个将 IP 地址与易记的名称及记录类型进行映射的目录：
- en: 'The `kube-dns` server watches the API server for the creation of a new Service.
    When a new server is created, the `kube-dns` server creates a set of DNS records.
    Kubernetes is configured to use the `kube-dns` server''s IP to resolve DNS names
    for Pods. Pods can resolve their Service IP by querying the `kube-dns` server
    using the Service name, the Pod''s namespace, and the default cluster domain:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`kube-dns` 服务器会监视 API 服务器以检测新的 Service 创建。当新服务创建时，`kube-dns` 服务器会创建一组 DNS 记录。Kubernetes
    配置为使用 `kube-dns` 服务器的 IP 来解析 Pods 的 DNS 名称。Pods 可以通过查询 `kube-dns` 服务器，使用 Service
    名称、Pod 的命名空间和默认集群域来解析它们的 Service IP：'
- en: If the Pod and Service are on the same namespace, then the Pod can resolve the
    Service IP by querying the `kube-dns` server using the Service name directly.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 Pod 和 Service 在同一个命名空间中，那么 Pod 可以通过直接使用 Service 名称查询 `kube-dns` 服务器来解析 Service
    的 IP。
- en: If the Pod and Service are not on the same namespace, then the Pod can resolve
    the Service IP by querying the `kube-dns` server using the Service and the Service
    namespace.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 Pod 和 Service 不在同一个命名空间中，那么 Pod 可以通过查询 `kube-dns` 服务器，使用 Service 名称和 Service
    命名空间来解析 Service 的 IP。
- en: A Pod in any other namespace can resolve the IP address of the Service by using
    the fully qualified domain name, `foo.bar.svc.cluster.local`.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在其他命名空间中的 Pod 可以通过使用完全限定域名 `foo.bar.svc.cluster.local` 来解析 Service 的 IP 地址。
- en: '`kube-dns` maintains the following types of DNS record for Pods and services:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`kube-dns` 为 Pods 和服务维护以下类型的 DNS 记录：'
- en: Every Service defined in the cluster is assigned a DNS A record.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群中定义的每个Service都会分配一个DNS A记录。
- en: Every named Pod in the cluster is assigned a DNS SRV record.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群中每个命名Pod都会分配一个DNS SRV记录。
- en: 'The following table represents `kube-dns` records where the hostname is `foo`
    and the namespace is `bar`:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格表示`kube-dns`记录，其中主机名为`foo`，命名空间为`bar`：
- en: '![](img/B15587_07_Table_01.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15587_07_Table_01.jpg)'
- en: This concludes a high-level overview of specific Kubernetes objects, and this
    should provide a good basis for discussing GKE in the next chapter. There are
    several other objects, such as job, CronJob, volumes, and persistent volumes,
    but a deep dive on those will be beyond the scope of the book.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分内容总结了特定Kubernetes对象的概述，并为下一章讨论GKE提供了良好的基础。还有其他一些对象，如作业、CronJob、卷和持久卷，但对这些对象的深入探讨超出了本书的范围。
- en: The next topic details several concepts with respect to scheduling and interacting
    with Pods.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将详细介绍与Pod调度和交互相关的几个概念。
- en: Scheduling and interacting with Pods
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调度和与Pod交互
- en: A Pod is the smallest unit of deployment in a Kubernetes cluster that runs containerized
    applications. The `kube-scheduler` master control plane component is responsible
    for finding a suitable node for the Pod and includes interactions with other components
    of the control plane. In addition, `kube-scheduler` needs to consider multiple
    configuration options, such as NodeSelector, NodeAffinity, and PodAffinity, to
    find the right node for the Pod. This section details the interactions that happen
    during a Pod creation and details the factors that need to be considered while
    scheduling Pods.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Pod是Kubernetes集群中最小的部署单元，运行容器化的应用程序。`kube-scheduler`主控平面组件负责为Pod找到合适的节点，并与其他控制平面组件进行交互。此外，`kube-scheduler`还需要考虑多个配置选项，如NodeSelector、NodeAffinity和PodAffinity，以找到适合Pod的节点。本节详细描述了Pod创建过程中发生的交互，并详细介绍了调度Pod时需要考虑的因素。
- en: Summarizing master plane interactions on Pod creation
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结Pod创建时主控平面交互
- en: 'A Pod is a workload that needs to be deployed in a Kubernetes cluster. A Pod
    needs to run on a node and will host an application. A Pod can be in various phases.
    The following is a summary of valid Pod phases:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Pod是需要在Kubernetes集群中部署的工作负载。Pod需要在节点上运行，并托管一个应用程序。Pod可以处于不同的阶段。以下是有效Pod阶段的总结：
- en: '**Pending**: A Pod is accepted by the Kubernetes cluster, but is waiting to
    be scheduled.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**待定**：Pod已被Kubernetes集群接受，但正在等待调度。'
- en: '**Running**: A Pod is tied to a node and the container in the Pod is running.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运行中**：Pod已经绑定到某个节点，并且Pod中的容器正在运行。'
- en: '**Succeeded** or **Completed**: All containers in a Pod have terminated successfully
    and will not be restarted.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成功**或**已完成**：Pod中的所有容器都已成功终止，并且不会重新启动。'
- en: '**Failed**: All containers in the Pod have terminated and at least one container
    exited with a non-zero status or failure.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**失败**：Pod中的所有容器都已终止，并且至少一个容器以非零状态或失败退出。'
- en: '**Unknown**: The state of the Pod cannot be obtained due to a communication
    error between the node where the Pod should be running.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未知**：由于Pod应该运行的节点之间发生通信错误，无法获取Pod的状态。'
- en: 'Right from the time a request is received to create a Pod to the time the Pod
    is created, there is a series of interactions between the components of the master
    plane that will create the Pod on the worker node. The sequence of interactions
    is listed as follows. This reflects a scenario where a Pod is being created. The
    sequence of steps for other interactions, such as list or delete, or even other
    workloads, such as job or deployment, follow the same pattern:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 从接收到创建Pod的请求到Pod被创建完成，主控平面各个组件之间会有一系列的交互，这些组件将会在工作节点上创建Pod。交互的顺序如下所示。这反映了一个Pod正在创建的场景。其他交互（如列出或删除Pod），或者其他工作负载（如作业或部署），也遵循相同的模式：
- en: '`kube-apiserver` receives a request to create a Pod. The request can come from
    a `kubectl` command or a direct API interaction.'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kube-apiserver`接收到创建Pod的请求。该请求可以来自`kubectl`命令或直接的API交互。'
- en: '`kube-apiserver` authenticates and authorizes the incoming request.'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kube-apiserver`对传入请求进行身份验证和授权。'
- en: Upon successful validation, `kube-apiserver` creates a Pod object but will not
    assign the newly created Pod object to any node.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在成功验证后，`kube-apiserver`创建Pod对象，但不会将新创建的Pod对象分配给任何节点。
- en: '`kube-apiserver` will update the information about the newly created Pod object
    against the `etcd` database and sends a response to the original request for Pod
    creation that a Pod has been created.'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kube-apiserver` 会更新关于新创建的 Pod 对象的信息到 `etcd` 数据库，并向原始请求返回响应，表示 Pod 已创建。'
- en: '`kube-scheduler` continuously monitors and realizes that there is a new Pod
    object but with no node assigned.'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kube-scheduler` 持续监控并发现有新的 Pod 对象，但没有分配节点。'
- en: '`kube-controller` identifies the right node to put the Pod and communicates
    this back to `kube-apiserver`.'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kube-controller` 确定合适的节点以放置 Pod，并将此信息传回 `kube-apiserver`。'
- en: '`kube-apiserver` updates the node for the Pod object against the `etcd` database.'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kube-apiserver` 会更新 Pod 对象在 `etcd` 数据库中的节点信息。'
- en: '`kube-apiserver` passes instructions to `kubelet` on the node (worker) to physically
    create the Pod object.'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kube-apiserver` 将指令传递给节点上的 `kubelet`（工作节点），以物理创建 Pod 对象。'
- en: '`kubelet` creates the Pod on the node and instructs the container runtime engine
    to deploy the application image.'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kubelet` 在节点上创建 Pod，并指示容器运行时引擎部署应用程序镜像。'
- en: '`kubelet` updates the status back to `kube-apiserver` and `kube-apiserver`
    updates the `etcd` database.'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kubelet` 将状态更新回 `kube-apiserver`，并且 `kube-apiserver` 更新 `etcd` 数据库。'
- en: This summarizes the interactions between master plane components when a request
    is sent to the Kubernetes cluster through `kubectl` or the Kubernetes client.
    The next sub-section focuses on critical factors that should be considered while
    scheduling Pods against the node.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过 `kubectl` 或 Kubernetes 客户端向 Kubernetes 集群发送请求时，主控平面组件之间的交互总结。下一节将重点讨论在将
    Pod 调度到节点时需要考虑的重要因素。
- en: Critical factors to consider while scheduling Pods
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调度 Pod 时需要考虑的关键因素
- en: There are multiple factors that `kube-scheduler` considers when scheduling a
    Pod against a node. One such common factor is resource requests and maximum limits.
    A Pod optionally allows the specification of CPU/memory requests and sets the
    respective maximum limits on a container basis. These requests and limits at container
    level are summed up for a Pod and are used by `kube-scheduler` to determine the
    appropriate node for the Pod. `kube-scheduler` schedules a Pod on the node where
    the Pod's requests and limits are within the node's available capacity.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`kube-scheduler` 在调度 Pod 到节点时会考虑多个因素。一个常见的因素是资源请求和最大限制。Pod 可选地允许指定 CPU/内存请求，并在容器级别设置相应的最大限制。这些容器级别的请求和限制会汇总到
    Pod 中，并由 `kube-scheduler` 用来确定适合的节点。`kube-scheduler` 会将 Pod 调度到一个节点，该节点的资源请求和限制在节点的可用容量范围内。'
- en: 'A Pod provides additional properties that exercise more control in forcing
    `kube-scheduler` to schedule Pods only if certain conditions are met. A node also
    provides properties that are considered during scheduling. The following are such
    properties:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 提供了额外的属性，以便在某些条件满足时，强制 `kube-scheduler` 只调度这些 Pod。节点也提供了一些在调度时会被考虑的属性。以下是这些属性：
- en: '**NodeSelector**: Schedules a Pod against the node with matching label values'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NodeSelector**：将 Pod 调度到具有匹配标签值的节点上'
- en: '**NodeAffinity**: Schedules a Pod against the node with matching flexible conditions;
    also considers Anti-Affinity conditions to avoid scheduling a Pod against specific
    node(s)'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点亲和性**：将 Pod 调度到具有匹配灵活条件的节点上；还会考虑反亲和性条件，避免将 Pod 调度到特定节点。'
- en: '**Inter-pod affinity and Anti-Affinity**: Schedules a Pod on nodes with Pods
    having matching attributes; also considers Anti-Affinity conditions that avoid
    scheduling Pods against specific node(s) that have Pods with specific attributes'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pod 间亲和性和反亲和性**：将 Pod 调度到具有匹配属性的节点上的 Pod；还会考虑反亲和性条件，避免将 Pod 调度到具有特定属性的 Pod
    所在的节点。'
- en: '**NodeName**: Schedules a Pod against a very specific node'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NodeName**：将 Pod 调度到一个特定的节点上。'
- en: '**Taints and Tolerations**: Avoids scheduling Pods on nodes that are tainted,
    but can make an exception if tolerations are defined on the Pod'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**污点和容忍**：避免将 Pod 调度到已打上污点的节点上，但如果 Pod 上定义了容忍规则，则可以做例外处理。'
- en: The upcoming sub-sections will detail the aforementioned attributes.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的子章节将详细介绍上述属性。
- en: Node Selector
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 节点选择器
- en: '`kube-scheduler` to schedule a Pod only against a node with a matching label
    and corresponding value for the label.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`kube-scheduler` 只将 Pod 调度到具有匹配标签和值的节点上。'
- en: For example, consider a cluster where nodes in the cluster belong to different
    CPU platforms. The nodes are labeled with a label selector and an appropriate
    value indicating the CPU platform of the node. If there is a need to run a Pod
    on a node with a specific CPU platform, then the Pod attribute, `nodeSelector`,
    can be used. `kube-scheduler` will find a node that matches the `nodeSelector`
    specification on the Pod against the matching label on the node. If no such node
    is found, then the Pod will not be scheduled.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个集群，其中集群中的节点属于不同的 CPU 平台。这些节点通过标签选择器和一个适当的值标记，指示节点的 CPU 平台。如果需要在特定 CPU
    平台的节点上运行一个 Pod，则可以使用 Pod 属性 `nodeSelector`。`kube-scheduler` 会查找与 Pod 上的 `nodeSelector`
    规范匹配的节点，并与节点上的匹配标签进行比较。如果没有找到符合条件的节点，则该 Pod 不会被调度。
- en: '*Figure 7.9* shows the use of `nodeSelector` in a Pod and its matching relevance
    to a node specification:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.9* 显示了在 Pod 中使用 `nodeSelector` 以及它与节点规范匹配的相关性：'
- en: '![Figure 7.9 – Specifying a nodeSelector on a Pod that matches a node'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.9 – 在 Pod 上指定 nodeSelector 与节点匹配](img/B15587_07_09.jpg)'
- en: '](img/B15587_07_09.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15587_07_09.jpg)'
- en: Figure 7.9 – Specifying a nodeSelector on a Pod that matches a node
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.9 – 在 Pod 上指定 nodeSelector 与节点匹配](img/B15587_07_09.jpg)'
- en: In the preceding example, `kube-scheduler` will schedule the Pod on a node where
    the node label is `cpuPlatform` and the corresponding value is `Skylake`.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，`kube-scheduler` 将把 Pod 调度到标签为 `cpuPlatform` 且对应值为 `Skylake` 的节点上。
- en: Node Affinity and Anti-Affinity
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 节点亲和性和反亲和性
- en: '`nodeSelector`, where only an exact value for a label match can be specified.
    These preferences are only considered during scheduling and are ignored during
    execution. This means that once a Pod is scheduled on a node, the Pod continues
    to run on the node even though the node labels have changed.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '`nodeSelector`，其中只能指定标签匹配的精确值。这些偏好只会在调度时考虑，在执行时会被忽略。这意味着，一旦 Pod 被调度到某个节点，尽管节点的标签发生了变化，Pod
    仍会继续在该节点上运行。'
- en: 'In addition, the Node Affinity and Anti-Affinity preferences can be set against
    two properties that serve as a hard or soft constraint. These two properties are
    as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，节点亲和性和反亲和性偏好可以设置为两种属性，这两种属性可以作为硬性或软性约束。以下是这两种属性：
- en: '**requiredDuringSchedulingIgnoredDuringExecution**: This is a hard limit where
    a Pod is scheduled only if the criterion is met.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**requiredDuringSchedulingIgnoredDuringExecution**：这是一个硬性限制，只有满足条件时才会调度 Pod。'
- en: '**preferredDuringSchedulingIgnoredDuringExecution**: This is a soft limit where
    the scheduler tries to deploy the Pod on the node that matches the specified criterion.
    The Pod is still deployed on a node even if a match is not found.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**preferredDuringSchedulingIgnoredDuringExecution**：这是一个软性限制，调度器会尝试将 Pod 部署到符合指定条件的节点上。如果找不到匹配的节点，Pod
    仍然会部署到节点上。'
- en: 'The following is a Pod specification involving the use of `nodeAffinity`. The
    Pod specification indicates that the Pod should not be scheduled on nodes with
    a specific CPU platform:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个涉及使用 `nodeAffinity` 的 Pod 规范。该 Pod 规范指示该 Pod 不应该调度到具有特定 CPU 平台的节点上：
- en: '[PRE15]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the preceding example, `kube-scheduler` will not schedule the Pod on nodes
    where the CPU platform is either `Skylake` or `Broadwell`.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，`kube-scheduler` 不会将 Pod 调度到 CPU 平台为 `Skylake` 或 `Broadwell` 的节点上。
- en: Inter-pod affinity and Anti-Affinity
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pod 之间的亲和性和反亲和性
- en: This is an extension of Node Affinity with the same fundamentals. This specification
    allows the scheduling of Pods to nodes based on the labels on the Pods that are
    already running on the nodes. Similarly, Anti-Affinity will ensure that a Pod
    is not scheduled on a node if there are other Pods of specific labels running
    on a node.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对节点亲和性（Node Affinity）的扩展，基本原理相同。该规范允许根据已经在节点上运行的 Pod 上的标签，将 Pod 调度到节点上。同样，反亲和性（Anti-Affinity）将确保，如果节点上已经运行了具有特定标签的其他
    Pod，则不会将一个 Pod 调度到该节点。
- en: 'The rules for Pod affinity and Anti-Affinity can be illustrated as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 亲和性和反亲和性的规则可以如下所示：
- en: '**pod-affinity**: Pod P should be scheduled on Node N only if Node N has other
    Pods running with matching rule A.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pod-affinity**：仅当节点 N 上运行有与规则 A 匹配的其他 Pod 时，Pod P 才应该调度到节点 N 上。'
- en: '**pod-anti-affinity**: Pod P should not be scheduled on Node N if Node N has
    other Pods running with matching rule B.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pod-anti-affinity**：如果节点 N 上有其他与规则 B 匹配的 Pod 运行，则 Pod P 不应该调度到节点 N 上。'
- en: '*Figure 7.10* shows a Pod specification with Pod affinity and Anti-Affinity
    definitions:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.10* 显示了一个带有 Pod 亲和性和反亲和性定义的 Pod 规范：'
- en: '![Figure 7.10 – Pod definition with inter-pod and anti-pod affinity'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.10 – 带有 Pod 内部亲和性和反 Pod 亲和性的 Pod 定义](img/B15587_07_10.jpg)'
- en: '](img/B15587_07_10.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15587_07_10.jpg)'
- en: Figure 7.10 – Pod definition with inter-pod and anti-pod affinity
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 – 带有 Pod 间亲和性和反亲和性的 Pod 定义
- en: In the preceding example, `kube-scheduler` will schedule Pods on nodes where
    other Pods that are already running on the node have matching labels that reflect
    `app` as either `webserver` or `elasticserver`. On the other hand, `kube-scheduler`
    will not attempt to schedule Pods on nodes where other Pods that are already running
    on the nodes have matching labels that reflect `app` as a database. In short,
    this Pod specification tries to schedule Pods on nodes that don't run database
    applications.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，`kube-scheduler` 会在其他已在节点上运行的 Pod 标签与 `app` 匹配且为 `webserver` 或 `elasticserver`
    的节点上调度 Pod。另一方面，`kube-scheduler` 不会在其他已在节点上运行的 Pod 标签与 `app` 匹配且为数据库的节点上调度 Pod。简而言之，这个
    Pod 定义试图将 Pod 调度到不运行数据库应用程序的节点上。
- en: Node name
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 节点名称
- en: '**nodeName** is an attribute that can be specified in a Pod definition file
    and is also the simplest way to specify constraints regarding node selection.
    The biggest limitation of this kind of specification is that it is an all-or-nothing
    proposition.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '**nodeName** 是在 Pod 定义文件中可以指定的属性，也是指定节点选择约束的最简单方式。这种指定方式的最大局限性在于它是一个全有或全无的方案。'
- en: 'For example, if the node is available for scheduling, then the Pod can be scheduled
    on that specific node. However, if the node is not available, then the Pod doesn''t
    have a choice of any other node. A Pod cannot be scheduled until the node can
    take further workloads. In addition, nodes can be ephemeral, specifically, when
    the nodes are VMs. So, specifying a node name might not be a good design to start
    with, and hence this method is the least preferred. The following is a Pod specification
    with the `nodeName` attribute:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果节点可用于调度，则 Pod 可以在该特定节点上调度。然而，如果节点不可用，则 Pod 不会有其他节点可选。只有当节点能够处理更多工作负载时，Pod
    才能被调度。此外，节点可以是临时的，尤其是当节点是虚拟机时。因此，指定节点名称可能不是一个好的设计方案，因此这种方法是最不推荐的。以下是带有 `nodeName`
    属性的 Pod 定义：
- en: '[PRE16]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In the preceding example, `kube-scheduler` will attempt to schedule Pods on
    nodes where the node name is `node01`.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，`kube-scheduler` 会尝试将 Pod 调度到节点名称为 `node01` 的节点上。
- en: Taints and tolerations
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 污染与容忍
- en: 'Node Affinity and Pod affinity are properties of a Pod for finding a set of
    nodes. Taint is a property of a node that can repel one or more Pods. A node is
    tainted with a specific effect, based on a defined combination of a key, operator,
    and optionally, a value attribute. Possible indications that a node may be tainted
    are as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 节点亲和性和 Pod 亲和性是 Pod 用于寻找节点集合的属性。污点是节点的一个属性，可以排斥一个或多个 Pod。节点根据定义的键、操作符以及可选的值属性与特定效果一起被污点化。以下是可能指示节点被污染的情况：
- en: '**NoSchedule**: Indicates that no more Pods can be scheduled on this node'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NoSchedule**：表示不能再在该节点上调度 Pod。'
- en: '**NoExecute**: Indicates that no more Pods can run on this node and existing
    ones should be terminated'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NoExecute**：表示不能再在该节点上运行 Pod，且现有 Pod 应该被终止。'
- en: '**PreferNoSchedule**: Indicates a soft limit that no more Pods can be scheduled
    on this node'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PreferNoSchedule**：表示一个软限制，即不能再在该节点上调度更多 Pod。'
- en: Tolerations is a feature that allows Pods to be scheduled on nodes with matching
    taints. So, tolerations are a way to counter the impact of a taint.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 容忍是一个功能，允许 Pod 在具有匹配污点的节点上进行调度。因此，容忍是抵消污点影响的一种方式。
- en: 'The following is the CLI command to taint a node:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是污染节点的 CLI 命令：
- en: '[PRE17]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following is a Pod specification that defines toleration against the tainted
    node, which makes the Pod still eligible to be scheduled against the node:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个定义容忍已污染节点的 Pod 规范，使得 Pod 仍然能够被调度到该节点：
- en: '[PRE18]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This is a two-part example for tainting a node:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个分为两部分的例子，用于污染一个节点：
- en: The CLI command taints `node01` by specifying not to schedule Pods with a matching
    label key-value pair as `sky=blue`.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLI 命令通过指定不调度具有匹配标签键值对 `sky=blue` 的 Pod 来污染 `node01`。
- en: However, the Pod specification defines a toleration for `node01`.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，Pod 定义为 `node01` 定义了一个容忍。
- en: So, the Pod can be potentially scheduled on `node01` by `kube-scheduler`. This
    completes the deep dive into critical factors that need to be considered while
    scheduling Pods on nodes.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`kube-scheduler` 可能会在 `node01` 上调度 Pod。这就完成了在调度 Pod 到节点时需要考虑的关键因素的深入探讨。
- en: In a Kubernetes deployment, application changes in terms of new features or
    bug fixes are reflected by deploying updated container images. There are several
    strategies to enforce a change in deployment or apply a new deployment. These
    will be discussed in the upcoming section.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 部署中，新的功能或 bug 修复通过部署更新的容器镜像来反映。更改部署或应用新部署有几种策略。这些将在接下来的章节中讨论。
- en: Kubernetes deployment strategies
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 部署策略
- en: If a change is required to horizontally scale an application by increasing the
    number of replicas or if a change is required to the application by updating the
    container image, then a change is required to the deployment specification in
    Kubernetes. This will lead to automatic updates, either resulting in deploying
    additional Pods to scale horizontally or deploying a new Pod with the updated
    image and replace the current running Pod.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要通过增加副本数量来水平扩展应用程序，或者需要通过更新容器镜像来更改应用程序，则需要更改 Kubernetes 中的部署规范。这将导致自动更新，可能是通过部署额外的
    Pods 来水平扩展，或部署新的 Pod 来更新镜像并替换当前运行的 Pod。
- en: 'Changes to deployment can either happen by applying an updated deployment spec
    or by editing an existing deployment or specifically updating the image on the
    deployment. All of these can be done through the `kubectl` commands. However,
    the strategy used to perform the deployment makes an immense difference in terms
    of how end users of the application are impacted. There are four specific deployment
    strategies. Each of these strategies offers a different use case. These are mentioned
    as follows and will be illustrated in detail in the upcoming sub-sections:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 部署的更改可以通过应用更新的部署规范、编辑现有部署或专门更新部署中的镜像来完成。所有这些操作都可以通过 `kubectl` 命令进行。然而，执行部署所使用的策略对最终用户的影响非常大。部署策略有四种，每种策略有不同的使用场景。以下将详细说明这些策略，并在接下来的子章节中进行详细阐述：
- en: Recreate
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Recreate
- en: Rolling update
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滚动更新
- en: Blue/Green
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蓝绿部署
- en: Canary
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金丝雀发布
- en: The first deployment strategy that will be detailed will be the *Recreate* strategy.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个详细介绍的部署策略将是 *Recreate* 策略。
- en: Recreate strategy
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Recreate 策略
- en: The **Recreate** strategy is a basic strategy and is also the most straightforward
    compared to other strategies. Essentially, the current running Pods are all destroyed
    or brought down first and then the desired number of Pods are brought up against
    a new ReplicaSet.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '**Recreate** 策略是一种基本策略，相比其他策略，它也是最直接的。基本上，当前运行的 Pods 会被销毁或关闭，然后根据新的 ReplicaSet
    启动所需数量的 Pods。'
- en: 'The following is an example snippet that illustrates a Recreate update:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例代码片段，说明了一个 Recreate 更新：
- en: '[PRE19]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Based on the preceding example snippet, Kubernetes will first bring down all
    four running Pods on the current ReplicaSet. Following that, Kubernetes will create
    a new ReplicaSet and will start four new Pods. Refer to *Figure 7.11*:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的示例代码片段，Kubernetes 将首先关闭当前 ReplicaSet 上的所有四个运行中的 Pods。然后，Kubernetes 会创建一个新的
    ReplicaSet，并启动四个新的 Pods。请参见 *图 7.11*：
- en: '![Figure 7.11 – Illustrating the ''Recreate'' strategy in Kubernetes deployment'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.11 – 说明 Kubernetes 部署中的 ''Recreate'' 策略'
- en: '](img/B15587_07_11.jpg)'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15587_07_11.jpg)'
- en: Figure 7.11 – Illustrating the 'Recreate' strategy in Kubernetes deployment
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 – 说明 Kubernetes 部署中的 'Recreate' 策略
- en: The Recreate strategy results in downtime as the application will remain unavailable
    for a brief period. This will result in disruptions and is therefore not a suggested
    strategy for applications that have a live user base. However, this strategy is
    used in scenarios where old and new versions of the application should or can
    never serve user traffic at the exact same time.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: Recreate 策略会导致停机，因为应用程序将暂时不可用。这将导致中断，因此对于拥有活跃用户群的应用程序，建议避免使用该策略。然而，在某些场景中，旧版本和新版本的应用程序无法或不应该在同一时间为用户提供服务时，仍然会使用该策略。
- en: This completes the Recreate strategy. The clear downside is unavoidable downtime.
    This downside can be handled by another deployment strategy, called the rolling
    update strategy, and will be covered in the next sub-section.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了 Recreate 策略。明显的缺点是不可避免的停机。这个缺点可以通过另一种部署策略来解决，称为滚动更新策略，下一小节将介绍该策略。
- en: Rolling update strategy
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 滚动更新策略
- en: The **Rolling update** strategy enables the incremental deployment of applications
    with zero downtime. The current running Pod instances are gradually updated with
    a new Pod instance until all of them are replaced. The application stays available
    at all times. The rolling update strategy is the default deployment strategy in
    Kubernetes. However, this strategy doesn't exercise control in terms of specifying
    or having control over the amount of traffic directed to the new Pod instances
    versus old Pod instances.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '**滚动更新（Rolling update）**策略允许应用程序进行增量部署而不会导致停机。当前运行的Pod实例将逐渐被新的Pod实例替代，直到所有实例都被替换。应用程序始终保持可用。滚动更新策略是Kubernetes的默认部署策略。然而，这种策略并不会控制新Pod实例与旧Pod实例之间流量的分配。'
- en: The deployment also gets updated over time, and the process is time-consuming
    and gradual. There are specific fields that control the rolling update strategy,
    and these are detailed as follows, starting with **Max unavailable**.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 部署过程会随着时间推移而逐步更新，这个过程是耗时且渐进的。有特定字段控制滚动更新策略，具体内容如下，从**最大不可用（Max unavailable）**开始。
- en: Max unavailable
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最大不可用（Max unavailable）
- en: '`.spec.strategy.rollingUpdate.maxUnavailable` is an optional field and refers
    to the maximum number of Pods that can be unavailable during the deployment process.
    This can be specified as an absolute number, or as a percentage of the desired
    Pods. If the field is not explicitly specified, then the default value is 25%.
    In addition, the default value is always considered if the value is explicitly
    specified as `0`.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '`.spec.strategy.rollingUpdate.maxUnavailable`是一个可选字段，表示在部署过程中，最多可以不可用的Pods数量。这个值可以作为一个绝对数值或所需Pods数量的百分比来指定。如果未明确指定此字段，则默认值为25%。此外，如果该字段被明确指定为`0`，则始终认为使用默认值。'
- en: Let's consider an example. If the desired set of Pods is 5, and `maxUnavailable`
    is 2, this means that at any point in time, the total number of minimum Pods running
    across the old and new versions should be 3.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子。如果所需的Pods数量是5，而`maxUnavailable`为2，这意味着在任何时刻，旧版本和新版本的Pods总数的最小值应该为3。
- en: The next sub-section will cover **Max surge**. This indicates the maximum number
    of Pods that can exist at any time across current and new replica sets.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 下一小节将介绍**最大突增（Max surge）**。这表示在当前和新的副本集之间，任何时候最多可以存在的Pods数量。
- en: Max surge
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最大突增（Max surge）
- en: '`.spec.strategy.rollingUpdate.maxSurge` is an optional field and refers to
    the maximum number of Pods that can be created in addition to the desired number
    of Pods during the deployment process. This can be specified as an absolute number
    or a percentage of the desired Pods. If the field is not explicitly specified,
    then the default value is 25%. In addition, the default value is always considered
    if the value is explicitly specified as `0`.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '`.spec.strategy.rollingUpdate.maxSurge`是一个可选字段，表示在部署过程中，除了所需Pods数量外，最多可以创建的Pods数量。这个值可以作为一个绝对数值或所需Pods数量的百分比来指定。如果未明确指定此字段，则默认值为25%。此外，如果该字段被明确指定为`0`，则始终认为使用默认值。'
- en: Let's consider an example. If the desired set of Pods is 5 and the maximum surge
    is 3, this means that the deployment process can get started by rolling out three
    new Pods and ensure that the total number of running Pods doesn't exceed 8 (desired
    Pods + maximum surge). If the maximum surge is specified in terms of a percentage
    and the value is set to 20%, then the total number of running Pods across old
    and new deployment will not exceed 6 (desired Pods + 10% of desired Pods).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子。如果所需的Pods数量是5，最大突增为3，这意味着部署过程可以通过先推出三个新Pods来开始，并确保总的运行Pods数量不会超过8（所需Pods
    + 最大突增）。如果最大突增以百分比形式指定，且值设为20%，则新旧部署中总的运行Pods数量不会超过6（所需Pods + 所需Pods的10%）。
- en: The next sub-section will cover **Min Ready**. This indicates the minimum time
    that the container should run for, indicating the Pod to be ready.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 下一小节将介绍**最小就绪时间（Min Ready）**。这表示容器应该运行的最小时间，以确保Pod准备就绪。
- en: Min ready
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最小就绪时间（Min ready）
- en: '`.spec.minReadySeconds` is an optional field and refers to the minimum number
    of seconds that a newly created Pod should be in the ready state where containers
    are running without any failures or crashes. The default value, if not specified,
    is `0` and indicates that the Pod is ready as soon as it is created. However,
    if a value of `10` seconds is specified, for example, then the Pod needs to be
    in the ready state for 10 seconds without any containers failing in order to consider
    the Pod as available.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '`.spec.minReadySeconds` 是一个可选字段，表示新创建的 Pod 在就绪状态下应持续的最少秒数，即容器运行且没有任何失败或崩溃。如果未指定该值，默认值为
    `0`，表示 Pod 在创建后即视为就绪。然而，如果指定了 `10` 秒，例如，Pod 必须在就绪状态下持续 10 秒，且没有任何容器失败，才会被视为可用。'
- en: The next sub-section will cover **Progress Deadline**; the minimum wait time
    before concluding that a deployment is not progressing.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 下一小节将介绍**进度截止时间**；这是判断部署是否没有进展的最小等待时间。
- en: Progress deadline
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进度截止时间
- en: '`.spec.progressDeadlineSeconds` is an optional field and refers to the waiting
    period before a deployment reports that it has failed to progress. The default
    value, if not explicitly specified, is 600 (in seconds). If explicitly specified,
    then this value needs to be greater than `.spec.minReadySeconds`.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '`.spec.progressDeadlineSeconds` 是一个可选字段，表示在报告部署失败进展之前的等待时间。如果未明确指定，默认值为 600（秒）。如果明确指定，则此值必须大于
    `.spec.minReadySeconds`。'
- en: 'During a rolling update strategy, a new ReplicaSet is always created. New Pods
    are created in the new ReplicaSet and currently running Pods are gradually removed
    from the old ReplicaSet. The following is an example snippet that illustrates
    a rolling update strategy and includes the key fields that impact the way the
    rolling update will be performed internally:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在滚动更新策略中，始终会创建一个新的副本集。新 Pods 会在新的副本集中创建，当前正在运行的 Pods 会逐步从旧副本集中移除。以下是一个示例片段，展示了滚动更新策略，并包含了影响滚动更新执行方式的关键字段：
- en: '[PRE20]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Based on the preceding example snippet, the following are the specific values
    that will be used to illustrate the example:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的示例片段，以下是用于说明该示例的具体值：
- en: Desired number of Pods = 8 Pods.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 期望的 Pods 数量 = 8 个 Pods。
- en: Maximum surge = 4\. At any point, the total number of running Pods across old
    and new running Pods cannot exceed 12.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大突增 = 4。在任何时刻，旧副本集和新副本集中的 Pods 总数不能超过 12。
- en: Maximum unavailable = 50% of desired = 4\. At any point, there should be a minimum
    of 4 running Pods across old and new replica sets.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大不可用 = 期望的 50% = 4。在任何时刻，旧副本集和新副本集中应该至少有 4 个 Pods 正在运行。
- en: 'Kubernetes will create a new ReplicaSet and will launch 4 new Pods. Kubernetes
    will then wait for 5 seconds once the Pods have been created to consider whether
    the Pods are available. So, at this moment, the total number of Pods across old
    and new replica sets is 12, which is the maximum value allowed. This is illustrated
    in *Figure 7.12*:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 会创建一个新的副本集，并启动 4 个新的 Pods。然后，Kubernetes 会在 Pods 创建后等待 5 秒，以判断这些 Pods
    是否可用。因此，此时，旧副本集和新副本集中的 Pods 总数为 12，这是允许的最大值。这在*图 7.12*中得到了说明：
- en: '![Figure 7.12 – Rolling update; creating Pods up to the maximum surge in a
    new ReplicaSet'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.12 – 滚动更新；在新副本集中创建最多突增的 Pods'
- en: '](img/B15587_07_12.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15587_07_12.jpg)'
- en: Figure 7.12 – Rolling update; creating Pods up to the maximum surge in a new
    ReplicaSet
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 – 滚动更新；在新副本集中创建最多突增的 Pods
- en: 'Now, given that the minimum number of running Pods is four in this example,
    across the old and new replica sets, Kubernetes can potentially kill all eight
    of the old replica sets since it will still leave four in the new ReplicaSet.
    So, the core values are still not violated. This is illustrated in *Figure 7.13*:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，鉴于在此示例中，最少有 4 个 Pods 正在运行，跨旧副本集和新副本集，Kubernetes 有可能会终止所有 8 个旧副本集中的 Pods，因为它会在新副本集中保留
    4 个。因此，核心值仍然没有违反。这在*图 7.13*中得到了说明：
- en: '![Figure 7.13 – Rolling update; removing Pods up to the maximum unavailable
    in the current ReplicaSet'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.13 – 滚动更新；在当前副本集（ReplicaSet）中删除最多不可用的 Pods'
- en: '](img/B15587_07_13.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15587_07_13.jpg)'
- en: Figure 7.13 – Rolling update; removing Pods up to the maximum unavailable in
    the current ReplicaSet
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 – 滚动更新；在当前副本集中删除最多不可用的 Pods
- en: 'Now, Kubernetes will launch four more new Pods in the new ReplicaSet and will
    reach the desired number of Pods as well. This is illustrated in *Figure 7.14*.
    This completes the rolling update, where the specified limits were met throughout
    the process:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Kubernetes 将在新的 ReplicaSet 中启动四个新的 Pods，并将达到期望的 Pods 数量。这在 *图 7.14* 中得到了说明。这完成了滚动更新，整个过程中满足了指定的限制：
- en: '![Figure 7.14 – Rolling update; creating new Pods up to the desired number
    in a new ReplicaSet'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.14 – 滚动更新；在新的 ReplicaSet 中创建新的 Pods 直到达到期望的数量'
- en: '](img/B15587_07_14.jpg)'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15587_07_14.jpg)'
- en: Figure 7.14 – Rolling update; creating new Pods up to the desired number in
    a new ReplicaSet
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14 – 滚动更新；在新的 ReplicaSet 中创建新的 Pods 直到达到期望的数量
- en: The rolling update strategy ensures zero downtime, but the downside is that
    there is no control in terms of the time taken for the deployment to complete,
    or no control in terms of the traffic going across old and new versions. The next
    strategy solves this specific downside.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新策略确保零停机，但缺点是无法控制部署完成所需的时间，也无法控制流量在旧版本和新版本之间的切换时间。下一个策略解决了这一特定缺点。
- en: Blue/Green strategy
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝绿策略
- en: In the case of the **Blue/Green** strategy, there will be two versions of the
    deployment running. That means that there are two replica sets, one ReplicaSet
    per deployment. However, each ReplicaSet will have a different set of labels that
    differentiate the Pods. Traffic to the Pods is sent through a Service. The Service
    will initially have labels that send traffic to the first deployment or ReplicaSet.
    The second deployment will also be running, but traffic will not be served. When
    the Service is patched, and the labels are updated on the Service, matching the
    labels of Pods on the second deployment, then traffic will be diverted to the
    second deployment without any downtime.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **蓝绿** 策略中，将有两个版本的部署在运行。这意味着有两个副本集，每个部署一个副本集。然而，每个副本集会有一组不同的标签来区分 Pods。流量通过服务发送到
    Pods。服务最初会有标签，将流量发送到第一个部署或副本集。第二个部署也会运行，但不会处理任何流量。当服务被打补丁，并且服务上的标签被更新以匹配第二个部署中
    Pods 的标签时，流量将被切换到第二个部署，而不会发生任何停机。
- en: The following is an example of two running deployments in the Kubernetes cluster.
    In this example, the name of the deployment is `demo-app`. Both deployments are
    running the same application, but different versions of the application image.
    The difference in the deployment is also reflected by the Pod label selector,
    where the current version of the deployment has a label selector with the version
    as *blue*, whereas the new deployment has a label selector with the version as
    *green*.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 Kubernetes 集群中两个运行中的部署示例。在此示例中，部署的名称为 `demo-app`。这两个部署都在运行相同的应用程序，但使用不同版本的应用镜像。部署中的差异也通过
    Pod 标签选择器得以体现，其中当前版本的部署有一个标签选择器，版本为 *blue*，而新部署的标签选择器则是版本为 *green*。
- en: 'The example is illustrated in *Figure 7.15*. The Service initially points to
    the deployment where the version is blue. This is because the label selector on
    the Service has the version as *blue* and matches the label selectors on the Pods
    in the current deployment. Hence, the incoming traffic to the Service is only
    handled by Pods in the *blue* deployment. The Pods in the *green* deployment are
    running, but they are not serving any incoming traffic:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 示例如 *图 7.15* 所示。服务最初指向版本为蓝色的部署。这是因为服务的标签选择器将版本设置为 *blue*，并与当前部署中的 Pod 标签选择器匹配。因此，流量仅由
    *blue* 部署中的 Pods 处理。虽然 *green* 部署中的 Pods 正在运行，但它们没有处理任何流量：
- en: '![Figure 7.15 – Blue/Green deployment; traffic served only by the blue version'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.15 – 蓝绿部署；仅由蓝色版本处理流量'
- en: '](img/B15587_07_15.jpg)'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15587_07_15.jpg)'
- en: Figure 7.15 – Blue/Green deployment; traffic served only by the blue version
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.15 – 蓝绿部署；仅由蓝色版本处理流量
- en: '*Figure 7.16* shows a snippet that reflects an update to the Service spec,
    where the Service label selector is updated to reflect the new version as green
    from blue:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.16* 显示了一个反映服务规范更新的片段，其中服务的标签选择器被更新为将版本从蓝色改为绿色：'
- en: '![Figure 7.16 – Updating the Service specification to switch to a new deployment
    version'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.16 – 更新服务规范以切换到新部署版本'
- en: '](img/B15587_07_16.jpg)'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15587_07_16.jpg)'
- en: Figure 7.16 – Updating the Service specification to switch to a new deployment
    version
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16 – 更新服务规范以切换到新部署版本
- en: '*Figure 7.17* reflects how the traffic is served after the Service label selector
    is updated. In this case, incoming traffic will now be served by the Pods in the
    green deployment, as the Pod label selectors match those of the Service. The Pods
    in the blue deployment will no longer serve incoming traffic, although the Pods
    can continue to run:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.17* 反映了更新服务标签选择器后流量如何分配。在这种情况下，传入的流量将由绿色部署中的 Pods 提供，因为 Pod 标签选择器与服务的匹配。蓝色部署中的
    Pods 将不再提供传入流量，尽管这些 Pods 可以继续运行：'
- en: '![Figure 7.17 – Blue/Green deployment; traffic served only by the green version'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.17 – 蓝绿部署；流量仅由绿色版本提供'
- en: '](img/B15587_07_17.jpg)'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15587_07_17.jpg)'
- en: Figure 7.17 – Blue/Green deployment; traffic served only by the green version
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.17 – 蓝绿部署；流量仅由绿色版本提供
- en: Rolling out the deployment is as simple as updating the labels on the Service
    to point back to Pods matching the blue deployment.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 部署的推出就像更新服务的标签，将流量指向匹配蓝色部署的 Pods 一样简单。
- en: Blue/Green deployment is alternatively known as red/black deployment, or A/B
    deployment. Although Blue/Green deployment provides control over the specific
    deployment against which traffic can be sent or rolled back, the downside is that
    double the number of applications are always running, thereby increasing infrastructure
    costs significantly. Also, it's an all-or-nothing scenario where a bug or issue
    in the application related to the updated deployment impacts all users of the
    application. This downside is solved by using the next deployment strategy, canary
    deployment.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝绿部署也被称为红黑部署，或 A/B 部署。虽然蓝绿部署能够控制流量发送到特定部署或回滚到哪个部署，但缺点是总是有两倍数量的应用程序在运行，这显著增加了基础设施成本。此外，它是一种全有或全无的场景，应用程序与更新部署相关的
    bug 或问题会影响所有用户。这一缺点通过使用下一个部署策略——金丝雀部署来解决。
- en: Canary deployment
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 金丝雀部署
- en: '**Canary deployment** provides more control in terms of how much traffic can
    be sent to the new deployment. This ensures that a change in the application only
    impacts a subset of the users. If the change is not as desired, then it also impacts
    only a small percentage of total active users, thereby controlling customers''
    perceptions. Canary deployment is increasingly used in a continuous deployment
    process as it''s a slow change where new features can be constantly added to active
    users, but in a controlled fashion.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '**金丝雀部署**提供了更多的控制，能够决定向新部署发送多少流量。这确保了应用程序的更改只会影响部分用户。如果更改效果不如预期，那么它也只会影响一小部分活跃用户，从而控制客户的感知。金丝雀部署在持续部署过程中被越来越多地使用，因为它是一种渐进式变化，新的功能可以持续添加到活跃用户中，但以受控的方式进行。'
- en: '*Figure 7.18* illustrates a canary deployment where only 10% of the traffic
    is sent to the new deployment (version=green), whereas the remaining 90% is going
    to the current deployment (version=blue):'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.18* 展示了金丝雀部署的情况，其中只有 10% 的流量被发送到新的部署（版本=绿色），而剩余的 90% 流量则发送到当前的部署（版本=蓝色）：'
- en: '![Figure 7.18 – Canary deployment; traffic sent to both versions based on the
    weighted percentage'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.18 – 金丝雀部署；流量根据加权百分比发送到两个版本'
- en: '](img/B15587_07_18.jpg)'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15587_07_18.jpg)'
- en: Figure 7.18 – Canary deployment; traffic sent to both versions based on the
    weighted percentage
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.18 – 金丝雀部署；流量根据加权百分比发送到两个版本
- en: Canary deployment is a true and reliable reflection of the continuous deployment
    model since a change to the application can flow through the CI/CD pipeline and
    can be deployed to production. In addition, deployment can also be targeted at
    just a specific set of users or a user base. This ensures that the new features
    are tested by live users (like beta users), but also ensures that a break in the
    new feature doesn't negatively impact the entire user base. Canary deployment
    is popularly implemented in the real world by using a resource such as Istio.
    Istio can split and route traffic between two versions based on predefined weights.
    As the new version becomes more stable, the traffic can gradually be shifted to
    the new deployment by changing the weighted percentage.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀部署是真正可靠的持续部署模型的体现，因为对应用程序的更改可以通过 CI/CD 管道流动，并可以部署到生产环境。此外，部署也可以仅针对特定用户群体或用户基础进行。这确保了新功能会被实际用户（如
    beta 用户）测试，同时也能确保新功能的故障不会对整个用户基础产生负面影响。金丝雀部署在现实世界中通常通过使用 Istio 这类资源来实现。Istio 可以根据预定义的权重在两个版本之间拆分和路由流量。随着新版本变得更加稳定，流量可以通过改变加权百分比逐渐转移到新部署。
- en: This completes a detailed illustration of the possible deployment strategies
    in Kubernetes. This also concludes a chapter that primarily focused on understanding
    the essential Kubernetes constructs for containerized deployments.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了对Kubernetes中可能的部署策略的详细说明。也总结了这一章，主要关注理解Kubernetes中用于容器化部署的关键构造。
- en: Summary
  id: totrans-369
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed Kubernetes workloads in detail and considered
    Kubernetes as an option for deploying containerized applications. We learned about
    Kubernetes cluster anatomy, with a specific focus on understanding the key components
    that form the master control plane and the node control plane. In addition, we
    focused on learning key Kubernetes objects that are critical to deploying applications
    in the cluster, along with possible deployment strategies. Finally, we deep dived
    into how the master plane components interact while performing an action against
    an object such as Pod and discussed various factors involved in scheduling Pods
    onto Kubernetes nodes.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们详细讨论了Kubernetes工作负载，并将Kubernetes作为部署容器化应用程序的选项。我们了解了Kubernetes集群的构造，特别是关注理解构成主控制平面和节点控制平面的关键组件。此外，我们重点学习了对在集群中部署应用至关重要的Kubernetes对象，以及可能的部署策略。最后，我们深入探讨了主平面组件在执行针对对象（如Pod）操作时如何交互，并讨论了调度Pod到Kubernetes节点上的各种因素。
- en: The next chapter focuses on the managed version of Kubernetes, called GKE, or
    GKE. The fundamental constructs of Kubernetes studied in this chapter, such as
    cluster anatomy or Kubernetes objects, are essentially the same for GKE. However,
    GKE makes cluster creation a lot easier and, in addition, GKE provides additional
    features for cluster management. Topics specifc to GKE, such as node pools, cluster
    configuration choices and autoscaling will also be detailed.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将重点介绍Kubernetes的托管版本，称为GKE或GKE。我们在本章中研究的Kubernetes基本构造，例如集群构造或Kubernetes对象，对于GKE来说基本相同。然而，GKE使得集群创建变得更容易，此外，GKE还提供了额外的集群管理功能。本章将详细介绍与GKE特定的主题，例如节点池、集群配置选项和自动扩展。
- en: Points to remember
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 需要记住的要点
- en: 'The following are some important points to remember:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些需要记住的重要事项：
- en: A node in a Kubernetes cluster is categorized as a *master* or *worker* node.
    The master node runs the control plane components.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes集群中的节点被分类为*主节点*或*工作节点*。主节点运行控制平面组件。
- en: The key components of the Kubernetes control plane are `kube-apiserver`, `etcd`,
    `kube-scheduler`, `kube-controller-manager`, and `cloud-controller-manager`.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes控制平面的关键组件包括`kube-apiserver`、`etcd`、`kube-scheduler`、`kube-controller-manager`和`cloud-controller-manager`。
- en: It is recommended to run the control plane components on the same node and avoid
    any user-specific containers on that node.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建议将控制平面组件运行在同一节点上，并避免在该节点上运行任何用户特定的容器。
- en: A highly available cluster can have multiple control planes.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高可用集群可以有多个控制平面。
- en: '`kube-apiserver` handles any queries or changes to the cluster and can be horizontally
    scaled.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-apiserver`处理对集群的任何查询或更改，并且可以进行水平扩展。'
- en: '`etcd` is a distributed key-value store used by Kubernetes to store cluster
    configuration data.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd`是一个分布式键值存储，用于Kubernetes存储集群配置数据。'
- en: '`kube-scheduler` chooses a suitable node where an application can be deployed.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-scheduler`选择一个合适的节点来部署应用程序。'
- en: '`kube-controller-manager` runs several controller functions to ensure that
    the current state of the cluster matches the desired state.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-controller-manager`运行多个控制器功能，确保集群的当前状态与期望状态一致。'
- en: '`cloud-controller-manager` includes controller functions that allow Kubernetes
    to integrate with services from a cloud provider.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cloud-controller-manager`包括控制器功能，使Kubernetes能够与云提供商的服务进行集成。'
- en: Key components of the worker node include `kubelet` (a Kubernetes agent that
    listens to instructions from `kube-apiserver` and runs containers as per the Pod
    specification), `kube-proxy` (a network proxy to enable communication between
    services), and container runtime (software responsible for running containers).
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作节点的关键组件包括`kubelet`（一个Kubernetes代理，监听来自`kube-apiserver`的指令，并根据Pod规范运行容器）、`kube-proxy`（一个网络代理，用于启用服务之间的通信）以及容器运行时（负责运行容器的软件）。
- en: Deployment, ReplicaSet, StatefulSet, DaemonSet, Jobs, and CronJobs are categorized
    as workload resources, and each run one or more Pods.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deployment、ReplicaSet、StatefulSet、DaemonSet、Jobs和CronJobs被归类为工作负载资源，每个资源运行一个或多个Pod。
- en: A Pod is the smallest deployable unit in a Kubernetes cluster and can contain
    one or more containers that share filesystem, namespace, and network resources.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 是 Kubernetes 集群中最小的可部署单元，可以包含一个或多个共享文件系统、命名空间和网络资源的容器。
- en: Deployment provides a declarative way to manage a set of Pods that are replicas.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署提供了一种声明性方式来管理一组 Pod，这些 Pod 是副本。
- en: StatefulSets manage stateful applications and can scale a set of Pods, but each
    replica is unique and has its own state.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StatefulSets 管理有状态应用程序，可以扩展一组 Pod，但每个副本是独特的，并具有自己的状态。
- en: DaemonSets ensure that a copy of the Pod runs on every node or a certain subset
    of nodes in the cluster.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DaemonSets 确保 Pod 的副本在集群的每个节点或某些子集节点上运行。
- en: The EndPoint object will maintain a list of all IPs for the Pods that match
    the label selector and is constantly updated as Pods are deleted and created.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EndPoint 对象将维护与标签选择器匹配的 Pod 所有 IP 地址的列表，并会随着 Pod 的删除和创建而不断更新。
- en: ExternalName is a Service type where the Service uses DNS names instead of label
    selectors.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ExternalName 是一种服务类型，其中服务使用 DNS 名称而非标签选择器。
- en: Critical factors to consider while scheduling Pods are NodeSelector, NodeAffinity,
    inter-pod affinity and Anti-Affinity, taints and tolerations, and NodeName.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度 Pods 时需要考虑的关键因素包括 NodeSelector、NodeAffinity、Pod 间亲和性与反亲和性、污点与容忍度以及 NodeName。
- en: Possible Kubernetes deployment strategies are Recreate, Rolling update, Blue/Green,
    and Canary.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 部署的可能策略包括重建、滚动更新、蓝绿发布和金丝雀发布。
- en: Further reading
  id: totrans-393
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more information on GCP''s approach to DevOps, read the following article:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 GCP 在 DevOps 方面的方法，阅读以下文章：
- en: '**Kubernetes**: [https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/)'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes**: [https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/)'
- en: Practice test
  id: totrans-396
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践测试
- en: 'Answer the following questions:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 回答以下问题：
- en: 'A user changes the image of a container running in a Pod against a deployment
    in a Kubernetes cluster. A user updates the deployment specification. Select the
    option that describes the accurate behavior:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户更改 Kubernetes 集群中运行的 Pod 中容器的镜像，并更新部署规范。选择描述准确行为的选项：
- en: a) The container image of the Pod tied to the deployment will get instantly
    updated and the running Pods will use the new container image.
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 与部署绑定的 Pod 的容器镜像会立即更新，运行中的 Pods 会使用新的容器镜像。
- en: b) A new ReplicaSet will be created with the new image running inside a new
    Pod and will run in parallel with the older ReplicaSet with the older image.
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 将创建一个新的 ReplicaSet，新的镜像会在新的 Pod 中运行，并与旧的 ReplicaSet 中的旧镜像并行运行。
- en: c) The current running Pod will stop instantly, and a new Pod will be created
    with the new image. There will be some downtime.
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 当前运行的 Pod 会立即停止，新的 Pod 会使用新的镜像创建，并会有一些停机时间。
- en: d) A new ReplicaSet will be created with the new image running inside a new
    Pod and will gradually replace Pods from the old ReplicaSet.
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 将创建一个新的 ReplicaSet，新的镜像会在新的 Pod 中运行，并会逐渐替换旧的 ReplicaSet 中的 Pods。
- en: 'Select the smallest unit of deployment in Kubernetes:'
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 Kubernetes 中最小的部署单元：
- en: a) Deployment
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 部署
- en: b) Container
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 容器
- en: c) Pod
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) Pod
- en: d) ReplicaSet
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) ReplicaSet
- en: 'Select the object and the basis on which a Service object directs traffic:'
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择对象以及服务对象指引流量的依据：
- en: a) The Service object sends traffic to Deployments based on metadata.
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 服务对象根据元数据将流量发送到部署。
- en: b) The Service object sends traffic to Pods based on label selectors.
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 服务对象根据标签选择器将流量发送到 Pods。
- en: c) The Service object sends traffic to containers based on label selectors.
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 服务对象根据标签选择器将流量发送到容器。
- en: d) The Service object sends traffic to Pods based on using the same name for
    the Pod and Service.
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 服务对象根据 Pod 和 Service 使用相同名称来将流量发送到 Pods。
- en: 'A Pod is in a ready state, but performing some actions internally when started,
    and is thereby unable to serve incoming traffic. Traffic from a Service is failing.
    Select the option that could be a potential solution:'
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pod 虽处于就绪状态，但在启动时进行一些内部操作，因此无法提供传入流量。来自服务的流量失败。选择可能的解决方案：
- en: a) Configure a start up probe.
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 配置启动探针。
- en: b) Configure a liveness probe.
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 配置存活探针。
- en: c) Configure a readiness probe.
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 配置就绪探针。
- en: d) None of the above.
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 以上都不是。
- en: 'There is a need to deploy multiple applications in a GKE cluster that could
    scale independently based on demand. Some of these applications are memory-intensive,
    some are I/O-intensive, and some are CPU-intensive. Select the option that represents
    the most appropriate cluster design:'
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 需要在GKE集群中部署多个可以根据需求独立扩展的应用程序。这些应用程序中有一些是内存密集型的，有些是I/O密集型的，还有些是CPU密集型的。选择表示最合适集群设计的选项：
- en: a) Select the majority category that applications fall under and create a cluster
    with either a CPU-intensive machine type, or memory-intensive or I/O-intensive.
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 选择应用程序所属的主要类别，并创建一个集群，该集群使用CPU密集型、内存密集型或I/O密集型的机器类型。
- en: b) Create a cluster where the nodes have the maximum possible CPU and memory.
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 创建一个节点具有最大CPU和内存的集群。
- en: c) Create a cluster with multiple node pools. Each node pool can be used to
    run a specific type of application with specific CPU, RAM, or I/O requirements.
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 创建一个具有多个节点池的集群。每个节点池可以用来运行具有特定CPU、内存或I/O要求的特定类型的应用程序。
- en: d) (b) and (c).
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) (b) 和 (c)。
- en: Which specific deployment option allows the testing of a new version of the
    application in production with a small percentage of actual traffic?
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个特定的部署选项允许在生产环境中以少量实际流量测试应用程序的新版本？
- en: a) Percentage deployment
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 百分比部署
- en: b) Rolling update
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 滚动更新
- en: c) Canary deployment
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 金丝雀部署
- en: d) Blue/Green deployment
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 蓝绿部署
- en: 'Select the appropriate Service type where a Service gets an internal IP address:'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择适当的服务类型，服务获取一个内部IP地址：
- en: a) ClusterIP
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) ClusterIP
- en: b) NodePort
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) NodePort
- en: c) LoadBalancer
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 负载均衡器
- en: d) All of the above
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 以上所有选项
- en: Which of the following deployment options enables running the last successful
    deployment on standby so that it could be used if the latest deployment has an
    issue? (Select all applicable options)
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪个部署选项可以在备用状态下运行最后一个成功的部署，以便如果最新的部署出现问题时可以使用？（选择所有适用选项）
- en: a) Rolling update
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 滚动更新
- en: b) A/B deployment
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) A/B部署
- en: c) Canary deployment
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 金丝雀部署
- en: d) Red/black deployment
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 红黑部署
- en: Which of the following controllers allows multiple development teams to use
    the same cluster, but with specific controls on the consumption of CPU and memory?
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪个控制器允许多个开发团队使用同一个集群，但对CPU和内存的使用进行特定控制？
- en: a) Authorization controller
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 授权控制器
- en: b) `kube-controller-manager`
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `kube-controller-manager`
- en: c) ResourceQuota admission controller
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) ResourceQuota准入控制器
- en: d) `cloud-controller-manager`
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) `cloud-controller-manager`
- en: What is the function of a DaemonSet?
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DaemonSet的功能是什么？
- en: a) It runs a specific Pod on every node in the cluster.
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 它在集群的每个节点上运行特定的Pod。
- en: b) It runs multiple copies of the specific Pod on every node in the cluster.
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 它在集群的每个节点上运行特定Pod的多个副本。
- en: c) It runs a specific Pod on every node in the cluster or a subset of selected
    nodes in the cluster.
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 它在集群的每个节点或选定节点的子集上运行特定的Pod。
- en: d) It runs multiple copies of the Pod on every node in the cluster or a subset
    of selected nodes in the cluster.
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 它在集群的每个节点或选定节点的子集上运行Pod的多个副本。
- en: 'There is a specific requirement where Container C1 should be terminated if
    the memory or CPU currently utilized is three times more than the specified request
    limits. Select all possible options that match the specified requirements and
    should be added to the Pod spec:'
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有一个特定的需求，如果容器C1当前使用的内存或CPU是指定请求限制的三倍，则应终止容器C1。选择所有符合指定要求的选项并应添加到Pod规范中：
- en: 'a) Requests: CPU=1000m, Memory=500Mi'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 请求：CPU=1000m, 内存=500Mi
- en: 'Limits: CPU=3000m, Memory=1250Mi'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 限制：CPU=3000m, 内存=1250Mi
- en: 'b) Limits: CPU=3000m, Memory=1500Mi'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 限制：CPU=3000m, 内存=1500Mi
- en: 'Requests: CPU=1000m, Memory=500Mi'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请求：CPU=1000m, 内存=500Mi
- en: 'c) Requests: CPU=750m, Memory=1000Mi'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 请求：CPU=750m, 内存=1000Mi
- en: 'Limits: CPU=2250m, Memory=3000Mi'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 限制：CPU=2250m, 内存=3000Mi
- en: 'd) Limits: CPU=1200m, Memory=500Mi'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 限制：CPU=1200m, 内存=500Mi
- en: 'Requests: CPU=3600m, Memory=1500Mi'
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请求：CPU=3600m, 内存=1500Mi
- en: A StatefulSet called `log-collector` consists of three replicas. Assume the
    Pods are labeled as `log-collector-0`, `log-collector-1`, and `log-collector-2`.
    The replica count is now scaled down to two replicas. Which of the following Pods
    will be deleted?
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个名为`log-collector`的StatefulSet包含三个副本。假设Pods被标记为`log-collector-0`、`log-collector-1`和`log-collector-2`。现在副本数缩减到两个副本。以下哪个Pod将被删除？
- en: a) The first Pod that was created in sequence will be deleted (`log-collector-0`).
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 按顺序创建的第一个Pod将被删除（`log-collector-0`）。
- en: b) A random Pod will be deleted.
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 随机删除一个Pod。
- en: c) The last Pod that was created will be deleted (`log-collector-2`).
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 最后创建的 Pod 将被删除（`log-collector-2`）。
- en: d) It's not possible to scale down a StatefulSet.
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 无法缩减 StatefulSet 的规模。
- en: 'Select the option that depicts the reason for a CrashLoopBackOff error:'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择描述 CrashLoopBackOff 错误原因的选项：
- en: a) Containers are terminated when an update is made to the Pod.
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 在对 Pod 进行更新时，容器将被终止。
- en: b) A container in a Pod failed to start successfully following repeated attempts.
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) Pod 中的一个容器在多次尝试后未能成功启动。
- en: c) Containers are terminated when an update is made to the deployment.
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 在对部署进行更新时，容器将被终止。
- en: d) None of the above.
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 以上都不是。
- en: 'Select the option that depicts the state where all containers in a Pod have
    terminated successfully and will not be restarted:'
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择描述所有容器在 Pod 中成功终止并且不会被重启的状态的选项：
- en: a) Unknown
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 未知
- en: b) Pending
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 等待中
- en: c) Completed
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 已完成
- en: d) Failed
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 失败
- en: 'Select the appropriate Service type where a Service gets a cluster-wide port:'
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择适当的服务类型，其中服务获得集群范围内的端口：
- en: a) ClusterIP
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) ClusterIP
- en: b) NodePort
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) NodePort
- en: c) LoadBalancer
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) LoadBalancer
- en: d) All of the above
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 以上所有
- en: Answers
  id: totrans-477
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 答案
- en: (d) – A new ReplicaSet will be created with the new image running inside a new
    Pod and will gradually replace Pods from the old ReplicaSet.
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (d) – 将创建一个新的 ReplicaSet，并在新的 Pod 中运行新的镜像，逐步替换旧 ReplicaSet 中的 Pods。
- en: (c) – Pod.
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c) – Pod。
- en: (b) – The Service object sends traffic to Pods based on label selectors.
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b) – 服务对象根据标签选择器将流量发送到 Pods。
- en: (c) – Configure a readiness probe.
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c) – 配置就绪探针。
- en: (c) – Create a cluster with multiple node pools.
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c) – 创建一个包含多个节点池的集群。
- en: (c) – Canary deployment.
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c) – 金丝雀部署。
- en: (d) – All of the above.
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (d) – 以上所有。
- en: (b) and (d) – A/B and Red/Black is the same as Blue/Green.
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b) 和 (d) – A/B 和 Red/Black 与 Blue/Green 相同。
- en: (b) – ResourceQuota is an example of an admission controller where a namespace
    can be restricted to only use up to a certain capacity of memory and CPU. Each
    development team can work exclusively in its own namespace.
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b) – ResourceQuota 是一种入驻控制器的示例，其中一个命名空间可以被限制为仅使用一定容量的内存和 CPU。每个开发团队可以在自己的命名空间中独立工作。
- en: (c) – It runs a specific Pod on every node in the cluster or a subset of selected
    nodes in the cluster.
  id: totrans-487
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c) – 它会在集群中的每个节点或选定节点的子集上运行一个特定的 Pod。
- en: (b) and (c).
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b) 和 (c)。
- en: (c) – The last Pod that was created will be deleted (`log-collector-2`).
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c) – 最后创建的 Pod 将被删除（`log-collector-2`）。
- en: (b) – A container in a Pod failed to start successfully following repeated attempts.
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b) – Pod 中的一个容器在多次尝试后未能成功启动。
- en: (c) – Completed.
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c) – 已完成。
- en: (b) – NodePort.
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b) – NodePort。
