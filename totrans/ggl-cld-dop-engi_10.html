<html><head></head><body>
		<div id="_idContainer124">
			<h1 id="_idParaDest-182"><em class="italic"><a id="_idTextAnchor182"/>Chapter 8</em>: Understanding GKE Essentials to Deploy Containerized Applications</h1>
			<p>Kubernetes or K8s <a id="_idIndexMarker853"/>is an open source container orchestration system for automating the application deployment, scaling, and management of a cluster running containerized applications. The previous chapter introduced K8s fundamentals, including cluster anatomy, master plane components, Kubernetes objects (such as Pods and Services), workloads such as Deployments, StatefulSets, DaemonSets, and so on, and deep-dived into deployment strategies. However, setting up an open source Kubernetes cluster involves a lot of work at the infrastructure level and will also take a lot of time to set up. This also includes post-maintenance activities such as updating, upgrading, or repairing the cluster. GCP <a id="_idIndexMarker854"/>provides a compute offering that provides a managed Kubernetes or K8s environment called <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>).</p>
			<p>The chapter introduces Google Kubernetes Engine as the managed Kubernetes option in GCP and uses the concepts introduced in <a href="B15587_07_Final_ASB_ePub.xhtml#_idTextAnchor154"><em class="italic">Chapter 7</em></a>, <em class="italic">Understanding Kubernetes Essentials to Deploy Containerized Applications</em>, to create a managed GKE cluster, deploy a containerized application into the cluster, and expose the application to be accessible from external clients. The chapter later details key GKE features, including the following topics:</p>
			<ul>
				<li><strong class="bold">Google Kubernetes Engine (GKE) – introduction </strong></li>
				<li><strong class="bold">GKE – core features </strong></li>
				<li><strong class="bold">GKE Autopilot – hands-on lab</strong></li>
			</ul>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor183"/>Technical requirements</h1>
			<p>There are four main technical requirements:</p>
			<ul>
				<li>A valid <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>) account to go hands-on with GCP services: <a href="https://cloud.google.com/free">https://cloud.google.com/free</a> </li>
				<li>Install Google Cloud SDK: <a href="https://cloud.google.com/sdk/docs/quickstart">https://cloud.google.com/sdk/docs/quickstart</a> </li>
				<li>Install Git: <a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git">https://git-scm.com/book/en/v2/Getting-Started-Installing-Git</a> </li>
				<li>Install Docker: <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a> </li>
			</ul>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor184"/>Google Kubernetes Engine (GKE) – introduction</h1>
			<p>GKE <a id="_idIndexMarker855"/>is managed K8s and abstracts away the need to manage the master plane components from a user's standpoint. Creating a GKE cluster is much easier than creating a K8s cluster. This is because GKE cluster creation removes the need to manually create nodes, configure nodes and certificates, and establish network communication between the nodes. GKE also offers options to autoscale and manage auto-upgrades of the cluster's node software.</p>
			<p>The following are the key <a id="_idIndexMarker856"/>features of a GKE cluster. These features differentiate GKE from open source Kubernetes or K8s:</p>
			<ul>
				<li>Fully managed and abstracts away the need for a user to provide underlying resources.</li>
				<li>Uses a container-optimized OS, an OS that is maintained by Google and is built to scale quickly with minimal resource requirements.</li>
				<li>Supports auto-upgrade and provides options to either get the latest available features or a more stable version without manual intervention.</li>
				<li>Provides the ability to auto-repair nodes by continuously monitoring the status of the nodes. If unhealthy, the nodes are gracefully drained and recreated.</li>
				<li>Automatically scales <a id="_idIndexMarker857"/>the cluster by adding more nodes as needed.</li>
			</ul>
			<p>In addition to the preceding list, here are some additional key features that are available in K8s but need to be added and explicitly maintained as add-ons. These come as standard with GKE, thus making GKE a more viable and preferred option when compared to K8s:</p>
			<ul>
				<li>Load balancer – GKE provides a HTTP(S) load balancer.</li>
				<li>DNS – GKE implements service discovery and provides a managed DNS.</li>
				<li>Logging, monitoring, and dashboard – GKE provides these features built in due to its integration with Google Cloud operations.</li>
			</ul>
			<p>Until recently, GKE offered <a id="_idIndexMarker858"/>only one mode of operation called <strong class="bold">Standard</strong> (also referred to as default). The Standard mode allows users to select the configurations needed to run workloads such as the node's machine type. This mode also allows you to select security configuration features, provides the ability to group nodes that run similar workloads, provides options to configure networking, and so on. Essentially, creating a cluster through GKE Standard mode is much easier than in open source K8s, but there is still a learning curve.</p>
			<p>GKE recently introduced <a id="_idIndexMarker859"/>a new mode of operation called Autopilot. Autopilot has many of the configurations pre-selected and essentially creates a production-grade cluster that is hardened from a security standpoint. There are a few options to configure but, most importantly, the nodes are provisioned only when workloads are deployed. Autopilot mode will be discussed in detail later in this chapter through a hands-on lab.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The current chapter focuses on <em class="italic">Standard</em> mode unless explicitly specified. This will help you to understand the available options while creating a GKE cluster and provides insights into GKE features. Later in this chapter, the Autopilot mode will be elaborated on, calling out the key differences between the Standard and Autopilot modes, along with a hands-on lab.</p>
			<p>GKE <a id="_idIndexMarker860"/>provides seamless integration with multiple service offerings from GCP. GKE provides options to automate deployment by building code stored in a source code repository using Cloud Build, which results in private container images that could be stored in Google's Container Registry. In addition, access to the cluster and the ability to configure GKE cluster options can be controlled via <a id="_idIndexMarker861"/>Google's <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>). GKE integrates with GCP's network offerings as a GKE cluster is created as part of Google's Virtual Private Cloud or VPC. GCP provides insights into a GKE cluster and its resources as GKE integrates with Google's Cloud operations, a suite of tools from Google aimed at providing integrated services related to monitoring and logging.</p>
			<p>We will start by creating a GKE cluster through a step-by-step process. This will provide an insight into the possible configuration options. Once the cluster is created, the user will be able to deploy an application through the concept of a Deployment and expose the application through the concept of a Service. The application runs inside a container wrapped by a Pod. The Deployment specification will manage the Pod. The Pod is then exposed using the concept of a Service. The concepts of Pods, Deployments, and services are K8s fundamentals that were discussed in <a href="B15587_07_Final_ASB_ePub.xhtml#_idTextAnchor154"><em class="italic">Chapter 7</em></a>, <em class="italic">Understanding Kubernetes Essentials to Deploy Containerized Applications</em>, and these concepts will be put into action on an actual GKE cluster.</p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor185"/>Creating a GKE cluster</h2>
			<p>There are <a id="_idIndexMarker862"/>multiple ways to create a GKE cluster – the Cloud Console, CLI, or REST. To create a cluster, the user or the service account should have one of the following pre-defined roles: Kubernetes Engine Admin or Kubernetes Engine Cluster Admin. </p>
			<p>The following is a step-by-step process to create a GKE cluster from the Google Cloud Console. The mode of operation will be <strong class="bold">Standard</strong> in this specific example:</p>
			<ol>
				<li>Navigate to the GCP Console and select the compute service – <strong class="bold">Kubernetes Engine</strong>.</li>
				<li>Select the option to create a cluster and choose the <strong class="bold">Standard</strong> mode. </li>
				<li>Enter the name for the cluster as <strong class="source-inline">my-first-cluster</strong>.</li>
				<li>Leave the <a id="_idIndexMarker863"/>default selections for the rest of the options. Refer to <em class="italic">Figure 8.1</em>:<p class="figure-caption"> </p><div id="_idContainer098" class="IMG---Figure"><img src="image/B15587_08_01.jpg" alt="Figure 8.1 – Creating a GKE Cluster from the GCP Console&#13;&#10;"/></div><p class="figure-caption">Figure 8.1 – Creating a GKE Cluster from the GCP Console</p></li>
				<li>Select the option to <strong class="bold">CREATE</strong> the cluster. This will initiate the cluster creation process.</li>
				<li>The newly created cluster will be displayed on the cluster home page. Refer to <em class="italic">Figure 8.2</em>:</li>
			</ol>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="image/B15587_08_02.jpg" alt="Figure 8.2 – The GKE cluster list page displays the newly created cluster&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – The GKE cluster list page displays the newly created cluster</p>
			<p>The newly created <a id="_idIndexMarker864"/>cluster used the default options. Nothing really was changed during the cluster creation except for the cluster name. The following are some important points to know when a GKE cluster is created with default options. Each of the default options mentioned in the following list can be explicitly changed during cluster creation:</p>
			<ul>
				<li>The default <strong class="bold">Location type</strong> of the cluster is <strong class="bold">Zonal</strong>. <strong class="bold">Location type</strong> refers to the cluster based on availability requirements. The options are <strong class="bold">Zonal</strong> and <strong class="bold">Regional</strong>. </li>
				<li>The default <strong class="bold">Control plane zone</strong> is <strong class="source-inline">us-central1-c</strong>. This indicates the zone where the control plane components are created.</li>
				<li>The default <strong class="bold">Node Location</strong> is <strong class="source-inline">us-central1-c</strong>. This indicates where the nodes are created. Multiple locations within a region can be selected to form a cluster where the location type is a multi-zonal cluster.</li>
				<li>The default <strong class="bold">Control plane version</strong> is <strong class="bold">Release channel</strong>. <strong class="bold">Control plane version</strong> provides options to signify the cluster version. The cluster version is indicative of the preferred feature set in terms of stability.</li>
				<li>The <strong class="bold">default size</strong> of the cluster is <strong class="source-inline">3</strong>, indicating the number of worker nodes. The cluster, by default, only has 1 node pool. It's important to note that the cluster size doesn't include the master node count. Customers only pay for the worker nodes. The master node and the associated master plane components are entirely managed by GKE.</li>
				<li>The <strong class="bold">default node pool</strong> is named <strong class="source-inline">default-pool</strong>. A node pool is a collection of VMs.</li>
				<li>The default node pool consists of 3 nodes and the machine type for the node is <strong class="source-inline">e2-medium</strong> (2 vCPU, 4 GB memory).</li>
				<li>The default <strong class="bold">Maintenance Window</strong> is <strong class="bold">anytime</strong>. This implies that GKE maintenance can run at any time on the cluster. This is not the preferred option when running production workloads.</li>
				<li>The <a id="_idIndexMarker865"/>default cluster type based on networking is <strong class="bold">Public cluster</strong> and the default <strong class="bold">VPC network</strong> is <strong class="bold">default</strong>. This indicates how clients can reach the control plane and how applications in the cluster communicate with each other and with the control plane. </li>
				<li>Advanced networking options such as <strong class="bold">VPC-native traffic routing</strong> and <strong class="bold">HTTP Load Balancing</strong> are <em class="italic">enabled</em> by default. These options are discussed in detail in the sub-section <em class="italic">Networking in GKE</em>, later in this chapter</li>
				<li><strong class="bold">Maximum pods per node</strong> defaults to <strong class="source-inline">110</strong>.</li>
				<li>The security feature <strong class="bold">Shielded GKE Node</strong> is <em class="italic">enabled</em>. This feature provides strong cryptographic identity for nodes joining a cluster and is discussed in detail as part of <a href="B15587_09_Final_ASB_ePub.xhtml#_idTextAnchor201"><em class="italic">Chapter 9</em></a>, <em class="italic">Securing the Cluster Using GKE Security Constructs</em>.</li>
				<li>Cloud operations for GKE are enabled and are set to <strong class="bold">System, workload logging and monitoring</strong>. This feature aggregates logs, events, and metrics for both infrastructure and application-level workloads.</li>
			</ul>
			<p>A cluster can also be created from the <strong class="bold">Command-Line Interface</strong> (<strong class="bold">CLI</strong>). The following is the CLI command to create a cluster with default options. The default options used in the CLI are the same as the default options used while creating a cluster from the console as described previously. One significant difference, however, is that it is mandatory to explicitly specify a zone while executing through the CLI. However, the zone is auto-filled in the UI unless modified:</p>
			<p class="source-code"># Create a GKE cluster with default options</p>
			<p class="source-code">gcloud container clusters create my-first-cli-cluster --zone us-central1-c</p>
			<p>This CLI command can be run from the terminal window of your local machine, which has Google Cloud SDK installed and configured. Alternatively, the CLI command can also be executed using Google Cloud Shell, activated through the Google Cloud Console.</p>
			<p>Given that a GKE cluster is created, the next step is to deploy an application onto the GKE cluster and expose the application to an external client. This is discussed as the next topic.</p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor186"/>GKE cluster – deploying and exposing an application</h2>
			<p>In <a href="B15587_06_Final_ASB_ePub.xhtml#_idTextAnchor131"><em class="italic">Chapter 6</em></a>, <em class="italic">Building code using Cloud Build, and Pushing to Container Registry</em>, we created a<a id="_idIndexMarker866"/> container image, and the container image was deployed using Cloud Run. In this chapter and in this sub-section, we <a id="_idIndexMarker867"/>will reuse this image and deploy it to the newly created GKE cluster by creating appropriate workloads. Once the application is deployed, the application will be exposed via a Service so that the application can be reached via an external client such as a web browser.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">For continuity from an example standpoint, we will be using the container image created in <a href="B15587_06_Final_ASB_ePub.xhtml#_idTextAnchor131"><em class="italic">Chapter 6</em></a>, <em class="italic">Building code using Cloud Build, and Pushing to Container Registry</em> – <strong class="source-inline">gcr.io/gcp-devops-2021/cloud-build-trigger</strong>. It's recommended to use an appropriate container image of your choice that you have access to. For example, if you followed the step-by-step instructions in <a href="B15587_06_Final_ASB_ePub.xhtml#_idTextAnchor131"><em class="italic">Chapter 6</em></a>, <em class="italic">Building code using Cloud Build, and Pushing to Container Registry</em>, and ended up creating a container image in your project, you can reuse the same image in this chapter.  </p>
			<p>We will deploy the application and expose the application in two different ways:</p>
			<ul>
				<li>GKE Console</li>
				<li>The CLI approach via Cloud Shell</li>
			</ul>
			<p>It's important to note that a cluster is typically deployed in most cases through the command line. However, we will first explore the GKE Console approach as this will give us insights into the available configuration options. This is covered as the next topic.</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor187"/>GKE Console</h2>
			<p>The first step is to<a id="_idIndexMarker868"/> deploy the application to the GKE cluster through the GKE Console.</p>
			<h3>Deploying an application to the GKE cluster</h3>
			<p>The following is the step-by-step <a id="_idIndexMarker869"/>process to deploy an application through the GKE Console:</p>
			<ol>
				<li value="1">Navigate to the <strong class="bold">Clusters</strong> page in the <strong class="bold">Kubernetes Engine</strong> section of the GCP Console. </li>
				<li>Select the cluster that was previously created – <strong class="source-inline">my-first-cluster</strong>.</li>
				<li>On the left-hand pane, select the section <strong class="bold">Workloads</strong>. From a GKE perspective, workloads refer to Deployments, StatefulSets, DaemonSets, Jobs, and CronJobs. There are no workloads at this moment and the current state will be as shown in <em class="italic">Figure 8.3</em>:<div id="_idContainer100" class="IMG---Figure"><img src="image/B15587_08_03.jpg" alt="Figure 8.3 – The Workloads section of a newly created cluster&#13;&#10;"/></div><p class="figure-caption">Figure 8.3 – The Workloads section of a newly created cluster</p></li>
				<li>Create a workload by <a id="_idIndexMarker870"/>selecting the <strong class="bold">DEPLOY</strong> option. This action allows you to create a Deployment object in a two-step process.</li>
				<li>The first step to create a Deployment is to define the containers required for the Deployment. Select the container image created in <a href="B15587_06_Final_ASB_ePub.xhtml#_idTextAnchor131"><em class="italic">Chapter 6</em></a>, <em class="italic">Building code using Cloud Build, and Pushing to Container Registry</em>. For this example, select the container image <strong class="source-inline">gcr.io/gcp-devops-2021/cloud-build-trigger</strong>. Refer to <em class="italic">Figure 8.4</em>. Optionally, add environment variables for the container and click on <strong class="bold">Done</strong>: <div id="_idContainer101" class="IMG---Figure"><img src="image/B15587_08_04.jpg" alt="Figure 8.4 – Selecting container image while defining a container for Deployment&#13;&#10;"/></div><p class="figure-caption">Figure 8.4 – Selecting container image while defining a container for Deployment</p></li>
				<li>Optionally, multiple containers <a id="_idIndexMarker871"/>can be added to the Pod by using the <strong class="bold">ADD CONTAINER</strong> option. Refer to <em class="italic">Figure 8.5</em>:<div id="_idContainer102" class="IMG---Figure"><img src="image/B15587_08_05.jpg" alt="Figure 8.5 – The option to add multiple containers to a Deployment&#13;&#10;"/></div><p class="figure-caption">Figure 8.5 – The option to add multiple containers to a Deployment</p></li>
				<li>The second step in creating a Deployment is to configure the Deployment. This includes specifying the application name, namespace, labels, and the cluster to which the application should be deployed. For this specific example, set <strong class="bold">Application name</strong> as <strong class="source-inline">hello-world</strong>, <strong class="bold">Namespace</strong> as <strong class="source-inline">default</strong>, <strong class="bold">Labels</strong> with <strong class="bold">Key</strong> as <strong class="source-inline">app</strong> and <strong class="bold">Value</strong> as <strong class="source-inline">hello-world</strong>, and select the cluster called <strong class="source-inline">my-first-cluster</strong>. Refer to <em class="italic">Figure 8.6</em>:<div id="_idContainer103" class="IMG---Figure"><img src="image/B15587_08_06.jpg" alt="Figure 8.6 – Configuring a Deployment by specifying the required attributes&#13;&#10;"/></div><p class="figure-caption">Figure 8.6 – Configuring a Deployment by specifying the required attributes</p></li>
				<li>Before selecting the <strong class="bold">DEPLOY</strong> option, the configuration YAML can be viewed by selecting the <strong class="bold">VIEW YAML</strong> option as shown in <em class="italic">Figure 8.6</em>. By default, the number of replicas is defined as 3. This can optionally be changed to the desired replica count.</li>
				<li>Initiate the deployment creation process by selecting the <strong class="bold">DEPLOY</strong> option.</li>
				<li>The newly <a id="_idIndexMarker872"/>created Deployment – <strong class="source-inline">hello-world</strong> – will be displayed as follows. This Deployment created three replicas with the same image. Refer to <em class="italic">Figure 8.7</em>:</li>
			</ol>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/B15587_08_07.jpg" alt="Figure 8.7 – Details of the newly created Deployment&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.7 – Details of the newly created Deployment</p>
			<p>It is important to note that the newly created Deployment – <strong class="source-inline">hello-world</strong> – cannot be accessed from external clients (such as a web browser or through a <strong class="source-inline">ping</strong> command) as the Deployment is not exposed as a Service. However, the application can still be tested by using the <strong class="source-inline">port-forward</strong> option. The CLI commands required to execute this option are shown in the following snippet. These commands can be executed through Google Cloud Shell:</p>
			<p class="source-code"># Connect to the cluster – 'my-first-cluster'</p>
			<p class="source-code">gcloud container clusters get-credentials my-first-cluster --zone us-central1-c --project gcp-devops-2021</p>
			<p class="source-code"># Find the list of pods for the deployment hello-world</p>
			<p class="source-code">kubectl get pods</p>
			<p class="source-code"># For a specific pod, create a port-forward option to access the application running inside the pod</p>
			<p class="source-code">kubectl port-forward hello-world-6755d97c-dlq7m 10080:8080</p>
			<p>Once the preceding <strong class="source-inline">port-forward</strong> command is executed, traffic coming on <strong class="source-inline">127.0.0.1:10080</strong> will be forwarded to port <strong class="source-inline">8080</strong>. Port <strong class="source-inline">8080</strong> is the container port related to the <strong class="source-inline">hello-world</strong> Deployment. Refer to <em class="italic">Figure 8.8</em>:</p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/B15587_08_08.jpg" alt="Figure 8.8 – Forwarding traffic to a container inside a Pod&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.8 – Forwarding traffic to a container inside a Pod</p>
			<p>To test <a id="_idIndexMarker873"/>whether traffic is getting forwarded, open another Cloud Shell window and run the <strong class="source-inline">curl</strong> command as shown. This will do a REST call invocation against the application running inside the container of a Pod. Refer to <em class="italic">Figure 8.9</em>:</p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/B15587_08_09.jpg" alt="Figure 8.9 – Result of accessing the application in a Pod through port-forwarding&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.9 – Result of accessing the application in a Pod through port-forwarding</p>
			<p>Alternatively, you can also use the web preview option on port <strong class="source-inline">10080</strong> in Cloud Shell to view the application. Given that the application is now deployed and is working as expected, the next step is to expose the application as a Service.</p>
			<h3>Exposing the application as a Service</h3>
			<p>The following is a<a id="_idIndexMarker874"/><a id="_idIndexMarker875"/> step-by-step process to expose the application as a Service through the GCP Console:</p>
			<ol>
				<li value="1">Navigate to the <strong class="bold">Clusters</strong> page in the <strong class="bold">Kubernetes Engine</strong> section of the GCP Console. </li>
				<li>Select the cluster that was previously created – <strong class="source-inline">my-first-cluster</strong>.</li>
				<li>Select the Deployment that was previously created – <strong class="source-inline">hello-world</strong>.</li>
				<li>Under the <strong class="bold">Actions</strong> menu on the deployment details page, select the <strong class="bold">EXPOSE</strong> option. This will open a pop-up window where <strong class="bold">Port</strong>, <strong class="bold">Target port</strong>, <strong class="bold">Protocol</strong>, and <strong class="bold">Service type</strong> need to be selected.</li>
				<li>Enter <strong class="bold">Port</strong> as <strong class="source-inline">80</strong> (this represents the port where the Service will be listening for incoming traffic), <strong class="bold">Target port</strong> as <strong class="source-inline">8080</strong> (this is the port the container will be listening on), <strong class="bold">Protocol</strong> as <strong class="source-inline">TCP</strong>, and <strong class="bold">Service type</strong> as <strong class="source-inline">Load balancer</strong>. Select the <strong class="bold">EXPOSE</strong> option. Refer to <em class="italic">Figure 8.10</em>:<div id="_idContainer107" class="IMG---Figure"><img src="image/B15587_08_10.jpg" alt="Figure 8.10 – Specifying port mapping to expose a Pod as a Service of type Load balancer&#13;&#10;"/></div><p class="figure-caption">Figure 8.10 – Specifying port mapping to expose a Pod as a Service of type Load balancer</p></li>
				<li>Once the Pod is <a id="_idIndexMarker876"/><a id="_idIndexMarker877"/>exposed, a Service will be created as shown in the following screenshot. Given the Service is of type <strong class="bold">LoadBalancer</strong>, the Service will have an external endpoint. Refer to <em class="italic">Figure 8.11</em>:<div id="_idContainer108" class="IMG---Figure"><img src="image/B15587_08_11.jpg" alt="Figure 8.11 – The LoadBalancer Service created by exposing the Pod&#13;&#10;"/></div><p class="figure-caption">Figure 8.11 – The LoadBalancer Service created by exposing the Pod</p></li>
				<li>Select the external endpoint. This will open the application in the browser as shown in the following screenshot. This essentially is the output of deploying the application to the GKE cluster. The output is the same as the output in <a href="B15587_06_Final_ASB_ePub.xhtml#_idTextAnchor131"><em class="italic">Chapter 6</em></a>, <em class="italic">Building code using Cloud Build, and Pushing to Container Registry</em>, when the same container image was deployed to Cloud Run. Refer to <em class="italic">Figure 8.12</em>:</li>
			</ol>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/B15587_08_12.jpg" alt="Figure 8.12 – Output of accessing the application through the load balancer Service&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.12 – Output of accessing the application through the load balancer Service</p>
			<p>This completes the topic on deploying an application to the GKE cluster and exposing the application via a <a id="_idIndexMarker878"/>load balancer Service through the GKE Console. The next sub-section essentially works on a similar example but provides insights on how the same thing can be done through Cloud Shell using the CLI approach.</p>
			<h3>The CLI approach via Cloud Shell</h3>
			<p>In this sub-section, we will <a id="_idIndexMarker879"/>deploy an application and expose the application as a load balancer Service through the CLI using Cloud Shell. We will use the same cluster as was previously created – <strong class="source-inline">my-first-cluster</strong>. It is also recommended to use the container image created as part of the exercise in <a href="B15587_06_Final_ASB_ePub.xhtml#_idTextAnchor131"><em class="italic">Chapter 6</em></a>, <em class="italic">Building code using Cloud Build, and Pushing to Container Registry</em>. For this example, the container image <strong class="source-inline">gcr.io/gcp-devops-2021/cloud-build-trigger</strong> will be used. </p>
			<h3>Deploying an application to the GKE cluster</h3>
			<p>The following is <a id="_idIndexMarker880"/>the step-by-step process to deploy an application via Cloud Shell:</p>
			<ol>
				<li value="1">Open Cloud Shell and connect to the cluster using the following CLI command:<p class="source-code"># Connect to the cluster</p><p class="source-code">gcloud container clusters get-credentials my-first-cluster --zone us-central1-c --project gcp-devops-2021</p></li>
				<li>Create a new file called <strong class="source-inline">hello-world-cli.yaml</strong> with contents as follows. This file essentially creates a Deployment that has the container and respective image to be deployed. The replica count is also specified and in this case, is 1:<p class="source-code">apiVersion: "apps/v1"</p><p class="source-code">kind: "Deployment"</p><p class="source-code">metadata:</p><p class="source-code">  name: "hello-world-cli"</p><p class="source-code">  namespace: "default"</p><p class="source-code">  labels:</p><p class="source-code">    app: "hello-world-cli"</p><p class="source-code">spec:</p><p class="source-code">  replicas: 1</p><p class="source-code">  selector:</p><p class="source-code">    matchLabels:</p><p class="source-code">      app: "hello-world-cli"</p><p class="source-code">  template:</p><p class="source-code">    metadata:</p><p class="source-code">      labels:</p><p class="source-code">        app: "hello-world-cli"</p><p class="source-code">    spec:</p><p class="source-code">      containers:</p><p class="source-code">      - name: "cloud-build-trigger-sha256-1"</p><p class="source-code">        image: "gcr.io/gcp-devops-2021/cloud-build-trigger:latest"</p></li>
				<li>Create the Deployment by running the following command: <p class="source-code">kubectl apply -f hello-world-cli.yaml</p></li>
			</ol>
			<p>Once the Deployment is<a id="_idIndexMarker881"/> created, the Deployment and its respective Pod can be queried as follows through the CLI. Please note that this Deployment will create only one Pod. Refer to <em class="italic">Figure 8.13</em>:</p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/B15587_08_13.jpg" alt="Figure 8.13 – Querying the Deployment through the CLI&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.13 – Querying the Deployment through the CLI</p>
			<p>The deployed application cannot be accessed through an external client. However, the port-forward approach explained in the previous sub-section can be exactly applied in this context as well. Given that the application is now deployed, the next step is to expose the application as a Service.</p>
			<h3>Exposing the application as a Service</h3>
			<p>The following is the<a id="_idIndexMarker882"/> step-by-step process to expose the application as a Service through Cloud Shell:</p>
			<ol>
				<li value="1">Create a new file called <strong class="source-inline">hello-world-cli-service.yaml</strong> with a definition as follows. This will create a load balancer Service that will expose a Pod with matching label selectors:<p class="source-code">apiVersion: v1</p><p class="source-code">kind: Service</p><p class="source-code">metadata:</p><p class="source-code">  name: hello-world-cli-service</p><p class="source-code">spec:</p><p class="source-code">  type: LoadBalancer</p><p class="source-code">  selector:</p><p class="source-code">    app: hello-world-cli</p><p class="source-code">  ports:</p><p class="source-code">  - protocol: TCP </p><p class="source-code">    port: 80</p><p class="source-code">    targetPort: 8080</p></li>
				<li>Create the load balancer Service by running the following command:<p class="source-code">kubectl apply -f hello-world-cli-service.yaml</p></li>
				<li>Once the Service<a id="_idIndexMarker883"/> is created, a load balancer will be created with an external endpoint. As per the Service definition, the Service will listen to traffic on port <strong class="source-inline">80</strong> and will forward the traffic to the container on port <strong class="source-inline">8080</strong>. The external endpoint of the Service can be found out by querying the Service as follows. Refer to <em class="italic">Figure 8.14</em>:<div id="_idContainer111" class="IMG---Figure"><img src="image/B15587_08_14.jpg" alt="Figure 8.14 – Query the load balancer Service to fetch the external endpoint&#13;&#10;"/></div><p class="figure-caption">Figure 8.14 – Query the load balancer Service to fetch the external endpoint</p></li>
				<li>Access the external endpoint through a browser window. The output will be the same as the output from <a href="B15587_06_Final_ASB_ePub.xhtml#_idTextAnchor131"><em class="italic">Chapter 6</em></a>, <em class="italic">Building code using Cloud Build, and Pushing to Container Registry</em>, or the output from the application deployed in GKE through the console. This is because we are using the same image. Refer to <em class="italic">Figure 8.15</em>:</li>
			</ol>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B15587_08_15.jpg" alt="Figure 8.15 – Viewing the output of the load balancer Service via an external endpoint&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.15 – Viewing the output of the load balancer Service via an external endpoint</p>
			<p>This concludes this section, which introduced GKE and took a deep dive into the step-by-step process to create a GKE cluster, deploy an application to the cluster, and expose the deployed application as a Service to be accessed by external clients. Essentially, the output of this approach is the same as the output from the console approach. The goal is to understand the process of creating a cluster, deploying workloads, and exposing the workloads through a Service via the CLI.</p>
			<p>The concepts used while creating the cluster or deploying the application are the same concepts that form the fundamentals of K8s (learned about in <a href="B15587_07_Final_ASB_ePub.xhtml#_idTextAnchor154"><em class="italic">Chapter 7</em></a>, <em class="italic">Understanding Kubernetes Essentials to Deploy Containerized Applications</em>). However, the cluster creation is much simpler in nature since the maintenance of the master plane components is completely abstracted and is not the responsibility of the user. The upcoming section focuses on core GKE features and possible cluster types, and provides an introduction to integration with networking and cloud operations in GKE.</p>
			<h1 id="_idParaDest-188"><a id="_idTextAnchor188"/>GKE – core features</h1>
			<p>This section <a id="_idIndexMarker884"/>covers the following topics. These topics will provide a considerable amount of information, which is required to build a good understanding and working knowledge of GKE. Most of these GKE concepts are an extension of topics learned about in the Kubernetes section. The topics that will be covered are as follows:</p>
			<ul>
				<li>GKE node pools</li>
				<li>GKE cluster types</li>
				<li>Autoscaling in GKE</li>
				<li>Networking in GKE</li>
				<li>Cloud operations for GKE</li>
			</ul>
			<p>The first of the GKE constructs that will be detailed in the upcoming sub-section is GKE node pools.</p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor189"/>GKE node pools</h2>
			<p>Nodes (that is, worker nodes) in a <a id="_idIndexMarker885"/>Kubernetes cluster<a id="_idIndexMarker886"/> deploy workloads. The nature of workloads deployed across all nodes might not be the same. Some workloads might be CPU-intensive, others might be memory-intensive, and some might need a minimum version of the CPU platform. Workloads can also be fault-tolerant batch jobs or might need a specific type of storage such as SSD.</p>
			<p>A <strong class="bold">node pool</strong> represents <a id="_idIndexMarker887"/>a group of nodes in a GKE cluster that have the same configuration in terms of a specific CPU family, minimum CPU platform, preemptible VMs, or a specific storage requirement. A node pool is defined using a <strong class="source-inline">nodeConfig</strong> specification. All matching nodes that match the <strong class="source-inline">nodeConfig</strong> specification will be labeled using a node label where the key is <strong class="source-inline">cloud.google.com/gke-nodepool</strong> and the value is the name of the node pool.</p>
			<p>The <a id="_idIndexMarker888"/>following is an example of a <strong class="source-inline">nodeConfig</strong> specification with a specific machine type, OAuth scopes, and a disk type:</p>
			<p class="source-code">nodeConfig: {</p>
			<p class="source-code">        machineType: "n2-highmem-32",</p>
			<p class="source-code">        oauthScopes: [</p>
			<p class="source-code">            "https://www.googleapis.com/auth/compute",</p>
			<p class="source-code">            "https://www.googleapis.com/auth/logging.write",</p>
			<p class="source-code">            "https://www.googleapis.com/auth/monitoring"</p>
			<p class="source-code">        ],</p>
			<p class="source-code">        diskType: "pd-ssd"</p>
			<p class="source-code">    }</p>
			<p>A cluster is always created with a default node pool with a specific number of nodes and a specific machine type (along with other attributes). Additional custom node pools can be added based on their respective <strong class="source-inline">nodeConfig</strong> and workload requirements. </p>
			<p>The following are some of the key <a id="_idIndexMarker889"/>characteristics of a node pool:</p>
			<ul>
				<li>A new node pool, by default, runs the latest stable Kubernetes version.</li>
				<li>The Kubernetes version on existing node pools can either be configured for auto-upgrade or can be manually upgraded.</li>
				<li>A node pool can be individually resized, upgraded, or deleted without impacting other node pools. Any change to the node pool impacts all nodes within the pool. </li>
			</ul>
			<p>The following are a few CLI commands that can perform actions on a node pool. These commands can be executed on the cluster that was previously created in this chapter – <strong class="source-inline">my-first-cluster</strong>.</p>
			<p>The following CLI command creates a node pool with a specific machine type:</p>
			<p class="source-code">gcloud container node-pools create my-high-mem-pool --machine-type=n1-highmem-8 --cluster=my-first-cluster --num-nodes=2 –zone=us-central1-c</p>
			<p>The created node pool will be reflected on the GKE Console against the cluster (refer to <em class="italic">Figure 8.16</em>):</p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B15587_08_16.jpg" alt="Figure 8.16 – New custom node pool – my-high-mem-pool created&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.16 – New custom node pool – my-high-mem-pool created</p>
			<p>The following are other CLI commands to resize a node pool, upgrade to a specific version, or delete a node pool:</p>
			<p class="source-code"># Resize node pool</p>
			<p class="source-code">gcloud container clusters resize my-first-cluster --node-pool=my-high-mem-pool --num-nodes=1 –zone=us-central1-c</p>
			<p class="source-code"># Upgrading node pool to specific cluster version</p>
			<p class="source-code">gcloud container clusters upgrade my-first-cluster --cluster-version="1.17.17-gke.3000" --node-pool=my-high-mem-cluster --zone=us-central1-c</p>
			<p class="source-code"># Delete a node pool</p>
			<p class="source-code">gcloud container node-pools delete my-high-mem-pool --cluster=my-first-cluster --zone=us-central1-c</p>
			<p>Node pools <a id="_idIndexMarker890"/>in a regional or <a id="_idIndexMarker891"/>multi-zonal cluster are replicated to multiple zones. Additionally, the workload can be deployed to a specific node pool by explicitly specifying the node pool name using a <strong class="source-inline">nodeSelector</strong> or by finding a node pool that satisfies the resource requests as defined for the workload.</p>
			<p>If the node pool name is explicitly specified using the <strong class="source-inline">nodeSelector</strong> attribute, then <strong class="source-inline">kube-scheduler</strong> will deploy workloads to the specified node. Otherwise, <strong class="source-inline">kube-scheduler</strong> will find the node pool that meets the intended resource request for the workload. </p>
			<p>This completes the overview of GKE node pools. The next topic deep-dives into the various cluster configurations available in GKE.</p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor190"/>GKE cluster configuration</h2>
			<p>GKE offers <a id="_idIndexMarker892"/>multiple cluster configuration choices based on cluster availability type, cluster version, network isolation, and Kubernetes features. Each of these configuration choices is discussed in the following sub-sections.</p>
			<h3>Cluster availability type</h3>
			<p>GKE allows you <a id="_idIndexMarker893"/>to create a cluster based on the availability requirements of the workloads. There are two types of cluster configuration based on availability types – zonal clusters (single-zone or multi-zonal) and regional clusters. These are discussed in the following sub-sections.</p>
			<h4>Zonal clusters</h4>
			<p>A <strong class="bold">zonal cluster</strong> will <a id="_idIndexMarker894"/>have a single <a id="_idIndexMarker895"/>control plane running in a single zone. The nodes (that is, worker nodes) can run either in a single zone or run across multiple zones. If the nodes run in the same zone as the control plane, then it represents <a id="_idIndexMarker896"/>a <strong class="bold">single-zone cluster</strong>. However, if nodes run across multiple zones, then it represents <a id="_idIndexMarker897"/>a <strong class="bold">multi-zonal cluster</strong>. Note that GKE allows up to 50 clusters per zone.</p>
			<p>A multi-zonal cluster <a id="_idIndexMarker898"/>will only have a single replica of the control plane. The choice between a single zone or multi-zonal cluster is based on the level of availability required for an application. Specific to a multi-zonal cluster and in the event of a cluster upgrade or a zone outage, the workloads running on the nodes will continue to run, but a new node or workload cannot be configured till the cluster control plane is available.</p>
			<p>The following are CLI commands to create a zonal cluster (single zone and multi zonal): </p>
			<p class="source-code">#Syntax</p>
			<p class="source-code">gcloud containers clusters create CLUSTER_NAME \</p>
			<p class="source-code">  --zone COMPUTE_ZONE \</p>
			<p class="source-code">  --node-locations COMPUTE_ZONE, COMPUTE_ZONE, [..]</p>
			<p class="source-code">#Single Zone Cluster</p>
			<p class="source-code">gcloud containers clusters create single-zone-cluster \</p>
			<p class="source-code">  --zone us-central1-a</p>
			<p class="source-code">#Multi Zonal Cluster</p>
			<p class="source-code">gcloud containers clusters create single-zone-cluster \</p>
			<p class="source-code">  --zone us-central1-a \</p>
			<p class="source-code">  --node-locations us-central1-a,us-central1-b, us-central1-c</p>
			<p>The input parameter specific to the zone refers to the location of the control plane. The node locations refer to the locations of the worker node(s) and are not required for a single <a id="_idIndexMarker899"/>zone cluster as it will be the same as the master control plane.</p>
			<p>This completes a brief overview of GKE zonal clusters. The next topic will provide an overview of GKE regional clusters.</p>
			<h4>Regional clusters</h4>
			<p>A<a id="_idIndexMarker900"/> regional cluster provides high <a id="_idIndexMarker901"/>availability both in terms of worker nodes as well as the control plane. A regional cluster has multiple replicas of the control plane running across multiple zones in a region. The worker nodes are also replicated across multiple zones and the worker nodes run in conjunction in the same zone as the control plane. A regional cluster cannot be converted into a zonal cluster.</p>
			<p>The following is the CLI command to create a regional cluster:</p>
			<p class="source-code">#Syntax</p>
			<p class="source-code">gcloud containers clusters create CLUSTER_NAME \</p>
			<p class="source-code">  --region COMPUTE_REGION \</p>
			<p class="source-code">  --node-locations COMPUTE_ZONE, COMPUTE_ZONE, [..]</p>
			<p class="source-code">#Regional Cluster</p>
			<p class="source-code">gcloud containers clusters create single-zone-cluster \</p>
			<p class="source-code">  --region us-central1 \</p>
			<p class="source-code">  --node-locations us-central1-a,us-central1-b, us-central1-c</p>
			<p>The input parameter specific to <strong class="source-inline">region</strong> refers to the location of the control plane. The node locations refer to the locations of the worker node. This is required for a multi-zone cluster as node locations could be in multiple zones.</p>
			<p>This completes a brief overview of GKE cluster configuration based on cluster availability type. The next topic will provide an overview of GKE cluster configuration based on cluster version.</p>
			<h3>Cluster versions</h3>
			<p>GKE allows <a id="_idIndexMarker902"/>you to choose the cluster version. The cluster version can be a very specific version, the current default version, or can be based on a release channel, which is a combination of features based on early availability and stability. These cluster version configurations are discussed in the following sub-sections.</p>
			<h3>Specific versions</h3>
			<p>A <a id="_idIndexMarker903"/>GKE cluster can be created by specifying a specific version. This information can be provided as part of the <em class="italic">Static Version</em> selection while creating the cluster from the console. The user will be provided with a choice of cluster versions and can select an available version.</p>
			<h3>Release channels</h3>
			<p>Open source <a id="_idIndexMarker904"/><a id="_idIndexMarker905"/>Kubernetes or K8s has a constant stream of releases. These could be required for the following purpose:</p>
			<ul>
				<li>To fix known issues</li>
				<li>To add new features</li>
				<li>To address any security risks/concerns</li>
			</ul>
			<p>Kubernetes users who run applications on a Kubernetes cluster will prefer to exercise control in terms of how frequently the releases should be applied or the rate at which new features should be adopted. Google<a id="_idIndexMarker906"/> provides this choice to customers using the concept of a <strong class="bold">release channel</strong>. </p>
			<p>Each of the release channels provides <strong class="bold">generally available</strong> (<strong class="bold">GA</strong>) features but the maturity of the features in terms of their original release date will vary from one channel to another. In addition, Google can also add the latest GKE-specific features depending on the type of release channel. This ensures that a specific feature or fix has potentially gone through the grind and is vetted in terms of its correctness and consistency over a period.</p>
			<p>GKE provides three release channels:</p>
			<ul>
				<li><strong class="bold">Rapid</strong>: This <a id="_idIndexMarker907"/>release channel includes the latest Kubernetes and GKE features when compared to other release channels, but the features are still several weeks old after their respective open source GA release.</li>
				<li><strong class="bold">Regular</strong>: This<a id="_idIndexMarker908"/> is the default release channel, which includes Kubernetes and GKE-specific features that are reasonably new but are more stable in nature. The features are at least 2-3 months old after their release in the rapid channel and several months old from their open source GA release.</li>
				<li><strong class="bold">Stable</strong>: This <a id="_idIndexMarker909"/>is the most stable of the release channels since the features added to this channel are added at least 2-3 months after being added to the regular channel. Essentially, the features are thoroughly validated and tested to provide the utmost stability.</li>
			</ul>
			<p>The following is the CLI command to enroll a cluster in a release channel:</p>
			<p class="source-code">#Syntax</p>
			<p class="source-code">gcloud containers clusters create CLUSTER_NAME \</p>
			<p class="source-code">  --zone COMPUTE_ZONE \</p>
			<p class="source-code">  --release-channel CHANNEL \</p>
			<p class="source-code">  ADDITIONAL_FLAGS</p>
			<p class="source-code"># Release Channel Example</p>
			<p class="source-code">gcloud containers clusters create my-cluster \</p>
			<p class="source-code">  --zone us-central1-a \</p>
			<p class="source-code">  --release-channel rapid</p>
			<p>To summarize, new Kubernetes versions and GKE features are promoted from the rapid to the regular to the stable channel, providing users with the choice to use newer features over stable features. GKE handles the availability of versions and the upgrade cadence once a cluster is added to the release channel. Each of the release channels continues to receive critical security updates.</p>
			<h3>The default version</h3>
			<p>If a specific <a id="_idIndexMarker910"/>version or a release channel is not specified, then GKE creates a cluster with the current default version. GKE selects a default version based on usage and real-world performance. GKE is responsible for changing the default version on a regular basis. Historically, new versions of Kubernetes are released every 3 months.</p>
			<p>This completes a brief overview of GKE cluster configuration based on cluster version. The next topic will provide an overview of GKE cluster configuration based on network isolation choices.</p>
			<h3>Network isolation choices</h3>
			<p>There are <a id="_idIndexMarker911"/>two specific choices related to network isolation – a public cluster or a private cluster. A public cluster is the default configuration. However, this does not enforce network isolation and the cluster is accessible from any public endpoint. This makes the cluster vulnerable from a security standpoint. The drawbacks of configuring a public cluster can be handled through a private cluster, which is introduced in the following sub-sections.</p>
			<h3>Private clusters</h3>
			<p>GKE provides <a id="_idIndexMarker912"/>an option to create a private cluster <a id="_idIndexMarker913"/>where the nodes only have internal IP addresses. This means that the nodes and the pods running on the nodes are isolated from the internet and inherently will not have inbound or outbound connectivity to the public internet.</p>
			<p>A private cluster will have a control plane that includes a private endpoint, in addition to a public endpoint. Access to the public endpoint can be controlled through multiple options. In addition, the control plane will run on a VM that is in a VPC network in a Google-owned project. The details surrounding private clusters will be discussed in depth as part of <a href="B15587_09_Final_ASB_ePub.xhtml#_idTextAnchor201"><em class="italic">Chapter 9</em></a>, <em class="italic">Securing the Cluster Using GKE Security Constructs</em>.</p>
			<h3>Kubernetes features – alpha clusters</h3>
			<p>New features in Kubernetes are<a id="_idIndexMarker914"/> rolled out to GKE as part of the release channel in most cases. The release channel includes choices of rapid, regular, and stable. However, alpha features are only available in special GKE alpha clusters. This is discussed in the following sub-sections.</p>
			<h4>Alpha clusters</h4>
			<p>Alpha clusters <a id="_idIndexMarker915"/>are a specific feature of GKE that is designed for adopting new features that are not production-ready or generally available as open source. GKE creates alpha clusters as short-lived clusters and they are automatically deleted after 30 days. </p>
			<p>The following is the CLI command to create an alpha cluster:</p>
			<p class="source-code">#Syntax</p>
			<p class="source-code">gcloud container clusters create cluster-name \</p>
			<p class="source-code">    --enable-kubernetes-alpha \</p>
			<p class="source-code">    [--zone compute-zone] \</p>
			<p class="source-code">    [--cluster-version version]</p>
			<p class="source-code">#Alpha Cluster Example</p>
			<p class="source-code">gcloud container clusters create my-cluster \</p>
			<p class="source-code">    --enable-kubernetes-alpha \</p>
			<p class="source-code">    --region us-central1</p>
			<p>These clusters do not receive security updates, cannot be auto-upgraded or auto-repaired, and are not covered by any GKE-specific SLAs. Hence, alpha clusters are never recommended for production workloads.</p>
			<p>This completes a brief overview of GKE cluster configuration based on network isolation choices. This also concludes the sub-section on GKE cluster configuration in general. The next topic details possible autoscaling options in GKE.</p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor191"/>AutoScaling in GKE</h2>
			<p>There are three potential <a id="_idIndexMarker916"/>options to perform autoscaling in GKE. Each of these options is suitable for specific needs and<a id="_idIndexMarker917"/> situations:</p>
			<ul>
				<li><strong class="bold">Cluster autoscaler</strong>: A scaling <a id="_idIndexMarker918"/>option to resize a node pool in a GKE cluster</li>
				<li><strong class="bold">Horizontal Pod Autoscaler</strong> (<strong class="bold">HPA</strong>): An <a id="_idIndexMarker919"/>option that indicates when application instances should be autoscaled based on their current utilization</li>
				<li><strong class="bold">Vertical Pod Autoscaler</strong> (<strong class="bold">VPA</strong>): An <a id="_idIndexMarker920"/>option that suggests recommended resources for a Pod based on the current utilization</li>
			</ul>
			<p>The upcoming topics detail the preceding autoscaling mechanisms, starting with the cluster autoscaler.</p>
			<h3>The cluster autoscaler</h3>
			<p>The <strong class="bold">cluster autoscaler</strong> is a <a id="_idIndexMarker921"/>scaling mechanism<a id="_idIndexMarker922"/> to automatically resize a node pool in a GKE cluster. The scaling is based on the demands of workloads deployed within the node pool. This allows you to implement the core concept of cloud computing, called elasticity, and removes the need to over-provision or under-provision nodes.</p>
			<p>The cluster autoscaler works on a per-node pool basis and is based on resource requests (defined as part of the Pod specification) rather than the actual resource utilization. When a new Pod needs to be deployed, the Kubernetes scheduler works out of the Pod resource requests and attempts to find a node to deploy the Pod. If there is no node that matches the Pod resource requirement in terms of available capacity, then the Pod goes into a pending state until any of the existing pods are terminated or a new node is added.</p>
			<p>The cluster autoscaler keeps track of the pods that are in the pending state and subsequently tries to scale up the number of nodes. Similarly, the cluster autoscaler also scales down the number of nodes if the nodes are under-utilized. A minimum or maximum number of nodes can be defined for the cluster autoscaler, which allows it to operate within the specified limits.</p>
			<p>When a cluster is scaled down, there is a possibility that new workloads might have to wait till new nodes are added. This could cause a potential disruption. GKE profile types provide a choice of options to choose between balanced and aggressive scale-down:</p>
			<ul>
				<li><strong class="bold">Balanced</strong>: The default profile option, which is not aggressive in nature.</li>
				<li><strong class="bold">Optimize-utilization</strong>: Scaling down is more aggressive and removes underutilized nodes faster.</li>
			</ul>
			<p>The following are some CLI commands related to the cluster autoscaler:</p>
			<p class="source-code"># Create cluster with autoscaler limits</p>
			<p class="source-code">gcloud container clusters create my-autoscaler-cluster \</p>
			<p class="source-code">  --zone us-central1-b \</p>
			<p class="source-code">  --num-nodes 3 --enable-autoscaling --min-nodes 1 --max-nodes 5</p>
			<p class="source-code"># Update autoscaling profile to optimize-utilization</p>
			<p class="source-code">gcloud beta container clusters update my-autoscaler-cluster \</p>
			<p class="source-code">  -autoscaling-profile optimize-utilization</p>
			<p>The following are some<a id="_idIndexMarker923"/> limitations that need to be considered when using the cluster autoscaler:</p>
			<ul>
				<li>There is a graceful termination of 10 minutes for rescheduling pods on to a different node before forcibly terminating the original node.</li>
				<li>The node pool scaling limits are determined by zone availability. If a cluster has 3 nodes (with <strong class="source-inline">min_nodes</strong> = <strong class="source-inline">1</strong> and <strong class="source-inline">max_nodes</strong> = <strong class="source-inline">5</strong>) across 4 zones, then if 1 of the zones fails, the size of the cluster can vary from 4-20 nodes per cluster to 3-15 nodes per cluster.</li>
			</ul>
			<p>This concludes the overview of the cluster autoscaler. The next topic focuses on the <strong class="bold">Horizontal Pod Autoscaler</strong> (<strong class="bold">HPA</strong>).</p>
			<h3>The Horizontal Pod Autoscaler</h3>
			<p>The HPA is a <a id="_idIndexMarker924"/>Kubernetes controller object that <a id="_idIndexMarker925"/>automatically scales the number of pods in a replication controller, Deployment, ReplicaSet, or StatefulSet based on the observed CPU or memory utilization. The HPA indicates the Deployment or StatefulSet against which scaling needs to happen. The HPA doesn't apply to DaemonSets.</p>
			<p>To implement the HPA, the following factors need to be considered:</p>
			<ul>
				<li>One HPA object needs to be defined per Deployment or StatefulSet.</li>
				<li>The attribute <strong class="source-inline">--horizontal-pod-autoscaler-sync-period</strong> allows you to implement the HPA as a control loop. The default value is 15 seconds per period.</li>
				<li><strong class="source-inline">kube-controller-manager</strong> (on a per-period basis) obtains metrics from the resource manager API or the custom metrics API and compares them against the metrics specified in each HPA definition.</li>
			</ul>
			<p>The following are few key parameters that can define the HPA configuration:</p>
			<ul>
				<li><strong class="source-inline">--horizontal-pod-autoscaler-initial-readiness-delay</strong>: A configurable window to ensure that a Pod is transitioned to the ready state.</li>
				<li><strong class="source-inline">--horizontal-pod-autoscaler-cpu-initialization-period</strong>: A configurable window to set the CPU initialization period, once the Pod is transitioned to the ready state. The default is 5 minutes.</li>
				<li><strong class="source-inline">--horizontal-pod-autoscaler-downscale-stabilization</strong>: A configurable window that autoscaler needs to wait before initiating a downscale operation after the current one is completed. The default is 5 minutes. This prevents thrashing.</li>
			</ul>
			<p>The following<a id="_idIndexMarker926"/> is the sample definition of an HPA<a id="_idIndexMarker927"/> object based on CPU utilization:</p>
			<p class="source-code">apiVersion: autoscaling/v1</p>
			<p class="source-code">kind: HorizontalPodAutoscaler</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: nginx</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  scaleTargetRef:</p>
			<p class="source-code">    apiVersion: apps/v1</p>
			<p class="source-code">    kind: Deployment</p>
			<p class="source-code">    name: my-nginx</p>
			<p class="source-code">  minReplicas: 1</p>
			<p class="source-code">  maxReplicas: 5</p>
			<p class="source-code">  targetCPUUtilizationPercentage: 75</p>
			<p>In the preceding example, <strong class="source-inline">kube-controller-manager</strong> will scale up the Deployment based on the HPA object specification, to a maximum of 5 instances if the target CPU utilization exceeds 75%. This concludes the overview of the HPA. The next topic focuses on the <strong class="bold">Vertical Pod Autoscaler</strong> (<strong class="bold">VPA</strong>).</p>
			<h3>The Vertical Pod Autoscaler (VPA)</h3>
			<p>The cluster <a id="_idIndexMarker928"/>autoscaler functions based on the workload's CPU and memory request limits. If these limits are not defined appropriately, then there is always a chance of over-provisioning or <a id="_idIndexMarker929"/>under-provisioning as the reference values will not be accurate.</p>
			<p>The VPA is a Kubernetes resource that recommends values for CPU and memory requests/limits. Additionally, the VPA can automatically update workloads if the <strong class="source-inline">updateMode</strong> attribute is set to <em class="italic">On</em> on the VPA. This will potentially evict the existing Pod as a change is required to the pod's resource requests and will result in a new Pod with the updated recommendations.</p>
			<p>This ensures that the cluster nodes are optimally utilized and potentially removes the need to run benchmark tests to determine the correct values for CPU and memory requests. VPA communicates with the cluster autoscaler to perform the appropriate operations on the nodes tied to the node pools.</p>
			<p>The following is a sample definition of a VPA object:</p>
			<p class="source-code">apiVersion: autoscaling.k8s.io/v1</p>
			<p class="source-code"><strong class="bold">kind: VerticalPodAutoscaler</strong></p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: my-vpa</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  targetRef:</p>
			<p class="source-code">    apiVersion: "apps/v1"</p>
			<p class="source-code">    kind:       Deployment</p>
			<p class="source-code">    name:       my-nginx</p>
			<p class="source-code">  updatePolicy:</p>
			<p class="source-code">    <strong class="bold">updateMode: "On"</strong></p>
			<p>The <strong class="source-inline">kind</strong> attribute in the preceding snippet indicates that the Kubernetes resource is a VPA object. The <strong class="source-inline">updateMode</strong> attribute indicates that the recommendations suggested by the VPA are automatically applied against the running workloads.</p>
			<p>The following are some CLI commands specific to the VPA:</p>
			<p class="source-code"># To view recommendations of VPA is updateMode was set to Off</p>
			<p class="source-code">kubectl get vpa my-vpa --output yaml</p>
			<p class="source-code"># To disable VPA</p>
			<p class="source-code">gcloud container clusters update my-cluster --no-enable-vertical-pod-autoscaling</p>
			<p>If an HPA object is configured to evaluate metrics for CPU or memory, it's recommended that HPA should not be used with VPA.</p>
			<p class="callout-heading">Multi-dimensional Pod autoscaling (MPA)</p>
			<p class="callout">This is a new<a id="_idIndexMarker930"/> autoscaling option that is currently in pre-GA. As per this option, it is possible to configure autoscaling to horizontally scale based on CPU and vertically scale based on memory at the same time. MPA is supported for clusters that are 1.19.4-gke.1700 or later.</p>
			<p>This concludes the section on autoscaling in GKE where multiple mechanisms were detailed out. The next section focuses on networking constructs with respect to GKE. This will cover details about Pod networking, Service networking, and will deep dive into the usage of GKE load balancers to expose services for external consumption.</p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor192"/>Networking in GKE</h2>
			<p>Applications are <a id="_idIndexMarker931"/>deployed in Kubernetes <a id="_idIndexMarker932"/>as containers. Pods run containers. The desired state of the pods is controlled by Deployments and the applications are exposed for both internal and external networking through Services. The deployed pods run in GKE on nodes. Nodes in GKE are represented by virtual machines or VMs. These nodes are deployed in<a id="_idIndexMarker933"/> a <strong class="bold">Virtual Private Cloud</strong> (<strong class="bold">VPC</strong>).</p>
			<p>A VPC defines a virtual network topology that closely resembles a traditional network. It is a logically isolated network and provides connectivity between deployed resources. A VPC also provides complete control in terms of launching resources, selecting a range of RFC 1918 addressing, the creation of subnets, and so on. </p>
			<p>A VPC on GCP has a pre-allocated IP subnet, for every GCP region. When a GKE cluster is deployed within the VPC, a specific region or zone can be selected. Since GKE nodes are made up of Compute Engine VMs and these VMs need an IP address, the range of IP addresses is allocated from the IP subnet pre-allocated to the region. A VPC on GCP is considered a global resource since a single Google Cloud VPC can span multiple regions without communicating across the public internet. It is not required to have a connection in every region.</p>
			<p>GCP provides the option of configuring alias IP ranges. This allows VMs to have an additional secondary IP address. As a result, a VM can have multiple services running with a separate IP address. These secondary IP addresses are routable within the VPC without the need to configure additional routes.</p>
			<p>A GKE cluster might need to run cluster-wide services. GCP recommends deploying a GKE cluster as a <strong class="bold">VPC-native cluster</strong>. A <a id="_idIndexMarker934"/>VPC-native cluster uses three unique subnet IP address ranges:</p>
			<ul>
				<li>A primary IP address range of subnet for node IP addresses</li>
				<li>A secondary IP address range for all Pod IP addresses</li>
				<li>An additional secondary IP address range for all Service IP addresses</li>
			</ul>
			<p>GKE provides flexibility where the number of nodes in a cluster and the maximum number of pods per node are configurable. The next topic details how pods are assigned IP addresses when pods are deployed in a GKE cluster.</p>
			<h3>Pod networking</h3>
			<p>When a Pod <a id="_idIndexMarker935"/>is scheduled on a node, Kubernetes <a id="_idIndexMarker936"/>creates a network namespace for the Pod on the node's Linux kernel and connects the node's physical network interface to the Pod with a virtual network interface, thus allowing communication among pods within the same node.</p>
			<p>Kubernetes assigns an IP address (the Pod's IP) to the virtual network interface in the Pod's network namespace from a range of addresses reserved for Pods on the node. This address range is a subset of the IP address range assigned to the cluster for Pods, which can be configured when creating a cluster.</p>
			<p>GKE automatically configures VPC to recognize this range of IP addresses as an authorized secondary subnet of IP addresses. As a result, the pod's traffic is permitted to pass the anti-spoofing filters on the network. Also, because each node maintains a separate IP address base for its pods, the nodes don't need to perform network address translation on the pod's IP address. The next topic details Service networking, specifically, how services can effectively receive traffic from external sources via the use of GKE load balancers.</p>
			<h3>Service networking</h3>
			<p>A<a id="_idIndexMarker937"/> Service is a Kubernetes <a id="_idIndexMarker938"/>resource that creates a dynamic collection of IP addresses called endpoints. These <a id="_idIndexMarker939"/>IP addresses belong to the Pod that matches the Service label selector. Kubernetes creates a Service by assigning a static virtual IP address and this IP address is assigned from the pool of IP addresses reserved for services by the cluster. </p>
			<p>Out of the available<a id="_idIndexMarker940"/> Service types, the <strong class="source-inline">LoadBalancer</strong> Service type is implemented in GKE using GCP's <strong class="bold">Network Load Balancer</strong> (<strong class="bold">NLB</strong>). The NLB supports TCP and UDP traffic. GCP creates a network load balancer when a Service of type <strong class="source-inline">LoadBalancer</strong> is created within the GKE cluster. GCP will subsequently assign a static <strong class="source-inline">LoadBalancer</strong> IP address that is accessible from outside the cluster and the project.</p>
			<p>For traffic sent to the GCP NLB, <em class="italic">Figure 8.17</em> depicts the interactions between the NLB and the nodes within the GKE cluster. These interactions are listed as follows in a step-by-step manner:</p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B15587_08_17.jpg" alt="Figure 8.17 – Interactions between the NLB and a GKE cluster within a VPC&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.17 – Interactions between the NLB and a GKE cluster within a VPC</p>
			<p><strong class="bold">Step-by-step interactions</strong>:</p>
			<ol>
				<li value="1">NLB will pick a <a id="_idIndexMarker941"/>random node in the cluster and forwards the traffic (say <strong class="bold">Node 2</strong> as per <em class="italic">Figure 8.17</em>)</li>
				<li>The Service might be tied to multiple pods spread across the cluster nodes. The <strong class="source-inline">kube-proxy</strong> Service on the node receives the client request and will select a Pod matching the Service at random. The selected Pod can be on the same node or a different node.</li>
				<li>If the selected Pod is on a different node (say <strong class="bold">Pod 8</strong>), then the client request will be sent to the other node (<strong class="bold">Node 4</strong>) from the original node (<strong class="bold">Node 2</strong>). The response goes back to the original node (<strong class="bold">Node 2</strong>) that received the request and subsequently goes back to the client.</li>
			</ol>
			<p>The preceding process provides a way to access services from an external client and maintains an even balance with respect to Pod usage. However, there is a possibility that within the Kubernetes cluster, a response might have to go through multiple nodes as the request was directed from one node to the other, resulting in a <strong class="bold">double hop</strong>.</p>
			<p>To avoid <strong class="bold">double hop</strong>, Kubernetes natively provides an option called <strong class="source-inline">externalTrafficPolicy</strong>. If set to local, <strong class="source-inline">kube-proxy</strong> will pick a Pod on the local node (either <strong class="bold">Pod 3</strong> or <strong class="bold">Pod 4</strong>) and will not forward the client request to another node. However, this creates an imbalance and users must choose between better balance versus low-latency communication. GKE solves this by using the concept of container-native load balancing.</p>
			<h3>Container-native load balancing</h3>
			<p>The essence of <a id="_idIndexMarker942"/>container-native load <a id="_idIndexMarker943"/>balancing is that instead of directing traffic to nodes, traffic will be sent to pods directly, avoiding an additional hop. The connection is made directly between the load balancer and the pods. GKE accomplishes this by leveraging <strong class="bold">GCP HTTP(S) Load Balancing</strong> and<a id="_idIndexMarker944"/> the use of a data model called a <strong class="bold">Network Endpoint Group</strong> (<strong class="bold">NEG</strong>). GKE needs to <a id="_idIndexMarker945"/>run in VPC-native mode to use the container-native load balancing feature.</p>
			<p>A NEG is a set of network endpoints representing IP to port pairs. So instead of load balancing traffic using node IPs, the combination of Pod IPs and a port is used as a tuple. This information is maintained in the NEG. <em class="italic">Figure 8.18</em> depicts the interactions between GKE container-native load balancing and pods in GKE nodes through an NEG. As per <em class="italic">Figure 8.18</em>, a request to the container-native load balancer is forwarded to the NEG. The NEG then chooses the specific Pod based on the request, and directly forwards the traffic to the node associated with the Pod in a single hop, thus avoiding the <em class="italic">double hop</em>:</p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/B15587_08_18.jpg" alt="Figure 8.18 – Solving the double hop problem using container-native load balancing&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.18 – Solving the double hop problem using container-native load balancing</p>
			<p>Apart from establishing a direct connection to the Pod, container-native load balancing allows direct visibility of Pods, leading to the possibility of accurate health checks. The source IP address is preserved thus giving insights into the roundtrip time between the client and the load balancer.</p>
			<p>This concludes a high-level overview of networking constructs specific to GKE. The next section summarizes the storage options available for containerized applications deployed in GKE.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor193"/>Storage options for GKE</h2>
			<p>Kubernetes offers storage <a id="_idIndexMarker946"/>abstractions in the form of Volumes and Persistent Volumes. These are used as storage options providing file system capacity that is directly accessible by applications running in a Kubernetes cluster. Persistent Volumes exist beyond the life of a container and can further be used as durable file storage or as a database backing store.</p>
			<p>In GKE, Compute <a id="_idIndexMarker947"/>Engine persistent disks are used as persistent volumes. GKE also provides various managed backing stores such as Cloud SQL, Cloud Datastore, and so on, which removes the need to run a database as an application inside the GKE cluster, connecting applications in a GKE cluster to a managed datastore instead. For example, a frontend application in a GKE Cluster can be connected to Cloud SQL rather than the frontend application connecting to another application running a MySQL server. To be more specific, the frontend application can connect to Cloud SQL for database needs through a Cloud SQL proxy. This can be run inside the frontend application's Pod as a side-car container.</p>
			<p>This abstracts away infrastructure requirements and reduces maintenance, allowing you to focus on the application. GCP offers managed services across relational, non-relational, and caching services that applications running in a GKE cluster can connect to.</p>
			<p>In addition to applications that might require a backend data store, there could be applications running in a GKE cluster that might need object storage. <strong class="bold">Google Cloud Storage</strong> (<strong class="bold">GCS</strong>) is an object <a id="_idIndexMarker948"/>storage Service. Object-based storage refers to the storage of an ordered group of bytes where the structure and semantics of those bytes are not important. It can be used for a variety of applications, such as the following:</p>
			<ul>
				<li>Serving images for a website</li>
				<li>Streaming music, videos, and media hosting</li>
				<li>Constructing data lakes for analytics and machine learning workloads</li>
			</ul>
			<p>Applications within the GKE cluster can access Cloud Storage using Cloud Storage APIs. This concludes the summary of the storage options available in GCP for applications deployed in GKE. The next section summarizes details on cloud operations from a GKE perspective.</p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor194"/>Cloud Operations for GKE</h2>
			<p><strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>) provides native integration with <strong class="bold">Google's Cloud operations</strong> – a suite of tools that allows you to monitor workloads, collect application logs, capture metrics and provide alerting or notification options on key metrics. Cloud operations and the respective suite of services are elaborated on in detail as part of <a href="B15587_10_Final_ASB_ePub.xhtml#_idTextAnchor218"><em class="italic">Chapter 10</em></a>, <em class="italic">Exploring GCP Cloud Operations</em>.</p>
			<p>Cloud Operations for GKE is <a id="_idIndexMarker949"/>enabled by default at the time of cluster creation. However, it is possible to configure if the user chooses to disable Cloud Monitoring or Cloud Logging as part of the GKE<a id="_idIndexMarker950"/> cluster configuration. Cloud Operations for GKE monitors GKE clusters and provides a tailored, out-of-the-box dashboard that includes the following capabilities:</p>
			<ul>
				<li>Viewing cluster resources categorized by infrastructure, workloads, or services</li>
				<li>Inspecting namespaces, nodes, workloads, services, pods, and containers</li>
				<li>Viewing application logs for pods and containers</li>
				<li>Viewing key metrics related to clusters, such as CPU utilization, memory utilization, and so on</li>
			</ul>
			<p>Logging and monitoring are two critical aspects of reliably running a Service or application in a GKE cluster. These will be covered as part of upcoming topics from the aspect of Cloud Operations for GKE.</p>
			<h3>Logging for GKE</h3>
			<p>GKE deploys<a id="_idIndexMarker951"/> applications and orchestrates multiple actions or events within a cluster. This results in a variety of logs such as application logs, system logs, event logs, and so on. Logging provides visibility of various actions that happen and is also considered a passive form of monitoring.</p>
			<p>There are two options to view logs for a GKE cluster:</p>
			<ul>
				<li>Kubernetes Native Logging</li>
				<li>GKE Cloud Logging</li>
			</ul>
			<h3>Kubernetes Native Logging</h3>
			<p>Kubernetes supports <a id="_idIndexMarker952"/>native logging to standard output and standard error. In Kubernetes, the <em class="italic">container engine</em> can be <a id="_idIndexMarker953"/>used to redirect stdin/out and standard error streams from the containers to a logging driver. This driver is configured to write these container logs in JSON format and store them in the <strong class="source-inline">/var/log</strong> directory at the node level. This includes logs from containers and logs from node control plane components such as <strong class="source-inline">kubelet</strong> and <strong class="source-inline">kube-proxy</strong>. These logs can be retrieved using the <strong class="source-inline">kubectl logs</strong> command.</p>
			<p>The <strong class="source-inline">kubectl logs</strong> command can be used to retrieve logs for a Pod or a specific container within a Pod. The command also provides options to retrieve logs for a specific period or you can retrieve a portion of logs using the <strong class="source-inline">tail</strong> option. A few of such examples are provided as follows:</p>
			<p class="source-code"># Stdout container logs; pod has a single container</p>
			<p class="source-code">kubectl logs &lt;pod-name&gt;</p>
			<p class="source-code"># Stdout container logs; pod has multiple containers</p>
			<p class="source-code">kubectl logs &lt;pod-name&gt; -c &lt;container-name&gt;</p>
			<p class="source-code"># Stdout container logs – most recent 50 lines</p>
			<p class="source-code">kubectl logs --tail=50 &lt;pod-name&gt;</p>
			<p class="source-code"># Stdout most recent container logs in the last 1 hour</p>
			<p class="source-code">kubectl logs --since=1h &lt;pod-name&gt;</p>
			<p class="source-code"># Stream pod logs</p>
			<p class="source-code">kubectl logs -f &lt;pod-name&gt;</p>
			<p>Kubernetes native logging can lead to node saturation as the log files continue to grow in the node's storage directory. GKE solves this to an extent by running the Linux log rotate utility to clean up the log files. Any log files older than a day or more than 100 MB will be automatically compressed and copied into an archive file.</p>
			<p>GKE only stores the five most recently archived log files on the nodes and will delete the previous archived log files. Though this ensures that the node doesn't saturate in terms of disk space, it still poses a problem if older application logs need to be analyzed or researched.</p>
			<p>By default, open source Kubernetes or K8s will delete logs related to a container either when a container is deleted or when a Pod tied to the container is deleted. GKE resolves problems related to node saturation and provides the ability to analyze logs related to deleted pods/containers by streaming the logs to Cloud Logging, as part of Cloud Operations. Application logs, system logs, and log events can be streamed to Cloud Logging, which will be discussed as part of upcoming topics.</p>
			<h3>GKE Cloud Logging</h3>
			<p>Open source Kubernetes or K8s<a id="_idIndexMarker954"/> doesn't provide a log storage solution for cluster-level logging. GKE handles this by <a id="_idIndexMarker955"/>streaming log events to Cloud Logging. <strong class="bold">Cloud Logging</strong> is a centralized log management utility and a fully managed Service. Cloud Logging<a id="_idIndexMarker956"/> can automatically scale and can ingest terabytes of log data per second. </p>
			<p>GKE streams to Cloud Logging by using <strong class="source-inline">FluentD</strong> logging agents. A <strong class="source-inline">FluentD</strong> agent is implemented as a DaemonSet because it needs to run on every node in the cluster.</p>
			<p>Logging agents are pre-installed on each node as a DaemonSet and are pre-configured to push log data to Cloud Logging. <strong class="source-inline">FluentD</strong> collects container logs and system logs from the node. FluentD aggregates the logs, appends additional metadata, and pushes them to Cloud Logging.</p>
			<p><em class="italic">Figure 8.19</em> illustrates the interactions of logs being sent from GKE to Cloud Logging using the <strong class="source-inline">FluentD</strong> DaemonSet Pod on each node in the cluster:</p>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/B15587_08_19.jpg" alt="Figure 8.19 – FluentD agent capturing logs and sending to Cloud Logging&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.19 – FluentD agent capturing logs and sending to Cloud Logging</p>
			<p><strong class="bold">Event logs</strong> are <a id="_idIndexMarker957"/>also streamed to Cloud Logging. Event logs refers to logs from operations that take place on the cluster such as the creation/deletion of a Pod, scaling of deployments, and so on. Events are stored as API objects on the Kubernetes master or control plane. GKE uses an event exporter in the cluster master to capture the events and automatically pushes them to Cloud Logging.</p>
			<p>Cloud Logging provides the ability to capture metrics from streaming logs and create alerting policies<a id="_idIndexMarker958"/> as needed. Cluster actions such as autoscaling can be configured based on custom metrics. By default, GKE-specific logs related to a cluster are available in Cloud Logging for 30 days. For longer retention, Cloud Logging offers options to export logs to Cloud Storage or Big Query using the concept of log sinks. <a href="B15587_10_Final_ASB_ePub.xhtml#_idTextAnchor218"><em class="italic">Chapter 10</em></a>, <em class="italic">Exploring GCP Cloud Operations</em>, elaborates on topics related to Cloud Logging in depth.</p>
			<h3>Monitoring for GKE</h3>
			<p>Monitoring provides <a id="_idIndexMarker959"/>insights into how an application or Service functions based on key internal metrics related to a GKE cluster. In addition, monitoring also provides insights from a user's perspective based on the user's interaction with the Service. The previous chapters on site reliability engineering (<a href="B15587_01_Final_ASB_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">DevOps, SRE, and Google Cloud Services for CI/CD</em>, to <a href="B15587_04_Final_ASB_ePub.xhtml#_idTextAnchor087"><em class="italic">Chapter 4</em></a>, <em class="italic">Building SRE Teams and Applying Cultural Practices</em>), clearly call out Service reliability as one of the key aspects. Monitoring is the fundamental input to ensure that a Service runs reliably.</p>
			<p>Monitoring provides data that is critical to make decisions about applications. This data can be used further to resolve an ongoing incident and perform a blameless postmortem, and you can use it further to improve an existing test suite and provide inputs to the product and development team for any further improvements or fine-tuning.</p>
			<p><strong class="bold">Cloud Monitoring</strong> is Google's <a id="_idIndexMarker960"/>managed solution that provides a solution to monitor the state of services using key parameters such as latency, throughput, and so on, and identify performance bottlenecks. From a GKE perspective, monitoring can be divided into two domains:</p>
			<ul>
				<li><strong class="bold">Cluster-Level Monitoring</strong>: This<a id="_idIndexMarker961"/> includes monitoring cluster-level components such as nodes and components from the master control plane such as <strong class="source-inline">kube-apiserver</strong>, <strong class="source-inline">etcd</strong>, and other infrastructure elements.</li>
				<li><strong class="bold">Pod-Level Monitoring</strong>: This<a id="_idIndexMarker962"/> includes monitoring resources using container-specific metrics, tracking deployment-specific system metrics, tracking instances, monitoring uptime checks, and monitoring application-specific metrics designed by the application's developer(s).</li>
			</ul>
			<p>Kubernetes uses the concept of labels to group or track resources. The same concept can be extended, and resources can be filtered in Cloud Monitoring using labels. Cloud Monitoring provides ways to track all relevant metrics and put them on a customized dashboard, thus giving visibility of a GKE cluster. <em class="italic">Figure 8.20</em> shows the built-in <strong class="bold">GKE Dashboard</strong> from <a id="_idIndexMarker963"/>Cloud Monitoring (with options displayed in collapsed mode). The GKE dashboard summarizes information about clusters, namespaces, nodes, workloads, Kubernetes services, Pods, and Containers:</p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/B15587_08_20.jpg" alt="Figure 8.20 – Built-in GKE Dashboard from Cloud Monitoring&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.20 – Built-in GKE Dashboard from Cloud Monitoring</p>
			<p>This completes the topic on Cloud Operations for GKE and concludes the section on GKE where many key concepts and core features were discussed in detail. The next section elaborates on the latest operation mode in GKE, called <strong class="bold">Autopilot</strong>.</p>
			<h1 id="_idParaDest-195"><a id="_idTextAnchor195"/>GKE Autopilot – hands-on lab</h1>
			<p><strong class="bold">GKE Autopilot or Autopilot</strong> is<a id="_idIndexMarker964"/> one of the two modes of operation supported by GKE. The other mode being the standard mode (which was elaborated on at the start of this chapter). Autopilot removes the need to perform <strong class="bold">do-it-yourself</strong> (<strong class="bold">DIY</strong>) actions during cluster creation and instead creates a cluster with the industry-standard recommendations regarding networking and security. In addition, Autopilot removes the need to configure node pools or estimate the size of the cluster upfront. Nodes are automatically provisioned based on the types of deployed workloads and the user is essentially charged for the running workloads.</p>
			<p>Autopilot is not only managed but is also a serverless K8s offering from GKE. Autopilot, however, does not offer all cluster configuration choices offered by the standard mode. The following table represents the configuration choices offered by Autopilot in comparison to the standard mode:</p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/Table_01.jpg" alt=""/>
				</div>
			</div>
			<p>The following is a step-by-step guide to creating a<a id="_idIndexMarker965"/> GKE cluster in Autopilot mode:</p>
			<ol>
				<li value="1">Navigate to the GCP<a id="_idIndexMarker966"/><a id="_idIndexMarker967"/> Console and select the compute Service – <strong class="bold">Kubernetes Engine</strong>.</li>
				<li>Select the option to create a cluster and choose <strong class="bold">Autopilot</strong> mode. Refer to <em class="italic">Figure 8.21</em>:<p class="figure-caption"> </p><div id="_idContainer119" class="IMG---Figure"><img src="image/B15587_08_21.jpg" alt="Figure 8.21 – Select Autopilot mode during cluster creation&#13;&#10;"/></div><p class="figure-caption">Figure 8.21 – Select Autopilot mode during cluster creation</p></li>
				<li>Enter the name for the cluster as <strong class="source-inline">my-autopilot-cluster</strong>. Leave the default selections for the <a id="_idIndexMarker968"/>rest of the <a id="_idIndexMarker969"/>options and select the <strong class="bold">CREATE</strong> action. Refer to <em class="italic">Figure 8.22</em>:<div id="_idContainer120" class="IMG---Figure"><img src="image/B15587_08_22.jpg" alt="Figure 8.22 – Creating a cluster in Autopilot mode&#13;&#10;"/></div><p class="figure-caption">Figure 8.22 – Creating a cluster in Autopilot mode</p></li>
				<li>This will <a id="_idIndexMarker970"/>initiate the cluster creation process but in Autopilot mode. Once the cluster is created, the cluster will be listed on the<a id="_idIndexMarker971"/> cluster list page as shown in <em class="italic">Figure 8.23</em>:</li>
			</ol>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B15587_08_23.jpg" alt="Figure 8.23 – New cluster created in Autopilot mode&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.23 – New cluster created in Autopilot mode</p>
			<p>Here are some observations from the newly created Autopilot cluster. These observations differentiate the <a id="_idIndexMarker972"/>Autopilot cluster from a <a id="_idIndexMarker973"/>Standard mode cluster:</p>
			<ul>
				<li>An autopilot cluster is created without pre-assigning any nodes upfront.</li>
				<li>An autopilot cluster is always created as a regional cluster.</li>
				<li>The release channel for an autopilot cluster is the <em class="italic">Regular channel</em>.</li>
				<li>Node auto-provisioning and vertical Pod autoscaling are enabled by default.</li>
				<li>Advanced networking options such as intranode visibility, NodeLocal DNSCache, and HTTP load balancing are enabled by default.</li>
				<li>Security options such as Workload Identity and shielded GKE nodes are enabled by default. These security options are discussed in <a href="B15587_09_Final_ASB_ePub.xhtml#_idTextAnchor201"><em class="italic">Chapter 9</em></a>, <em class="italic">Securing the Cluster Using GKE Security Constructs</em>.</li>
			</ul>
			<p>Once a cluster is created in Autopilot mode, workloads can be deployed to the Autopilot cluster in the exact same way that workloads were previously deployed to a cluster in Standard mode. <em class="italic">Figure 8.24</em> refers to a Deployment created on the Autopilot cluster:</p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/B15587_08_24.jpg" alt="Figure 8.24 – Deployment details in an Autopilot cluster&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.24 – Deployment details in an Autopilot cluster</p>
			<p>The resources<a id="_idIndexMarker974"/> required to run the workloads are allocated to the Autopilot cluster. <em class="italic">Figure 8.25</em> displays the cluster list page with resources allocated to <strong class="source-inline">my-autopilot-cluster</strong>. In this specific case, 0.5 vCPUs and 2 GB memory are allocated to run a single Pod. So, the user is only charged for this workload:</p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="image/B15587_08_25.jpg" alt="Figure 8.25 – Resource allocation for the Autopilot cluster after deploying a workload&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.25 – Resource allocation for the Autopilot cluster after deploying a workload</p>
			<p>This completes the hands-on lab related to GKE Autopilot. This lab provides insights into the Autopilot configuration and how resources are allocated to the cluster after the deployment of workloads. This also brings us to the end of the chapter.</p>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor196"/>Summary</h1>
			<p>Given that open source Kubernetes or K8s involves a lot of setup and upkeep, we deep-dived into Google Kubernetes Engine or GKE, a GCP compute Service that runs containerized applications. The Kubernetes concepts learned in <a href="B15587_07_Final_ASB_ePub.xhtml#_idTextAnchor154"><em class="italic">Chapter 7</em></a>, <em class="italic">Understanding Kubernetes Essentials to Deploy Containerized Applications</em>, apply to GKE. We additionally explored GKE core features such as GKE node pools, GKE cluster configurations, autoscaling, and GKE's ability to integrate with other GCP services across networking and operations. The next chapter focuses on security-specific features related to the Google Kubernetes Engine, with the goal of hardening a cluster's security.</p>
			<h1 id="_idParaDest-197"><a id="_idTextAnchor197"/>Points to remember</h1>
			<p>The following are some important points to remember:</p>
			<ul>
				<li>GKE is fully managed, uses a container-optimized OS, and supports autoscaling, the auto-repair of nodes, and auto-upgrades.</li>
				<li>GKE supports two modes of operations – Standard and Autopilot.</li>
				<li>GKE Standard mode supports VPC-native traffic routing and HTTP load balancing as default options.</li>
				<li>Cloud operations for GKE are enabled as a default setting.</li>
				<li>A private Kubernetes engine cluster cannot be accessed publicly.</li>
				<li>A node pool represents a group of nodes with the same configuration.</li>
				<li>By default, a new node pool runs the latest Kubernetes version and can be configured for auto-upgrade or can be manually upgraded.</li>
				<li>Node pools in a regional or multi-zonal cluster are replicated to multiple zones.</li>
				<li>A multi-zonal cluster will only have a single replica of the control plane.</li>
				<li>A regional cluster has multiple replicas of the control plane running across multiple zones in a region.</li>
				<li>A release channel is used to fix known issues or add new features or address any security risks or concerns.</li>
				<li>GKE creates a cluster with the default version if a specific version or release channel is not specified.</li>
				<li>Alpha features are only available in special GKE alpha clusters and are not available as part of release channels.</li>
				<li>Options to autoscale in GKE include the cluster autoscaler, HPA, VPA, and MPA (pre-GA).</li>
				<li>The cluster autoscaler automatically resizes a node pool in a GKE cluster.</li>
				<li>The HPA indicates when application instances should be scaled based on the current utilization.</li>
				<li>The HPA is not supported for DaemonSets.</li>
				<li>The VPA suggests recommended resources for a Pod based on the current utilization.</li>
				<li>The VPA can automatically update workloads if the <strong class="source-inline">updateMode</strong> attribute is set to <em class="italic">On</em>.</li>
				<li>MPA allows you to horizontally scale based on CPU and vertically scale based on memory at the same time. This is a pre-GA feature.</li>
				<li>Autoscaler provides two profile options to scale down: balanced and optimize-utilization. </li>
				<li>Kubernetes' native option to avoid double hop is to set <strong class="source-inline">externalTrafficPolicy</strong> to <strong class="source-inline">local</strong>.</li>
				<li>GKE avoids double hop using GCP HTTP(S) Load Balancer and an NEG.</li>
				<li>An NEG is a set of network endpoints representing IP to port pairs.</li>
				<li>GKE runs Linux's log rotate utility to clean up log files. Any log files older than a day or more than 100 MB will be automatically compressed and copied into an archive file.</li>
				<li>GKE only stores the five most recently archived log files on the nodes and will delete the previously archived log files.</li>
				<li>GKE streams to Cloud Logging by using FluentD logging agents.</li>
				<li>Event logs refers to logs from operations that take place on a cluster.</li>
				<li>Events are stored as API objects on the cluster master. GKE uses an event exporter to push events to Cloud Logging.</li>
				<li>GKE cluster-specific logs are available in Cloud Logging for 30 days.</li>
				<li>For longer retention, Cloud Logging can export logs using log sinks.</li>
				<li>GKE Autopilot mode supports cluster configurations where the availability type is <em class="italic">Regional</em>, the version is <em class="italic">Release Channel</em>, network isolation is <em class="italic">Private</em> or <em class="italic">Public</em>, and Kubernetes features are <em class="italic">Production</em>.</li>
			</ul>
			<h1 id="_idParaDest-198"><a id="_idTextAnchor198"/>Further reading</h1>
			<p>For more information on GCP's approach to DevOps, read the following articles:</p>
			<ul>
				<li><strong class="bold">Kubernetes</strong>: <a href="https://kubernetes.io/docs/home/ ">https://kubernetes.io/docs/home/</a></li>
				<li><strong class="bold">Google Kubernetes Engine</strong>: <a href="https://cloud.google.com/kubernetes-engine ">https://cloud.google.com/kubernetes-engine</a></li>
			</ul>
			<h1 id="_idParaDest-199"><a id="_idTextAnchor199"/>Practice test</h1>
			<p>Answer the following questions:</p>
			<ol>
				<li value="1">How do you create control plane components in GKE?<p>a) Create worker nodes and then create control plane components on the worker nodes.</p><p>b) A GKE cluster does not mandate the creation of control plane components.</p><p>c) Create control plane components on a node group called <strong class="source-inline">master</strong> and the worker nodes are placed in a node group called <strong class="source-inline">worker</strong>.</p><p>d) The control plane components are automatically created and managed by GKE on behalf of the user.</p></li>
				<li>Pod <strong class="source-inline">p1</strong> has three containers – <strong class="source-inline">c1</strong>, <strong class="source-inline">c2</strong>, and <strong class="source-inline">c3</strong>. The user wants to view the logs of container <strong class="source-inline">c2</strong>. Select the option that represents the appropriate CLI command to view the logs:<p>a) <strong class="source-inline">kubectl logs -p p1 -c c2</strong></p><p>b) <strong class="source-inline">kubectl logs p1 -c c2</strong></p><p>c) <strong class="source-inline">kubectl logs pod=p1 container=c2</strong></p><p>d) <strong class="source-inline">kubectl logs p1 container=c2</strong></p></li>
				<li>The company <em class="italic">Alpha</em> is about to launch a stateless web application to offer a new e-commerce Service. The web application will have steady traffic with occasional peaks, especially when special offers are announced for customers. Select the option that depicts an appropriate cluster design in this case:<p>a) Deploy a standard cluster and use a Deployment with the HPA.</p><p>b) Deploy a cluster with autoscaling and use a Deployment with the HPA.</p><p>c) Deploy a standard cluster and use a Deployment with the VPA.</p><p>d) Deploy a cluster with autoscaling and use a Deployment with the VPA.</p></li>
				<li>Choose the cluster configuration that could withstand it if there was a loss of a GCP zone:<p>a) Create a regional cluster.</p><p>b) Create a Redis cluster that can cache the resource information of the zone where cluster resources are hosted.</p><p>c) Create two clusters in separate zones and create a load balancer between them.</p><p>d) None of the above.</p></li>
				<li>Select the Google Cloud Service where private GKE clusters can use Docker images from?<p>a) Cloud Source Repositories</p><p>b) Container Registry</p><p>c) Cloud Build</p><p>d) All of the above</p></li>
				<li>Select the allowed maximum clusters per zone:<p>a) 25</p><p>b) 50</p><p>c) 100</p><p>d) Unlimited</p></li>
				<li>Select the command to get authentication credentials to interact with a cluster named <strong class="source-inline">my-cluster</strong>:<p>a) <strong class="source-inline">gcloud containers clusters get-credentials my-cluster</strong></p><p>b) <strong class="source-inline">gcloud container clusters get-credentials my-cluster</strong></p><p>c) <strong class="source-inline">gcloud container cluster get-credentials my-cluster</strong></p><p>d) <strong class="source-inline">gcloud containers cluster get-credentials my-cluster</strong></p></li>
				<li>Select the command that can retrieve pods in a cluster:<p>a) <strong class="source-inline">gcloud get pods</strong></p><p>b) <strong class="source-inline">kubectl list pods</strong></p><p>c) <strong class="source-inline">gcloud list pods</strong></p><p>d) <strong class="source-inline">kubectl get pods</strong></p></li>
				<li>The company <em class="italic">Real World</em> decides to use a third-party monitoring solution to monitor an application deployed in a GKE cluster. Select the best approach to deploy the third-party monitoring solution:<p>a) It is not possible to use a third-party monitoring solution in GKE.</p><p>b) Download the monitoring solution for Cloud Marketplace.</p><p>c) Deploy the monitoring solution in a Pod as a DaemonSet.</p><p>d) Deploy the monitoring solution in a Pod as a ReplicaSet.</p></li>
				<li>A VPC on Google Cloud is a: <p>a) Zonal resource</p><p>b) Global resource</p><p>c) Regional resource</p><p>d) Multi-Regional resource</p></li>
				<li>An application called <em class="italic">my-app</em> in GKE needs access to a managed MySQL database. Select the most appropriate option:<p>a) Run MySQL as an application in the cluster. The <em class="italic">my-app</em> application will connect with the MySQL application through the ClusterIP Service.</p><p>b) Use Cloud SQL to run MySQL database. Run the Cloud SQL proxy as a side-car container insider the application's Pod.</p><p>c) Run MySQL as an application in the cluster. The <em class="italic">my-app</em> application will connect with the MySQL application through the <strong class="source-inline">LoadBalancer</strong> Service.</p><p>d) Use Cloud SQL for running MySQL Database. Run the Cloud SQL proxy as a ClusterIP Service.</p></li>
				<li>Google Network Load Balancing distributes the following traffic: <p>a) TCP</p><p>b) UDP</p><p>c) TCP or UDP</p><p>d) None of the above</p></li>
				<li>From an availability-type point of view, a cluster created in <em class="italic">Autopilot</em> mode is: <p>a) Zonal</p><p>b) Multi-zonal</p><p>c) Regional</p><p>d) Zonal and regional</p></li>
				<li>Select the option that is not a supported release channel in GKE: <p>a) Regular</p><p>b) Alpha</p><p>c) Rapid</p><p>d) Stable</p></li>
				<li>Select the possible cluster configurations based on network isolation: <p>a) Standard and Private</p><p>b) Standard and Public</p><p>c) Standard and Default</p><p>d) Private and Public</p></li>
			</ol>
			<h1 id="_idParaDest-200"><a id="_idTextAnchor200"/>Answers</h1>
			<ol>
				<li value="1">(d) – The control plane components such as the <strong class="source-inline">kube-api</strong> server, scheduler, and so on form the cluster master and are set up and managed by GKE.</li>
				<li>(b) – <strong class="source-inline">kubectl logs p1 -c c2</strong></li>
				<li>(b) – Deploy a cluster with autoscaling and use Deployment with HPA.</li>
				<li>(a) – Create a regional cluster as the workload is spread across multiple zones in one region.</li>
				<li>(b) – Container Registry</li>
				<li>(b) – 50</li>
				<li>(b) - <strong class="source-inline">gcloud container clusters get-credentials my-cluster</strong></li>
				<li>(d) - <strong class="source-inline">kubectl get pods</strong></li>
				<li>(c) - Deploy the monitoring solution in a Pod as a DaemonSet.</li>
				<li>(b) – Global resource</li>
				<li>(c)</li>
				<li>(c) – TCP or UDP</li>
				<li>(c) – Regional</li>
				<li>(b) – Alpha</li>
				<li>(d) – Private and Public</li>
			</ol>
		</div>
	</body></html>