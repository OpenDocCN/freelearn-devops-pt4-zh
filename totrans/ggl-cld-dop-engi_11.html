<html><head></head><body>
		<div id="_idContainer126">
			<h1 id="_idParaDest-201"><em class="italic"><a id="_idTextAnchor201"/>Chapter 9</em>: Securing the Cluster Using GKE Security Constructs</h1>
			<p>Kubernetes, or K8s, is an open source container orchestration system that runs containerized applications but requires significant effort to set up and maintain. <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>) is an enhanced version of K8s that is managed in nature, abstracts the master plane components from the user, provides the ability to auto-upgrade, and supports features such as DNS, logging, and monitoring dashboards as built-ins rather than maintaining them as external plugins. Kubernetes has a lot of critical concepts, jargon, and objects. The last two chapters (<a href="B15587_07_Final_ASB_ePub.xhtml#_idTextAnchor154"><em class="italic">Chapter 7</em></a>, <em class="italic">Understanding Kubernetes Essentials to Deploy Containerized Applications</em>, and <a href="B15587_08_Final_ASB_TD_ePub.xhtml#_idTextAnchor182"><em class="italic">Chapter 8</em></a>, <em class="italic">Understanding GKE Essentials to Deploy Containerized Applications</em>) focused on native Kubernetes features such as cluster anatomy, elaborated on key Kubernetes objects, and discussed how applications are scheduled on a cluster. In addition, the focus was extended to learning about specific GKE features such as node pools, cluster configurations, options to auto scale workloads, and understand how GKE interacts with other GCP services.</p>
			<p>This chapter specifically focuses on understanding the basic security constructs in Kubernetes, their application, and then specific GKE security features that are fundamental to hardening a cluster's security. The key here is to secure the applications running inside the cluster using GKE-specific features.</p>
			<p>This chapter will cover the following topics:</p>
			<ul>
				<li><strong class="bold">Essential security patterns in Kubernetes</strong>: This section deep dives into fundamental security constructs in native Kubernetes, such as authentication, authorization, securing the control plane, and Pod security. We will also look at each of the security constructs with respect to their GKE implementations.</li>
				<li><strong class="bold">Hardening cluster security</strong>: This section deep dives into GKE-specific security features that provide options for securing applications running inside the GKE cluster. This includes features such as private cluster, binary authorization, container-optimized OS, and more.</li>
			</ul>
			<h1 id="_idParaDest-202"><a id="_idTextAnchor202"/>Technical requirements</h1>
			<p>There are two main technical requirements for this chapter:</p>
			<ul>
				<li>A valid <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>) account to go hands-on with GCP services: <a href="https://cloud.google.com/free">https://cloud.google.com/free</a></li>
				<li>Install Google Cloud SDK: <a href="https://cloud.google.com/sdk/docs/quickstart">https://cloud.google.com/sdk/docs/quickstart</a></li>
			</ul>
			<h1 id="_idParaDest-203"><a id="_idTextAnchor203"/>Essential security patterns in Kubernetes</h1>
			<p>A Kubernetes cluster can run multiple types of workloads. This includes stateful applications, stateless applications, jobs, and<a id="_idIndexMarker975"/><a id="_idIndexMarker976"/> DaemonSets. However, it is critical to secure these workloads from potential security attacks. Native Kubernetes provides some essential security constructs that focus on the fundamentals, including a request being sent to the cluster and how the<a id="_idIndexMarker977"/><a id="_idIndexMarker978"/> request is authenticated and authorized. Additionally, it is important to understand how the master plane components are secured and how the pods running the applications can also be secured. We will cover these from a native Kubernetes standpoint, but their implementation in GKE will also be discussed. The first such security construct we will deep dive into is authentication.</p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor204"/>Authentication</h2>
			<p><strong class="bold">Authentication</strong> is the process of <a id="_idIndexMarker979"/><a id="_idIndexMarker980"/>determining the<a id="_idIndexMarker981"/><a id="_idIndexMarker982"/> identity of the user. It essentially confirms that the user is who they say they are and eventually provides access to eligible resources once authentication is successful. </p>
			<p>Kubernetes supports two categories of authentication or user:</p>
			<ul>
				<li>User accounts</li>
				<li>Kubernetes service accounts</li>
			</ul>
			<p>Let's look at these in more detail.</p>
			<h3>User accounts</h3>
			<p>By default, Kubernetes does not <a id="_idIndexMarker983"/><a id="_idIndexMarker984"/>have any objects that can support normal user accounts. Hence, these can never be created through an API call. Normal or regular users in Kubernetes are created in any of the following ways:</p>
			<ul>
				<li>By an admin distributing private keys</li>
				<li>With a file that contains a list of usernames and their associated passwords</li>
				<li>Through external identity service providers</li>
			</ul>
			<p>In the case of GKE, normal <strong class="bold">user accounts</strong> can be provisioned by Cloud IAM users. These user accounts are referred to as members. Members can also be defined as part of a G Suite domain or a Cloud Identity domain. It is also possible to add members or users to Cloud IAM by linking to an existing active directory through Google Cloud Directory Sync. In addition to Cloud IAM users, GCP service accounts are also considered members, like users. These are different from Kubernetes service accounts. </p>
			<p><strong class="bold">GCP service accounts</strong> are managed by<a id="_idIndexMarker985"/><a id="_idIndexMarker986"/> Google Cloud IAM and specifically used if GCP resources need to have identities that are tied to an application or a virtual machine, instead of a human being. In contrast, Kubernetes service accounts provide an identity to a process running inside a pod and provides access to the cluster.</p>
			<h3>Kubernetes service accounts</h3>
			<p><strong class="bold">Kubernetes service accounts</strong> are users that are managed by the Kubernetes API. This means that unlike regular user accounts, the<a id="_idIndexMarker987"/><a id="_idIndexMarker988"/> service accounts can be created and managed through API calls. In fact, every namespace in Kubernetes has a default Kubernetes service account. These are automatically created<a id="_idIndexMarker989"/><a id="_idIndexMarker990"/> by the API server. The <em class="italic">service account admission controller</em> associates the created service accounts with the running pods. In fact, service accounts are stored as secrets and are mounted onto pods when they're created. These secrets are used by processes running inside the pod for in-cluster access to the API server. </p>
			<p>In addition, Kubernetes service accounts are used to create identities for long-running jobs where there is a need to talk to the Kubernetes API, such as running a Jenkins server. You can use the following CLI <a id="_idIndexMarker991"/><a id="_idIndexMarker992"/>command to create a Kubernetes service account:</p>
			<p class="source-code"># Create kubernetes service account</p>
			<p class="source-code">kubectl create serviceaccount jenkins </p>
			<p>The preceding <a id="_idIndexMarker993"/><a id="_idIndexMarker994"/>command creates a <strong class="source-inline">serviceaccount</strong> object, generates a token for the service account, and creates a secret object to store the token. The secret bearing the token can be retrieved using the following CLI command:</p>
			<p class="source-code"># Get the definition of the service account</p>
			<p class="source-code">kubectl get serviceaccounts jenkins -o yaml</p>
			<p>The preceding command will result in the following output:</p>
			<p class="source-code">apiVersion: v1</p>
			<p class="source-code">kind: ServiceAccount</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  # ...</p>
			<p class="source-code">secrets:</p>
			<p class="source-code">- name: jenkins-token-78abcd</p>
			<p>The secret that's displayed under the secrets section will contain the public <strong class="bold">Certificate Authority</strong> (<strong class="bold">CA</strong> – an entity that issues digital <a id="_idIndexMarker995"/><a id="_idIndexMarker996"/>certificates) of the API server, the specific namespace, and a signed <strong class="bold">JSON Web Token</strong> (<strong class="bold">JWT</strong>). The signed JWT can be used as the bearer account to authenticate the provided service account. This service account can eventually be used either <a id="_idIndexMarker997"/><a id="_idIndexMarker998"/>for in-cluster communication or even to authenticate from outside the cluster, as in the case of a Jenkins server.</p>
			<p>Every request to Kubernetes needs to be authenticated before it can serve requests. The incoming request is handled by the <strong class="source-inline">kube-api</strong> server by listening on port <strong class="source-inline">443</strong> using HTTPS. Authentication can be done in various ways. GKE supports the following authentication methods:</p>
			<ul>
				<li>OpenID Connect tokens </li>
				<li>x509 client certs</li>
				<li>Basic authentication using static passwords</li>
			</ul>
			<p><strong class="bold">OpenID Connect</strong> is a layer on top of the OAuth 2.0 protocol and allows clients to verify the identity of an end user by querying the authorization server. <strong class="bold">x509 client certificates and static passwords</strong> present a wider <a id="_idIndexMarker999"/><a id="_idIndexMarker1000"/>surface of attack than OpenID. In GKE, both x509 and static password authentication is disabled by default, specifically in clusters created with Kubernetes 1.12 and later. This helps improve the default security posture as the area of impact in the event of an <a id="_idIndexMarker1001"/><a id="_idIndexMarker1002"/>attack is significantly reduced or lowered.</p>
			<p>This completes this topic on authentication in Kubernetes. The next topic will cover authorization in Kubernetes.</p>
			<h2 id="_idParaDest-205"><a id="_idTextAnchor205"/>Authorization</h2>
			<p><strong class="bold">Authorization</strong> is the <a id="_idIndexMarker1003"/><a id="_idIndexMarker1004"/>process of determining whether a user has permission to access a specific resource or perform a specific function. In Kubernetes, a user must be authenticated or logged in and authorized to access or use specific resources. It's generally recommended to enforce the principle of least privilege as a security best practice, as this ensures that a user only has the required level of access to the resource based on the access requirements.</p>
			<p>Specific to GKE, a user authenticating via Cloud Identity can be authorized using two approaches. In fact, GKE recommends using both approaches to authorize access to a specific resource:</p>
			<ul>
				<li>Cloud <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>)</li>
				<li>Kubernetes <strong class="bold">Role-Based Access Control</strong> (<strong class="bold">RBAC</strong>)</li>
			</ul>
			<p><strong class="bold">Cloud IAM</strong> is the access control <a id="_idIndexMarker1005"/><a id="_idIndexMarker1006"/>system for managing GCP resources. Google Account, service account, and Google Group are entities that have an identity in Cloud IAM. Cloud IAM allows users to perform operations at the project level (such as listing all GKE clusters in the project) or at the cluster level (such as viewing the cluster) but specifically outside the cluster. This includes adding specific GKE security configuration options to an existing cluster or even to a new <a id="_idIndexMarker1007"/><a id="_idIndexMarker1008"/>cluster. However, <strong class="bold">Kubernetes RBAC</strong> provides access to inside the cluster, even specifically at the namespace level. RBAC allows you to fine-tune rules to provide granular access to resources within the cluster.</p>
			<p>To summarize, Cloud IAM defines who can view or change the configuration of a GKE cluster, while Kubernetes RBAC defines who can view or change Kubernetes objects inside the specific GKE cluster. GKE integrates Cloud IAM and Kubernetes RBAC to authorize users to perform actions on resources if they have the required permissions. Now, let's look at both authorization methods, starting with GKE authorization via Cloud IAM.</p>
			<p>GKE authorization via Cloud IAM</p>
			<p>There are three main elements that <a id="_idIndexMarker1009"/><a id="_idIndexMarker1010"/>comprise Cloud IAM access controls. They are as follows:</p>
			<ul>
				<li><strong class="bold">Who</strong>: This refers to<a id="_idIndexMarker1011"/><a id="_idIndexMarker1012"/> authentication; specifically, the identity of the member making the request.</li>
				<li><strong class="bold">What</strong>: This refers to authorization; specifically, the set of permissions that are required to authorize the request. Permissions cannot be directly assigned to members; instead, a set of permissions comprises a role that is assigned to members.</li>
				<li><strong class="bold">Which</strong>: This refers to the resources that the request is authenticated and authorized to access. In the case of GKE, this refers to GKE resources such as the clusters or objects inside the cluster.</li>
			</ul>
			<p>GKE provides several predefined Cloud IAM roles that provide granular access to Kubernetes engine resources. The <a id="_idIndexMarker1013"/><a id="_idIndexMarker1014"/>following table summarizes the critical pre-defined IAM roles required to authorize or perform actions on GKE:</p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="image/B15587_09_Table_9.1.jpg" alt=""/>
				</div>
			</div>
			<p>You can always use custom roles with the minimum required set of permissions. This is specifically true in situations where the GKE pre-defined roles are too permissive or do not fit the use case at hand to meet the principle of least privilege. </p>
			<p>Next, we will look at Kubernetes RBAC.</p>
			<h3>Kubernetes RBAC</h3>
			<p><strong class="bold">Kubernetes RBAC</strong> is an<a id="_idIndexMarker1015"/><a id="_idIndexMarker1016"/> authorization mechanism that can limit access to specific resources based on roles that have been assigned to individual users. RBAC is a native Kubernetes security feature that provides options to manage user account permissions. Kubernetes RBAC can be used as an added supplement to Cloud IAM. If Cloud IAM can define roles to operate on clusters and API objects within the cluster, then RBAC can be used to define granular access to <a id="_idIndexMarker1017"/><a id="_idIndexMarker1018"/>specific API objects inside the cluster.</p>
			<p>There are three main elements to Kubernetes RBAC. These are as follows:</p>
			<ul>
				<li><strong class="bold">Subjects</strong>: This refers to a set of users or processes (including Kubernetes service accounts) that can make requests to the Kubernetes API.</li>
				<li><strong class="bold">Resources</strong>: This refers to <a id="_idIndexMarker1019"/><a id="_idIndexMarker1020"/>a set of Kubernetes API objects, such as Pod, Deployment, Service, and so on.</li>
				<li><strong class="bold">Verbs</strong>: This refers to a set <a id="_idIndexMarker1021"/><a id="_idIndexMarker1022"/>of operations that can be performed on resources such as get, list, create, watch, describe, and so on.</li>
			</ul>
			<p>The preceding elements are connected by two RBAC API objects:</p>
			<ul>
				<li><strong class="bold">Roles</strong>: Connects API resources and verbs</li>
				<li><strong class="bold">RoleBindings</strong>: Connects Roles to subjects</li>
			</ul>
			<p>Roles and RoleBindings can be applied at either the cluster level or at the namespace level. These will be discussed in the upcoming subsections, starting with <em class="italic">Roles</em>.</p>
			<h3>Roles</h3>
			<p><strong class="bold">Roles</strong> connect API<a id="_idIndexMarker1023"/><a id="_idIndexMarker1024"/> resources <a id="_idIndexMarker1025"/><a id="_idIndexMarker1026"/>and verbs. There are two types of roles in RBAC. RBAC Roles are defined at the namespace level, while RBAC ClusterRole are defined at the cluster level. We'll look at these in the following sub-sections, starting with RBAC Roles.</p>
			<p>RBAC Roles</p>
			<p>The following is an <a id="_idIndexMarker1027"/><a id="_idIndexMarker1028"/>RBAC Role that's been<a id="_idIndexMarker1029"/><a id="_idIndexMarker1030"/> defined for a specific namespace:</p>
			<p class="source-code">apiVersion: rbac.authorization.k8s.io/v1</p>
			<p class="source-code">kind: <strong class="bold">Role</strong></p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: viewer</p>
			<p class="source-code">  namespace: production</p>
			<p class="source-code">rules:</p>
			<p class="source-code"><strong class="bold">apiGroups</strong>: [""]</p>
			<p class="source-code">   <strong class="bold">resources</strong>: ["pods"]</p>
			<p class="source-code">   <strong class="bold">verbs</strong>: ["get", "list"]</p>
			<p>The definition represents a role of <strong class="source-inline">viewer</strong> that connects the resource pod with specific verbs, <strong class="source-inline">get</strong> and <strong class="source-inline">list</strong>, in the <strong class="source-inline">production</strong> namespace. Only one namespace can be defined per role. For core groups, the <strong class="source-inline">apiGroups</strong> section is optional. However, <strong class="source-inline">apiGroups</strong> should be specified for groups other than core groups. In addition, it is also possible to define a granular role where a specific resource name is also specified.</p>
			<p>Multiple rules can be added to a role. Rules are additive in nature. An RBAC Role doesn't support deny rules. The following is an extension of the earlier RBAC Role, which now includes multiple rules and specifies a resource name:</p>
			<p class="source-code">apiVersion: rbac.authorization.k8s.io/v1</p>
			<p class="source-code">kind: <strong class="bold">Role</strong></p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: viewer</p>
			<p class="source-code">  namespace: production</p>
			<p class="source-code">rules:</p>
			<p class="source-code">apiGroups: [""]</p>
			<p class="source-code">   resources: ["pods"]</p>
			<p class="source-code">   verbs: ["get", "list"]</p>
			<p class="source-code"><strong class="bold">apiGroups</strong>: [""]</p>
			<p class="source-code"><strong class="bold">resources</strong>: ["ConfigMap"]</p>
			<p class="source-code"><strong class="bold">resourceNames</strong>: ["prodEnvironmentVariables"]</p>
			<p class="source-code"><strong class="bold">verbs</strong>: ["get", "list"]</p>
			<p>In the preceding specification, the viewer RBAC Role can now perform <strong class="source-inline">get</strong> and <strong class="source-inline">list</strong> actions on <em class="italic">Pods</em> and <strong class="source-inline">ConfigMap</strong>. However, the operations on <strong class="source-inline">ConfigMap</strong> are strictly restricted to a specific <strong class="source-inline">ConfigMap</strong> named <strong class="source-inline">prodEnvironmentVariables</strong>.</p>
			<p>This completes this sub-section <a id="_idIndexMarker1031"/><a id="_idIndexMarker1032"/>on RBAC Role, one of the two possible RBAC roles. The other – <em class="italic">RBAC ClusterRole</em> – will be detailed in the following sub-section.</p>
			<h4>RBAC ClusterRole</h4>
			<p>RBAC ClusterRole grants permissions<a id="_idIndexMarker1033"/><a id="_idIndexMarker1034"/> at the cluster level, so you don't need to define a specific namespace. The rest of the elements <a id="_idIndexMarker1035"/><a id="_idIndexMarker1036"/>and their usage is the same as RBAC Role.</p>
			<p class="callout-heading">Namespace Scope versus Cluster Scope</p>
			<p class="callout">There are specific resources that are scoped at the namespace level and others that are scoped at the cluster level. Pods, Deployments, Services, Secrets, ConfigMaps, PersistentVolumeClaim, Roles, and RoleBindings are namespace scoped. Nodes, PersistentVolume, CertificateSigningRequests, Namespaces, ClusterRoles, and ClusterRoleBindings are cluster scoped.</p>
			<p>The following is the definition of an RBAC ClusterRole, where the intent is to define a role that can perform <strong class="source-inline">list</strong>, <strong class="source-inline">get</strong>, <strong class="source-inline">create</strong>, and <strong class="source-inline">delete</strong> operations against nodes in the cluster:</p>
			<p class="source-code">apiVersion: rbac.authorization.k8s.io/v1</p>
			<p class="source-code">kind: <strong class="bold">ClusterRole</strong></p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: node-administrator</p>
			<p class="source-code">rules:</p>
			<p class="source-code">apiGroups: [""]</p>
			<p class="source-code">   resources: ["nodes"]</p>
			<p class="source-code">   verbs: ["get", "list", "create", "delete"]</p>
			<p>This completes this sub-section on Roles. The following sub-section explains how roles and users are tied through the <em class="italic">RoleBindings</em> Kubernetes API object.</p>
			<h3>RoleBindings</h3>
			<p><strong class="bold">RoleBindings</strong> connect the <a id="_idIndexMarker1037"/><a id="_idIndexMarker1038"/>subject to a role through a Kubernetes API object. There are two types of RoleBindings in RBAC. RBAC <a id="_idIndexMarker1039"/><a id="_idIndexMarker1040"/>RoleBindings are defined at the namespace level, while RBAC ClusterRoleBindings are defined at the cluster level. Both will be discussed in the following sub-sections.</p>
			<p>RBAC RoleBindings</p>
			<p>The following is the definition <a id="_idIndexMarker1041"/><a id="_idIndexMarker1042"/>of an RBAC RoleBinding that's been defined for a specific namespace that connects users to RBAC Roles:</p>
			<p class="source-code">apiVersion: rbac.authorization.k8s.io/v1</p>
			<p class="source-code">kind: RoleBinding</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: viewer-rolebinding</p>
			<p class="source-code">  namespace: production</p>
			<p class="source-code">subjects:</p>
			<p class="source-code">- kind: User</p>
			<p class="source-code">  name: joe@organization.com</p>
			<p class="source-code">  apiGroup: rbac.authorization.k8s.io</p>
			<p class="source-code">roleRef:</p>
			<p class="source-code">  kind: Role</p>
			<p class="source-code">  name: viewer</p>
			<p class="source-code">  apiGroup: rbac.authorization.k8s.io</p>
			<p>The preceding RBAC RoleBinding has been defined for a production namespace and connects the elements defined under subjects to elements defined under <strong class="source-inline">roleRef</strong>. To be specific, the RBAC RoleBinding connects the user <strong class="source-inline">joe@organization.com</strong> to the <strong class="source-inline">viewer</strong> RBAC Role.</p>
			<p>It's important to note that <strong class="source-inline">kind</strong> under the subject section can be of the <em class="italic">User</em>, <em class="italic">Group</em>, or <em class="italic">ServiceAccount</em> type. These values, from a GKE perspective, can either be from a Cloud IAM User, Cloud IAM service account, or a Kubernetes service account.</p>
			<h4>RBAC ClusterRoleBindings</h4>
			<p>RBAC ClusterRoleBindings bind subjects <a id="_idIndexMarker1043"/><a id="_idIndexMarker1044"/>to RBAC ClusterRoles at the cluster level and are not restricted at the namespace level. You can only bind resources that are cluster scoped, not namespace scoped.</p>
			<p>The following is the definition of RBAC <strong class="source-inline">ClusterRoleBindings</strong>, where the intent is to bind a specific admin user to the RBAC <strong class="source-inline">ClusterRole</strong>, called <strong class="source-inline">node-administrator</strong>, to perform operations against GKE nodes:</p>
			<p class="source-code">apiVersion: rbac.authorization.k8s.io/v1</p>
			<p class="source-code">kind: ClusterRoleBinding</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: node-administrator-clusterrolebinding</p>
			<p class="source-code">subjects:</p>
			<p class="source-code">- kind: User</p>
			<p class="source-code">  name: theadmin@organization.com</p>
			<p class="source-code">  apiGroup: rbac.authorization.k8s.io</p>
			<p class="source-code">roleRef:</p>
			<p class="source-code">  kind: ClusterRole</p>
			<p class="source-code">  name: node-administrator</p>
			<p class="source-code">  apiGroup: rbac.authorization.k8s.io</p>
			<p>This completes this sub-section on <a id="_idIndexMarker1045"/><a id="_idIndexMarker1046"/>RoleBindings, where both kinds of RoleBindings were explained. Overall, this also concludes the sub-section on Kubernetes RBAC and authorization in Kubernetes in particular. The upcoming sub-section discusses another key Kubernetes security construct – <em class="italic">control plane security</em> – which focuses on securing master control plane components.</p>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor206"/>Control plane security</h2>
			<p>As per GCP's shared<a id="_idIndexMarker1047"/><a id="_idIndexMarker1048"/> responsibility model, GKE's <strong class="bold">master control plane components</strong> such as API server, <strong class="source-inline">etcd</strong> database, controller manager, and so on are all managed by Google. So, Google is responsible for securing the control plane, while the end user is responsible for securing nodes, containers, and pods.</p>
			<p>Every GKE cluster has its own root CA. This CA represents an entity that issues a trusted certificate. This trusted certificate is used to secure the connection between machines. The root keys for a CA are managed by an internal service from Google. Communication between the master and the nodes in a cluster is secured based on the shared root of trust provided by the certificates issued by the CA. By default, GKE uses a separate per-cluster CA to provide certificates for the <strong class="source-inline">etcd</strong> databases within a cluster. Since separate CAs are used for each separate cluster, a compromised CA in one cluster cannot be used to compromise another cluster. </p>
			<p>The Kubernetes API server and <strong class="source-inline">kubelet</strong> use secured network communication protocols such as TLS and SSH. They do this by using the certificates issued by the cluster root CA. When a new node is created in the cluster, the node is injected with a shared secret at the time of its creation. This secret is then used by its <strong class="source-inline">kubelet</strong> to submit certificate signing requests to the cluster root CA. This allows <strong class="source-inline">kubelet</strong> to get client certificates when the node is created, and new certificates when they need to be renewed or rotated. <strong class="source-inline">kubelet</strong> uses these client certificates to communicate securely with the API server. </p>
			<p>You must periodically rotate the certificates or credentials to limit the impact of a breach. But sometimes, it might be difficult to strike a balance in terms of how often the credentials should be rotated. This is because the cluster API server will remain unavailable for a short <a id="_idIndexMarker1049"/><a id="_idIndexMarker1050"/>period of time. Note that the credentials that are used by the API server and the clients can be rotated except for the <strong class="source-inline">etcd</strong> certificates, since these are managed by Google.</p>
			<p><strong class="bold">The following is the step-by-step process you should follow to rotate credentials</strong>:</p>
			<ol>
				<li>The rotation process starts by creating a new IP address for the cluster master, along with its existing IP address.</li>
				<li><strong class="source-inline">kube-apiserver</strong> will not be available during the rotation process, but existing pods will continue to run.</li>
				<li>New credentials are issued to the control plane as the result of a new IP address.</li>
				<li>Once GKE has reconfigured the masters, the nodes are automatically updated by GKE to use the new IP and credentials. In addition, the node version is also updated to the closest supported version.</li>
				<li>Each API client must be updated with the new address. Rotation must be completed for the cluster master to start serving with the new IP address and new credentials and to remove the old IP address and old credentials.</li>
				<li>The master node will stop serving the old IP address.</li>
				<li>If the rotation process is started but not completed within 7 days, then GKE will automatically complete the rotation.</li>
			</ol>
			<p>Pods run on nodes and by default, pods can access the metadata of the nodes they are running on. This includes node secrets, which are used for node configuration. So, if a pod is compromised, the node secret also gets compromised, thus negatively impacting the entire cluster. The following steps should be taken to prevent such a compromised event and to protect cluster metadata:</p>
			<ul>
				<li>The service account tied to the nodes should not include the <strong class="source-inline">compute.instance.get</strong> permission. This blocks Compute Engine API calls to those nodes.</li>
				<li>The legacy Compute Engine API endpoint should be disabled (versions 0.1 and v1-beta-1) as these endpoints support metadata being queried directly.</li>
				<li>Use a workload identity to access Google Cloud services from applications running within GKE. This prevents pods from accessing the Compute Engine metadata server.</li>
			</ul>
			<p>This completes this sub-section <a id="_idIndexMarker1051"/><a id="_idIndexMarker1052"/>on how master control plane components are secured in GKE. Next, we'll look at how to secure pods running in a cluster by looking at pod security.</p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor207"/>Pod security</h2>
			<p>One or more <a id="_idIndexMarker1053"/><a id="_idIndexMarker1054"/>containers run inside a pod. By default, these containers can be deployed with privileged elevation. These are also known as privileged containers. <strong class="bold">Privileged containers</strong> have the root capabilities of a<a id="_idIndexMarker1055"/><a id="_idIndexMarker1056"/> host machine and can access resources that can otherwise not be accessed by <a id="_idIndexMarker1057"/><a id="_idIndexMarker1058"/>ordinary containers. The following are a few use cases for privileged containers:</p>
			<ul>
				<li>Running a Docker daemon inside a Docker container</li>
				<li>Requiring direct hardware access to the container</li>
				<li>Automating CI/CD tasks on an open source automation server, such as Jenkins</li>
			</ul>
			<p>Running privileged containers is convenient but undesirable from a security perspective as it allows critical access to host resources. This privilege can be a disadvantage if it's exploited by cybercriminals. The attackers will have root access, which means they can identify and exploit software vulnerabilities and possible misconfigurations, such as containers with no authentication or minimum strength credentials. It is essentially a playground for coin miners to use this privilege for unauthorized needs.</p>
			<p>There are two potential ways to <a id="_idIndexMarker1059"/><a id="_idIndexMarker1060"/>define restrictions on what containers in a pod can do. They are as follows:</p>
			<ul>
				<li>Pod security context</li>
				<li>Pod security policy</li>
			</ul>
			<p>Let's look at these options in more detail.</p>
			<p>Pod Security Context</p>
			<p>The security settings for <a id="_idIndexMarker1061"/><a id="_idIndexMarker1062"/>a pod can be specified using the <strong class="source-inline">securityContext</strong> field in the pod specification. This applies to all the containers inside the pod and enforces the use of specific security measures. They can define whether privileged containers can run and whether the code in the container can be escalated to root privileges. </p>
			<p>A security context can be defined both at the pod level and the container level. The container-level security context takes precedence over the pod-level security context. The following is an extract from the pod manifest YAML for <strong class="source-inline">securityContext</strong>:</p>
			<p class="source-code">apiVersion: v1</p>
			<p class="source-code">kind: Pod</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: my-pod</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  <strong class="bold">securityContext:</strong></p>
			<p class="source-code"><strong class="bold">    runAsUser: 3000</strong></p>
			<p class="source-code">  containers:</p>
			<p class="source-code">  - name: nginx</p>
			<p class="source-code">    image: nginx</p>
			<p class="source-code">    <strong class="bold">securityContext:</strong></p>
			<p class="source-code"><strong class="bold">      runAsUser: 1000</strong></p>
			<p class="source-code"><strong class="bold">      allowPrivilegeEscalation: false</strong></p>
			<p class="source-code">  - name: hello</p>
			<p class="source-code">    image: hello-world</p>
			<p>The preceding specification represents a pod with two containers: <strong class="source-inline">nginx</strong> and <strong class="source-inline">hello</strong>. The <strong class="source-inline">securityContext</strong> definition on the pod specifies that processes inside containers run with a user ID of <strong class="source-inline">3000</strong>. It is important to specify a non-zero number as 0 in Linux as this represents a privileged user's user ID. Not specifying 0 takes away the root privilege of the code running inside the container. <strong class="source-inline">securityContext</strong> on the pod applies to all the containers inside the pod, unless each individual container has an optional <strong class="source-inline">securityContext</strong> defined. In that case, <strong class="source-inline">securityContext</strong> on the container takes precedence. So, in the preceding example, the <strong class="source-inline">hello</strong> container will run the process inside its container while using <strong class="source-inline">3000</strong> as the user ID, whereas the <strong class="source-inline">nginx</strong> container will run the process while using <strong class="source-inline">1000</strong> as the user ID.</p>
			<p class="callout-heading">Using allowPrivilegeEscalation on a Container</p>
			<p class="callout">There are various ways we can use this field. One such scenario is that this field can be explicitly used where <strong class="source-inline">securityContext</strong> is not defined at the pod level but privilege escalation needs to be avoided at a specific container level.</p>
			<p>Security contexts allow<a id="_idIndexMarker1063"/><a id="_idIndexMarker1064"/> you to exercise control over the use of host namespaces, networking, filesystems, and volume types. A security context can be used to control additional security settings:</p>
			<ul>
				<li><strong class="bold">Provide specific capabilities</strong>: If you don't want to give root capabilities, specific capabilities can also be specified at the container level. In the following example, <strong class="source-inline">NET_ADMIN</strong> allows you to perform network-related operations, such as modifying routing tables, enabling multicasting, and so on. <strong class="source-inline">SYS_TIME</strong> allows you to set the system clock:<p class="source-code">. . .</p><p class="source-code">spec:</p><p class="source-code">  containers:</p><p class="source-code">  - name: security-context-example</p><p class="source-code">    image: gcr.io/demo/security-context-example</p><p class="source-code">    <strong class="bold">securityContext:</strong></p><p class="source-code"><strong class="bold">      capabilities:</strong></p><p class="source-code"><strong class="bold">        add: ["NET_ADMIN", "SYS_TIME"]</strong></p></li>
				<li><strong class="bold">Enable seccomp</strong>: Blocks code that's running in containers from making system calls.</li>
				<li><strong class="bold">Enable AppArmor</strong>: Restricts individual program actions using security profiles.</li>
			</ul>
			<p>The downside of configuring <strong class="source-inline">securityContext</strong> for each pod and, sometimes, at each container level is that it involves a lot of effort, especially when hundreds of pods are involved in a cluster. This can be solved by using <em class="italic">Pod Security Policies</em>.</p>
			<h3>Pod Security Policies</h3>
			<p>A <strong class="bold">Pod Security Policy</strong> is a cluster-level <a id="_idIndexMarker1065"/><a id="_idIndexMarker1066"/>resource that manages access for creating and updating pods, based on <a id="_idIndexMarker1067"/><a id="_idIndexMarker1068"/>defined policies. A policy is a set of conditions that need to be met. A pod security policy makes it easier to define and manage security configurations separately. This allows you to apply security restrictions to multiple pods, without having to specify and manage those details in individual pod definitions. </p>
			<p>Pod security Policies can enforce the following:</p>
			<ul>
				<li><strong class="bold">Disable privileged containers</strong>: This can be disabled and can be optionally applied against a specific namespace and specific service account.</li>
				<li><strong class="bold">Enable read-only filesystems</strong>: Containers such as web applications potentially write to a database and not necessarily to a filesystem. So, in such cases, <strong class="source-inline">readOnlyRootFilesystem</strong> can be set to <strong class="source-inline">true</strong>.</li>
				<li><strong class="bold">Enforce non-root users</strong>: This can be enforced to not allow applications to run as root. You can do this by setting the <strong class="source-inline">MustRunAsNonRoot</strong> flag to <strong class="source-inline">true</strong>.</li>
				<li><strong class="bold">Prevent hostpath volumes</strong>: This can<a id="_idIndexMarker1069"/><a id="_idIndexMarker1070"/> be prevented by using <strong class="source-inline">hostpath</strong> for specific directories and not the <a id="_idIndexMarker1071"/><a id="_idIndexMarker1072"/>entire filesystem.</li>
			</ul>
			<p>There are two elements you <a id="_idIndexMarker1073"/><a id="_idIndexMarker1074"/>need in order to define a Pod Security Policy:</p>
			<ul>
				<li><strong class="bold">PodSecurityPolicy object</strong>: The <strong class="source-inline">PodSecurityPolicy</strong> object represents a set of restrictions, requirements, and defaults that are defined similar to a security context inside a pod. This object also specifies all the security conditions that need to be met for a pod to be admitted into a cluster. These rules are specifically applied when a pod is created or updated.</li>
				<li><strong class="bold">PodSecurityPolicy controller</strong>: The <strong class="source-inline">PodSecurityPolicy</strong> controller is an admission controller. The admission controller validates and modifies requests against one or more Pod Security Policies. The controller essentially determines whether a pod can be created or modified.</li>
			</ul>
			<h4>Creating a PodSecurityPolicy object</h4>
			<p>If you need to<a id="_idIndexMarker1075"/><a id="_idIndexMarker1076"/> create a <strong class="source-inline">PodSecurityPolicy</strong> object where privileged containers cannot be run in a specific namespace and by a specific service account, then you should follow these steps:</p>
			<ol>
				<li value="1">Define a <strong class="source-inline">PodSecurityPolicy</strong> Kubernetes object using the <strong class="source-inline">pod-security-policy.yaml</strong> file. The following is an example specification:<p class="source-code"><strong class="bold">apiVersion: policy/v1beta1</strong></p><p class="source-code"><strong class="bold">kind: PodSecurityPolicy</strong></p><p class="source-code"><strong class="bold">metadata:</strong></p><p class="source-code"><strong class="bold">  name: my-pod-security-policy</strong></p><p class="source-code"><strong class="bold">spec:</strong></p><p class="source-code"><strong class="bold">  privileged: false #Prevents creation of privileged Pods</strong></p><p class="source-code"><strong class="bold">  runAsUser:</strong></p><p class="source-code"><strong class="bold">    rule: RunAsAny #Indicates any valid values to be used</strong></p><p>Create the <strong class="source-inline">PodSecurityPolicy</strong> resource using the following CLI command:</p><p class="source-code"><strong class="bold"># Create pod security policy</strong></p><p class="source-code"><strong class="bold">kubectl apply -f pod-security-policy.yaml</strong></p></li>
				<li>To authorize the specific <strong class="source-inline">PodSecurityPolicy</strong>, define a ClusterRole with the resource set to <strong class="source-inline">podsecuritypolicies</strong> and against the specific policy's resource name. An <a id="_idIndexMarker1077"/><a id="_idIndexMarker1078"/>example <strong class="source-inline">ClusterRole</strong> specification for <strong class="source-inline">my-cluster-role.yaml</strong> is as follows:<p class="source-code"><strong class="bold">apiVersion: rbac.authorization.k8s.io/v1</strong></p><p class="source-code"><strong class="bold">kind: ClusterRole</strong></p><p class="source-code"><strong class="bold">metadata:</strong></p><p class="source-code"><strong class="bold">  name: my-cluster-role</strong></p><p class="source-code"><strong class="bold">rules:</strong></p><p class="source-code"><strong class="bold">- apiGroups:</strong></p><p class="source-code"><strong class="bold">  - policy</strong></p><p class="source-code"><strong class="bold">  resources:</strong></p><p class="source-code"><strong class="bold">  - podsecuritypolicies</strong></p><p class="source-code"><strong class="bold">  verbs:</strong></p><p class="source-code"><strong class="bold">  - use</strong></p><p class="source-code"><strong class="bold">  resourceNames:</strong></p><p class="source-code"><strong class="bold">  - my-pod-security-policy</strong></p></li>
				<li>Create your <strong class="source-inline">ClusterRole</strong> using the following CLI command:<p class="source-code"><strong class="bold"># Create ClusterRole</strong></p><p class="source-code"><strong class="bold">kubectl apply -f my-cluster-role.yaml</strong></p></li>
				<li>To authorize the created ClusterRole against a specific subject (which could be a service account) and, optionally, in a specific namespace, define a <strong class="source-inline">RoleBinding</strong>. An example specification for <strong class="source-inline">my-role-binding.yaml</strong> is as follows, where a RoleBinding is being applied to a specific service account:<p class="source-code"><strong class="bold"># Bind the ClusterRole to the desired set of service accounts</strong></p><p class="source-code"><strong class="bold">apiVersion: rbac.authorization.k8s.io/v1</strong></p><p class="source-code"><strong class="bold">kind: RoleBinding</strong></p><p class="source-code"><strong class="bold">metadata:</strong></p><p class="source-code"><strong class="bold">  name: my-role-binding</strong></p><p class="source-code"><strong class="bold">  namespace: my-namespace</strong></p><p class="source-code"><strong class="bold">roleRef:</strong></p><p class="source-code"><strong class="bold">  apiGroup: rbac.authorization.k8s.io</strong></p><p class="source-code"><strong class="bold">  kind: ClusterRole</strong></p><p class="source-code"><strong class="bold">  name: my-cluster-role</strong></p><p class="source-code"><strong class="bold">subjects:</strong></p><p class="source-code"><strong class="bold">  - kind: ServiceAccount</strong></p><p class="source-code"><strong class="bold">    name: sa@example.com</strong></p><p class="source-code"><strong class="bold">    namespace: my-namespace</strong></p></li>
				<li>Create your <strong class="source-inline">RoleBinding</strong> using the following CLI command:<p class="source-code"><strong class="bold"># Create RoleBinding</strong></p><p class="source-code"><strong class="bold">kubectl apply -f my-role-binding.yaml</strong></p></li>
				<li>Enable the <strong class="source-inline">PodSecurityPolicy</strong> controller either at the time of cluster creation or while you're updating<a id="_idIndexMarker1079"/><a id="_idIndexMarker1080"/> an existing cluster. The following are the CLI commands for both options:<p class="source-code"><strong class="bold"># To enable at the time of cluster creation</strong></p><p class="source-code"><strong class="bold">gcloud beta container clusters create &lt;cluster-name&gt; --enable-pod-security-policy</strong></p><p class="source-code"><strong class="bold"># To enable on an existing cluster</strong></p><p class="source-code"><strong class="bold">gcloud beta container clusters update &lt;cluster-name&gt; --enable-pod-security-policy</strong></p></li>
				<li>If you ever need to disable the <strong class="source-inline">PodSecurityPolicy</strong> controller, use the following CLI command:<p class="source-code"><strong class="bold">To disable PodSecurityPolicy controller</strong></p><p class="source-code"><strong class="bold">gcloud beta container clusters update &lt;cluster-name&gt; --no-enable-pod-security-policy</strong></p></li>
			</ol>
			<p>In GKE, the <strong class="source-inline">PodSecurityPolicy</strong> controller is disabled by default or is not enabled at the time of cluster creation. So, it needs to be explicitly enabled. However, the controller should only be enabled once all the relevant <strong class="source-inline">PodSecurityPolicy</strong> objects have been defined, along with their authorization requirements. If there are multiple pod security policies, then these are evaluated alphabetically.</p>
			<p>This concludes this section, which discussed the essential security concepts in the control plane (authentication), worker nodes, and deployments (authorization). This section also drew references to <a id="_idIndexMarker1081"/><a id="_idIndexMarker1082"/>GKE and how these concepts are also implemented in GKE. The upcoming section focuses on specific GKE recommendations around hardening cluster security to ensure the applications running inside the cluster are secure. GKE offers certain features to support these recommendations, all of which will be outlined in detail.</p>
			<h1 id="_idParaDest-208"><a id="_idTextAnchor208"/>Hardening cluster security in GKE</h1>
			<p>Securing the Kubernetes<a id="_idIndexMarker1083"/><a id="_idIndexMarker1084"/> cluster should be your topmost priority when it comes to securing applications running inside your cluster. GKE supports many such features to harden the cluster. For example, the GKE control plane is patched and upgraded automatically as part of the shared responsibility model. In addition, node auto-upgrades are also enabled for a newly created GKE cluster.</p>
			<p>The following are some key<a id="_idIndexMarker1085"/><a id="_idIndexMarker1086"/> additional GKE features that can be used to secure and harden clusters. Some of these features are enabled by default while you're creating a GKE cluster:</p>
			<ul>
				<li>GKE supports a cluster type called <strong class="bold">Private Cluster</strong>, which provides options to restrict access to control planes and nodes. This needs to be specified at the time of cluster creation.</li>
				<li>GKE supports <strong class="bold">container-optimized OS</strong> images. It is a container-optimized OS that has been custom-built, optimized, and hardened specifically for running containers.</li>
				<li>GKE supports <strong class="bold">shielded GKE nodes</strong> as they help increase cluster security using verifiable node identities and integrity. This feature can be enabled on cluster creation or can be updated for an existing cluster.</li>
				<li>GKE allows you to enforce the use of <strong class="bold">Network Policies</strong> on a new or existing cluster. A network policy can restrict pod-to-pod communication within a cluster, thus reducing your footprint in the event of a security incident.</li>
				<li>GKE recommends using <strong class="bold">binary authorization</strong>, a process that ensures supply chain software security. Here, you have the option to exercise control so that only trusted images in the cluster are deployed.</li>
				<li>GKE can authenticate with other Google services and APIs through <strong class="bold">Workload Identity</strong>. This is the recommended way of doing things, instead of using the service account keys at the node level.</li>
				<li>GKE provides an additional layer of protection for sensitive data such as secrets by integrating with Google Secret Manager. <strong class="bold">Secret Manager</strong> is a GCP service that's used to secure API keys, passwords, certificates, and other sensitive data. GKE also supports the use of<a id="_idIndexMarker1087"/><a id="_idIndexMarker1088"/> third-party secret managers such as HashiCorp Vault.</li>
			</ul>
			<p>Each of the preceding GKE features will be covered in their respective sections. We will start with GKE private clusters.</p>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor209"/>GKE private clusters</h2>
			<p><strong class="bold">GKE private clusters</strong> are one of the possible cluster configurations in GKE, especially when it comes to network<a id="_idIndexMarker1089"/><a id="_idIndexMarker1090"/> isolation. This cluster configuration isolates node connectivity to the public internet. This includes both inbound traffic to the cluster and outbound traffic from the cluster. This is <a id="_idIndexMarker1091"/><a id="_idIndexMarker1092"/>because the nodes inside the cluster will not have a public-facing IP address and will only have an internal IP address.</p>
			<p>If nodes require outbound internet access, then a managed <strong class="bold">Network Address Translation</strong> (<strong class="bold">NAT</strong>) gateway can be used. Cloud NAT is<a id="_idIndexMarker1093"/><a id="_idIndexMarker1094"/> GCP's managed NAT gateway. For inbound internet access, external clients can reach applications inside the cluster through services. The service type can either be of the <strong class="source-inline">NodePort</strong> or <strong class="source-inline">LoadBalancer</strong> type. If the service is of the <strong class="source-inline">LoadBalancer</strong> type, GCP's HTTP(S) load balancer can be used and will provide an external IP address to allow inbound traffic into the cluster. The key to GKE private clusters is the functionality of their control planes, since this is the main differentiating factor compared to a non-private cluster. We will look at this next.</p>
			<h3>Control plane in private clusters</h3>
			<p>In GKE, <strong class="source-inline">kube-apiserver</strong> is <a id="_idIndexMarker1095"/><a id="_idIndexMarker1096"/>managed by the control plane. Google runs the control plane on a VM that is in a VPC network in a Google-owned project. In the case of a private cluster, the master control plane sitting on a Google-owned VPC network connects to your cluster's VPC network through VPC network peering. The traffic between the nodes and the control plane is routed through an internal IP address.</p>
			<p>You can access the control plane through<a id="_idIndexMarker1097"/><a id="_idIndexMarker1098"/> endpoints. In general, there are two types of endpoints:</p>
			<ul>
				<li><strong class="bold">Public endpoint</strong>: This represents the<a id="_idIndexMarker1099"/><a id="_idIndexMarker1100"/> external IP address of the control plane. Commands via the <strong class="source-inline">kubectl</strong> tool go through the public endpoint. </li>
				<li><strong class="bold">Private endpoint</strong>: This<a id="_idIndexMarker1101"/><a id="_idIndexMarker1102"/> represents the internal IP address in the control plane's VPC network. This is very specific to private clusters. The nodes in the private cluster communicate with the components in the control plane through internal IP addresses.</li>
			</ul>
			<p>To summarize, a public cluster control plane has an internet-facing endpoint, while a private cluster control plane can be accessed both through private and public endpoints. In addition, a private cluster can only be created in a VPC-native mode (refer to <a href="B15587_08_Final_ASB_TD_ePub.xhtml#_idTextAnchor182"><em class="italic">Chapter 8</em></a>, <em class="italic">Understanding GKE Essentials for Deploying Containerized Applications</em>). The level of access to a <a id="_idIndexMarker1103"/><a id="_idIndexMarker1104"/>private cluster via endpoints can be controlled through one of the following three configurations:</p>
			<ul>
				<li>Public endpoint access disabled</li>
				<li>Public endpoint access enabled; authorized networks enabled for limited access</li>
				<li>Public endpoint access enabled; authorized networks disabled</li>
			</ul>
			<p>Each of the preceding configurations will be discussed in detail in the upcoming sub-sections, starting with <em class="italic">Public endpoint access disabled</em>.</p>
			<h4>Public endpoint access disabled</h4>
			<p>This configuration represents a private GKE cluster with no access to a public endpoint. This is very secure as there is no access to the control plane via public internet. The cluster can only be accessed from the subnet and a secondary range used for pods. A VM in the same region can be added by updating the master authorized networks with the private IP of the VM in CIDR format. </p>
			<p>If the cluster needs to be accessed from outside, then connect to the GKE private cluster's VPC network through Cloud VPN or Cloud Interconnect. The connection gets established through internal IP addresses. The list of internal IP addresses that can access the control plane can also be limited by using <strong class="source-inline">master-authorized-networks</strong>. This does not include public IP addresses as access to public endpoints is disabled.</p>
			<p>Use the following CLI command if you need to create a private GKE cluster where you don't want client access to the public endpoint:</p>
			<p class="source-code"># For Standard Clusters</p>
			<p class="source-code">gcloud container clusters create my-private-cluster \</p>
			<p class="source-code">    --create-subnetwork name=my-subnet \</p>
			<p class="source-code">    --enable-master-authorized-networks \</p>
			<p class="source-code">    --enable-ip-alias \</p>
			<p class="source-code">    --enable-private-nodes \</p>
			<p class="source-code">    --enable-private-endpoint \</p>
			<p class="source-code">    --master-ipv4-cidr 172.20.4.32/28</p>
			<p>The key flags in the preceding CLI command are as follows:</p>
			<ul>
				<li><strong class="source-inline">--enable-master-authorized-networks</strong>: Access to the cluster control plane is restricted to the list of internal IP addresses. Cannot include external IP addresses.</li>
				<li><strong class="source-inline">--enable-private-nodes</strong>: This indicates that the cluster nodes do not have external IP addresses.</li>
				<li><strong class="source-inline">--enable-private-endpoint</strong>: This indicates that the cluster is only managed by the private IP address of the master API endpoint.</li>
			</ul>
			<p>The next sub-section focuses on a configuration where public endpoint access is enabled but access is restricted.</p>
			<h4>Public endpoint access enabled; authorized networks enabled for limited access</h4>
			<p>This configuration represents a<a id="_idIndexMarker1105"/><a id="_idIndexMarker1106"/> private GKE cluster configuration where there is restricted access to the control plane from both internal and external IP addresses. The specific set of internal and external IP addresses can be specified as part of the authorized networks. So, a machine with an external IP address can only communicate with a GKE Private Cluster if that IP address is included in the authorized networks. </p>
			<p>Use the following CLI command if you need to create a private GKE cluster where there is limited access to a public endpoint:</p>
			<p class="source-code"># For Standard Clusters</p>
			<p class="source-code">gcloud container clusters create my-private-cluster-1 \</p>
			<p class="source-code">    --create-subnetwork name=my-subnet-1 \</p>
			<p class="source-code">    --enable-master-authorized-networks \</p>
			<p class="source-code">    --enable-ip-alias \</p>
			<p class="source-code">    --enable-private-nodes \</p>
			<p class="source-code">    --master-ipv4-cidr 172.20.8.0/28</p>
			<p>Note that most of these flags are the same as they were in the previous sub-section, except for the omission of the <strong class="source-inline">--enable-private-endpoint</strong> flag. Omitting this flag implies that the cluster control plane can be reached both by private and public endpoints, but access is restricted only to the allowed IP address as part of the master authorized networks.</p>
			<p>The next sub-section focuses on a configuration where public endpoint access is enabled and access is not restricted.</p>
			<p>Public endpoint access enabled; authorized networks disabled</p>
			<p>This is the default configuration option while creating a private GKE cluster. Essentially, the cluster will have access to the control plane from any IP address. This is the least restrictive option.</p>
			<p>Use the following CLI command if you<a id="_idIndexMarker1107"/><a id="_idIndexMarker1108"/> need to create a private GKE cluster where you wish there to be unrestricted access to the public endpoint:</p>
			<p class="source-code"># For Standard Clusters</p>
			<p class="source-code">gcloud container clusters create my-private-cluster-2 \</p>
			<p class="source-code">    --create-subnetwork name=my-subnet-2 \</p>
			<p class="source-code">    --no-enable-master-authorized-networks \</p>
			<p class="source-code">    --enable-ip-alias \</p>
			<p class="source-code">    --enable-private-nodes \</p>
			<p class="source-code">    --master-ipv4-cidr 172.20.10.32/28</p>
			<p>Note that most of these flags are the same as the ones in the configuration where public endpoint access is enabled but master authorized networks are not enabled. As a result, there are no restricts in terms of the IP addresses that can access the control plane of the private GKE cluster either via a private endpoint or a public endpoint.</p>
			<p>This completes this sub-section on private clusters, where nodes in the cluster can potentially be isolated or restricted from the public internet. The next topic shifts focus to container-optimized OS, which essentially protects the application by hardening the images that are used in containers with key security features. This feature is available in GKE.</p>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor210"/>Container-optimized OS</h2>
			<p><strong class="bold">Container-optimized OS</strong> (also known as a <strong class="source-inline">cos_containerd</strong> image) is a Linux-based kernel that is custom-built from Google and is based on Chromium OS. It can continuously scan <a id="_idIndexMarker1109"/><a id="_idIndexMarker1110"/>vulnerabilities at the kernel level or against any package of the OS. It can patch and update any package in case of a vulnerability. It is optimized and hardened specifically for running containers in production. The following are some of its key features:</p>
			<ul>
				<li><strong class="bold">Minimal OS footprint</strong>: Doesn't include packages that are not required, thereby reducing the OS attack surface.</li>
				<li><strong class="bold">Immutable root system and verified boot</strong>: The root filesystem is always mounted as read-only. This prevents attackers from making changes on the filesystem. Checksum is also computed at build time and verified by the kernel on each boot.</li>
				<li><strong class="bold">Support for stateless configuration</strong>: The root filesystem can be customized to allow writes against a specific directory, such as <strong class="source-inline">/etc/</strong>. This is useful as you can allow write configuration at runtime, such as adding users to the filesystem. However, these changes are not persisted across reboots.</li>
				<li><strong class="bold">Security-hardened kernel</strong>: Supports features such as seccomp and AppArmor to enforce fine-grained security policies.</li>
				<li><strong class="bold">Automatic updates</strong>: Supports automatic updates for new security features or security patches for running GCE VMs.</li>
				<li><strong class="bold">Firewall</strong>: By default, container-optimized OS doesn't allow any incoming traffic except SSH on port <strong class="source-inline">22</strong>.</li>
			</ul>
			<p>Container-optimized <a id="_idIndexMarker1111"/><a id="_idIndexMarker1112"/>OS ensures that the base image<a id="_idIndexMarker1113"/><a id="_idIndexMarker1114"/> that's used to containerize the applications is secure and has a minimal footprint, but it is also important that these containers run on nodes that are equally secured or shielded. We will cover this in the next topic.</p>
			<h2 id="_idParaDest-211"><a id="_idTextAnchor211"/>Shielded GKE nodes</h2>
			<p><strong class="bold">Shielded GKE nodes</strong> is a GKE feature that<a id="_idIndexMarker1115"/><a id="_idIndexMarker1116"/> increases cluster security by providing strong, verifiable node identity and integrity. These nodes are based on Compute Engine Shielded VMs.</p>
			<p class="callout-heading">Shielded VMs</p>
			<p class="callout">Shielded VMs is a GCP feature where VM instances are ensured they won't be compromised <a id="_idIndexMarker1117"/><a id="_idIndexMarker1118"/>at the boot or kernel level. GCP makes this possible by using secure boot and <strong class="bold">virtual Trusted Platform Modules</strong> (<strong class="bold">vTPMs</strong>). Shielded VMs enforce and verify the digital signature of all the components at the time of boot process and halt the boot process on failure.</p>
			<p>The shielded GKE nodes feature prevents the attacker from impersonating nodes in a cluster in the event of a pod vulnerability being exploited. If the shielded GKE nodes feature is enabled, the GKE control plane will cryptographically verify the following and limit the ability of the attacker to impersonate a node in the cluster:</p>
			<ul>
				<li>Every node in the GKE cluster is a GCE VM running in a Google data center.</li>
				<li>Every node is part of the cluster-provisioned managed instance group.</li>
				<li><strong class="source-inline">kubelet</strong> authenticates with the node with a cluster-provisioned certificate.</li>
			</ul>
			<p>You can use the following CLI commands to enable shielded GKE nodes in a new/existing cluster, to verify whether shielded GKE nodes are enabled, and to disable shielded GKE nodes:</p>
			<p class="source-code"># Enable Shielded GKE nodes on new cluster</p>
			<p class="source-code">gcloud container clusters create &lt;cluster-name&gt; --enable-shielded-nodes</p>
			<p class="source-code"># Enable Shielded GKE nodes on existing cluster</p>
			<p class="source-code">gcloud container clusters update &lt;cluster-name&gt; --enable-shielded-nodes</p>
			<p class="source-code"># Verify that Shielded GKE nodes are enabled (check for enabled under shieldedNodes as true)</p>
			<p class="source-code">gcloud container clusters describe &lt;cluster-name&gt;</p>
			<p class="source-code"># Disable Shielded GKE nodes (This will recreate the control plane and nodes thus leading to downtime)</p>
			<p class="source-code">gcloud container clusters update &lt;cluster-name&gt; --no-enable-shielded-nodes</p>
			<p>There is no extra cost in running shielded <a id="_idIndexMarker1119"/><a id="_idIndexMarker1120"/>GKE nodes. However, they produce more logs than regular nodes, thus leading to an overall increase in costs with respect to Cloud Logging. The next topic explains another GKE security feature where the surface area of the attack is reduced in case of a security threat, by restricting the traffic among pods in a cluster.</p>
			<h2 id="_idParaDest-212"><a id="_idTextAnchor212"/>Network Policies – restricting traffic among pods</h2>
			<p>All the pods within a Kubernetes <a id="_idIndexMarker1121"/><a id="_idIndexMarker1122"/>cluster can communicate with each other. However, Kubernetes provides an option for when the traffic between pods needs to be controlled at the IP address or port level. This thought process is strongly recommended to ensure that the entire cluster is not compromised, and that the surface area is controlled in case of a security attack. Kubernetes Network Policies helps you restrict traffic among pods within the cluster.</p>
			<p><strong class="bold">Network Policies</strong> in Kubernetes allow <a id="_idIndexMarker1123"/><a id="_idIndexMarker1124"/>you to specify how a pod can communicate with various network entities based on pods with matching label selectors, namespaces with matching label selectors, or specific IP addresses with port combinations (including the ability to specify exception IP addresses). This can be defined for either ingress or egress traffic flowing in both directions.</p>
			<p>GKE provides options to enforce the use of a network policy when a cluster is created,like so:</p>
			<p class="source-code"># Enforce network policy for a new GKE cluster</p>
			<p class="source-code">gcloud container clusters create &lt;cluster-name&gt; --enable-network-policy</p>
			<p>Optionally, we can enforce this on an existing cluster by using the following CLI commands:</p>
			<p class="source-code"># Enable add-on to enforce network policy on existing cluster</p>
			<p class="source-code">gcloud container clusters update &lt;cluster-name&gt; --update-addons=NetworkPolicy=ENABLED</p>
			<p class="source-code"># Enforce network policy after enabling add-on for existing cluster. This will recreate the cluster node pools</p>
			<p class="source-code">gcloud container clusters update &lt;cluster-name&gt; --enable-network-policy</p>
			<p>A sample network policy can be<a id="_idIndexMarker1125"/><a id="_idIndexMarker1126"/> found at <a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">https://kubernetes.io/docs/concepts/services-networking/network-policies/</a>.</p>
			<p>In addition to specifying a pinpointed policy, Kubernetes allows you to define default network policies. The <a id="_idIndexMarker1127"/><a id="_idIndexMarker1128"/>following are some <a id="_idIndexMarker1129"/><a id="_idIndexMarker1130"/>of the supported default policies:</p>
			<ul>
				<li>Default deny all ingress traffic</li>
				<li>Default deny all egress traffic</li>
				<li>Default deny all ingress and all egress traffic</li>
				<li>Default allow all ingress traffic</li>
				<li>Default allow all egress traffic</li>
			</ul>
			<p>If a specific network policy or a default policy is not defined, then the cluster will allow both ingress and egress traffic to and from pods.</p>
			<p>The next topic details another key GKE feature known as Binary Authorization, which can exercise control to ensure only trusted images are deployed to the GKE cluster.</p>
			<p>Binary Authorization</p>
			<p><strong class="bold">Binary Authorization</strong> is a deploy-time security service provided by Google. It ensures that only trusted containers are deployed in the GKE cluster using deployment policies. The goal of the policy is to <a id="_idIndexMarker1131"/><a id="_idIndexMarker1132"/>determine which images<a id="_idIndexMarker1133"/><a id="_idIndexMarker1134"/> to allow and which to exempt. To accomplish this goal, Binary Authorization integrates with Container Analysis – a GCP service that scans container images stored in a Container Registry for vulnerabilities. In addition, <strong class="bold">Container Analysis</strong> also stores trusted metadata that's used in the authorization process. </p>
			<p>Binary Authorization policies are security-oriented and comprise one or more rules. Rules are constraints that need to pass before the images can be deployed to the GKE cluster. An attested image is one that has been verified or guaranteed by an <em class="italic">attestor</em>. The most common rule that is used is the need for a digitally signed attestation to verify whether the image has been attested. When a container image is built through Cloud Build, the image's digest is digitally signed by a signer, which creates an attestation. At the time of deployment, Binary Authorization enforces the use of an attestor to verify the attestation. Binary Authorization only allows <em class="italic">attested</em> images to be deployed to the cluster. Any unauthorized images that do not match the Binary Authorization policy are rejected. Additionally, a <em class="italic">Denied by Attestor</em> error can also be returned if no attestations are found that were valid and were signed by a key trusted by the attestor. To overcome the <em class="italic">Denied by Attestor</em> error, create an attestation and submit it to Binary Authorization.</p>
			<p>The following are some common use cases that include attestations:</p>
			<ul>
				<li><strong class="bold">Build verification</strong>: To verify <a id="_idIndexMarker1135"/><a id="_idIndexMarker1136"/>whether the container image was built by a specific build system or from a specific <strong class="bold">continuous integration</strong> (<strong class="bold">CI</strong>) pipeline.</li>
				<li><strong class="bold">Vulnerability scanning</strong>: To verify <a id="_idIndexMarker1137"/><a id="_idIndexMarker1138"/>whether the CI-built container image has been scanned for vulnerabilities by Container Analysis and the findings have been defined at an acceptable level.</li>
			</ul>
			<p>Configuring Binary Authorization is a multi-step process. The following is a high-level summary of the steps involved:</p>
			<ol>
				<li value="1">Enabled the required APIs. This includes APIs for GKE, Container Analysis, and Binary Authorization.</li>
				<li>Create a GKE cluster with binary authorization enabled.</li>
				<li>Set up a note. This is a piece of metadata in Container Analysis storage that is associated with an attestor.</li>
				<li>Set up cryptographic keys using PKIX keys, to securely verify the identity of attestors; only enforce verified parties to authorize a container image. <strong class="bold">Public-Key Infrastructure</strong> (<strong class="bold">PKIX</strong>) keys refer to public key<a id="_idIndexMarker1139"/><a id="_idIndexMarker1140"/> certificates defined in the X.509 standard.</li>
				<li>Create an attestor; that is, a person or process that attests to the authenticity of the image.</li>
				<li>Create a Binary Authorization policy. The default policy is to allow all images. The other option includes denying all images or denying images from a specific attestor.</li>
				<li>Optionally, images can be configured so that they're exempt from the binary authorization policy.</li>
			</ol>
			<p>As we mentioned <a id="_idIndexMarker1141"/><a id="_idIndexMarker1142"/>previously, Binary Authorization can deny images from being deployed if the policy conditions are violated or not met. However, you can specify the break-glass flag as an annotation in the pod deployment, which allows pod creation even if the images violate the policy. The break-glass annotation flag is also logged and can be identified by incident response teams through audit logs while they're reviewing or debugging the deployments. The following is a snippet of a pod specification that includes the break-glass flag annotation:</p>
			<p class="source-code">apiVersion: v1</p>
			<p class="source-code">kind: Pod</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: my-break-glass-pod</p>
			<p class="source-code">  annotations:</p>
			<p class="source-code">    <strong class="bold">alpha.image-policy.k8s.io/break-glass: "true"</strong></p>
			<p>This concludes this topic on Binary Authorization. The next topic details another key GKE security feature that allows Google Cloud IAM service accounts to be used as Kubernetes service accounts through Workload Identity, thus providing more secure access to GCP services from applications running inside the GKE cluster.</p>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor213"/>Workload Identity</h2>
			<p>GKE clusters can run applications <a id="_idIndexMarker1143"/><a id="_idIndexMarker1144"/>that might need access to Google-specific APIs, such as compute APIs, storage APIs, database APIs, and more. GKE recommends using <em class="italic">Workload Identity</em> to access GCP services from<a id="_idIndexMarker1145"/><a id="_idIndexMarker1146"/> applications running within GKE. Workload Identity allows you to use a Kubernetes service account as a Google service account. This allows each application to have distinct identities and fine-grained authorization.</p>
			<p>Workload Identity uses the concept of a cluster workload identity pool, which allows Google Cloud IAM to trust and understand Kubernetes service account credentials. The cluster's workload identity pool will be set to <strong class="source-inline">PROJECT_ID.svc.id.goog</strong> and is automatically created<a id="_idIndexMarker1147"/><a id="_idIndexMarker1148"/> at the project level. In such a scenario, Cloud IAM will authenticate the Kubernetes service account with the following member name:</p>
			<p class="source-code">serviceAccount:PROJECT_ID.svc.id.goog[K8S_NAMESPACE/KSA_NAME]</p>
			<p class="source-code"># PROJECT_ID.svc.id.good - workload identity pool on the cluster</p>
			<p class="source-code"># KSA_NAME Kubernetes - service account making the request</p>
			<p class="source-code"># K8S_NAMESPACE Kubernetes - namespace with Kube SA is defined</p>
			<p>The preceding member's name is unique<a id="_idIndexMarker1149"/><a id="_idIndexMarker1150"/> due to the cluster's Workload Identity pool, service account name, and Kubernetes namespace. So, multiple service accounts with the matching three tuples will map to the same member name.</p>
			<h3>Enabling Workload Identity</h3>
			<p>Follow these steps to enable<a id="_idIndexMarker1151"/><a id="_idIndexMarker1152"/> Workload Identity on a GKE cluster:</p>
			<ol>
				<li value="1">Navigate to <strong class="bold">APIs &amp; Services</strong> under the GCP console. Then, search for the <strong class="source-inline">IAM service account Credentials</strong> API and enable it.</li>
				<li>Create a new cluster with Workload Identity enabled via the following CLI command:<p class="source-code"><strong class="bold"># Create cluster with workload identity enabled</strong></p><p class="source-code"><strong class="bold">gcloud container clusters create &lt;CLUSTER_NAME&gt; \</strong></p><p class="source-code"><strong class="bold">  --workload-pool=&lt;PROJECT_ID&gt;.svc.id.goog</strong></p></li>
				<li>Update an existing cluster with Workload Identity enabled via the following CLI command:<p class="source-code"><strong class="bold"># Update existing cluster with workload identity enabled</strong></p><p class="source-code"><strong class="bold">gcloud container clusters update &lt;CLUSTER_NAME&gt; \</strong></p><p class="source-code"><strong class="bold"> --workload-pool=&lt;PROJECT_ID&gt;.svc.id.goog</strong></p></li>
			</ol>
			<p>This concludes this section on Workload Identity, as well as this major section on the key GKE security features that are recommended by Google to harden cluster security.</p>
			<p>Summary</p>
			<p>In this chapter, we discussed some fundamental security concepts from a native Kubernetes or K8s standpoint. Each of these concepts was extended as we looked at their equivalent usage or implementation in GKE. Later, we did a deep dive into certain GKE-specific security features that are critical to hardening cluster security. This included using node auto upgrades to ensure that the nodes are running the latest version of Kubernetes, or using Google's container-optimized OS instead of a general-purpose Linux distribution system. We also looked at using private clusters, where access to the cluster master can be restricted for enhanced security or can be controlled so that it's only accessed from authorized networks. We also looked at Binary Authorization, which ensures that only trusted images can be deployed to the cluster, and Workload Identity, which allows us to use a Cloud IAM service account as a Kubernetes service account, thus providing more flexibility in terms of which applications in the GKE cluster can easily interact with other GCP services, such as Cloud Storage, Secret Management, and more. </p>
			<p>In the next chapter, we'll look at services that are tied to Cloud Operations, along with a specific feature that was introduced in Google Cloud to track the reliability of services: Service Monitoring. This specific feature/option links the SRE technical practices (SLIs, SLOs, and error budget) to the features that are available in Google Cloud Operations so that we can monitor services and alert others about their reliability.</p>
			<h1 id="_idParaDest-214"><a id="_idTextAnchor214"/>Points to remember</h1>
			<p>The following are some important points to remember:</p>
			<ul>
				<li>GCP service accounts are used if GCP resources must have an identity that is tied to an application or a virtual machine.</li>
				<li>Kubernetes service accounts are users that are managed by the Kubernetes API.</li>
				<li>Cloud IAM defines who can view or change the configuration of a GKE cluster and Kubernetes RBAC defines who can view or change Kubernetes objects inside the specific GKE cluster.</li>
				<li>Workload Identity is used to access Google Cloud services from applications running within GKE. This prevents pods from accessing the Compute Engine metadata server.</li>
				<li>In RBAC, a Role connects API resources and verbs. An RBAC Role is cluster-wide scoped, while an RBAC ClusterRole is namespace scoped.</li>
				<li>In RBAC, RoleBindings connect Roles to subjects. A RoleBinding is cluster-wide scoped, while a ClusterRoleBinding is namespace scoped.</li>
				<li>Every GKE cluster has its own root <strong class="bold">CA</strong>.</li>
				<li>Pod Security Context and Pod Security Policy are two ways we can define restrictions regarding what the containers inside a pod can do.</li>
				<li>A GKE Private Cluster allows you to restrict access to control planes and nodes.</li>
				<li>The break-glass flag is used in deployments as an annotation; it allows pod creation, even if the images violate a policy.</li>
				<li><strong class="source-inline">enable-private-nodes</strong>: The nodes do not have external IP addresses.</li>
				<li><strong class="source-inline">enable-private-endpoint</strong>: The cluster is managed by the private IP address of the master endpoint.</li>
				<li><strong class="source-inline">enable-master-authorized-networks</strong>: Access to the cluster's public endpoint is restricted to a specific set of source IP addresses.</li>
				<li>Container Analysis is a service that provides vulnerability scanning and metadata storage for software artifacts.</li>
				<li>Container Analysis stores trusted metadata that's used in the authorization process.</li>
				<li>Binary Authorization allows or blocks images from being deployed to GKE based on a policy you've configured.</li>
				<li>A container-optimized OS or <strong class="source-inline">cos_containerd</strong> image is a Linux-based kernel that can continuously scan for vulnerabilities at the kernel level.</li>
				<li>Shielded GKE nodes increase cluster security by using verifiable node identity and integrality. This can be enabled using the <strong class="source-inline">--enable-shielded-nodes</strong> option.</li>
				<li>You can restrict traffic among pods with Network Policies.</li>
				<li>You can configure a secret manager that has been integrated with GKE clusters.</li>
				<li>You can use admission controllers to enforce a Pod Security Policy.</li>
				<li>In terms of Workload Identity, you can use K8's service account and namespace as a GCP service account to authenticate GCP APIs.</li>
			</ul>
			<h1 id="_idParaDest-215"><a id="_idTextAnchor215"/>Further reading</h1>
			<p>For more information on GCP's approach to DevOps, read the following articles:</p>
			<ul>
				<li><strong class="bold">Kubernetes</strong>: <a href="https://kubernetes.io/docs/home/">https://kubernetes.io/docs/home/</a></li>
				<li><strong class="bold">Google Kubernetes Engine</strong>: <a href="https://cloud.google.com/kubernetes-engine">https://cloud.google.com/kubernetes-engine</a></li>
			</ul>
			<h1 id="_idParaDest-216"><a id="_idTextAnchor216"/>Practice test</h1>
			<p>Answer the following questions:</p>
			<ol>
				<li value="1">Network Policies are used to restrict traffic among which of the following?<p>a) Deployments</p><p>b) Containers</p><p>c) Pods</p><p>d) Container images</p></li>
				<li>Select the RBAC option that connects a user and a role:<p>a) UserRoleBinding</p><p>b) RoleBindings</p><p>c) Roles</p><p>d) RoleUserBinding</p></li>
				<li>In a private cluster, which Google service can download a Docker image?<p>a) Cloud Build</p><p>b) Cloud Source Repository</p><p>c) Elastic Container Registry </p><p>d) Container Registry</p></li>
				<li>What will happen if the process of rotating credentials started but never completed?<p>a) GKE will not complete the cluster rotation.</p><p>b) GKE will pause the cluster rotation.</p><p>c) GKE will complete the cluster rotation in 7 days.</p><p>d) GKE will instantly complete the cluster rotation.</p></li>
				<li>Which of the following possible policies will disable privileged containers?<p>a) Network Policy</p><p>b) Pod Security Policy</p><p>c) Network Security Policy</p><p>d) Pod Policy</p></li>
				<li>Select the GKE Role that allows you to manage clusters, including creating, deleting, getting, listing, or updating clusters. No access is given to cluster resources or API objects:<p>a) GKE Admin</p><p>b) GKE Cluster Admin</p><p>c) GKE Developer</p><p>d) GKE Cluster Developer</p></li>
				<li>With regards to a <strong class="bold">Pod Security Policy</strong> (<strong class="bold">PSP</strong>), select the order of operations:<p>a) Enable PSP Controller, Create PSP, Define Authorization Requirements</p><p>b) Create PSP, Enable PSP Controller, Define Authorization Requirements </p><p>c) Create PSP, Define Authorization Requirements, Enable PSP Controller</p><p>d) Enable PSP Controller, Define Authorization Requirements, Create PSP</p></li>
				<li>If a specific network policy or a default policy is not defined, then which of the following is true?<p>a) Deny all ingress and all egress traffic.</p><p>b) Allow all ingress and all egress traffic.</p><p>c) Deny all ingress and allow all egress traffic.</p><p>d) Allow all ingress and deny all egress traffic.</p></li>
				<li>Select the option that enforces a deploy time policy to GKE:<p>a) Cloud IAM Policies</p><p>b) AppArmor</p><p>c) Cloud Armor</p><p>d) Binary Authorization</p></li>
				<li>The <em class="italic">service account admission controller</em> associates the created service accounts with the running pods. How are the service accounts stored and accessed? <p>a) Stored as plain text and accessed as environment variables at runtime</p><p>b) Stored as a Kubernetes secret and accessed through the key management service</p><p>c) Stored as a Kubernetes secret and accessed as an environment variable at runtime</p><p>d) Stored as plain text and accessed through the key management service</p></li>
			</ol>
			<h1 id="_idParaDest-217"><a id="_idTextAnchor217"/>Answers</h1>
			<ol>
				<li value="1">(c): Pods</li>
				<li>(b): RoleBindings</li>
				<li>(d): Container Registry</li>
				<li>(c): GKE will complete the cluster rotation in 7 days.</li>
				<li>(b): Pod Security Policy</li>
				<li>(b): GKE Cluster Admin</li>
				<li>(c): Create PSP, Define Authorization Requirements, Enable PSP Controller</li>
				<li>(b): Allow all ingress and all egress traffic.</li>
				<li>(d): Binary Authorization</li>
				<li>(c): Stored as a Kubernetes secret and accessed as an environment variable at runtime</li>
			</ol>
		</div>
	</body></html>