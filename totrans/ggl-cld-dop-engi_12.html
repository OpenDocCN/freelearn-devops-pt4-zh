<html><head></head><body>
		<div id="_idContainer174">
			<h1 id="_idParaDest-218"><em class="italic"><a id="_idTextAnchor218"/>Chapter 10</em>: Exploring GCP Cloud Operations</h1>
			<p><strong class="bold">Reliability</strong> is the most critical feature of a service or a system. <strong class="bold">Site Reliability Engineering</strong> (<strong class="bold">SRE</strong>) prescribes specific technical tools or practices that help in measuring characteristics that define and track reliability, such as <strong class="bold">SLAs</strong>, <strong class="bold">SLOs</strong>, <strong class="bold">SLIs</strong>, and <strong class="bold">error budgets</strong>.</p>
			<p>In <a href="B15587_02_Final_ASB_ePub.xhtml#_idTextAnchor038"><em class="italic">Chapter 2</em></a>, <em class="italic">SRE Technical Practices – Deep Dive</em>, we discussed the key constructs of SLAs in detail, the need for SLOs to achieve SLAs, the guidelines for setting SLOs, and the need for SLIs to achieve SLOs. In addition, we learned about the different types of SLIs based on user journey categorization, different sources to measure SLIs, the importance of error budgets, and the ways to set error budgets to make a service reliable.</p>
			<p>This raises a series of fundamental questions:</p>
			<ul>
				<li>How can we observe SLIs for a service so that the SLOs are not violated?</li>
				<li>How can we track whether our error budgets are getting exhausted?</li>
				<li>How can we maintain harmony between the key SRE technical tools?</li>
			</ul>
			<p>SRE's answer to the preceding questions is observability. Observability on <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>) is established through operations. From Google's point of view, Cloud Operations is about monitoring, troubleshooting, and improving application performance in the Google Cloud environment. The key objectives of Cloud Operations are as follows:</p>
			<ul>
				<li>Gather logs, metrics, and traces from any source.</li>
				<li>Query the captured metrics and analyze traces.</li>
				<li>Visualize information on built-in or customizable dashboards.</li>
				<li>Establish performance and reliability indicators.</li>
				<li>Trigger alerts and report errors in situations where reliability indicators are not met, or issues are encountered.</li>
			</ul>
			<p>Google achieves key objectives of operations through a collection of services called Cloud Operations. Cloud Operations is a suite of GCP services that includes Cloud Monitoring, Cloud Logging, Error Reporting, and <strong class="bold">Application Performance Management</strong> (<strong class="bold">APM</strong>). Furthermore, APM includes Cloud Debugger, Cloud Trace, and Cloud Profiler. This chapter will explore services tied to Cloud Operations. Post that, we will focus on a specific feature that was introduced in Google Cloud to track the reliability of services through Service Monitoring. This specific feature/option links the SRE technical practices (SLIs, SLOs, and Error Budget) to features available in Google Cloud Operations that monitor the service and tell us about its reliability.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li><strong class="bold">Cloud Monitoring</strong>: Workspaces, dashboards, Metrics explorer, uptime checks, and alerting.</li>
				<li><strong class="bold">Cloud Logging</strong>: Audit Logs, Logs Ingestion, Logs Explorer, and Logs-Based Metrics.</li>
				<li><strong class="bold">Cloud Debugger</strong>: Setting up, Usage, Debug Logpoints, and Debug Snapshots.</li>
				<li><strong class="bold">Cloud Trace</strong>: Trace Overview, Trace List, and Analysis Reports.</li>
				<li><strong class="bold">Cloud Profiler</strong>: Profile Types.</li>
				<li><strong class="bold">Binding SRE and Cloud Operations</strong>: We will measure service reliability using Cloud Operations by linking them to SRE technical practices via a hands-on lab.</li>
			</ul>
			<h1 id="_idParaDest-219"><a id="_idTextAnchor219"/>Cloud Monitoring</h1>
			<p><strong class="bold">Cloud Monitoring</strong> is a GCP service that <a id="_idIndexMarker1153"/>collects metrics, events, and metadata from multi-cloud and hybrid infrastructures in real time. Cloud Monitoring helps us understand how well our resources are performing and if there is something wrong that requires immediate attention. Cloud Monitoring is a medium through which SRE best practices can be implemented and to ensure that applications are meeting their set SLAs. Cloud Monitoring consists of out-of-the-box dashboards. These can be used to visualize insights into key factors that impact SLIs and SLOs such as latency, throughput, and more. Cloud Monitoring is also critical for incident management as alerts can be generated from key metrics, and these alerts can be sent as notifications to configured notification channels.</p>
			<p>This section on <a id="_idIndexMarker1154"/>Cloud Monitoring deep dives into several key areas/properties, such as workspaces, dashboards, Metrics explorer, uptime checks, alerting, access controls, and the Monitoring agent. We will start with workspaces.</p>
			<h2 id="_idParaDest-220"><a id="_idTextAnchor220"/>Workspaces</h2>
			<p><strong class="bold">Google Workspace</strong> is a <a id="_idIndexMarker1155"/>centralized hub that's used to organize and display monitoring information about GCP and AWS resources. Workspace <a id="_idIndexMarker1156"/>provides a centralized view and acts as a <a id="_idIndexMarker1157"/>single point of entry to resource dashboards, uptime checks, groups, incidents, events, and charts. Google describes this centralized view as a <em class="italic">single pane of glass</em> (please refer to the following screenshot). The actions that can be performed against a workspace include the ability to <a id="_idIndexMarker1158"/>view content, which is controlled by <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>):</p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="image/B15587_10_01.jpg" alt="Figure 10.1 – Overview of a Cloud Monitoring workspace&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – Overview of a Cloud Monitoring workspace</p>
			<p>The following is some key terminology we need to know about before we elaborate on the relationship between workspaces and projects:</p>
			<ul>
				<li><strong class="bold">Host project</strong>: This refers to the <a id="_idIndexMarker1159"/>project where Workspace is created.</li>
				<li><strong class="bold">Monitored project</strong>: This refers to the <a id="_idIndexMarker1160"/>GCP projects or AWS accounts that the workspace can monitor.</li>
				<li><strong class="bold">AWS connector project</strong>: This <a id="_idIndexMarker1161"/>refers to the GCP project that connects the monitored AWS account to the workspace.</li>
			</ul>
			<p>The upcoming <a id="_idIndexMarker1162"/>subsection <a id="_idIndexMarker1163"/>provides insights into the relationship between Workspace and Project.</p>
			<h3>Workspace/project relationship</h3>
			<p>The following are some <a id="_idIndexMarker1164"/>key pointers with respect to a workspace/project relationship:</p>
			<ul>
				<li>A workspace is always created inside a project. This is known as the host project.</li>
				<li>A workspace is part of a single host project and is named after the host project.</li>
				<li>A workspace can monitor resources from multiple monitored projects simultaneously. This could include about 200 GCP projects/AWS accounts.</li>
				<li>A workspace can access other monitored projects' metric data, but the actual data lives in monitored projects.</li>
				<li>A monitored project can only be associated with a single workspace, and a monitored project can be moved from one workspace to another.</li>
				<li>Multiple workspaces can be merged into a single workspace.</li>
				<li>There is no charge associated with creating a workspace. However, charges with respect to logging and ingesting metric data is charged to the billing account associated with the <em class="italic">monitored projects</em>.<p class="callout-heading">Tip – how to connect an AWS account to a workspace</p><p class="callout">A GCP connector project is required. This can be an existing project, or an empty project (preferred) created for this purpose. GCP connector project needs to be under the same parent organization as the workspace. A billing account should be tied to the connector project and this account will be charged to monitor the resources under the AWS account.</p></li>
			</ul>
			<p>The following section discusses potential strategies for creating a workspace.</p>
			<h3>Workspace creation – strategies</h3>
			<p>In a real-time scenario, it is <a id="_idIndexMarker1165"/>possible to have multiple projects where the projects are either differentiated by customers or differentiated by environment types such as development, test, staging, and production. Given that a workspace can monitor resources from multiple projects, the strategy/approach that's taken to create the workspace becomes critical. There are multiple strategies we can follow to create a workspace, as detailed in the following sections.</p>
			<h4>A single workspace for all monitored projects</h4>
			<p>Information about all monitored project resources is available from within a single workspace. The following diagram represents a single workspace that monitors an application, <strong class="source-inline">app</strong>, that's been deployed across multiple projects, such as <strong class="source-inline">app-dev</strong>, <strong class="source-inline">app-test</strong>, and <strong class="source-inline">app-prod</strong>. These have been categorized by environment type. This approach has its own pros and cons.</p>
			<p>The pro is that the workspace provides a single pane of glass for all the resources tied to the application across multiple projects representing multiple environment types. The con is that a non-production user can access resources from a production project, and this might not be acceptable in most cases. This approach is not suitable for organizations that have strict isolation between production and non-production environments:</p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/B15587_10_02.jpg" alt="Figure 10.2 – A single workspace for all related projects&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.2 – A single workspace for all related projects</p>
			<h4>Workspace per group of monitored projects</h4>
			<p>A workspace will monitor a specific group of projects. There could be more than one workspace monitoring the available projects. The following diagram is an alternative workspace creation strategy compared to the one shown in the preceding diagram. Specifically, the <a id="_idIndexMarker1166"/>following diagram represents two workspaces where one workspace monitors the non-production projects and the second workspace monitors the production project. Access controls to that specific group can be controlled by the host project of the individual workspace. This allows us to differentiate between users across environment types, such as production versus non-production or even across customers:</p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/B15587_10_03.jpg" alt="Figure 10.3 – Workspace per group of monitored projects&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.3 – Workspace per group of monitored projects</p>
			<h4>Single workspace per monitored project</h4>
			<p>Essentially, every project that needs to be monitored is hosted by a workspace within the same project. This can be seen in the following diagram. This means that the source and the host project will be the same. This approach provides the most granular control in terms of providing monitoring access to the project resources. However, this might also only provide a slice of information if an application is spread across multiple projects:</p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="image/B15587_10_04.jpg" alt="Figure 10.4 – Single workspace per monitored project&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.4 – Single workspace per monitored project</p>
			<p class="callout-heading">Important note – Workspace management</p>
			<p class="callout">One or more GCP project(s) or AWS account(s) can either be added or removed from the workspace. In addition, all projects within a selected workspace can be merged into the current workspace, but the configuration will be deleted in the selected workspace. These actions are performed through the settings page of Cloud Monitoring. A workspace can only be deleted if the workspace host project is deleted.</p>
			<p>This concludes this <a id="_idIndexMarker1167"/>subsection on workspace creation strategies. The preferred strategy depends on the organizational need. Workspace basic operations include creating a workspace, adding project(s) to a workspace, moving projects between workspaces, and merging workspaces. Detailed instructions on how to create a workspace can be found at <a href="https://cloud.google.com/monitoring/workspaces/create">https://cloud.google.com/monitoring/workspaces/create</a>. The upcoming subsection discusses IAM roles, which are used to determine who has access to monitor the resources inside the monitoring workspace.</p>
			<h3>Workspace IAM roles</h3>
			<p>The following are the IAM roles that <a id="_idIndexMarker1168"/>can be applied to a workspace project so that we can view monitoring data or perform actions on the workspace:</p>
			<ul>
				<li><strong class="bold">Monitoring Viewer</strong>: Read-only <a id="_idIndexMarker1169"/>access to view <a id="_idIndexMarker1170"/>metrics inside a workspace.</li>
				<li><strong class="bold">Monitoring Editor</strong>: Monitoring Viewer, plus the <a id="_idIndexMarker1171"/>ability to edit a <a id="_idIndexMarker1172"/>workspace, create alerts, and have write access to Monitoring Console and Monitoring API.</li>
				<li><strong class="bold">Monitoring Admin</strong>: Monitoring Editor, plus the <a id="_idIndexMarker1173"/>ability to <a id="_idIndexMarker1174"/>manage IAM roles for the workspace.</li>
				<li><strong class="bold">Monitoring Metric Writer</strong>: A service account <a id="_idIndexMarker1175"/>role that's given to applications instead of humans. This allows an application to <a id="_idIndexMarker1176"/>write data to a workspace but does not provide read access.</li>
			</ul>
			<p>This concludes our quick insight into workspaces and their concepts, such as workspace creation strategies. Next, we will look at dashboards.</p>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor221"/>Dashboards</h2>
			<p><strong class="bold">Dashboards</strong> provide a graphical <a id="_idIndexMarker1177"/>representation of key signal data, called metrics, in a manner that is suitable for end users or the operations team. It's <a id="_idIndexMarker1178"/>recommended that a single dashboard displays metrics depicting a specific viewpoint (for example, serverless resources with a specific label) or for a specific resource (for example, persistent disks, snapshots, and so on). There are two types of dashboards: predefined dashboards and custom dashboards.</p>
			<p>The following screenshot is of the <strong class="bold">Dashboards Overview</strong> page in Cloud Monitoring. This page displays the list of available dashboards, categorized by dashboard types, and provides quick links to the most recently used dashboards:</p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/B15587_10_05.jpg" alt="Figure 10.5 – Cloud Monitoring – Dashboards Overview&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.5 – Cloud Monitoring – Dashboards Overview</p>
			<p>Cloud Monitoring supports <a id="_idIndexMarker1179"/>both predefined dashboards and custom dashboards. The upcoming <a id="_idIndexMarker1180"/>subsection provides an overview of the different types of dashboards and steps for how to create a custom dashboard.</p>
			<h3>Predefined dashboards</h3>
			<p>Cloud Monitoring <a id="_idIndexMarker1181"/>provides a set of predefined dashboards that are <a id="_idIndexMarker1182"/>grouped by a specific GCP resource such as firewalls or GKE Clusters. These dashboards are categorized by <strong class="bold">Type</strong>. This is set to <strong class="bold">Google Cloud Platform</strong>, which is maintained by Google Cloud. They do not require any explicit setup or effort to configure.</p>
			<p>However, predefined dashboards are not customizable. These dashboards are organized in a specific manner with a set of predefined filters from the context of the dashboard. Users cannot change the contents of the view or add a new filter criterion. Users can only use the predefined filters to control the data being displayed.</p>
			<h3>Custom dashboards</h3>
			<p>Users or operation teams can create a <strong class="bold">custom dashboard</strong> that displays specific content of interest. These <a id="_idIndexMarker1183"/>dashboards are categorized by <strong class="bold">Type</strong> set to <strong class="bold">Custom</strong>. Content is added by configuring one or more widgets. There are <a id="_idIndexMarker1184"/>multiple types of widgets. Dashboards can either be created from Google Cloud Console or via the Cloud Monitoring API. In addition, the Cloud Monitoring API allows you to import a dashboard configuration from GitHub and modify it as needed.</p>
			<p>Custom dashboards represent information about a metric using a chart. This chart displays raw signal information from a metric that's aligned across a configurable time window. Each chart is of a specific widget type. Cloud Monitoring supports multiple widget types such as Line, Stacked area, Stacked bar, Heatmap, Gauge, Scorecard, and Textboxes. Let's take a brief look at the different types of widgets:</p>
			<ul>
				<li>Line charts, stacked area charts, and stacked bar charts are best utilized to display time series data. Each of these widget types can be configured so that they're displayed in Color/X-Ray/Stats/Outlier mode, along with an optional legend, using the display view options.</li>
				<li>Heatmap charts are used to represent metrics with a distribution value.</li>
				<li>Gauges display the most recent measurement in terms of a number. This is represented by a thick line around the gauge. This is visually categorized across good, warning, and danger zones.</li>
				<li>Scorecards are similar to gauges as they display the recent measurement in terms of a number, but they can be visually depicted using a different view other than a gauge, such as a spark line, spark bar, icon, or value.</li>
				<li>Textboxes allow us to add any custom information, such as quick notes or links, concerning the relevant resources.</li>
			</ul>
			<p>The upcoming subsection will show you how to create a custom dashboard.</p>
			<h4>Creating a custom dashboard</h4>
			<p>Follow these steps to create a <a id="_idIndexMarker1185"/>custom dashboard from the GCP console:</p>
			<ol>
				<li>Navigate to <strong class="bold">Cloud Monitoring</strong> | <strong class="bold">Dashboards</strong> and select the <strong class="bold">Create Dashboard</strong> option. Name the dashboard <strong class="source-inline">VM Instance – Mean CPU Utilization</strong>.</li>
				<li>Select a chart type or widget. This will open <strong class="bold">Metrics explorer</strong> on the left-hand pane and will add the chart to the dashboard.</li>
				<li>Select the options to choose a resource type, metric type, and grouping criteria. Then, <strong class="bold">Save</strong> the dashboard to add the chart type.</li>
				<li>Redo the preceding steps to add multiple charts to the same dashboard.</li>
			</ol>
			<p>The following screenshot shows a custom dashboard depicting the mean CPU utilization for all the VM instances, along with seven possible widget types:</p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/B15587_10_06.jpg" alt="Figure 10.6 – Custom dashboard with seven possible widget types&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.6 – Custom dashboard with seven possible widget types</p>
			<p>This concludes this section on Cloud Monitoring dashboards. The next section focuses on using <em class="italic">Metrics explorer</em> as an option to explore predefined and user-created metrics.</p>
			<h2 id="_idParaDest-222"><a id="_idTextAnchor222"/>Metrics explorer</h2>
			<p><strong class="bold">Metrics</strong> is one of the <a id="_idIndexMarker1186"/>critical sources for monitoring data. Metrics represents numerical measurements of resource usage or behavior that can be observed and collected across the system over many data points at regular time intervals. There are about 1,000 pre-created metrics in GCP. This <a id="_idIndexMarker1187"/>includes CPU utilization, network traffic, and more. However, some granular metrics such as memory usage can be collected using an <a id="_idIndexMarker1188"/>optional Monitoring agent. Additionally, custom metrics can be created either through the built-in Monitoring API or through OpenCensus – an open source library used to create metrics. It is always recommended to check if a default or pre-created metric exists before creating a custom metric.</p>
			<p>Metrics explorer provides options for exploring existing metrics (either predefined or user-created), using metrics to build charts, adding charts to an existing or new dashboard, sharing charts via a URL, or retrieving chart configuration data in JSON format. Metrics explorer is an interface that provides a DIY approach to building charts as you can select a metric of your choice.</p>
			<p>The following screenshot shows the <strong class="bold">Metrics explorer</strong> section, which charts the <strong class="bold">CPU Utilization</strong> metric for a <strong class="bold">VM Instance</strong>, grouped by system state. The left-hand side displays the configuration region, while the right-hand side depicts the chart for the selected metric. The configuration region has two tabs:</p>
			<ul>
				<li><strong class="bold">Metric</strong>: This tab is used to select the metric and explore it.</li>
				<li><strong class="bold">View Options</strong>: This tab is used to change the chart's display characteristics:</li>
			</ul>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/B15587_10_07.jpg" alt="Figure 10.7 – Cloud Monitoring – Metrics explorer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.7 – Cloud Monitoring – Metrics explorer</p>
			<p>The following section <a id="_idIndexMarker1189"/>discusses the <a id="_idIndexMarker1190"/>available options for configuring a metric using Metrics explorer.</p>
			<h3>Understanding metric configuration via Metrics explorer</h3>
			<p>To configure a metric for a <a id="_idIndexMarker1191"/>monitored resource, we can use the <a id="_idIndexMarker1192"/>following options.</p>
			<p><strong class="bold">Resource type</strong> and <strong class="bold">Metric</strong> option can be selected in either of the following ways:</p>
			<ul>
				<li><strong class="bold">Standard Mode</strong>: Select a specific metric type or browse the available metric types based on a specific GCP resource.</li>
				<li><strong class="bold">Direct Filter Mode</strong>: Manually enter a metric type and resource type in a text box.</li>
			</ul>
			<p>The <strong class="bold">Filter</strong> option can be used to filter out the results based on the filter criterion. The filter criterion can be defined using the available operators or regular expressions. There are two possible filter types:</p>
			<ul>
				<li><strong class="bold">By Resource Label</strong>: The available filter fields are specific to the selected resource type (for example, the VM Instance resource type will have <strong class="source-inline">project_id</strong>, <strong class="source-inline">instance_id</strong>, and <strong class="source-inline">zone</strong> as the available filter options).</li>
				<li><strong class="bold">By Metric Label</strong>: Refers to project-wide user-created labels.</li>
			</ul>
			<p>The <strong class="bold">Group By</strong> option can be used to group time series data by resource type and metric label. This creates new time series data based on the combination of group by values.</p>
			<p>The <strong class="bold">Aggregator</strong> option can be used to describe how to aggregate data points across multiple time series. Common options <a id="_idIndexMarker1193"/>include min, max, sum, count, and standard deviation. By default, the aggregation results in a single line by applying the <a id="_idIndexMarker1194"/>aggregator across all the time series. If <strong class="bold">Group By</strong> labels are selected, the aggregation results in a time series for each combination of matching labels.</p>
			<p>The <strong class="bold">Period</strong> option can be used to determine the time interval for which aggregation takes place. The default selection is 1 minute.</p>
			<p>The <strong class="bold">Aligner</strong> option can be used to bring the data points in each individual time series into equal periods of time.</p>
			<p>Additional options include the following:</p>
			<ul>
				<li><strong class="bold">Secondary Aggregator</strong>: Used in charts with multiple metrics</li>
				<li><strong class="bold">Legend Template</strong>: For better readability</li>
			</ul>
			<p>Multiple <strong class="bold">View Options</strong> can be used to plot metrics, and these are distinguished by the available chart modes, which are as follows:</p>
			<ul>
				<li><strong class="bold">Color mode</strong>: This is the default mode where graph lines are shown in color.</li>
				<li><strong class="bold">X-Ray mode</strong>: Shows graph lines in a translucent gray color but with brightness in the case of overlapping bands.</li>
				<li><strong class="bold">Stats mode</strong>: Shows common statistical values such as the 5th percentile, 95th percentile, average, median, and more.</li>
				<li><strong class="bold">Outlier mode</strong>: Allows you to choose a number of time series to display, along with the option to rank time series by ordering them from the top or bottom.</li>
			</ul>
			<p>Additionally, each chart mode supports the ability to specify a specific threshold and allows you to compare past time series data. In addition, it is possible to apply a log scale to the <em class="italic">y</em> axis for better separation between larger values in datasets where some values are much larger than the others.</p>
			<p class="callout-heading">Tip – Monitoring Query Language (MQL) – Advanced option to create charts</p>
			<p class="callout">Cloud Monitoring <a id="_idIndexMarker1195"/>supports <strong class="bold">MQL</strong>, an advanced option for creating a chart with a text-based interface and an expressive query language that can execute complex queries against time series data. Potential use cases include the ability to select a random sample of time series or compute the ratio of requests, resulting in a particular class of response codes.</p>
			<p>This completes this <a id="_idIndexMarker1196"/>section on Metrics explorer, which allows users to <a id="_idIndexMarker1197"/>explore predefined and custom metrics. These can potentially be used to create charts. The options related to configuring a metric were also discussed in detail. The upcoming section focuses on uptime checks – an option for validating whether a service is functioning.</p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor223"/>Uptime checks</h2>
			<p><strong class="bold">Uptime check</strong> is a Cloud Monitoring feature <a id="_idIndexMarker1198"/>where periodic requests are sent to monitor a resource to check if the resource is indeed up. Uptime checks can <a id="_idIndexMarker1199"/>check the uptime of GCP VMs, App Engine services, website URLs, and AWS Load Balancer. Uptime checks are also a way to track the Error Budget of services. Uptime checks essentially test the availability of an external facing service within a specific timeout interval and ensure that the Error Budget of the service is not burnt unnecessarily. It is possible to initiate these tests from one or more GCP geographic regions, and a minimum of three active locations must be selected as geographic regions. Alternatively, selecting the <strong class="bold">Global</strong> option will initiate tests from all available locations.</p>
			<p>The frequency at which uptime checks are performed can be configured and defaults to 1 minute. Uptime checks support multiple protocol options, such as HTTP, HTTPS, and TCP, and can be defined for the following resource types:</p>
			<ul>
				<li><strong class="bold">URL</strong>: Required to specify a hostname and path.</li>
				<li><strong class="bold">App Engine Service</strong>: Required to specify a service and path.</li>
				<li><strong class="bold">Instance</strong>: Required to specify a path for a single instance (GCP or EC2) or a predefined group. This group needs to be explicitly configured.</li>
				<li><strong class="bold">Elastic Load Balancer</strong>: Required to specify a path for AWS ELB.</li>
			</ul>
			<p>The configuration to create an uptime check includes options to perform response validation. These options include the following:</p>
			<ul>
				<li><strong class="bold">Provide response timeout</strong>: This is the time it takes for the request to complete. Must be between 1 and 60 seconds. The default is 10 seconds.</li>
				<li><strong class="bold">Enable response content</strong>: This option allows you to select a response content match type with specific operators that contain or do not contain specific text or matches on regex.</li>
				<li><strong class="bold">Log Check Failures</strong>: This option will save all the logs related to uptime checks failing to Cloud Logging.</li>
			</ul>
			<p>In addition, alerts and notifications <a id="_idIndexMarker1200"/>can be configured in situations <a id="_idIndexMarker1201"/>when the uptime check fails for the selected duration. It is mandatory that the alert policy already exists and that the notification channel has been pre-created. The following screenshot shows the summary configuration of an uptime check where the target resource type is a URL:</p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/B15587_10_08.jpg" alt="Figure 10.8 – Uptime checking a URL as the target resource type&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.8 – Uptime checking a URL as the target resource type</p>
			<p>The URL being used in the <a id="_idIndexMarker1202"/>preceding screenshot is the URL of the LoadBalancer service, <strong class="source-inline">hello-world-service</strong>, that was created as part of <a href="B15587_08_Final_ASB_TD_ePub.xhtml#_idTextAnchor182"><em class="italic">Chapter 8</em></a>, <em class="italic">Understanding GKE Essentials to Deploy Containerized Applications</em>. A configured <a id="_idIndexMarker1203"/>uptime check can result in a failure. The upcoming subsection lists the potential reasons for uptime check failures.</p>
			<h3>Potential reasons for uptime check failures</h3>
			<p>The following are some <a id="_idIndexMarker1204"/>potential reasons for uptime checks failing:</p>
			<ul>
				<li><strong class="bold">Connection errors</strong>: The hostname/service not found or responding, or the specified port is not open or valid.</li>
				<li><strong class="bold">40x Client Errors</strong>: Includes <strong class="source-inline">403</strong> (Forbidden Service), <strong class="source-inline">404</strong> (Incorrect Path), and <strong class="source-inline">408</strong> (port number is incorrect or service is not running).</li>
				<li><strong class="bold">Firewall rules are not configured</strong>: If the resource being monitored by an uptime check is not publicly available, then a firewall rule needs to be configured to allow incoming traffic from uptime check servers.<p class="callout-heading">Tip – How to identify an uptime check against service logs</p><p class="callout">Look for two specific fields in the logs: Ip field and User-agent. Ip field contains one or more addresses that are used by the uptime check server. User-agent will include some text stating <strong class="source-inline">GoogleStackdriverMonitoring-UptimeChecks</strong>.</p></li>
			</ul>
			<p>This concludes our detailed overview of uptime checks. The next topic deep dives into alerting – a Cloud Monitoring option that's key for Incident Management. Alerting provides options for reporting on monitored metrics and providing notifications appropriately.</p>
			<h2 id="_idParaDest-224"><a id="_idTextAnchor224"/>Alerting</h2>
			<p><strong class="bold">Alerting</strong> is the process of <a id="_idIndexMarker1205"/>processing the alerting rules, which track the SLOs and notify or perform certain actions when the rules are violated. <a href="B15587_03_Final_ASB_ePub.xhtml#_idTextAnchor064"><em class="italic">Chapter 3</em></a>, <em class="italic">Understanding Monitoring and Alerting to Target Reliability</em>, deep dived into alerting, described <a id="_idIndexMarker1206"/>how alerting allows us to convert SLOs into actionable alerts, discussed key alerting attributes, and elaborated on alerting strategies. The alerting UI in Cloud Monitoring hosts information with respect to incidents currently being fired, incidents being acknowledged, active alerting policies that have been configured, details of open and closed incidents, and all the incidents tied to the events. In addition, alerting allows us to create an alert policy and configure notification channels.</p>
			<h3>Configuring an alert policy</h3>
			<p>The steps involved in <a id="_idIndexMarker1207"/>configuring an alert policy are very similar to the ones for creating a chart using Metrics explorer. Essentially, an alert needs to be created against a metric. Configuring an alert includes adding a metric condition through Metrics explorer and setting a metric threshold condition.</p>
			<p>A metric threshold condition will define the specific value. If the specific metric value falls above or below the threshold value (based on how the policy is defined), an alert will be triggered, and we will be notified through the configured notification channels. If the policy is defined through the console, then the policy trigger field is used, while if the policy is defined through the API, then the combiner field is used.</p>
			<p>Alternatively, to define an alert policy based on a metric threshold condition, you can define an alert policy based on a metric absence condition. A metric absence condition is defined as a condition where time series data doesn't exist for a metric for a specific duration of time.</p>
			<p class="callout-heading">Important note – The alignment period is a lookback interval</p>
			<p class="callout">The alignment period is a lookback interval from a particular point in time. For example, if the alignment period is 5 minutes, then at 1:00 P.M., the alignment period contains the samples received between 12:55 P.M. and 1:00 P.M. At 1:01 P.M., the alignment period slides 1 minute and contains the samples received between 12:56 P.M. and 1:01 P.M.</p>
			<p>The next section describes the available notification channels that are used to send information that's specific to firing alerts.</p>
			<h3>Configuring notification channels</h3>
			<p>If an alerting policy violates the <a id="_idIndexMarker1208"/>specified condition, then an incident gets created with an Open status. Information about the incident can be sent to one or more notification channels. On receipt of the notification, the operations team can acknowledge the incident through the console. This changes the status of the incident to Acknowledged. This is an indication that the event is being inspected. The incident eventually goes to Closed status if either the conditions are no longer being violated, or no data is received for the specific incident over the course of the next 7 days.</p>
			<p>The supported notification channels are as follows:</p>
			<ul>
				<li><strong class="bold">Mobile Devices</strong>: Mobile devices should be registered via the incidents section of the Cloud Console Mobile App.</li>
				<li><strong class="bold">PagerDuty Services</strong>: Requires a service key to authenticate and authorize.</li>
				<li><strong class="bold">PagerDuty Sync</strong>: Requires a subdomain tied to <strong class="source-inline">pagerduty.com</strong> and the respective API key to authenticate and authorize.</li>
				<li><strong class="bold">Slack</strong>: Prompts the user to authenticate and authorize to a Slack channel through a custom URL, and then prompts the user to provide the Slack channel's name.</li>
				<li><strong class="bold">Webhooks</strong>: Requires the endpoint URL, along with optional usage of HTTP Basic Auth.</li>
				<li><strong class="bold">Email</strong>: Requires an email address to receive notifications when a new incident is created.</li>
				<li><strong class="bold">SMS</strong>: Requires a phone number to receive notifications.</li>
				<li><strong class="bold">Cloud Pub/Sub</strong>: Must specify a topic name for where the notification should be sent to. The topic should exist upfront.<p class="callout-heading">Tip – Additional configuration to enable alert notifications to cloud Pub/Sub</p><p class="callout">When the first Pub/Sub channel is created to configure alert notifications, Cloud Monitoring will create a service account via the monitoring notification service agent, specifically in the project where the channel was created. This service account will be structured as <strong class="source-inline">service-[PROJECT_NUMBER]@gcp-sa-monitoring-notification.iam.gserviceaccount.com</strong>. The <strong class="source-inline">pubsub.publisher</strong> role should be added to the preceding service account to configure alert notifications via Cloud Pub/Sub.</p></li>
			</ul>
			<p>This concludes this section on <a id="_idIndexMarker1209"/>alerting, where we looked at configuring an alerting policy and notification channels. The next section introduces the Cloud Monitoring agent.</p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor225"/>Monitoring agent</h2>
			<p>Cloud Monitoring <a id="_idIndexMarker1210"/>provides a lot of metrics out of the box, without any additional configuration, such as CPU utilization, network traffic, and more. However, more granular metrics such as memory usage, network traffic, and so on can be collected from unmanaged VMs or from third-party applications using an optional <strong class="bold">Monitoring agent</strong>. The Monitoring agent is based on the <strong class="source-inline">collectd</strong> daemon (daemon refers to a program that runs in the background) to collect system statistics from various sources, including operating systems, applications, logs, and external devices.</p>
			<p>The Monitoring agent can be installed on unmanaged GCE VMs or AWS EC2 VMs. Other Google compute services, such as App Engine, Cloud Run, and Cloud Functions, have built-in support for monitoring and do not require you to explicitly install the Monitoring agent. GKE also has built-in support for monitoring and can be enabled for new or existing clusters via <em class="italic">Cloud Operations for GKE</em>, an integrated monitoring and logging solution.</p>
			<p>Conceptually, you must follow this process to install/configure a Monitoring agent on unmanaged VMs:</p>
			<ul>
				<li>Add the agent's package repository via a provided script that detects the Linux distribution being run on the VM and configures the repository accordingly.</li>
				<li>Install the Monitoring agent using the <strong class="source-inline">stackdriver-agent</strong> agent for the latest version or by using <strong class="source-inline">stackdriver-agent-version-number</strong> for a very specific version.</li>
				<li>Restart the agent for the installed agent to come into effect.</li>
			</ul>
			<p>The step-by-step <a id="_idIndexMarker1211"/>process of installing a Logging agent on a single VM/GCE VM/AWS EC2 instance can be found at <a href="https://cloud.google.com/monitoring/agent/installation">https://cloud.google.com/monitoring/agent/installation</a>. This completes our brief overview of the Monitoring agent. The next subsection mentions the possible access controls with respect to Cloud Monitoring.</p>
			<h2 id="_idParaDest-226"><a id="_idTextAnchor226"/>Cloud Monitoring access controls</h2>
			<p>The following table <a id="_idIndexMarker1212"/>summarizes the critical IAM roles required to access or perform actions on Cloud Monitoring:</p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/B15587_10_Table_01.jpg" alt="Groups – A collection of resources that is defined as a Monitoring Group&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Groups – A collection of resources that is defined as a Monitoring Group</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Cloud Monitoring allows you to create a monitoring group. This is a convenient way to view the list of GCP resources, events, incidents, and visualizations as key metrics from a centralized place. A monitoring group is created by defining one or more criteria either against the name, resource type, tag, security group, cloud project, or region. If multiple criteria are specified, then an OR/AND operator can be specified.</p>
			<p>This concludes our <a id="_idIndexMarker1213"/>deep dive into Cloud Monitoring and its respective constructs, such as Workspace, dashboards, Metrics explorer, uptime checks, alerting policies, and access controls. The next section elaborates on another GCP construct that's part of Cloud Operations and focuses on logging; that is, Cloud Logging.</p>
			<h1 id="_idParaDest-227"><a id="_idTextAnchor227"/>Cloud Logging</h1>
			<p>A log is defined as a <a id="_idIndexMarker1214"/>record of a status or event. Logging essentially describes what happened and provides data so that we can investigate an issue. It is critical to be able to read and parse logs across a distributed infrastructure involving multiple services and products. <strong class="bold">Cloud Logging</strong> is a GCP service that allows you to store, search, analyze, monitor, and alert others about logging data and events from Google Cloud and AWS, third-party applications, or custom application code. The information in the log entry is structured as a payload. This payload consists of information related to a timestamp, a resource that the log entry applies to, and a log name. The maximum size of a log entry is 256 KB. Each log entry indicates the source of the resource, labels, namespaces, and status codes. Cloud Logging is also the source of input for other Cloud Operations services, such as Cloud Debug and Cloud Error Reporting.</p>
			<p>The following are the key <a id="_idIndexMarker1215"/>features of Cloud Logging:</p>
			<ul>
				<li><strong class="bold">Audit Logs</strong>: Logs are captured and categorized as Admin Activity, Data Access, System Event, and Access Transparency Logs, with each category having a default retention period.</li>
				<li><strong class="bold">Logs Ingestion</strong>: Logs can be ingested from many sources, including GCP services and on-premises or external cloud providers, by using the Cloud Logging API or through logging agents.</li>
				<li><strong class="bold">Logs Explorer</strong>: Logs can be searched for and analyzed through a guided filter configuration or flexible query language, resulting in effective visualization. Results can also be saved in JSON format.</li>
				<li><strong class="bold">Logs-based Metrics</strong>: Metrics can be created from log data and can be added to charts/dashboards using the Metrics explorer.</li>
				<li><strong class="bold">Logs Alerting</strong>: Alerts can be created based on the occurrence of log events and based on the created logs-based metrics.</li>
				<li><strong class="bold">Logs Retention</strong>: Logs can be retained for a custom retention period based on user-defined criteria.</li>
				<li><strong class="bold">Logs Export</strong>: Logs can be exported to Cloud Storage for archival, BigQuery for advanced analytics, Pub/Sub for event-based processing using GCP services, user-defined cloud logging sinks, or to initiate external third-party integrations so that you can export using services such as Splunk.</li>
			</ul>
			<p>Cloud Logging features will be discussed in detailed in the upcoming subsections, starting with Audit Logs.</p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor228"/>Audit Logs</h2>
			<p><strong class="bold">Cloud Audit Logs</strong> is a <a id="_idIndexMarker1216"/>fundamental source for <a id="_idIndexMarker1217"/>finding out about certain parts of a project (<em class="italic">who did what, where, and when?</em>). Cloud Audit Logs maintains logs for each project (including folder- and organization-level information). Cloud Audit Logs can be categorized into various categories. Let's take a look.</p>
			<h3>Admin activity logs</h3>
			<p><strong class="bold">Admin activity logs</strong> are specific to <a id="_idIndexMarker1218"/>any administrative actions that <a id="_idIndexMarker1219"/>modify the configuration or metadata of resources. Examples for admin activity logs include, but not are limited to, the following:</p>
			<ul>
				<li>Setting or changing permissions of a cloud storage bucket</li>
				<li>Assigning /unassigning IAM roles</li>
				<li>Changing any properties of a resource, such as tags/labels</li>
				<li>Creating/deleting resources for GCE, GKE, or Cloud Storage</li>
			</ul>
			<p>The following screenshot shows the admin activity logs for when a GCE VM or bucket was created. The easiest way to access these activity logs is from the <strong class="bold">Activity</strong> tab on the GCP console home page. This pulls a live feed of the admin activity logs but does not include data access logs by default:</p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/B15587_10_09.jpg" alt="Figure 10.9 – Admin activity logs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.9 – Admin activity logs</p>
			<p>The next subsection provides an overview of an audit log category specific to <em class="italic">data access</em>.</p>
			<h3>Data access logs</h3>
			<p><strong class="bold">Data access logs</strong> are <a id="_idIndexMarker1220"/>useful for <a id="_idIndexMarker1221"/>reading the configuration or metadata of resources. This also includes user-level API calls, which read or write resource data. Data access audit logs need to be enabled explicitly (except for Big Query), and this can be controlled by specifying the services whose audit logs should be captured. In addition, data access logs tied to actions performed by a specific set of users or groups can be exempted, thus providing granular control. Data access logs can be further <a id="_idIndexMarker1222"/>classified into three subtypes:</p>
			<ul>
				<li><strong class="bold">Admin read</strong>: Read attempts on <a id="_idIndexMarker1223"/>service metadata or configuration data. An example of this is listing the available buckets or listing the nodes within a cluster.</li>
				<li><strong class="bold">Data read</strong>: Read attempts of <a id="_idIndexMarker1224"/>data within a service. An example of this includes listing the objects within a bucket.</li>
				<li><strong class="bold">Data write</strong>: Write attempts of <a id="_idIndexMarker1225"/>data to a service. An example of this includes creating an object within a bucket.</li>
			</ul>
			<p>The following is a screenshot of granular data access being configured for an individual GCP service from IAM – the <strong class="bold">Audit Logs</strong> UI:</p>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="image/B15587_10_10.jpg" alt="Figure 10.10 – Configuring IAM Audit Logs for a service&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.10 – Configuring IAM Audit Logs for a service</p>
			<p>The preceding screenshot also shows the option to configure <strong class="bold">Exempted Users</strong>. This option allows you to exempt audit logs from being generated for certain users, as configured. Data access logs can be viewed either through the <strong class="bold">Activity</strong> tab of the GCP home page, where <strong class="bold">Activity Type</strong> is <strong class="bold">Data Access</strong>, or through the <strong class="bold">Logs Explorer</strong> UI (discussed later). The next subsection provides an overview of an audit log category specific to system events.</p>
			<h3>System event logs</h3>
			<p><strong class="bold">System event logs</strong> are used when <a id="_idIndexMarker1226"/>changes have been made to <a id="_idIndexMarker1227"/>resources by Google systems or services. They are not specific to user actions on the resources. Examples of system event logs include, but are not limited to, the following:</p>
			<ul>
				<li>Automatically restarting or resetting Compute Engine</li>
				<li>System maintenance operations, such as migration events, which are performed by Compute Engine to migrate applications to a different host</li>
			</ul>
			<p>The next subsection provides an overview on an audit log category specific to access transparency.</p>
			<h3>Access transparency logs</h3>
			<p><strong class="bold">Access transparency logs</strong> are used by <a id="_idIndexMarker1228"/>Google personnel when they're accessing a user's/customer's content. This situation typically arises when <a id="_idIndexMarker1229"/>Google's support team is working on a customer issue (such as a specific service not working as expected or an outage) and, as a result, needs to access the customer's project. This category of logs is critical if you wish to follow legal and regulatory obligations. In addition, you can trace events to look back on the actions that have been performed by Google support personnel. Access transparency logs can be enabled by contacting Google support and are available for customer support levels, excluding individual accounts. An example of access transparency logs could be the logs that are accessed by the support personnel while trying to resolve a support issue for a VM instance.</p>
			<h3>Policy denied logs</h3>
			<p><strong class="bold">Policy denied logs</strong> are logs that are <a id="_idIndexMarker1230"/>captured when access is <a id="_idIndexMarker1231"/>denied by a Google Cloud service to either a user or service account. Policy denied logs can be excluded from ingestion into Cloud Logging through Logs Exclusions.</p>
			<p>This completes this section on audit logs, where we provided an overview of the various subcategories. Before proceeding to the next section, take a look at the following table, which lists the IAM roles specific to accessing logs:</p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/B15587_10_Table_02.jpg" alt=""/>
				</div>
			</div>
			<p>The next section will <a id="_idIndexMarker1232"/>explain how logs are ingested into <a id="_idIndexMarker1233"/>Cloud Logging from multiple sources.</p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor229"/>Logs ingestion, routing, and exporting</h2>
			<p>Cloud Logging supports <a id="_idIndexMarker1234"/>logs ingestion <a id="_idIndexMarker1235"/>from multiple sources, such as audit logs, service logs, application logs, syslogs, and platform logs. These logs are <a id="_idIndexMarker1236"/>sent to the <strong class="bold">Cloud Logging API</strong>. The Cloud Logging API forwards the incoming log entries to a component called <strong class="bold">Logs Router</strong>. Logs Router is <a id="_idIndexMarker1237"/>fundamentally responsible for <a id="_idIndexMarker1238"/>routing logs to their respective destinations.</p>
			<p>These destinations can be grouped into four possible categories. Logs Router will check the incoming logs against existing rules to determine whether to ingest (store), export, or exclude them, and will route the logs to one of the four destination categories.</p>
			<p>These destination categories are as follows:</p>
			<ul>
				<li><strong class="bold">_Required log bucket</strong>: This is the primary destination for admin activity, system event, and access transparency logs. There are no charges associated with these logs and this bucket cannot be modified or deleted.</li>
				<li><strong class="bold">_Default log bucket</strong>: This is the primary destination for data access, policy denied, and user-specific logs. There are charges associated with these logs. The bucket cannot be deleted but the <strong class="source-inline">_Default</strong> log sink can be disabled.</li>
				<li><strong class="bold">User-managed log sinks</strong>: A user-managed <strong class="bold">log sink</strong> is an object that holds the filter <a id="_idIndexMarker1239"/>criteria and a destination. The destination could either be Cloud Storage, Cloud Logging, BigQuery, Pub/Sub, or Splunk. The user-managed log sink is the destination for incoming logs from the Cloud Logging API that satisfy the filter criteria <a id="_idIndexMarker1240"/>defined against the sink. These are also known as <strong class="bold">Inclusion Filters</strong>. This applies to logs that fall under the <strong class="source-inline">_Required</strong> log bucket and the <strong class="source-inline">_Default</strong> log bucket. The process of writing logs to <a id="_idIndexMarker1241"/>user-managed log sinks can also be <a id="_idIndexMarker1242"/>characterized as <strong class="bold">Log Exports</strong> (if the intent is to export for external processing) or <strong class="bold">Log Retention</strong> (if the intent is to export to retain logs for a longer <a id="_idIndexMarker1243"/>period from a compliance perspective).</li>
				<li><strong class="bold">Exclusions</strong>: Exclusions are governed by log exclusion filters. They only apply to entries that qualify for the <strong class="source-inline">_Default</strong> log bucket. In other words, logs that qualify for the <strong class="source-inline">_Required</strong> log bucket can never be excluded. If any of the log exclusion filters match with entries that qualify for the <strong class="source-inline">_Default</strong> log bucket, then the entries will be excluded and never be saved.<p class="callout-heading">Tip – What are log buckets?</p><p class="callout">Log buckets are a <a id="_idIndexMarker1244"/>form of object storage in Google Cloud projects that are used by Cloud Logging to store and organize logs data. All logs generated in the project are stored in these logs' buckets. Cloud Logging automatically creates two buckets in each project: <strong class="source-inline">_Required</strong> and <strong class="source-inline">_Default</strong>. <strong class="source-inline">_Required</strong> represents an audit bucket, which has a 400-day retention period, while <strong class="source-inline">_Default</strong> represents the <em class="italic">everything else</em> bucket, which has a 30-day retention period. In addition, a user can create custom logging buckets, also known as user-managed log sinks. </p><p class="callout">The log bucket per project can be <a id="_idIndexMarker1245"/>viewed via the <strong class="bold">Logs Storage UI</strong> in Cloud Logging. Additional actions such as creating a user-defined log bucket and a usage alert can also be initiated from the Logs Storage UI.</p></li>
			</ul>
			<p>The following diagram illustrates how logs are ingested from multiple sources via the Cloud Logging API and, subsequently, routed by Logs Router to possible destinations:</p>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/B15587_10_11.jpg" alt="Figure 10.11 – Illustrating logs ingesting and logs routing&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.11 – Illustrating logs ingesting and logs routing</p>
			<p>There are three steps we <a id="_idIndexMarker1246"/>must follow to export logs:</p>
			<ol>
				<li value="1">Create a sink.</li>
				<li>Create a filter that represents the criteria to identify logs to export.</li>
				<li>Create a destination – Cloud Storage Bucket, BigQuery, or Pub/Sub topics.<p class="callout-heading">IAM roles to Create/Modify/View a Sink</p><p class="callout">The Owner or Logging/Logs Configuration Writer role is required to create or modify a sink. The Viewer or Logging/Logs Viewer role is required to view existing sinks. The Project Editor role does not have access to create/edit sinks.</p></li>
			</ol>
			<p>To summarize, logs can originate from multiple sources, such as on-premises, Google Cloud, or a third-party cloud service. These logs are injected into Cloud Logging through the Cloud Logging API, which are then sent to Logs Router. Logs Router, which is based on configured filters, will route the logs to logged sinks (the <strong class="source-inline">_Required</strong> or <strong class="source-inline">_Default</strong> log bucket). Additionally, a copy of the logs can be sent to user-managed sinks based on the configured filter criteria, where the destination can either be Cloud Storage, BigQuery, or Pub/Sub. Log export <a id="_idIndexMarker1247"/>can be used for multiple purposes, such as long-term retention for compliance reasons (using Cloud Storage), Big Data analysis (using BigQuery), or to <a id="_idIndexMarker1248"/>stream to other applications (using Pub/Sub). If these logs are <a id="_idIndexMarker1249"/>sent to the Pub/Sub messaging service, then they can be exported outside Google Cloud to third-party tools such as Splunk, Elastic Stack, or SumoLogic. It is important to note that configured log sinks for export will only capture new logs, since the export was created but does not capture the previous logs or backfill.</p>
			<p class="callout-heading">How to export logs across folders/organizations</p>
			<p class="callout">Logs can be exported from all projects inside a specific folder or organization. This can currently only be done through the command line using the gcloud logging sink's <strong class="source-inline">create</strong> command. Apart from the sink's name, destination, and log filter, the command should include the <strong class="source-inline">--include-children</strong> flag and either the <strong class="source-inline">--folder</strong> or <strong class="source-inline">--organization</strong> attribute, along with its respective values.</p>
			<p>This completes this subsection on logs ingestion, routing, and exporting. The following subsection summarizes log characteristics across log buckets in the form of a table for ease of understanding.</p>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor230"/>Summarizing log characteristics across log buckets</h2>
			<p>Each log type is <a id="_idIndexMarker1250"/>destined to a specific Cloud Logging bucket. In addition, every log type has specific characteristics in terms of the minimum IAM roles required to access the logs, the default retention period, and the ability to configure a <a id="_idIndexMarker1251"/>custom retention period. The following table details the respective information:</p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="image/B15587_10_Table_03.jpg" alt=""/>
				</div>
			</div>
			<p class="callout-heading">All other logs</p>
			<p class="callout">This refers to either user logs generated by applications through a Logging agent or platform logs generated by GCP services or VPC Flow Logs or Firewall Logs.</p>
			<p>In addition to the preceding table, it is important to note the following:</p>
			<ul>
				<li>System event logs are system initiated, whereas admin activity, data access, and access transparency logs are user initiated.</li>
				<li>Admin activity and system event logs record the changes in the configuration of resources, whereas data access logs record the changes that were made inside the record.</li>
				<li>Admin activity, system event, and access transparency logs are always enabled.</li>
			</ul>
			<p>This completes our <a id="_idIndexMarker1252"/>overview on logs ingestion. The next topic focuses on the <strong class="bold">Logs Explorer</strong> UI, through which users can explore ingested logs.</p>
			<h2 id="_idParaDest-231"><a id="_idTextAnchor231"/>Logs Explorer UI</h2>
			<p><strong class="bold">Logs Explorer UI</strong> is the centralized way to <a id="_idIndexMarker1253"/>view logs that have been ingested into <a id="_idIndexMarker1254"/>Cloud Logging via the Cloud Logging API, and ultimately routed via Cloud Router to either Cloud Logging buckets or user-managed sinks. The UI allows us to filter logs by writing advanced search queries, visualize the time series data by configuring time windows, and perform critical actions to create log-based metrics or create users. The UI consists of multiple options and sections, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/B15587_10_12.jpg" alt="Figure 10.12 – Logs Explorer UI&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.12 – Logs Explorer UI</p>
			<p>To filter Cloud Audit Logs through the Logs Explorer UI, select the following options for the <strong class="bold">Log Name</strong> field:</p>
			<ul>
				<li><strong class="bold">Admin Activity</strong>: <strong class="source-inline">cloudaudit.googleapis.com%2Factivity</strong></li>
				<li><strong class="bold">Data Access</strong>: <strong class="source-inline">cloudaudit.googleapis.com%2Fdata_access</strong></li>
				<li><strong class="bold">System Event</strong>: <strong class="source-inline">cloudaudit.googleapis.com%2Fsystem_event</strong></li>
			</ul>
			<p>Let's take a look at some key <a id="_idIndexMarker1255"/>important information with <a id="_idIndexMarker1256"/>respect to navigating the options in the Logs Explorer UI.</p>
			<h3>Query builder</h3>
			<p>This section constructs <a id="_idIndexMarker1257"/>queries to filter logs. Queries can be expressed in query builder language by choosing an appropriate combination of field and value, as shown in the following screenshot. The user can provide input in two ways:</p>
			<ul>
				<li>By choosing options from the available drop-down menus with respect to <strong class="bold">Resource</strong>, <strong class="bold">Log Name</strong>, and <strong class="bold">Severity</strong>. This is the basic query interface.</li>
				<li>By choosing fields from the <strong class="bold">Log Fields</strong> section, starting by either selecting the <strong class="bold">Resource</strong> type or the <strong class="bold">Severity</strong> type. This is the advanced query interface:</li>
			</ul>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="image/B15587_10_13.jpg" alt="Figure 10.13 – Query builder section under Logs Explorer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.13 – Query builder section under Logs Explorer</p>
			<h3>Query results</h3>
			<p>This section displays the <a id="_idIndexMarker1258"/>results that match the filter criteria defined within query builder. If there is a match, the results are displayed in one or more rows. Each row represents a log entry, as shown here:</p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/B15587_10_14.jpg" alt="Figure 10.14 – Query results section&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.14 – Query results section</p>
			<h3>Log entries</h3>
			<p>Each query result that's returned is a <a id="_idIndexMarker1259"/>log entry that's displayed with a timestamp and summary text information. When expanded, the log entry displays further details in a JSON payload format. The JSON payload has multiple fields and can be elaborated on using the <strong class="bold">Expand nested fields</strong> option. Additionally, the user can copy the payload to a clipboard or share the specific payload by copying the shareable link, as shown here:</p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/B15587_10_15.jpg" alt="Figure 10.15 – Viewing the JSON payload for a log entry&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.15 – Viewing the JSON payload for a log entry</p>
			<h3>Payload-specific actions</h3>
			<p>There are multiple options that <a id="_idIndexMarker1260"/>perform actions on a specific payload on a specific field, as shown in the following screenshot. These are as follows:</p>
			<ul>
				<li><strong class="bold">Show matching entries</strong>: Adds the selected key-value pair from the JSON payload to the existing filter criteria and shows matching entries within the configured time window.</li>
				<li><strong class="bold">Hide matching entries</strong>: Adds the selected key-value pair from the JSON payload to the existing filter criteria in a negation form and removes the matching entries from user display, within the configured time window.</li>
				<li><strong class="bold">Add field to summary line</strong>: Adds the selected key to the summary section:</li>
			</ul>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/B15587_10_16.jpg" alt="Figure 10.16 – Possible payload-specific actions for a specific field&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.16 – Possible payload-specific actions for a specific field</p>
			<h3>Page layout</h3>
			<p>This option allows users to <a id="_idIndexMarker1261"/>configure the page layout and optionally include <strong class="bold">Log Fields</strong> and/or a <strong class="bold">Histogram</strong>. <strong class="bold">Query builder</strong> and <strong class="bold">Query results</strong> are mandatory sections and cannot be excluded:</p>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<img src="image/B15587_10_17.jpg" alt="Figure 10.17 – Options under the PAGE LAYOUT section&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.17 – Options under the PAGE LAYOUT section</p>
			<h3>Actions (to perform on a query filter)</h3>
			<p>Actions <a id="_idIndexMarker1262"/>allows user to operate on the potential results from the query filter definition. This includes <strong class="bold">Create Metrics</strong>, <strong class="bold">Download Logs</strong>, and <strong class="bold">Create Sinks</strong>:</p>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="image/B15587_10_18.jpg" alt="Figure 10.18 – Possible actions you can perform on a query filter&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.18 – Possible actions you can perform on a query filter</p>
			<p>This completes this section on Logs Explorer and all the possible UI options for filtering and analyzing logs. The next section provides an overview of <em class="italic">logs-based metrics</em>.</p>
			<h2 id="_idParaDest-232"><a id="_idTextAnchor232"/>Logs-based metrics</h2>
			<p><strong class="bold">Logs-based metrics</strong> are <a id="_idIndexMarker1263"/>Cloud Monitoring metrics <a id="_idIndexMarker1264"/>that are created based on the content of the log entries. They can be extracted from both included and excluded logs. As matching log entries are found, the information that's tied to the metrics is built over time. This forms the required time series data that is critical to metrics. Logs-based metrics are used in creating Cloud Monitoring charts and can also be added to Cloud Monitoring dashboards.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">To use logs-based metrics, a Google Cloud project is required with billing enabled. In addition, logs-based metrics are recorded for matching log entries, once the metric has been created. Metrics are not backfilled for log entries that are already in Cloud Logging.</p>
			<p>Logs-based metrics <a id="_idIndexMarker1265"/>can be <a id="_idIndexMarker1266"/>classified as either of the following:</p>
			<ul>
				<li>System (logs-based) metrics</li>
				<li>User-defined (logs-based) metrics</li>
			</ul>
			<p>Both of these logs-based metrics will be covered in the upcoming subsections.</p>
			<h3>System (logs-based) metrics</h3>
			<p><strong class="bold">System (logs-based) metrics</strong> are out-of-the-box, predefined <a id="_idIndexMarker1267"/>metrics from Google and are very specific to the current project. These metrics <a id="_idIndexMarker1268"/>record the number of events that occur within a specific period. A list of available system (logs-based) metrics can be found under the <strong class="bold">Logs-based Metrics</strong> UI in the <strong class="bold">Logging</strong> section of Cloud Operations. Examples include the following:</p>
			<ul>
				<li><strong class="bold">byte_count</strong>: Represents the total number of received bytes in log entries</li>
				<li><strong class="bold">excluded_byte_count</strong>: Represents the total number of excluded bytes from log entries</li>
			</ul>
			<p>The user can create an alert from a predefined metric or view the details of the metric, along with its current values, in Metrics explorer:</p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/B15587_10_19.jpg" alt="Figure 10.19 – System (logs-based) metrics and their qualifying actions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.19 – System (logs-based) metrics and their qualifying actions</p>
			<p>The next section provides an <a id="_idIndexMarker1269"/>overview of <a id="_idIndexMarker1270"/>user-defined metrics.</p>
			<h3>User-defined (logs-based) metrics</h3>
			<p><strong class="bold">User-defined (logs-based) metrics</strong>, as the <a id="_idIndexMarker1271"/>name suggests, are <a id="_idIndexMarker1272"/>defined by the user and are specific to the project where the user configures these metrics. These metrics can either of the <strong class="bold">Counter</strong> or <strong class="bold">Distribution</strong> type:</p>
			<ul>
				<li><strong class="bold">Counter</strong>: Counts the number of log entries that match on a query</li>
				<li><strong class="bold">Distribution</strong>: Accumulates numeric data from log entries that match on a query</li>
			</ul>
			<p>Users can create a user-defined metric either from <strong class="bold">Logs-based Metrics UI</strong> via the <strong class="bold">Create Metric</strong> action or the <strong class="bold">Logs Explorer UI</strong> via the actions menu above the query results. Once the user initiates these actions, they get to choose the type of metric in the Metric Editor panel; that is, <strong class="bold">Counter</strong> or <strong class="bold">Distribution</strong>.</p>
			<p>In addition, the user will have to configure fields such as the metric's name, description, and any optional labels and units. For a <strong class="bold">Counter,</strong> the units should be left blank. For a <strong class="bold">Distribution</strong>, the units should be <strong class="source-inline">s</strong>, <strong class="source-inline">ms</strong>, and so on:</p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/B15587_10_20.jpg" alt="Figure 10.20 – Creating a logs-based metric&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.20 – Creating a logs-based metric</p>
			<p>More details on <a id="_idIndexMarker1273"/>creating a <a id="_idIndexMarker1274"/>distribution metric can be found at <a href="https://cloud.google.com/logging/docs/logs-based-metrics/distribution-metrics">https://cloud.google.com/logging/docs/logs-based-metrics/distribution-metrics</a>.</p>
			<h3>Access control for logs-based metrics</h3>
			<p>The following table <a id="_idIndexMarker1275"/>displays the critical IAM roles required, along with their minimal permissions (in accordance with the principle of least privilege), to access or perform actions related to logs-based metrics:</p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/B15587_10_Table_04.jpg" alt=""/>
				</div>
			</div>
			<p>The following section will conclude this section on Cloud Logging by exploring the available network-based log types on Google Cloud.</p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor233"/>Network-based log types</h2>
			<p>There are <a id="_idIndexMarker1276"/>two network-based log types <a id="_idIndexMarker1277"/>that primarily capture logs related to network interactions. These are as follows:</p>
			<ul>
				<li>VPC Flow Logs</li>
				<li>Firewall logs</li>
			</ul>
			<p>Let's look at them in detail.</p>
			<h3>VPC Flow Logs</h3>
			<p><strong class="bold">VPC Flow Logs</strong> capture real-time network <a id="_idIndexMarker1278"/>activity (incoming/outgoing) against VPC resources on an enabled <a id="_idIndexMarker1279"/>subnet. Flow logs capture activity specific to the TCP/UDP protocols and are enabled at a VPC subnet level. Flow logs generate a large amount of chargeable log files, but they don't capture 100% of traffic; instead, traffic is sampled at 1 out of 10 packets and cannot be adjusted. Flow logs are used for Network Monitoring – to understand traffic growth from a forecasting capacity and for forensics – to evaluate network traffic (in/out) in terms of traffic source. Flow logs can be exported for analysis using BigQuery. In the case of a Shared VPC – where multiple service projects connect to a common VPC – flow logs flow into the host project, not the service projects.</p>
			<h3>Firewall logs</h3>
			<p><strong class="bold">Firewall logs</strong> capture the <a id="_idIndexMarker1280"/>effects of a specific firewall rule in <a id="_idIndexMarker1281"/>terms of the traffic that's allowed or denied by that firewall rule. Similar to VPC Flow Logs, firewall logs capture TCP/UDP traffic only and are used for auditing, verifying, and analyzing the effect of the configured rules. Firewall logs can be configured for an individual firewall rule. Firewall rules are applied for the entire VPC and cannot be applied at a specific subnet level like flow logs. Firewall logs attempt to capture every firewall connection attempt on a best effort basis. Firewall logs can also be exported to BigQuery for further analysis.</p>
			<p>Every VPC has a set of hidden implied pre-configured rules, with the lowest priority being <strong class="source-inline">65535</strong>. Firewall rules can have a priority between <strong class="source-inline">0</strong> and <strong class="source-inline">65535</strong> (<strong class="source-inline">0</strong> implies highest, while <strong class="source-inline">65535</strong> implies lowest). These are as follows:</p>
			<ul>
				<li><strong class="source-inline">deny all ingress</strong>: By default, this denies all incoming traffic to the VPC.</li>
				<li><strong class="source-inline">allow all egress</strong>: By default, this allows all outgoing traffic from the VPC.</li>
			</ul>
			<p>However, firewall logs <a id="_idIndexMarker1282"/>cannot be enabled for the <a id="_idIndexMarker1283"/>hidden rules. So, to capture the incoming traffic that is being denied or the outgoing traffic that is being allowed, it is recommended to explicitly configure a firewall rule for the denied/allowed traffic with an appropriate priority and enable firewall logs on that rule.</p>
			<p>This completes this subsection on network-based log types, where we introduced VPC Flow Logs and firewall logs.</p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor234"/>Logging agent</h2>
			<p>The <strong class="bold">Logging agent</strong> is optional and is used to <a id="_idIndexMarker1284"/>capture additional VM logs, such as <strong class="bold">Operating System</strong> (<strong class="bold">OS</strong>) logs (such as Linux syslogs or Windows Event Viewer logs) and logs from third-party applications. The Logging agent is based on <strong class="source-inline">fluentd</strong> – an open source log or data collector. The Logging agent can be installed on unmanaged GCE VMs or AWS EC2 VMs. Other Google Compute services such as App Engine, Cloud Run, and Cloud Functions have built-in support for logging and do not require you to explicitly install the Logging agent. GKE also has built-in support for logging and can be enabled for new or existing clusters by <em class="italic">Cloud Operations for GKE</em>, an integrated monitoring and logging solution.</p>
			<p>To configure the Logging agent, you must configure an additional configuration file, but a single configuration file acts as a catch all for capturing multiple types of logs, including OS logs and third-party application logs such as Apache, MySQL, Nginx, RabbitMQ, and so on. However, there are scenarios where the configuration file of the agent needs to be modified so that we can modify the logs. These are as follows:</p>
			<ul>
				<li>When reformatting log fields, either the order or combine multiple fields into one</li>
				<li>When removing <a id="_idIndexMarker1285"/>any <strong class="bold">Personally Identifiable Information</strong> (<strong class="bold">PII</strong>) or sensitive data</li>
				<li>When modifying records with <strong class="source-inline">fluentd</strong> plugins such as <strong class="source-inline">filter_record_transformer</strong>, a plugin for adding/modifying/deleting fields from logs before they're sent to Cloud Logging</li>
			</ul>
			<p>Conceptually, the <a id="_idIndexMarker1286"/>following is the process of installing/configuring an agent on a GCE VM:</p>
			<ol>
				<li value="1">Add the agent's package repository via a provided script that detects the Linux distribution being run on the VM and configures the repository accordingly.</li>
				<li>Install the Logging agent and install the <strong class="source-inline">google-fluentd-catch-all-config</strong> agent for unstructured logging and the <strong class="source-inline">google-fluentd-catch-all-config-structured</strong> agent for structured logging.</li>
				<li>Restart the agent for the installed agents to come into effect.</li>
			</ol>
			<p>The step-by-step process of installing a Logging agent on a single VM/GCE VM/AWS EC2 instance can be found at <a href="https://cloud.google.com/logging/docs/agent/installation">https://cloud.google.com/logging/docs/agent/installation</a>.</p>
			<p>This completes our high-level overview of logging agents. Subsequently, this also completes the section on Cloud Logging, where we looked at features such as audit log types, logs ingestion, the Logs Explorer UI, logs-based metrics, and access controls. The next section deep dives into <em class="italic">Cloud Debugger</em>, a GCP construct from Cloud Operations that can potentially inspect a production application by taking a snapshot of it, without stopping or slowing down.</p>
			<h1 id="_idParaDest-235"><a id="_idTextAnchor235"/>Cloud Debugger</h1>
			<p><strong class="bold">Cloud Debugger</strong> allows us to inspect the <a id="_idIndexMarker1287"/>state of a running application in real time. Cloud Debugger doesn't require the application to be stopped during this process and doesn't slow it down, either. Users can capture the call stack and variables at any location in the source code. This essentially allows the user to analyze the application state, especially in complex situations, without adding any additional log statements.</p>
			<p>In addition, Cloud Debugger can be used for production environments and is not limited to development or test environments. When Cloud Debugger captures the application state, it adds request latency that is less than 10 ms, which, practically, is not noticeable by users.</p>
			<p>Cloud Debugger is supported on applications running in GCP such as App Engine, Compute Engineer, GKE, Cloud Run, and so on, as well as those written in a number of languages, including Java, Python, Go, Node.js, Ruby, PHP, and .NET. Cloud Debugger needs access to the application code and supports reading the code from App Engine, Google Cloud <a id="_idIndexMarker1288"/>source repositories, or third-party repositories such as GitHub, Bitbucket, and so on.</p>
			<h2 id="_idParaDest-236"><a id="_idTextAnchor236"/>Setting up Cloud Debugger</h2>
			<p>Enabling/setting up <a id="_idIndexMarker1289"/>Cloud Debugger involves the following fundamental steps:</p>
			<ol>
				<li value="1">Enable the Cloud Debugger API as a one-time setup per project.</li>
				<li>Provide appropriate access so that the GCP service where Cloud Debugger will run has permission to upload telemetry data or call Cloud Debugger.</li>
				<li>App Engine and Cloud Run must already be configured for Cloud Debugger.</li>
				<li>A service account with the Cloud Debugger Agent role is required for applications running in Compute Engine, GKW, or external systems.</li>
				<li>If the application is running inside a Compute Engine VM or cluster nodes with a default service account, then the following access scopes should be added to the VMs or cluster nodes: <a href="https://www.googleapis.com/auth/cloud-platform">https://www.googleapis.com/auth/cloud-platform</a> and <a href="https://www.googleapis.com/auth/cloud_debugger">https://www.googleapis.com/auth/cloud_debugger</a>.</li>
				<li>Select the source code location. If there is no access to the source code, a debug snapshot can be taken that captures the call stack and local variables.</li>
				<li>If there is access to the source code, then App Engine standard will select the source code automatically. App Engine flex, GCE, GKE, and Cloud Run can automatically select the source code based on the configuration file in the application root folder; that is, <strong class="source-inline">source-context.json</strong>.</li>
				<li>Alternatively, select a source code location from the possible options, including local files, Cloud Source Repositories, GitHub, Bitbucket, and GitLab.</li>
				<li>To enable Cloud Debugger from application code, you must follow a set of instructions that are specific to the language that the application has been written in. The following is an example snippet:<p class="source-code"><strong class="bold">try:</strong></p><p class="source-code"><strong class="bold">  import googleclouddebugger</strong></p><p class="source-code"><strong class="bold">  googleclouddebugger.enable()</strong></p><p class="source-code"><strong class="bold">except ImportError:</strong></p><p class="source-code"><strong class="bold">  pass</strong></p></li>
			</ol>
			<p>Now that we've set up <a id="_idIndexMarker1290"/>Cloud Debugger, let's learn how to use it.</p>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor237"/>Using Cloud Debugger</h2>
			<p>Using Cloud Debugger involves <a id="_idIndexMarker1291"/>learning about the functionality of debug snapshots, debug logpoints, and accessing the logs panel.</p>
			<h3>Debug snapshots</h3>
			<p><strong class="bold">Snapshots</strong> capture local <a id="_idIndexMarker1292"/>variables and the <a id="_idIndexMarker1293"/>call stack at a specific location in the application's source code. The fundamental step prior to taking a snapshot is to set up a breakpoint. It takes about 40 seconds for a breakpoint to come into effect. Cloud Debugger breakpoints do not stop code execution. A non-intrusive snapshot is taken when the flow of execution passes the debug point. Additional conditions can be added so that a snapshot is only taken if a data condition passes. The captured snapshot will contain details of the local variables and the state of the call stack.</p>
			<p>In the following screenshot, the breakpoint was set to line 39 against a specific file. The breakpoint has a qualifying condition, and a snapshot is taken if its condition is met. The details of the variables are displayed in the <strong class="bold">Variables</strong> section:</p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/B15587_10_21.jpg" alt="Figure 10.21 – Taking a debug snapshot in Cloud Debugger&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.21 – Taking a debug snapshot in Cloud Debugger</p>
			<p>Optionally, expressions <a id="_idIndexMarker1294"/>can also be included while configuring a snapshot. Expressions <a id="_idIndexMarker1295"/>can be used as special variables to evaluate values when a snapshot is taken. These are especially useful in scenarios where the values being captured by the expressions are not usually captured by local variables.</p>
			<p>In the following screenshot, we can see that multiple expressions are defined while configuring a snapshot and are captured while taking a snapshot:</p>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/B15587_10_22.jpg" alt="Figure 10.22 – Defining expressions while configuring a snapshot&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.22 – Defining expressions while configuring a snapshot</p>
			<p>The following are some <a id="_idIndexMarker1296"/>key pointers related to snapshots:</p>
			<ul>
				<li>A snapshot is taken only once. To capture another snapshot of the application data for the same location in the code, the user needs to manually retake the snapshot through the camera icon in the snapshot panel.</li>
				<li>A snapshot location can be manually removed by clicking the <strong class="bold">x</strong> icon on the breakpoint.</li>
				<li>Cloud Debugger <a id="_idIndexMarker1297"/>generates a new URL for every snapshot that's been taken. It is valid for 30 days from the time it was taken. This URL can be shared with other members of the project.</li>
			</ul>
			<p>The next subsection provides an overview of debug logpoints and how they can be injected into a running application.</p>
			<h3>Debug logpoints</h3>
			<p>It's a common practice to add <a id="_idIndexMarker1298"/>log messages when you're trying to <a id="_idIndexMarker1299"/>solve complex problems. In such scenarios, developers often provide code changes to production that essentially include additional log statements that help with analysis. If the problem is complex, this process needs to be repeated multiple times, which means the production code needs to go through multiple changes to include log statements. Cloud Debugger steps away from the traditional approach to debugging an application and instead provides a dynamic way to add log messages using <strong class="bold">debug logpoints</strong>.</p>
			<p>Debug logpoints can inject logs into a <a id="_idIndexMarker1300"/>running application without stopping, editing, or restarting. A logpoint is added at a location of choice, as per the developer's wishes. When that particular portion of code is executed, Cloud Debugger logs a message, and the log message is sent to the appropriate service that is hosting the application. So, if the a<a id="_idIndexMarker1301"/>pplication is hosted in App Engine, then the log message can be found in the logs tied to App Engine.</p>
			<p>In the following screenshot, a logpoint has been added with a condition, with the message log level set to <strong class="bold">Info</strong>. The concept of specifying a condition along with a logpoint is called a <em class="italic">logpoint condition</em>. This is an expression in the application language that must evaluate to true for the logpoint to be logged. Logpoint conditions are evaluated each time that specific line is executed, if the logpoint is valid:</p>
			<div>
				<div id="_idContainer153" class="IMG---Figure">
					<img src="image/B15587_10_23.jpg" alt="Figure 10.23 – Adding a debug logpoint via Cloud Debugger&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.23 – Adding a debug logpoint via Cloud Debugger</p>
			<p>The following are some key pointers related to logpoints:</p>
			<ul>
				<li>A logpoint can be created even if direct access to the source code is not available. A logpoint can be created by specifying the name of the file, the line number to create the logpoint, the log level, an optional condition, and an appropriate message, as shown here:</li>
			</ul>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="image/B15587_10_24.jpg" alt="Figure 10.24 – Configuring a logpoint without access to the source code&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.24 – Configuring a logpoint without access to the source code</p>
			<ul>
				<li>Logpoints becomes inactive after 24 hours and post that, messages with respect to those logpoints will not be <a id="_idIndexMarker1302"/>evaluated or logged.</li>
				<li>Logpoints are automatically <a id="_idIndexMarker1303"/>deleted after 30 days from the time of creation. Optionally, users can manually delete logpoints at will.</li>
			</ul>
			<p>The next subsection illustrates the usage and options available in the <em class="italic">Logs panel</em>.</p>
			<h3>Logs panel</h3>
			<p>Cloud Debugger <a id="_idIndexMarker1304"/>includes <a id="_idIndexMarker1305"/>an in-page Logs panel that displays the running logs of the current application being inspected. This allows the developer to view logs next to the respective code. Users can use the logs panel to perform search variations, including text-based search, and can filter by either log level, request, or file. The results are highlighted in the context or are shown in the logs viewer:</p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/B15587_10_25.jpg" alt="Figure 10.25 – Logs panel for viewing logs while debugging in Cloud Debugger&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.25 – Logs panel for viewing logs while debugging in Cloud Debugger</p>
			<p>The upcoming subsection <a id="_idIndexMarker1306"/>provides an <a id="_idIndexMarker1307"/>overview of the access control that's required for Cloud Debugger.</p>
			<h2 id="_idParaDest-238"><a id="_idTextAnchor238"/>Access control for Cloud Debugger</h2>
			<p>The following table displays the <a id="_idIndexMarker1308"/>critical IAM roles required, along with their minimal permissions (in accordance with the principle of least privilege), to access or perform actions related to Cloud Debugger:</p>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="image/B15587_10_Table_05.jpg" alt=""/>
				</div>
			</div>
			<p class="callout-heading">Tip – How to hide sensitive data while using debugging</p>
			<p class="callout">Cloud Debugger has a feature in Pre-GA where sensitive data can be hidden through a configuration file. This configuration file consists of a list of rules that are either expressed as <strong class="source-inline">blacklist</strong> or <strong class="source-inline">blacklist_exception</strong> (to specify an inverse pattern). If the criteria match, then data is hidden and is reported by the debugger as <strong class="source-inline">blocked by admin</strong>. This feature is currently only supported for applications written in Java.</p>
			<p>This completes this section on <a id="_idIndexMarker1309"/>Cloud Debugger, where we learned how to set up Cloud Debugger, utilize debug logpoints to add log messages, and create snapshots to capture the call stack and its local values. We looked at the options that are available in the Logs panel and looked at the required access controls we can use to perform actions related to Cloud Debugger. In the next section, we will look at <em class="italic">Cloud Trace</em>, another GCP service that is part of Cloud Operations. Cloud Trace represents a distributed tracing system that collects latency data from applications to identify bottlenecks.</p>
			<h1 id="_idParaDest-239"><a id="_idTextAnchor239"/>Cloud Trace</h1>
			<p>A <strong class="bold">trace</strong> is a <a id="_idIndexMarker1310"/>collection of spans. A <strong class="bold">span</strong> is an <a id="_idIndexMarker1311"/>object that wraps latency-specific metrics and other contextual information around a unit of work in an application. <strong class="bold">Cloud Trace</strong> is a distributed tracing <a id="_idIndexMarker1312"/>system that captures latency data from an application, tracks the request's propagation, retrieves real-time performance insights, and displays the results in Google Cloud Console. This latency information can be either for a single request or can be aggregated for the entire application. This information helps us identify performance bottlenecks.</p>
			<p>Additionally, Cloud Trace can automatically analyze application traces that might reflect recent changes to the application's performance, identify degradations from latency reports, capture traces from containers, and create alerts as needed.</p>
			<p>Cloud Trace's language-specific SDKs are available for Java, Node.js, Ruby, and Go. These SDKs can analyze projects running on VMs. It is not necessary for these VMs to only be running on Google Cloud. Apart from the SDK, the Trace API can be used to submit and retrieve trace data from any source. A Zipkin collector is available, which allows Zipkin tracers to submit data to Cloud Trace. Additionally, Cloud Trace can generate trace information using OpenCensus or OpenTelemetry instrumentation. Cloud Trace consists of three main sections: Trace Overview, Trace List, and Analysis Reports. Let's look at them in detail.</p>
			<h2 id="_idParaDest-240"><a id="_idTextAnchor240"/>Trace Overview</h2>
			<p>The <strong class="bold">Trace Overview</strong> page <a id="_idIndexMarker1313"/>provides a <a id="_idIndexMarker1314"/>summary of latency data that's spread across various informational panes:</p>
			<ul>
				<li><strong class="bold">Insights</strong>: Displays a list of performance insights, if applicable</li>
				<li><strong class="bold">Recent Traces</strong>: Highlights the most recent traces for a project</li>
				<li><strong class="bold">Frequent URIs</strong>: Displays a list of URIs along with their average latency for the most frequent requests to the application in the last 7 days</li>
				<li><strong class="bold">Frequent RPCs</strong>: Displays a list of RPCs along with their average latency for the most frequent RPC calls made in the last 7 days</li>
				<li><strong class="bold">Chargeable trace spans</strong>: Summarizes the number of trace spans that have been created and received by Cloud Trace for the current and previous months:</li>
			</ul>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/B15587_10_26.jpg" alt="Figure 10.26 –Chargeable trace spans from the Trace Overview page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.26 –Chargeable trace spans from the Trace Overview page</p>
			<p>The next subsection provides an overview of the <strong class="bold">Trace List</strong> window, which can be used to examine traces in detail.</p>
			<h2 id="_idParaDest-241"><a id="_idTextAnchor241"/>Trace List</h2>
			<p>The <strong class="bold">Trace List</strong> window <a id="_idIndexMarker1315"/>allows <a id="_idIndexMarker1316"/>users to find, filter, and examine individual traces in detail. These traces are displayed in a heatmap, and a specific section of the heatmap can be selected if you wish to view these traces within that specific slice of the window:</p>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/B15587_10_27.jpg" alt="Figure 10.27 – List of all the traces filtered by the POST method in the last 30 days&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.27 – List of all the traces filtered by the POST method in the last 30 days</p>
			<p>Clicking on the individual trace (represented by a circle) provides details about the trace. It is represented by a waterfall graph:</p>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<img src="image/B15587_10_28.jpg" alt="Figure 10.28 – Waterfall graph of an individual trace&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.28 – Waterfall graph of an individual trace</p>
			<p>The next subsection <a id="_idIndexMarker1317"/>provides an <a id="_idIndexMarker1318"/>overview of trace analysis reports with respect to request latency.</p>
			<h2 id="_idParaDest-242"><a id="_idTextAnchor242"/>Analysis Reports</h2>
			<p>Analysis Reports shows an <a id="_idIndexMarker1319"/>overall view of the latency for all the requests or a <a id="_idIndexMarker1320"/>subset of requests with respect to the application. These reports are categorized either as daily reports or custom analysis reports.</p>
			<h3>Daily reports</h3>
			<p>Cloud Trace creates a <a id="_idIndexMarker1321"/>daily report automatically for the top three endpoints. Cloud Trace compares the previous days' performance with the performance from the same day of the previous week. The content of the report cannot be controlled by the user.</p>
			<h3>Custom analysis reports</h3>
			<p>The user can create a <a id="_idIndexMarker1322"/>custom analysis report, where the content of the report can be controlled from the aspect of which traces can be included. The report can include latency data either in histogram format or table format, with links to sample traces. The report can optionally include a bottleneck <a id="_idIndexMarker1323"/>pane that lists <strong class="bold">Remote Procedure Calls</strong> (<strong class="bold">RPCs</strong>), which are significant contributors to latency.</p>
			<p class="callout-heading">Condition to auto-generate or manually create a trace report</p>
			<p class="callout">For the daily report to auto-generate or for a user to create a custom report within a specific time range, it is mandatory that at least 100 traces are available in that time period. Otherwise, a trace report will not be generated.</p>
			<p>This completes this section on Cloud Trace, a GCP construct for representing a distributed tracing system, collecting latency data from applications, and identifying performance bottlenecks. The next section focuses on Cloud Profiler, a service that is part of Cloud Operations. Cloud Profiler is a low-impact production profiling system that presents call hierarchy and resource consumption through an interactive flame graph.</p>
			<h1 id="_idParaDest-243"><a id="_idTextAnchor243"/>Cloud Profiler</h1>
			<p>Cloud Profiler provides low-impact continuous profiling to help users understand the performance of a <a id="_idIndexMarker1324"/>production system. It provides insights into information such as CPU usage, memory consumption, and so on. Cloud Profiler allows developers to analyze applications running either in Google Cloud, other cloud providers, or on-premises.</p>
			<p>Cloud Profiler uses statistical techniques and extremely low-impact instrumentation to provide a complete picture of an application's performance, without slowing it down. Cloud Profiler runs across all production application instances, presents a call hierarchy, and explains the resource consumption of the relevant function in an interactive flame graph. This information is critical for developers to understand which paths consume the most resources and illustrates the different ways in which the code is actually called. The supported programming languages include Java, Go, Node.js, and Python.</p>
			<p>Cloud Profiler supports the <a id="_idIndexMarker1325"/>following types of profiles:</p>
			<ul>
				<li><strong class="bold">CPU time</strong>: The time the CPU spent executing a block of code. This doesn't include the time the CPU was waiting or processing instructions for something else.</li>
				<li><strong class="bold">Heap</strong>: Heap or heap usage is the amount of memory that's allocated to the program's heap when the profile is collected.</li>
				<li><strong class="bold">Allocated heap</strong>: Allocated heap or heap allocation is the total amount of memory that was allocated in the program's heap, including memory that has been freed and is no longer in use.</li>
				<li><strong class="bold">Contention</strong>: Contention provides information about the threads that are stuck and the ones waiting for other threads. Understanding contention behavior is critical to designing code and provides information for performance tuning.</li>
				<li><strong class="bold">Threads</strong>: Information related to threads gives insights into the threads that are created but never actually used. This forms the basis for identifying leaked threads, where the number of threads keeps increasing.</li>
				<li><strong class="bold">Wall time</strong>: Wall time is the <a id="_idIndexMarker1326"/>time it takes to run a block of code, including its wait time. The wall time for a block of code can never be less than the CPU time.</li>
			</ul>
			<p>The following screenshot summarizes the supported profile types by language:</p>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="image/B15587_10_29.jpg" alt="Figure 10.29 – Supported profile types by language&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.29 – Supported profile types by language</p>
			<p>The following screenshot shows the Profiler interface, which depicts a sample interactive flame graph for the <strong class="bold">CPU time</strong> profile type. The profile data is retained for 30 days and the profile information can be downloaded for long-term storage:</p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="image/B15587_10_30.jpg" alt="Figure 10.30 – Interactive flame graph with the profile type set to CPU time&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.30 – Interactive flame graph with the profile type set to CPU time</p>
			<p>The upcoming subsection <a id="_idIndexMarker1327"/>explains the access controls that are required to perform actions with respect to Cloud Profiler.</p>
			<h2 id="_idParaDest-244"><a id="_idTextAnchor244"/>Access control for Cloud Profiler</h2>
			<p>The following table <a id="_idIndexMarker1328"/>displays the critical IAM roles required, along with their minimal permissions (in accordance with the principle of least privilege), to access or perform actions related to Cloud Profiler:</p>
			<div>
				<div id="_idContainer162" class="IMG---Figure">
					<img src="image/B15587_10_Table_06.jpg" alt=""/>
				</div>
			</div>
			<p>This completes this section on Cloud Profiler, where we looked at the supported profile types and learned how to use an interactive flame graph.</p>
			<h1 id="_idParaDest-245"><a id="_idTextAnchor245"/>Binding SRE and Cloud Operations</h1>
			<p><a href="B15587_02_Final_ASB_ePub.xhtml#_idTextAnchor038"><em class="italic">Chapter 2</em></a>, <em class="italic">SRE Technical Practices – Deep Dive</em>, introduced SRE technical practices such as SLAs, SLOs, SLIs, and Error Budgets. To summarize, this chapter established a relationship <a id="_idIndexMarker1329"/>between these practices and tied them directly to the reliability of the service. To ensure that a <a id="_idIndexMarker1330"/>service meets its SLAs, the service needs to be reliable. SRE recommends using SLOs to measure the reliability of the service. SLOs require SLIs to evaluate the service's reliability. If these SLIs are not met, then the SLOs will miss their targets. This will eventually burn the Error Budget, which is a measure that calculates the acceptable level of unavailability or unreliability. <a href="B15587_03_Final_ASB_ePub.xhtml#_idTextAnchor064"><em class="italic">Chapter 3</em></a>, <em class="italic">Understanding Monitoring and Alerting to Target Reliability</em>, introduced concepts related to monitoring, alerting, logging, and tracing and established how these are critical to tracking the reliability of the service. However, both these chapters were conceptual in nature.</p>
			<p>This chapter's focus is Cloud Operations. So far, we've described how Google Cloud captures monitoring metrics, logging information, and traces and allows us to debug applications or services. Additionally, Cloud Operations has an option called SLO monitoring. This option allows you to <a id="_idIndexMarker1331"/>define and track the SLO of a service. This option currently supports three service types for <a id="_idIndexMarker1332"/>auto-ingestion: Anthos Service Mesh, Istio on GKE, and App Engine. However, this option also supports user-defined microservices. The next subsection deep dives into SLO monitoring.</p>
			<h2 id="_idParaDest-246"><a id="_idTextAnchor246"/>SLO monitoring</h2>
			<p>Given that SLOs are <a id="_idIndexMarker1333"/>measured using SLIs and SLOs are defined as quantifiable measures of service reliability that are measured over time, there are three specific steps in defining an SLO via SLO monitoring. These are as follows:</p>
			<ol>
				<li value="1">Setting a <strong class="bold">SLI</strong> </li>
				<li>Defining SLI details</li>
				<li>Setting a <strong class="bold">SLO</strong></li>
			</ol>
			<p>Let's look at these steps in more detail.</p>
			<h3>Setting an SLI</h3>
			<p>This is the <a id="_idIndexMarker1334"/>first step and has two specific goals: choosing a metric as an SLI and selecting a method of evaluation for measuring the chosen metric.</p>
			<h4>Choosing a metric</h4>
			<p>SLO monitoring allows you to <a id="_idIndexMarker1335"/>choose either <strong class="bold">Availability</strong> or <strong class="bold">Latency</strong> as an out-of-the-box SLI for a service that's been configured via Anthos Service Mesh, Istio on GKE, and App Engine. These options are not available for microservices on GKE that haven't been configured through the preceding options. These are also known as custom services. However, irrespective of how the service is configured, you have the option to choose <strong class="bold">Other</strong>. Here, the user can pick the metric of choice to track as the SLI.</p>
			<h4>Request-based or windows-based</h4>
			<p>There are two methods of <a id="_idIndexMarker1336"/>evaluation to choose from that <a id="_idIndexMarker1337"/>will affect how compliance against SLIs is measured. These are request-based and windows-based. The request-based option counts individual events and evaluates how a service performs over the compliance period, irrespective of how load is distributed. The windows-based option, on the other hand, measures performance in terms of time (good minutes versus bad minutes), irrespective of how load is distributed.</p>
			<h3>Defining SLI details</h3>
			<p>This is the second step and provides <a id="_idIndexMarker1338"/>options for the user to choose a performance metric. The user can either use the predefined metrics in Cloud Monitoring or any user-defined metrics that can be created from logs (through logs-based metrics). Once a metric has been chosen, the performance criteria for the metric need to be defined. The performance criteria for metrics related to services on Anthos Service Mesh, Istio on GKE, and App Engine are predefined. However, for custom services, this needs to be manually defined by the user by using two of the three filter options – <strong class="bold">Good</strong>, <strong class="bold">Bad</strong>, and <strong class="bold">Total</strong>.</p>
			<h3>Setting an SLO</h3>
			<p>This is the third and final step and has <a id="_idIndexMarker1339"/>two specific goals: setting the compliance period and setting the performance goal.</p>
			<h4>Compliance period</h4>
			<p>The compliance period <a id="_idIndexMarker1340"/>option allows you to set a time period to evaluate the SLO. There are two possible choices:</p>
			<ul>
				<li><strong class="bold">Calendar</strong>: Performance is measured from the start of the period, with a hard reset at the start of every new period. The available options for period length are Calendar day, Calendar week, Calendar fortnight, and Calendar month.</li>
				<li><strong class="bold">Rolling</strong>: Performance is measured for a fixed time period; say, the last 10 days. The user can specify the fixed time period in days.</li>
			</ul>
			<p>Now, let's look at setting the performance goal.</p>
			<h4>Performance goal</h4>
			<p>The performance goal indicates the <a id="_idIndexMarker1341"/>goal that's been set as a ratio of <em class="italic">good service</em> to <em class="italic">demanded service</em> over the compliance period. This goal can be refined as more information is known about the system's behavior.</p>
			<p>This completes our overview of SLO monitoring, which we can use to define an SLO to measure the reliability of a service. The next subsection provides a hands-on demonstration of how SLO monitoring can be configured against a GKE service (that we previously created in <a href="B15587_08_Final_ASB_TD_ePub.xhtml#_idTextAnchor182"><em class="italic">Chapter 8</em></a>, <em class="italic">Understanding GKE Essentials to Deploy Containerized Applications</em>).</p>
			<h2 id="_idParaDest-247"><a id="_idTextAnchor247"/>Hands-on lab – tracking service reliability using SLO monitoring</h2>
			<p>SLO monitoring allows us to <a id="_idIndexMarker1342"/>link the SRE technical practices with the practical options available in Google Cloud. These help us monitor the reliability of the <a id="_idIndexMarker1343"/>service and alert the on-call engineer if the service misses the reliability threshold.</p>
			<p>This subsection is a hands-on lab that will show you how to use the SLO monitoring option from Cloud Monitoring. The SLO monitoring option tracks service reliability by defining an SLO. In this lab, we will use <strong class="source-inline">hello-world-service</strong> from the <strong class="source-inline">my-first-cluster</strong> GKE cluster, which was created as part of <a href="B15587_08_Final_ASB_TD_ePub.xhtml#_idTextAnchor182"><em class="italic">Chapter 8</em></a>, <em class="italic">Understanding GKE Essentials to Deploy Containerized Applications</em>. This lab has three main goals:</p>
			<ul>
				<li>Defining an SLO for a service</li>
				<li>Creating an SLO burn rate alert policy</li>
				<li>Verifying SLO monitoring</li>
			</ul>
			<p>Let's take a look at these goals in more detail.</p>
			<h3>Defining a SLO for a service</h3>
			<p>Follow these steps to <a id="_idIndexMarker1344"/>define a SLO for <strong class="source-inline">hello-world-service</strong>:</p>
			<ol>
				<li value="1">Navigate to the <strong class="bold">Services</strong> UI under the <strong class="bold">Monitoring</strong> section of the GCP console. Select the <strong class="bold">Define Service</strong> action. Then, select the <strong class="source-inline">hello-world-service</strong> section from the <strong class="source-inline">my-first-cluster</strong> cluster, as shown in the following screenshot. Set the display name to <strong class="source-inline">hello-world-service</strong>. The system will create the service to be monitored and will navigate the user to the service overview dashboard:<div id="_idContainer163" class="IMG---Figure"><img src="image/B15587_10_31.jpg" alt="Figure 10.31 – Defining a custom service by selecting one&#13;&#10;"/></div><p class="figure-caption">Figure 10.31 – Defining a custom service by selecting one</p></li>
				<li>Select the <strong class="bold">Create SLO</strong> action to define an SLO. This action will open a pop-up window, as shown in the following screenshot. Note that, as discussed in <a href="B15587_02_Final_ASB_ePub.xhtml#_idTextAnchor038"><em class="italic">Chapter 2</em></a>, <em class="italic">SRE Technical Practices – Deep Dive</em>, an SLO requires an SLI. So, to define an SLO, we must first choose the SLI metric and then define it.</li>
				<li>Given that the service being used for this lab is not part of Anthos Service Mesh, Istio on GKE, or App Engine, the only option available is to choose <strong class="bold">Other</strong>. Here, the user can configure a metric of choice to measure the performance of the service. In addition, set the method of evaluation to <strong class="bold">Request-based</strong>:<div id="_idContainer164" class="IMG---Figure"><img src="image/B15587_10_32.jpg" alt="Figure 10.32 – Setting an SLI as part of SLO monitoring&#13;&#10;"/></div><p class="figure-caption">Figure 10.32 – Setting an SLI as part of SLO monitoring</p></li>
				<li>To define the SLI's details, select a <a id="_idIndexMarker1345"/>performance metric. In this case, we will select the <strong class="source-inline">kubernetes.io/container/restart_count</strong> metric. Set the filters to <strong class="bold">Total</strong> and <strong class="bold">Bad</strong>, as shown here:<div id="_idContainer165" class="IMG---Figure"><img src="image/B15587_10_33.jpg" alt="Figure 10.33 – Defining SLI details as part of SLO monitoring&#13;&#10;"/></div><p class="figure-caption">Figure 10.33 – Defining SLI details as part of SLO monitoring</p></li>
				<li>Select a compliance period; that is, either <strong class="bold">Calendar</strong> or <strong class="bold">Rolling</strong>. For this specific lab, set <strong class="bold">Period Type</strong> to <strong class="bold">Calendar</strong> and <strong class="bold">Period Length</strong> to <strong class="bold">Calendar day</strong>. Additionally, set the performance goal to <strong class="source-inline">90</strong>%, as shown here:<div id="_idContainer166" class="IMG---Figure"><img src="image/B15587_10_34.jpg" alt="Figure 10.34 – Setting an SLO as part of SLO monitoring&#13;&#10;"/></div><p class="figure-caption">Figure 10.34 – Setting an SLO as part of SLO monitoring</p></li>
				<li>Review the <a id="_idIndexMarker1346"/>configuration and save it by providing an appropriate display name, such as <strong class="source-inline">90% - Restart Count - Calendar Day</strong>, as shown here:<div id="_idContainer167" class="IMG---Figure"><img src="image/B15587_10_35.jpg" alt="Figure 10.35 – Reviewing and saving the SLO as part of SLO monitoring&#13;&#10;"/></div><p class="figure-caption">Figure 10.35 – Reviewing and saving the SLO as part of SLO monitoring</p></li>
				<li>Once saved, the SLO – <strong class="source-inline">90% - Restart Count - Calendar Day</strong> – will be created under the <strong class="source-inline">hello-world-service</strong> service, as shown in the following screenshot. At the moment, the error budget is <strong class="bold">100%</strong> since none of the containers <a id="_idIndexMarker1347"/>were restarted:</li>
			</ol>
			<div>
				<div id="_idContainer168" class="IMG---Figure">
					<img src="image/B15587_10_36.jpg" alt="Figure 10.36 – SLO created for a service as part of SLO monitoring&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.36 – SLO created for a service as part of SLO monitoring</p>
			<p>With this, we've learned the steps we need to take to define an SLO for a service. In the next topic, we'll explore the steps we need to create an SLO burn rate alert policy.</p>
			<h3>Creating a SLO burn rate alert policy</h3>
			<p>The concept of <a id="_idIndexMarker1348"/>alerting and notification channels from Cloud Monitoring (discussed earlier in this chapter) is used to create an alert. Before we look at the steps for this, let's recap on the critical jargon that was discussed in <a href="B15587_03_Final_ASB_ePub.xhtml#_idTextAnchor064"><em class="italic">Chapter 3</em></a>, <em class="italic">Understanding Monitoring and Alerting to Target Reliability</em>. We must configure these elements while defining an alert through Cloud Monitoring:</p>
			<ul>
				<li><strong class="bold">Lookback duration</strong> refers to how far you must go back in time to retrieve monitoring data.</li>
				<li><strong class="bold">Fast-burn alert</strong> refers to using shorter lookback durations that help with quickly detecting problems. However, this will lead to more frequent alerting and, potentially, false alarms.</li>
				<li><strong class="bold">Slow-burn alert</strong> refers to using a longer lookback duration to ensure that a problem exists for a longer duration and avoids false alarms. However, the downside is that the alert is fired after a longer duration, even though the problem has a current negative impact on the service.</li>
			</ul>
			<p>Follow these steps to set up an <a id="_idIndexMarker1349"/>alert for when the error budget for the SLO drops beyond a certain burn rate within a specified period of time:</p>
			<ol>
				<li value="1">Click the <strong class="bold">Create alerting policy</strong> button, as shown in the preceding screenshot, to create an SLO alerting policy. The SLO alerting policy tracks the error budget based on the configured burn rate, as shown in the following screenshot. Set <strong class="bold">Lookback duration</strong> to <strong class="source-inline">1</strong> minute(s) and <strong class="bold">Burn rate threshold</strong> to <strong class="source-inline">10</strong> minute(s). The following is our configuration for a fast-burn alert:<div id="_idContainer169" class="IMG---Figure"><img src="image/B15587_10_37.jpg" alt="Figure 10.37 – Setting an SLO alert condition&#13;&#10;"/></div><p class="figure-caption">Figure 10.37 – Setting an SLO alert condition</p></li>
				<li>Select a notification channel of choice (that has already been pre-configured). In this case, select an email notification channel, as shown here:<div id="_idContainer170" class="IMG---Figure"><img src="image/B15587_10_38.jpg" alt="Figure 10.38 – Selecting a notification channel to send an alert&#13;&#10;"/></div><p class="figure-caption">Figure 10.38 – Selecting a notification channel to send an alert</p><p>Now, create the SLO burn rate alert policy. Optionally, add documentation that references the alert in terms of what the on-job SRE engineer should check or do.</p></li>
				<li>Once the alert has been <a id="_idIndexMarker1350"/>configured, the SLO status will look as follows, where <strong class="bold">Error Budget</strong> is currently at 100% and none of the alerts are firing:</li>
			</ol>
			<div>
				<div id="_idContainer171" class="IMG---Figure">
					<img src="image/B15587_10_39.jpg" alt="Figure 10.39 – Showing the complete setup for SLO monitoring with its alert and initial error budget&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.39 – Showing the complete setup for SLO monitoring with its alert and initial error budget</p>
			<p>With that, we've created an SLO burn rate alert policy. Now, let's verify SLO monitoring by performing a test.</p>
			<h3>Verifying SLO monitoring</h3>
			<p>In the previous two <a id="_idIndexMarker1351"/>subsections of this hands-on lab on SLO monitoring, we created an SLO for a service (that was previously created in <a href="B15587_08_Final_ASB_TD_ePub.xhtml#_idTextAnchor182"><em class="italic">Chapter 8</em></a>, <em class="italic">Understanding GKE Essentials to Deploy Containerized Applications</em>) and then created an SLO burn rate alert policy. This section will show you how to test the configuration and verify if our SLO monitoring option verifies the health of our service; that is, <strong class="source-inline">hello-world-service</strong>:</p>
			<ol>
				<li value="1">Given that we previously selected the performance metric while defining our SLO as <strong class="source-inline">kubernetes.io/container/restart_count</strong>, let's restart the container and see if the error budget changes and, subsequently, if the alert gets fired. Use the following command to restart the container after connecting to the cluster. Replace <strong class="source-inline">pod-name</strong> and <strong class="source-inline">container-name</strong> accordingly. <strong class="source-inline">pod-name</strong> can be found via the service, while <strong class="source-inline">container-name</strong> can be found via <strong class="source-inline">pod-name</strong>:<p class="source-code"><strong class="bold">kubectl exec -it &lt;pod-name&gt; -c &lt;container-name&gt; -- /bin/sh -c "kill 1"</strong></p></li>
				<li>Once the command has been executed, the container inside the pod, with respect to <strong class="source-inline">hello-world-service</strong>, will restart. This means that the SLI that's been defined will not be met and, subsequently, the SLO will not be met. As a result, the error budget will be consumed. If the error budget is consumed by more than the burn rate that's been defined – which was 10 under 1 minute – then an alert will also be fired. The following screenshot shows the updated status of the SLO for <strong class="source-inline">hello-world-service</strong>. The status of the SLO has now been updated to <strong class="bold">Unhealthy</strong>:<div id="_idContainer172" class="IMG---Figure"><img src="image/B15587_10_40.jpg" alt="Figure 10.40 – Displaying the service as Unhealthy, alerts firing, and the reduced error budget&#13;&#10;"/></div><p class="figure-caption">Figure 10.40 – Displaying the service as Unhealthy, alerts firing, and the reduced error budget</p></li>
				<li>The alert triggers a notification that will be sent to the configured email, as shown in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer173" class="IMG---Figure">
					<img src="image/B15587_10_41.jpg" alt="Figure 10.41 – Alert notification set to the configured email address&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.41 – Alert notification set to the configured email address</p>
			<p>This completes our <a id="_idIndexMarker1352"/>detailed hands-on lab related to SLO monitoring, where we linked the SRE technical practices to options available in Google Cloud Operations to monitor and alert users about the reliability of the service. This also completes this chapter on Cloud Operations.</p>
			<h1 id="_idParaDest-248"><a id="_idTextAnchor248"/>Summary</h1>
			<p>In this chapter, we discussed the suite of tools that are part of Cloud Operations. Cloud Operations is critical for forming the feedback loop of the CI/CD process and is fundamental to establishing observability on GCP. Observability is key to ensuring that an SRE's technical practices – specifically, SLIs, SLOs, SLAs, and Error Budgets – are not violated. This is achieved by gathering logs, metrics, and traces from multiple sources and by visualizing this information on dashboards. This information is used to establish performance and reliability indicators. These indicators can then be tracked with configurable alerts. These alerts trigger when there is a potential violation, and the alerts will be notified on the configurable notification channels. Cloud Operations also offers services that allow us to debug the application, without slowing down, and capture trace information. The end goal is to ensure that the service is reliable. We concluded this chapter by providing a hands-on lab on SLO monitoring, a feature from Google Cloud that tracks the reliability of the service by bringing together Cloud Operations and SRE technical practices.</p>
			<p>This was the last chapter of this book. The next section provides insights into preparing to become a Professional Cloud DevOps Engineer, along with a summary on a few topics that might show up in the exam but were not covered in the last 10 chapters. We have also provided a mock exam, which will be useful as a preparation resource.</p>
			<h1 id="_idParaDest-249"><a id="_idTextAnchor249"/>Points to remember</h1>
			<p>The following are some important points to remember:</p>
			<ul>
				<li>Cloud Monitoring is a GCP service that collects metrics, events, and metadata from multi-cloud and hybrid infrastructures in real time.</li>
				<li>A workspace provides a <em class="italic">single pane of glass</em> related to GCP resources.</li>
				<li>A workspace can monitor resources from multiple monitored projects.</li>
				<li>A monitored project, however, can only be associated with a single workspace.</li>
				<li>Dashboards provide a graphical representation of key signal data, called metrics, in a manner that is suitable for end users or the operations team.</li>
				<li>Metrics represent numerical measurements of resource usage that can be observed and collected across the system at regular time intervals.</li>
				<li>MQL can be used to create a chart with a text-based interface and uses an expressive query language to execute complex queries against time series data.</li>
				<li>Uptime checks test the availability of an external facing service within a specific timeout interval.</li>
				<li>Connection errors, 40x client errors, and not configuring firewall rules are potential reasons for uptime check failures.</li>
				<li>Alerting is the process of processing the alerting rules that track the SLOs and notify or perform certain actions when the rules are violated.</li>
				<li>The alignment period is a lookback interval from a particular point in time.</li>
				<li>The Monitoring agent is based on the <strong class="source-inline">collectd</strong> daemon and is used to collect system statistics from various sources, including OSes, applications, logs, and external devices.</li>
				<li>Cloud Logging is a GCP service that allows you to store, search, analyze, monitor, and alert users about logging data and events from applications.</li>
				<li>Policy denied logs are specific to logs that are captured when access is denied by a Google Cloud service to either a user or service account.</li>
				<li>Logs-based metrics are metrics that are created based on the content of the log entries and can be extracted from both included and excluded logs.</li>
				<li>VPC Flow Logs capture real-time network activity (incoming/outgoing) against VPC resources on an enabled subnet.</li>
				<li>Firewall logs capture the effects of a specific firewall rule in terms of the traffic that's allowed or denied by that firewall rule.</li>
				<li>The Logging agent is based on <strong class="source-inline">fluentd</strong> and captures additional VM logs such as operating system (OS) logs and logs from third-party applications.</li>
				<li>The Monitoring and Logging agents can both be installed on unmanaged GCE VMs.</li>
				<li>GKE has built-in support for logging and can be enabled for new or existing clusters via <em class="italic">Cloud Operations for GKE</em>.</li>
				<li>Cloud Debugger inspects the state of a running application in real time.</li>
				<li>Snapshots capture local variables and the call stack at a specific location in the application's source code.</li>
				<li>A snapshot is only taken once, and the user needs to manually retake it if needed.</li>
				<li>Debug logpoints can inject a log into a running application without stopping, editing, or restarting.</li>
				<li>Logpoints can be created even if direct access to the source code is not available.</li>
				<li>Logpoints become inactive after 24 hours and are automatically deleted after 30 days.</li>
				<li>Cloud Trace is a collection of spans. A span is an object that wraps latency-specific metrics. Cloud Trace is a distributed tracing system.</li>
				<li>Cloud Trace's language-specific SDKs are available for Java, Node.js, Ruby, and Go.</li>
				<li>Cloud Profiler provides low-impact continuous profiling to help us understand the performance of a production system.</li>
				<li>The programming languages that are supported by Cloud Profiler include Java, Go, Node.js, and Python. Profile data is retained for 30 days by default.</li>
			</ul>
			<h1 id="_idParaDest-250"><a id="_idTextAnchor250"/>Further reading</h1>
			<p>For more information on GCP's approach toward DevOps, please read the following articles:</p>
			<ul>
				<li><strong class="bold">Cloud Operations</strong>: <a href="https://cloud.google.com/products/operations">https://cloud.google.com/products/operations</a></li>
				<li><strong class="bold">Cloud Monitoring</strong>: <a href="https://cloud.google.com/monitoring">https://cloud.google.com/monitoring</a></li>
				<li><strong class="bold">Cloud Logging</strong>: <a href="https://cloud.google.com/logging">https://cloud.google.com/logging</a></li>
				<li><strong class="bold">Cloud Debugger</strong>: <a href="https://cloud.google.com/debugger">https://cloud.google.com/debugger</a></li>
				<li><strong class="bold">Cloud Trace</strong>: <a href="https://cloud.google.com/trace">https://cloud.google.com/trace</a></li>
				<li><strong class="bold">Cloud Profiler</strong>: <a href="https://cloud.google.com/profiler">https://cloud.google.com/profiler</a></li>
			</ul>
			<h1 id="_idParaDest-251"><a id="_idTextAnchor251"/>Practice test</h1>
			<p>Answer the following questions:</p>
			<ol>
				<li value="1">A user has performed administrative actions that modify the configuration or metadata of resources. Which of the following is the most appropriate option to quickly get to the logs related to administrative actions?<p>a) Go to Error Reporting and view the administrative activity logs.</p><p>b) Go to Cloud Logging and view the administrative activity logs.</p><p>c) Go to Cloud Monitoring and view the administrative activity logs.</p><p>d) Go to the Activity tab on the Cloud Console and view the administrative activity logs.</p></li>
				<li>The default retention period for data access audit logs is ___________.<p>a) 7 days</p><p>b) 30 days</p><p>c) 400 days</p><p>d) Unlimited</p></li>
				<li>Select the most appropriate option for monitoring multiple GCP projects with resources through a single workspace.<p>a) Cannot monitor multiple GCP projects through a single workspace.</p><p>b) Configure a separate project as a <strong class="source-inline">host</strong> project for a Cloud Monitoring workspace. Configure metrics and logs from each project to the host project via Pub/Sub.</p><p>c) Configure a separate project as a <strong class="source-inline">host</strong> project for a Cloud Monitoring workspace. Use this host project to manage all other projects.</p><p>d) Configure a separate project as a <strong class="source-inline">host</strong> project for a Cloud Monitoring workspace. Configure the metrics and logs from each project for the host project via Cloud Storage.</p></li>
				<li>____________ logs record operations of instances that have been reset for Google Compute Engine.<p>a) Admin activity</p><p>b) System event</p><p>c) Data access</p><p>d) Access transparency</p></li>
				<li>The maximum size of a log entry is __________.<p>a) 64 KB</p><p>b) 128 KB</p><p>c) 256 KB</p><p>d) 512 KB</p></li>
				<li>The default retention period for access transparency logs is ___________.<p>a) 7 days</p><p>b) 30 days</p><p>c) 400 days</p><p>d) Unlimited</p></li>
				<li>___________ logs are specific to actions that are performed by Google personnel when accessing user's/customer's content.<p>a) Admin activity</p><p>b) System event</p><p>c) Data access</p><p>d) Access transparency</p></li>
				<li>The SRE team supports multiple production workloads in GCP. The SRE team wants to manage issues better by sending error reports and stack traces to a centralized service. Which of the following is best suited for accomplishing this goal?<p>a) Cloud Error Logging</p><p>b) Cloud Error Reporting</p><p>c) Cloud Tracing</p><p>d) Cloud Profiling</p></li>
				<li>___________ logs record the operations that are performed when assigning/unassigning IAM roles.<p>a) Admin activity</p><p>b) System event</p><p>c) Data access</p><p>d) Access transparency</p></li>
				<li>_________ logs analyze the network logs of an application.<p>a) VPC flow</p><p>b) Firewall</p><p>c) Audit</p><p>d) Activity</p></li>
				<li>Select the option that represents the right characteristics for log entry from Cloud Logging:<p>a) Timestamp</p><p>b) Log name</p><p>c) Resource tied to the log entry</p><p>d) All of the above</p></li>
				<li>Select two actions where the user will want to send a subset of logs for big data analysis:<p>a) Create a sink in Cloud Logging that identifies the subset of logs to send.</p><p>b) Export logs to Cloud Storage.</p><p>c) Export logs to BigQuery.</p><p>d) Export logs to Pub/Sub.</p></li>
				<li>The default retention period for admin activity logs is ___________.<p>a) 7 days</p><p>b) 30 days</p><p>c) 400 days</p><p>d) Unlimited</p></li>
				<li>Which of the following represents the right sequence of steps to export logs?<p>a) Choose destination, create sink, create filter</p><p>b) Create sink, create filter, choose destination</p><p>c) Create sink, choose destination, create filter</p><p>d) Choose destination, create filter, create Sink</p></li>
				<li>___________ logs will record how resources are created for Google Compute Engine:<p>a) Admin activity</p><p>b) System event</p><p>c) Data access</p><p>d) Access transparency</p></li>
				<li>Select the option that governs access to logs from Cloud Logging for a given user:<p>a) Service accounts</p><p>b) Cloud IAM roles</p><p>c) Both (a) and (b) </p><p>d) None of the above</p></li>
				<li>Select the role that allows us to manage IAM roles for a Monitoring workspace:<p>a) Monitoring Viewer</p><p>b) Monitoring Editor</p><p>c) Monitoring Admin </p><p>d) Monitoring Metric Writer</p></li>
				<li>Select the Cloud Monitoring widget that represents metrics with a distribution value:<p>a) Line charts</p><p>b) Heatmap charts</p><p>c) Gauges</p><p>d) Scorecards</p></li>
				<li>To perform uptime checks, what is the minimum number of active locations that need to be selected as geographic regions?<p>a) Two</p><p>b) Three</p><p>c) Four</p><p>d) Five</p></li>
				<li>The Monitoring agent is based on _________, while the Logging agent is based on __________.<p>a) <strong class="source-inline">fluentd</strong>, <strong class="source-inline">collectd</strong></p><p>b) <strong class="source-inline">google-collectd</strong>, <strong class="source-inline">google-fluentd</strong></p><p>c) <strong class="source-inline">collectd</strong>, <strong class="source-inline">fluentd</strong></p><p>d) <strong class="source-inline">google-fluentd</strong>, <strong class="source-inline">google-collectd</strong></p></li>
				<li>Which of the following is not a valid classification type for data access logs?<p>a) Admin read</p><p>b) Admin write</p><p>c) Data read</p><p>d) Data write</p></li>
				<li>Select the role that allows us to view data access and access transparency logs:<p>a) Logs Viewer</p><p>b) Private Logs Viewer</p><p>c) Project Viewer</p><p>d) Project Editor</p></li>
				<li>The default retention period for firewall logs is ___________.<p>a) 7 days</p><p>b) 30 days</p><p>c) 400 days</p><p>d) Unlimited</p></li>
				<li>Every VPC has a set of hidden, implied, pre-configured rules with the lowest priority. Select two valid pre-configured rules:<p>a) <strong class="source-inline">allow all ingress</strong></p><p>b) <strong class="source-inline">deny all ingress</strong></p><p>c) <strong class="source-inline">allow all egress</strong></p><p>d) <strong class="source-inline">deny all egress</strong></p></li>
				<li>The default retention period for system event audit logs is ___________.<p>a) 7 days</p><p>b) 30 days</p><p>c) 400 days</p><p>d) Unlimited</p></li>
			</ol>
			<h1 id="_idParaDest-252"><a id="_idTextAnchor252"/>Answers</h1>
			<ol>
				<li value="1">(d): Go to the Activity tab on the Cloud Console and view the administrative activity logs.</li>
				<li>(b): 30 days.</li>
				<li>(c): Configure a separate project as a <strong class="source-inline">host</strong> project for a Cloud Monitoring workspace. Use this host project to manage all other projects.</li>
				<li>(b): System event.</li>
				<li>(c): 256 KB.</li>
				<li>(c): 400 days.</li>
				<li>(d): Access transparency.</li>
				<li>(b): Cloud Error Reporting.</li>
				<li>(a): Admin activity.</li>
				<li>(a): VPC flow.</li>
				<li>(d): All of the above.</li>
				<li>(a) and (c).</li>
				<li>(c): 400 days.</li>
				<li>(b): Create sink, create filter, choose destination.</li>
				<li>(a): Admin activity.</li>
				<li>(b): Cloud IAM roles.</li>
				<li>(c): Monitoring Admin.</li>
				<li>(b): Heatmap chart.</li>
				<li>(b): Three.</li>
				<li>(c): <strong class="source-inline">collectd</strong>, <strong class="source-inline">fluentd</strong>.</li>
				<li>(b): Admin write; this is not a valid classification for data access logs.</li>
				<li>(b): Private Logs Viewer.</li>
				<li>(b): 30 days.</li>
				<li>(b) and (c): <strong class="source-inline">deny all ingress</strong> and <strong class="source-inline">allow all egress</strong>.</li>
				<li>(c): 400 days.</li>
			</ol>
		</div>
	</body></html>