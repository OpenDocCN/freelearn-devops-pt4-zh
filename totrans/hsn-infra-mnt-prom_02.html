<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Monitoring Fundamentals</h1>
                </header>
            
            <article>
                
<p class="mce-root">This chapter lays the foundation for several key concepts that will be used throughout this book. Starting with the definition of monitoring, we will explore various views and factors that emphasize why systematic analysis assumes different levels of importance and makes an impact on organizations. You will learn about the advantages and disadvantages of different monitoring mechanics, taking a closer look at the Prometheus approach regarding collecting metrics. Finally, we will discuss some of the controversial decisions that were vital for the design and architecture of the Prometheus stack and why you should take them into account when designing your own monitoring system.</p>
<p>We will be covering the following topics in this chapter:</p>
<ul>
<li><span>Defining of monitoring</span></li>
<li><span>Whitebox versus blackbox monitoring</span></li>
<li>Understanding metrics collection</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining of monitoring</h1>
                </header>
            
            <article>
                
<p>A consensual definition of monitoring is hard to come by because it quickly shifts between industry- or even job-specific contexts. The diversity of viewpoints, the components comprising the monitoring system, and even how the data is collected or used are all factors that contribute to the struggle of reaching a clear definition.</p>
<p>Without a common ground, it is difficult to sustain a discussion and, usually, expectations are mismatched. Therefore, in the following topics, we will outline a baseline, orientated to obtain a definition of monitoring that will guide us throughout this book.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The value of monitoring</h1>
                </header>
            
            <article>
                
<p>With the growing complexity of infrastructures, exponentially driven by the adoption of microservices-oriented architectures, it has become critical to attain a global view of all the different components of an infrastructure. It is unthinkable to manually validate the health of each instance, caching service, database, or load balancer. There are way too many moving pieces to count—let alone keep a close eye on.</p>
<p>Nowadays, it is expected that monitoring will keep track of data from those components. However, data might come in several forms, allowing it to be used for different purposes.</p>
<p class="mce-root"/>
<p>Alerting is one of the standard uses of monitoring data, but the application of such data can go far beyond it. You may require historical information to assist you in capacity planning or incident investigations, or you may need a higher resolution to drill down into a problem and even higher freshness to decrease the mean time to recovery during an outage.</p>
<p>You can look at monitoring as a source of information for maintaining healthy systems, production- and business-wise.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Organizational contexts </h1>
                </header>
            
            <article>
                
<p>Looking into an organizational context, roles such as system administrators, quality assurance engineers, <strong>Site Reliability Engineers</strong> (<strong>SREs</strong>), or product owners have different expectations from monitoring. Understanding the requirements of what each role surfaces makes it easier to comprehend why context is so useful when discussing monitoring. Let's expand the following statements while providing some examples:</p>
<ul>
<li>System administrators are interested in high-resolution, low-latency, and high-diversity data. For a system administrator, the main objective of monitoring is to obtain visibility across the infrastructure and manage data from CPU usage to <span><strong>Hypertext Transfer Protocol</strong> (<strong>HTTP</strong>) </span>request rate so that problems are quickly discovered and the root causes are identified as soon as possible. In this approach, exposing monitoring data in high resolution is critical to be able to drill down into the affected system. If a problem is occurring, you don't have the privilege to wait several hours for your next data point, and so data has to be provided in near real time or, in other words, with low latency. Lastly, since there is no easy way to identify or predict which systems are prone to be affected, we need to collect as much data as possible from all systems; namely, a high diversity of data.</li>
<li>Quality assurance engineers are interested in high-resolution, high-latency, and high-diversity data. Besides being important for quality assurance engineers to have high resolution monitoring data collected, which enables a deeper drill down into effects, the latency is not as critical as it is for system administrators. In this case, historical data is much more critical for comparing software releases than the freshness of the data. Since we can't wholly predict the ramifications of a new release, the available data needs to be spread across as much of the infrastructure as possible, touching every system the software release might use and invoke it or generally interact with it (directly or indirectly), so that we have as much data as possible.</li>
<li>SREs focused on capacity planning are interested in low-resolution, high-latency, and high-diversity data. In this scenario, historical data carries much more importance for SREs than the resolution that this data is presented in. For example, to predict the increase in infrastructure, it is not critical for a SRE to know that some months ago at 4 A.M., one of the nodes had a spike of CPU usage reaching 100% in 10 seconds, but is useful to understand the trend of the load across the fleet of nodes to infer the number of nodes required to handle new scale requirements. As such, it is also important for SREs to have a broad visualization of all the different parts of the infrastructure that are affected by those requirements to predict, for example, the amount of storage for logs, network bandwidth increase, and so on, making the high diversity of monitoring data mandatory.</li>
<li>Product owners are interested in low-resolution, high-latency, and low-diversity data. Where product owners are concerned, monitoring data usually steps away from infrastructure to the realm of business. Product owners strive to understand the trends of specific software products, where historical data is fundamental and resolution is not so critical. Keeping in mind the objective of evaluating the impact of software releases on the customers, latency is not as essential for them as it is for system administrators. The product owner manages a specific set of products, so a low diversity of monitoring data is expected, comprised mostly of business metrics.</li>
</ul>
<p>The following table sums up the previous examples in a much more condensed form:</p>
<div>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignLeft CDPAlign"/>
<td class="CDPAlignLeft CDPAlign">
<p><strong>Data resolution</strong></p>
</td>
<td class="CDPAlignLeft CDPAlign">
<p><strong>Data latency</strong></p>
</td>
<td class="CDPAlignLeft CDPAlign">
<p><strong>Data diversity</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>Infrastructure alerting</strong></p>
</td>
<td>
<p>High</p>
</td>
<td>
<p>Low</p>
</td>
<td>
<p>High</p>
</td>
</tr>
<tr>
<td>
<p><strong>Software release view</strong></p>
</td>
<td>
<p>High</p>
</td>
<td>
<p>High</p>
</td>
<td>
<p>High</p>
</td>
</tr>
<tr>
<td>
<p><strong>Capacity planning</strong></p>
</td>
<td>
<p>Low</p>
</td>
<td>
<p>High</p>
</td>
<td>
<p>High</p>
</td>
</tr>
<tr>
<td>
<p><strong>Product/business view</strong></p>
</td>
<td>
<p>Low</p>
</td>
<td>
<p>High</p>
</td>
<td>
<p>Low</p>
</td>
</tr>
</tbody>
</table>
</div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monitoring components</h1>
                </header>
            
            <article>
                
<p>The same way the monitoring definition changes across contexts, its components follow the same predicament. Depending on how broad you want to be, we can find some or all of these components in the following topics:</p>
<ul>
<li><strong>Metrics</strong><span>:</span><strong> </strong><span>This exposes a certain system resource, application action, or business characteristic as a specific point in time value. This information is obtained in an aggregated form; for example, you can find out how many requests per second were served but not the exact time for a specific request, and without context, you won't know the ID of the requests.</span></li>
<li><strong>Logging</strong>:<strong> </strong>Containing much more data than a metric, this manifests itself as an event from a system or application, containing all the information that's produced by such an event. This information is not aggregated and has the full context.</li>
<li><strong>Tracing</strong>:<strong> </strong>This is a special case of logging where a request is given a unique identifier so that it can be tracked during its entire life cycle across every system. Due to the increase of the dataset with the number of requests, it is a good idea to use samples instead of tracking all requests.</li>
<li><strong>Alerting</strong>:<strong> </strong><span>This is the continuous threshold validation of metrics or logs, and fires an action or notification in the case of a transgression of the said threshold.</span></li>
<li><strong>Visualization</strong>: <span>This is a graphical representation of metrics, logs, or traces.</span></li>
</ul>
<p>Recently, the term monitoring has been overtaken by a superset called <strong>observability</strong>, which is regarded as the evolution of monitoring, or a different wrapping to spring a hype and revive the concept (the same way it happened with DevOps<em>)</em>. From where it stands, observability does indeed include all the components we described here.</p>
<p>Throughout this book, our monitoring definition incorporates metrics, alerting, and visualization.</p>
<div class="packt_infobox">Monitoring is metrics with associated alerting and visualization.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Whitebox versus blackbox monitoring</h1>
                </header>
            
            <article>
                
<p>There are many ways we could go about monitoring, but they largely fall into two main categories, that is, blackbox and whitebox monitoring.</p>
<p class="mce-root"/>
<p>In blackbox monitoring, the application or host is observed from the outside and, consequently, this approach can be fairly limited. Checks are made to assess whether the system under observation responds to probes in a known way:</p>
<ul>
<li>Does the host respond to <strong>Internet Control Message Protocol</strong> (<strong>ICMP</strong>) echo requests (more commonly known as ping)?</li>
<li>Is a given TCP port open?</li>
<li>Does the application respond with the correct data and status code when it receives a specific HTTP request?</li>
<li>Is the process for a specific application running in its host?</li>
</ul>
<p>On the other hand, in whitebox monitoring, the system under observation surfaces data about its internal state and the performance of critical sections. This type of introspection can be very powerful as it exposes the operating telemetry, and consequently the health, of the different internal components, which otherwise would be difficult or even impossible to ascertain. This telemetry data is usually handled in the following ways:</p>
<ul>
<li><strong>Exported through logging</strong>: This is by far the most common case and how applications exposed their inner workings before instrumentation libraries were widespread. For instance, an HTTP server's access log can be processed to monitor request rates, latencies, and error percentages.</li>
<li><strong>Emitted as structured events</strong>: This approach is similar to logging but instead of being written to disk, the data is sent directly to processing systems for analysis and aggregation.</li>
<li><strong>Maintained in memory as aggregates</strong>: Data in this format can be hosted in an endpoint or read directly from command-line tools. Examples of this approach are <kbd><em>/</em>metrics</kbd> with Prometheus metrics, HAProxy's stats page, or the varnishstats command-line tool.</li>
</ul>
<p>Not all software is instrumented and ready to have its internal state exposed for metrics collection. For example, it can be a third-party, closed-source application that has no means of surfacing its inner workings. In these cases, external probing can be a viable option to gather the data that's deemed essential for a proper service state validation.</p>
<p>Regardless, not only third-party applications benefit from blackbox monitoring. It can be useful to validate your applications from their clients' standpoint by going through, for example, load balancers and firewalls. Probing can be your last line of defense—if all else fails, you can rely on blackbox monitoring to assess availability.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding metrics collection</h1>
                </header>
            
            <article>
                
<p>The process by which metrics are by monitoring systems can generally be divided into two approaches—push and pull. As we'll see in the following topics, both approaches are valid and have their pros and cons, which we will thoroughly discuss. Nonetheless, it is essential to have a solid grasp on how they differ to understand and fully utilize Prometheus. After understanding how collecting metrics works, we will delve into what should be collected. There are several proven methods to achieve this, and we will give an overview of each one.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An overview of the two collection approaches</h1>
                </header>
            
            <article>
                
<p>In push-based monitoring systems, emitted metrics or events are sent either directly from the producing application or from a local agent to the collecting service, like so:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/122f2a49-80ba-44fb-bc33-44565eee6864.png" style="width:25.42em;height:5.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 1.1: Pull-based monitoring system</div>
<p>Systems that handle raw event data generally prefer push since the frequency of event generation is very high—in the order of hundreds, thousands, or even tens of thousands per second, per instance—which would make polling data impractical and complex. Some sort of buffering mechanism would be needed to keep events generated between polls, and event freshness would still be a problem compared with just pushing data. Some examples that use this approach include Riemann, StatsD, and the <strong>Elasticsearch</strong>, <strong>Logstash</strong>, and the <strong>Kibana</strong> (<strong>ELK</strong>) stack.</p>
<p>That is not to say that only these types of systems use push. Some monitoring systems such as Graphite, OpenTSDB, and the <strong>Telegraf</strong>, <strong>InfluxDB</strong>, <strong>Chronograph</strong>, and <strong>Kapacitor</strong> (<strong>TICK</strong>) stack have been designed using this approach. Even good old Nagios supports push through the <strong>Nagios Service Check Acceptor</strong> (<strong>NSCA</strong>), commonly known as passive checks:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1a53e36e-c159-4238-bcc1-7a11b4f77a15.png" style="width:21.58em;height:4.92em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 1.2: Push-based monitoring system</div>
<p>In contrast, pull-based monitoring systems collect metrics directly from applications or from proxy processes that make those metrics available to the system. Some notable monitoring software that uses pull are Nagios and Nagios-style systems (Icinga, Zabbix, Zenoss, and Sensu, to name a few). Prometheus is also one of those that embraces the pull approach and is very opinionated about this.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Push versus pull</h1>
                </header>
            
            <article>
                
<p>There is much debate in the monitoring community around the merits of each of these design decisions. The main point of contention is usually about target discovery, which we will discuss in the following paragraphs.</p>
<p>In push-based systems, the monitored hosts and services make themselves known by reporting to the monitoring system. The advantage here is that no prior knowledge of new systems is required for them to be picked up. However, this means that the monitoring service's location needs to be propagated to all targets, usually with some form of configuration management. Staleness is a big drawback of this approach: if a system hasn't reported in for some time, does that mean it's having problems or was it purposely decommissioned?</p>
<p>Furthermore, when you manage a distributed fleet of hosts and services that push data to a central point, the risk of a thundering herd (overload due to many incoming connections at the same time) or a misconfiguration causing an unforeseen flood of data becomes much more complex and time-consuming to mitigate.</p>
<p>In pull-based monitoring, the system needs a definitive list of hosts and services to monitor so that their metrics are ingested. Having a central source of truth provides some level of assurance that everything is where it's supposed to be, with the drawback of having to maintain said source of truth and keeping it updated with any changes. With the rapid rate of change in today's infrastructures, some form of automated discovery is needed to keep up with the full picture. Having a centralized point of configuration enables a much faster response in the case of issues or misconfigurations.</p>
<p>In the end, most of the drawbacks from each approach can be reduced or effectively solved by clever design and automation. There are other more important factors when choosing a monitoring tool, such as flexibility, ease of automation, maintainability, or broad support for the technologies being used.</p>
<p>Even though Prometheus is a pull-based monitoring system, it also provides a way of ingesting pushed metrics by using a gateway that converts from push to pull. This is useful for monitoring a very narrow class of processes, which we will see later in this book.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What to measure</h1>
                </header>
            
            <article>
                
<p>When planning metrics collection, there is a question that's bound to come up, which is defining what metrics to observe. To answer this question, we should turn to the current best practices and methodologies. In the following topics, we will look at an overview of the most influential and best-regarded methods for reducing noise and improving visibility on performance and general reliability concerns.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Google's four golden signals</h1>
                </header>
            
            <article>
                
<p>Google's rationale regarding monitoring is quite simple. It states, pretty straightforwardly, that the four most important metrics to keep track of are the following:</p>
<ul>
<li><strong>Latency</strong>: The time required to serve a request</li>
<li><strong>Traffic</strong>: The number of requests being made</li>
<li><strong>Errors</strong>: The rate of failing requests</li>
<li><strong>Saturation</strong>: The amount of work not being processed, which is usually queued</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Brendan Gregg's USE method</h1>
                </header>
            
            <article>
                
<p>Brendan's method is more machine-focused and it states that for each resource (CPU, disk, network interface, and so on), the following metrics should be monitored:</p>
<ul>
<li style="font-weight: 400"><strong>Utilization</strong>: Measured as the percentage of the resource that was busy</li>
<li style="font-weight: 400"><strong>Saturation</strong>: The amount of work the resource was not able to process, which is usually queued</li>
<li style="font-weight: 400"><strong>Errors</strong>: Amount of errors that occurred</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tom Wilkie's RED method</h1>
                </header>
            
            <article>
                
<p>The RED method is more focused on a service-level approach and not so much on the underlying system itself. Obviously, being useful to monitor services, this strategy is also valuable to predict the experience of external clients. If a service's error rate increases, it is reasonable to assume those errors will impact, directly or indirectly, on the customer's experience. These are the metrics to be aware of:</p>
<ul>
<li style="font-weight: 400"><strong>Rate</strong>: Translated as requests per second</li>
<li style="font-weight: 400"><strong>Errors</strong>: The amount of failing requests per second</li>
<li style="font-weight: 400"><strong>Duration</strong>: The time taken by those requests</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we had the chance to understand the true value of monitoring and how to approach the term in a specific context, including the context that's used in this book. This will help you avoid any misunderstandings and ensure a clear perception of where the book stands on this topic. We also went through different aspects of monitoring, such as metrics, logging, tracing, alerting, and visualizations, while presenting observability and the benefits it brings. Whitebox and blackbox monitoring were addressed, which provide the basis to comprehend the benefits of using metrics. Armed with this knowledge about metrics, we went through the mechanics of push and pull and all the arguments regarding each one, before ending with what the metrics to track on the systems you manage are.</p>
<p>In the next chapter, we will look at an overview of the Prometheus ecosystem, and talk about several of its components.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li style="font-weight: 400">Why is monitoring definition so hard to clearly define?</li>
<li style="font-weight: 400">Does a high latency of metrics impact the work of a system administrator who's focused on fixing a live incident?</li>
<li style="font-weight: 400">What are the monitoring requirements to properly do capacity planning?</li>
<li style="font-weight: 400">Is logging considered monitoring?</li>
<li style="font-weight: 400">Regarding the available strategies for metrics collection, what are the downsides of using the push-based approach?</li>
<li style="font-weight: 400">If you had to choose three basic metrics from a generic web service to focus on, which would they be?</li>
<li style="font-weight: 400">When a check verifies whether a given process is running on a host by way of listing the running processes in said host, is that whitebox or blackbox monitoring?</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><strong>The Prometheus blog</strong>: <a href="https://prometheus.io/blog/2016/07/23/pull-does-not-scale-or-does-it/">https://prometheus.io/blog/2016/07/23/pull-does-not-scale-or-does-it</a></li>
<li><strong>Site Reliability Book</strong>: <a href="https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/">https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems</a></li>
<li><strong>The USE Method</strong>: <a href="http://www.brendangregg.com/usemethod.html">http://www.brendangregg.com/usemethod.html</a></li>
<li><strong>The RED Method</strong>: <a href="https://www.weave.works/blog/the-red-method-key-metrics-for-microservices-architecture/">https://www.weave.works/blog/the-red-method-key-metrics-for-microservices-architecture</a></li>
</ul>


            </article>

            
        </section>
    </body></html>