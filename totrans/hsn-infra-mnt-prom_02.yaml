- en: Monitoring Fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter lays the foundation for several key concepts that will be used
    throughout this book. Starting with the definition of monitoring, we will explore
    various views and factors that emphasize why systematic analysis assumes different
    levels of importance and makes an impact on organizations. You will learn about
    the advantages and disadvantages of different monitoring mechanics, taking a closer
    look at the Prometheus approach regarding collecting metrics. Finally, we will
    discuss some of the controversial decisions that were vital for the design and
    architecture of the Prometheus stack and why you should take them into account
    when designing your own monitoring system.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be covering the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining of monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whitebox versus blackbox monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding metrics collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining of monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A consensual definition of monitoring is hard to come by because it quickly
    shifts between industry- or even job-specific contexts. The diversity of viewpoints,
    the components comprising the monitoring system, and even how the data is collected
    or used are all factors that contribute to the struggle of reaching a clear definition.
  prefs: []
  type: TYPE_NORMAL
- en: Without a common ground, it is difficult to sustain a discussion and, usually,
    expectations are mismatched. Therefore, in the following topics, we will outline
    a baseline, orientated to obtain a definition of monitoring that will guide us
    throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: The value of monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the growing complexity of infrastructures, exponentially driven by the
    adoption of microservices-oriented architectures, it has become critical to attain
    a global view of all the different components of an infrastructure. It is unthinkable
    to manually validate the health of each instance, caching service, database, or
    load balancer. There are way too many moving pieces to count—let alone keep a
    close eye on.
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, it is expected that monitoring will keep track of data from those
    components. However, data might come in several forms, allowing it to be used
    for different purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting is one of the standard uses of monitoring data, but the application
    of such data can go far beyond it. You may require historical information to assist
    you in capacity planning or incident investigations, or you may need a higher
    resolution to drill down into a problem and even higher freshness to decrease
    the mean time to recovery during an outage.
  prefs: []
  type: TYPE_NORMAL
- en: You can look at monitoring as a source of information for maintaining healthy
    systems, production- and business-wise.
  prefs: []
  type: TYPE_NORMAL
- en: Organizational contexts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Looking into an organizational context, roles such as system administrators,
    quality assurance engineers, **Site Reliability Engineers** (**SREs**), or product
    owners have different expectations from monitoring. Understanding the requirements
    of what each role surfaces makes it easier to comprehend why context is so useful
    when discussing monitoring. Let''s expand the following statements while providing
    some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: System administrators are interested in high-resolution, low-latency, and high-diversity
    data. For a system administrator, the main objective of monitoring is to obtain
    visibility across the infrastructure and manage data from CPU usage to **Hypertext
    Transfer Protocol** (**HTTP**) request rate so that problems are quickly discovered
    and the root causes are identified as soon as possible. In this approach, exposing
    monitoring data in high resolution is critical to be able to drill down into the
    affected system. If a problem is occurring, you don't have the privilege to wait
    several hours for your next data point, and so data has to be provided in near
    real time or, in other words, with low latency. Lastly, since there is no easy
    way to identify or predict which systems are prone to be affected, we need to
    collect as much data as possible from all systems; namely, a high diversity of
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quality assurance engineers are interested in high-resolution, high-latency,
    and high-diversity data. Besides being important for quality assurance engineers
    to have high resolution monitoring data collected, which enables a deeper drill
    down into effects, the latency is not as critical as it is for system administrators.
    In this case, historical data is much more critical for comparing software releases
    than the freshness of the data. Since we can't wholly predict the ramifications
    of a new release, the available data needs to be spread across as much of the
    infrastructure as possible, touching every system the software release might use
    and invoke it or generally interact with it (directly or indirectly), so that
    we have as much data as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SREs focused on capacity planning are interested in low-resolution, high-latency,
    and high-diversity data. In this scenario, historical data carries much more importance
    for SREs than the resolution that this data is presented in. For example, to predict
    the increase in infrastructure, it is not critical for a SRE to know that some
    months ago at 4 A.M., one of the nodes had a spike of CPU usage reaching 100%
    in 10 seconds, but is useful to understand the trend of the load across the fleet
    of nodes to infer the number of nodes required to handle new scale requirements.
    As such, it is also important for SREs to have a broad visualization of all the
    different parts of the infrastructure that are affected by those requirements
    to predict, for example, the amount of storage for logs, network bandwidth increase,
    and so on, making the high diversity of monitoring data mandatory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Product owners are interested in low-resolution, high-latency, and low-diversity
    data. Where product owners are concerned, monitoring data usually steps away from
    infrastructure to the realm of business. Product owners strive to understand the
    trends of specific software products, where historical data is fundamental and
    resolution is not so critical. Keeping in mind the objective of evaluating the
    impact of software releases on the customers, latency is not as essential for
    them as it is for system administrators. The product owner manages a specific
    set of products, so a low diversity of monitoring data is expected, comprised
    mostly of business metrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following table sums up the previous examples in a much more condensed
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Data resolution** | **Data latency** | **Data diversity** |'
  prefs: []
  type: TYPE_TB
- en: '| **Infrastructure alerting** | High | Low | High |'
  prefs: []
  type: TYPE_TB
- en: '| **Software release view** | High | High | High |'
  prefs: []
  type: TYPE_TB
- en: '| **Capacity planning** | Low | High | High |'
  prefs: []
  type: TYPE_TB
- en: '| **Product/business view** | Low | High | Low |'
  prefs: []
  type: TYPE_TB
- en: Monitoring components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The same way the monitoring definition changes across contexts, its components
    follow the same predicament. Depending on how broad you want to be, we can find
    some or all of these components in the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Metrics**:This exposes a certain system resource, application action, or
    business characteristic as a specific point in time value. This information is
    obtained in an aggregated form; for example, you can find out how many requests
    per second were served but not the exact time for a specific request, and without
    context, you won''t know the ID of the requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logging**:Containing much more data than a metric, this manifests itself
    as an event from a system or application, containing all the information that''s
    produced by such an event. This information is not aggregated and has the full
    context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tracing**:This is a special case of logging where a request is given a unique
    identifier so that it can be tracked during its entire life cycle across every
    system. Due to the increase of the dataset with the number of requests, it is
    a good idea to use samples instead of tracking all requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alerting**:This is the continuous threshold validation of metrics or logs,
    and fires an action or notification in the case of a transgression of the said
    threshold.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visualization**: This is a graphical representation of metrics, logs, or
    traces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recently, the term monitoring has been overtaken by a superset called **observability**,
    which is regarded as the evolution of monitoring, or a different wrapping to spring
    a hype and revive the concept (the same way it happened with DevOps*)*. From where
    it stands, observability does indeed include all the components we described here.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, our monitoring definition incorporates metrics, alerting,
    and visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring is metrics with associated alerting and visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Whitebox versus blackbox monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many ways we could go about monitoring, but they largely fall into
    two main categories, that is, blackbox and whitebox monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: 'In blackbox monitoring, the application or host is observed from the outside
    and, consequently, this approach can be fairly limited. Checks are made to assess
    whether the system under observation responds to probes in a known way:'
  prefs: []
  type: TYPE_NORMAL
- en: Does the host respond to **Internet Control Message Protocol** (**ICMP**) echo
    requests (more commonly known as ping)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is a given TCP port open?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the application respond with the correct data and status code when it receives
    a specific HTTP request?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the process for a specific application running in its host?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the other hand, in whitebox monitoring, the system under observation surfaces
    data about its internal state and the performance of critical sections. This type
    of introspection can be very powerful as it exposes the operating telemetry, and
    consequently the health, of the different internal components, which otherwise
    would be difficult or even impossible to ascertain. This telemetry data is usually
    handled in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exported through logging**: This is by far the most common case and how applications
    exposed their inner workings before instrumentation libraries were widespread.
    For instance, an HTTP server''s access log can be processed to monitor request
    rates, latencies, and error percentages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Emitted as structured events**: This approach is similar to logging but instead
    of being written to disk, the data is sent directly to processing systems for
    analysis and aggregation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maintained in memory as aggregates**: Data in this format can be hosted in
    an endpoint or read directly from command-line tools. Examples of this approach
    are `*/*metrics` with Prometheus metrics, HAProxy''s stats page, or the varnishstats
    command-line tool.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not all software is instrumented and ready to have its internal state exposed
    for metrics collection. For example, it can be a third-party, closed-source application
    that has no means of surfacing its inner workings. In these cases, external probing
    can be a viable option to gather the data that's deemed essential for a proper
    service state validation.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless, not only third-party applications benefit from blackbox monitoring.
    It can be useful to validate your applications from their clients' standpoint
    by going through, for example, load balancers and firewalls. Probing can be your
    last line of defense—if all else fails, you can rely on blackbox monitoring to
    assess availability.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding metrics collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process by which metrics are by monitoring systems can generally be divided
    into two approaches—push and pull. As we'll see in the following topics, both
    approaches are valid and have their pros and cons, which we will thoroughly discuss.
    Nonetheless, it is essential to have a solid grasp on how they differ to understand
    and fully utilize Prometheus. After understanding how collecting metrics works,
    we will delve into what should be collected. There are several proven methods
    to achieve this, and we will give an overview of each one.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of the two collection approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In push-based monitoring systems, emitted metrics or events are sent either
    directly from the producing application or from a local agent to the collecting
    service, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/122f2a49-80ba-44fb-bc33-44565eee6864.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: Pull-based monitoring system'
  prefs: []
  type: TYPE_NORMAL
- en: Systems that handle raw event data generally prefer push since the frequency
    of event generation is very high—in the order of hundreds, thousands, or even
    tens of thousands per second, per instance—which would make polling data impractical
    and complex. Some sort of buffering mechanism would be needed to keep events generated
    between polls, and event freshness would still be a problem compared with just
    pushing data. Some examples that use this approach include Riemann, StatsD, and
    the **Elasticsearch**, **Logstash**, and the **Kibana** (**ELK**) stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'That is not to say that only these types of systems use push. Some monitoring
    systems such as Graphite, OpenTSDB, and the **Telegraf**, **InfluxDB**, **Chronograph**,
    and **Kapacitor** (**TICK**) stack have been designed using this approach. Even
    good old Nagios supports push through the **Nagios Service Check Acceptor** (**NSCA**),
    commonly known as passive checks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a53e36e-c159-4238-bcc1-7a11b4f77a15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: Push-based monitoring system'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, pull-based monitoring systems collect metrics directly from applications
    or from proxy processes that make those metrics available to the system. Some
    notable monitoring software that uses pull are Nagios and Nagios-style systems
    (Icinga, Zabbix, Zenoss, and Sensu, to name a few). Prometheus is also one of
    those that embraces the pull approach and is very opinionated about this.
  prefs: []
  type: TYPE_NORMAL
- en: Push versus pull
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is much debate in the monitoring community around the merits of each of
    these design decisions. The main point of contention is usually about target discovery,
    which we will discuss in the following paragraphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In push-based systems, the monitored hosts and services make themselves known
    by reporting to the monitoring system. The advantage here is that no prior knowledge
    of new systems is required for them to be picked up. However, this means that
    the monitoring service''s location needs to be propagated to all targets, usually
    with some form of configuration management. Staleness is a big drawback of this
    approach: if a system hasn''t reported in for some time, does that mean it''s
    having problems or was it purposely decommissioned?'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, when you manage a distributed fleet of hosts and services that
    push data to a central point, the risk of a thundering herd (overload due to many
    incoming connections at the same time) or a misconfiguration causing an unforeseen
    flood of data becomes much more complex and time-consuming to mitigate.
  prefs: []
  type: TYPE_NORMAL
- en: In pull-based monitoring, the system needs a definitive list of hosts and services
    to monitor so that their metrics are ingested. Having a central source of truth
    provides some level of assurance that everything is where it's supposed to be,
    with the drawback of having to maintain said source of truth and keeping it updated
    with any changes. With the rapid rate of change in today's infrastructures, some
    form of automated discovery is needed to keep up with the full picture. Having
    a centralized point of configuration enables a much faster response in the case
    of issues or misconfigurations.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, most of the drawbacks from each approach can be reduced or effectively
    solved by clever design and automation. There are other more important factors
    when choosing a monitoring tool, such as flexibility, ease of automation, maintainability,
    or broad support for the technologies being used.
  prefs: []
  type: TYPE_NORMAL
- en: Even though Prometheus is a pull-based monitoring system, it also provides a
    way of ingesting pushed metrics by using a gateway that converts from push to
    pull. This is useful for monitoring a very narrow class of processes, which we
    will see later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: What to measure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When planning metrics collection, there is a question that's bound to come up,
    which is defining what metrics to observe. To answer this question, we should
    turn to the current best practices and methodologies. In the following topics,
    we will look at an overview of the most influential and best-regarded methods
    for reducing noise and improving visibility on performance and general reliability
    concerns.
  prefs: []
  type: TYPE_NORMAL
- en: Google's four golden signals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Google''s rationale regarding monitoring is quite simple. It states, pretty
    straightforwardly, that the four most important metrics to keep track of are the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Latency**: The time required to serve a request'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Traffic**: The number of requests being made'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Errors**: The rate of failing requests'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Saturation**: The amount of work not being processed, which is usually queued'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brendan Gregg's USE method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Brendan''s method is more machine-focused and it states that for each resource
    (CPU, disk, network interface, and so on), the following metrics should be monitored:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Utilization**: Measured as the percentage of the resource that was busy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Saturation**: The amount of work the resource was not able to process, which
    is usually queued'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Errors**: Amount of errors that occurred'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tom Wilkie's RED method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The RED method is more focused on a service-level approach and not so much
    on the underlying system itself. Obviously, being useful to monitor services,
    this strategy is also valuable to predict the experience of external clients.
    If a service''s error rate increases, it is reasonable to assume those errors
    will impact, directly or indirectly, on the customer''s experience. These are
    the metrics to be aware of:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rate**: Translated as requests per second'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Errors**: The amount of failing requests per second'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Duration**: The time taken by those requests'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we had the chance to understand the true value of monitoring
    and how to approach the term in a specific context, including the context that's
    used in this book. This will help you avoid any misunderstandings and ensure a
    clear perception of where the book stands on this topic. We also went through
    different aspects of monitoring, such as metrics, logging, tracing, alerting,
    and visualizations, while presenting observability and the benefits it brings.
    Whitebox and blackbox monitoring were addressed, which provide the basis to comprehend
    the benefits of using metrics. Armed with this knowledge about metrics, we went
    through the mechanics of push and pull and all the arguments regarding each one,
    before ending with what the metrics to track on the systems you manage are.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at an overview of the Prometheus ecosystem,
    and talk about several of its components.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why is monitoring definition so hard to clearly define?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does a high latency of metrics impact the work of a system administrator who's
    focused on fixing a live incident?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the monitoring requirements to properly do capacity planning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is logging considered monitoring?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Regarding the available strategies for metrics collection, what are the downsides
    of using the push-based approach?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you had to choose three basic metrics from a generic web service to focus
    on, which would they be?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When a check verifies whether a given process is running on a host by way of
    listing the running processes in said host, is that whitebox or blackbox monitoring?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**The Prometheus blog**: [https://prometheus.io/blog/2016/07/23/pull-does-not-scale-or-does-it](https://prometheus.io/blog/2016/07/23/pull-does-not-scale-or-does-it/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Site Reliability Book**: [https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems](https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The USE Method**: [http://www.brendangregg.com/usemethod.html](http://www.brendangregg.com/usemethod.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The RED Method**: [https://www.weave.works/blog/the-red-method-key-metrics-for-microservices-architecture](https://www.weave.works/blog/the-red-method-key-metrics-for-microservices-architecture/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
