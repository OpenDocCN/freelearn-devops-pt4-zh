<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Running a Prometheus Server</h1>
                </header>
            
            <article>
                
<p class="mce-root">It's time to get our hands on some Prometheus configurations. This chapter will explore the core component of the stack, you will be introduced to common patterns of usage and full setup process scenarios under virtual machines and containers. This will allow you to truly validate the knowledge you've gathered so far and provide you with real examples to test your <span>knowledge</span>.</p>
<p>In brief, the following topics will be covered in this chapter:</p>
<ul>
<li>Deep dive into the Prometheus configuration</li>
<li>Managing Prometheus in a standalone server</li>
<li>Managing Prometheus in Kubernetes</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep dive into the Prometheus configuration</h1>
                </header>
            
            <article>
                
<p>One of the key features of Prometheus is, owing to incredibly sane default configurations, that it can scale from a quick test running on a local computer to a production-grade instance, handling millions of samples per second without having to touch almost any of its many knobs and dials. Having said <span>that</span>, it is very useful to know what configuration options are available to be able to get the most value out of Prometheus.</p>
<p>There are two main types of configuration on a Prometheus server—command-line flags and operating logic that provided through configuration files. Command-line flags control the parameters that cannot be changed at runtime, such as the storage path or which TCP port to bind to, and need a full server restart to apply any change done at this level. The configuration files control runtime configuration, such as scrape job definitions, rules files locations, or remote storage setup. In the following sections, we're going to explore both of these types of configurations in depth.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Prometheus startup configuration</h1>
                </header>
            
            <article>
                
<p>While running a Prometheus server with no startup configuration can be good enough for local instances, it is advisable to configure a couple of basic command-line flags for any serious deployment.</p>
<p>At the time of writing, Prometheus has almost 30 command-line flags for tweaking several aspects of its operational configuration, grouped by the following namespaces: <kbd>config</kbd>, <kbd>web</kbd>, <kbd>storage</kbd>, <kbd>rules</kbd>, <kbd>alertmanager</kbd>, <kbd>query</kbd>, and <kbd>log</kbd>. The <kbd>--help</kbd> flag does a good job of describing most options, but it can be a bit terse in a few places, so we're going to highlight the ones that are either important for any deployment or whose function is not readily apparent. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The config section</h1>
                </header>
            
            <article>
                
<p>The first thing that is usually important to set is the Prometheus configuration file path, through the <kbd>--config.file</kbd> flag. By default, Prometheus will look for a file named <kbd>prometheus.yml</kbd> in the current working directory. While this is great for local tests, production deployments usually place server binaries and configuration files in their own paths, and so this flag is commonly needed. As a side note, this and a storage directory are the only hard requirements for starting a Prometheus server; without a configuration file, Prometheus refuses to start.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The storage section</h1>
                </header>
            
            <article>
                
<p>Following the same logic from the previous section, the <kbd>--storage.tsdb.path</kbd> flag should be set to configure the base path to the data storage location. This defaults to <kbd>data/</kbd> on the current working directory, and so it is advisable to point this to a more appropriate path—possibly to a different drive/volume, where data can be safely persisted and I/O contention can be mitigated. To note that NFS (AWS EFS included) is not supported, as it doesn't support the POSIX locking primitives needed for safe database files management. Placing the Prometheus data storage directory in a network share is also ill-advised as transient network failures would impact the monitoring system's ability to keep functioning - just when you'd need it the most.</p>
<p>The Prometheus local storage can only be written to by a single Prometheus instance at a time. <span>To make sure this is the case, it uses a lock file in the data directory. On startup, it tries to lock this file using OS-specific system calls, and will refuse to start if the file is already locked by another process.</span></p>
<p>There can be an edge case to this behavior; when using persistent volumes to store the data directory, there is a chance that, when relaunching Prometheus as another container instance using the same volume, the previous instance might not have unlocked the database. This problem would make a setup of this kind susceptible to race conditions. Luckily, there is the <kbd>--storage.tsdb.no-lockfile</kbd> flag, which can be used in exactly this type of situation. Be warned though that, in general (and namely, in most Prometheus deployments), it is a bad idea to disable the lock file, as doing so makes unintended data corruption easier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The web section</h1>
                </header>
            
            <article>
                
<p>The next step is to configure what address users are going to utilize to get to the Prometheus server. The <kbd>--web.external-url</kbd> flag sets this base URL so that weblinks generated both in the web user interface and in outgoing alerts link back to the Prometheus server or servers correctly. This might be the DNS name for a load balancer/reverse proxy, a Kubernetes service, or, in the simplest deployments, the publicly accessible, fully qualified domain name of the host running the server. For completeness, and as stated in the official documentation, a URL path can also be supplied here when Prometheus is behind some layer seven reverse proxy with content switching (also referred to as location-based switching or URL prefix routing).</p>
<p>The Prometheus server behaves as a conventional <kbd>*nix</kbd> daemon by reloading its configuration file (along with rules files) when it receives a <kbd>SIGHUP</kbd>. However, there are situations where sending this signal isn't convenient (for example, when running in a container orchestration system such as Kubernetes or using custom-built automation) or even impossible (when running Prometheus on Windows). In these situations, the <kbd>--web.enable-lifecycle</kbd> flag can be used to enable the <kbd>/-/reload</kbd> and <kbd>/-/quit</kbd> HTTP endpoints, which can be used to control, reload, and shut down, respectively. To prevent accidental triggering of these endpoints, and because a <kbd>GET</kbd> wouldn't be semantically correct,  a <kbd>POST</kbd> request is needed. This flag is turned off by default as unfettered access to these endpoints pose a security concern.</p>
<p>Similarly, the <kbd>--web.enable-admin-api</kbd> flag is also turned off by default for the same reason. This flag enables HTTP endpoints that provide some advanced administration actions, such as creating snapshots of data, deleting time series, and cleaning tombstones.</p>
<p>As you may have noticed in <a href="8f575e82-4713-45c7-8eb3-0af3e0e61ed3.xhtml">Chapter 3</a>, <em>Setting Up a Test Environment</em>, the official Prometheus tarballs also bring two additional directories, <kbd>consoles</kbd> and <kbd>console_libraries</kbd>. These are needed to enable the native dashboarding capabilities of Prometheus, which are often overlooked. These directories contain some preconfigured dashboards (referred to as consoles) and support template libraries, written in the Go templating language. Prometheus can be configured to load these by using the <kbd>--web.console.templates</kbd> and <kbd>--web.console.libraries</kbd> flags. After that, those dashboards will be available at the <kbd>/consoles</kbd> endpoint (a link will be available in the main web UI if an index.html file exists).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The query section</h1>
                </header>
            
            <article>
                
<p>This section is all about tuning the inner workings of the query engine. Some are fairly straightforward to understand, such as how long a given query can run before being aborted (<kbd>--query.timeout</kbd>), or how many queries can run simultaneously (<kbd>--query.max-concurrency</kbd>).</p>
<p>However, two of them set limits that can have non-obvious consequences. The first is <kbd>--query.max-samples</kbd>, which was introduced in Prometheus 2.5.0, that sets the maximum number of samples that can be loaded onto memory. This was done as a way of capping the maximum memory the query subsystem can use (by using it together with <kbd>--query.max-concurrency</kbd>) to try and prevent the dreaded <em>query-of-death</em>—a query that loaded so much data to memory that it made Prometheus hit a memory limit and then killing the process. The behavior post 2.5.0 is that if any query hits the limit set by this flag (which defaults to 50,000,000 samples), the query simply fails.</p>
<p>The second one is <kbd>--query.lookback-delta</kbd>. Without going into too much detail regarding how PromQL works internally, this flag sets the limit of how far back Prometheus will look for time series data points before considering them stale. This implicitly means that if you collect data at a greater interval than what's set here (the default being five minutes), you will get inconsistent results in alerts and graphs, and as such, two minutes is the maximum sane value to allow for failures.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Prometheus configuration file walkthrough</h1>
                </header>
            
            <article>
                
<p>The configuration file we mentioned in the previous section declares the runtime configuration for the Prometheus instance. As we will see, everything related to scrape jobs, rule evaluation, and remote read/write configuration is all defined here. As we mentioned previously, these configurations can be reloaded without shutting down the Prometheus server by either sending a <kbd>SIGHUP</kbd> to the process, or by sending an HTTP POST request to the <kbd>/-/reload</kbd> endpoint (when <kbd>--web.enable-lifecycle</kbd> is used at startup).</p>
<p>At a high level, we can split the configuration file into the following sections:</p>
<ul>
<li style="font-weight: 400"><kbd>global</kbd></li>
<li style="font-weight: 400"><kbd>scrape_configs</kbd></li>
<li style="font-weight: 400"><kbd>alerting</kbd></li>
<li style="font-weight: 400"><kbd>rule_files</kbd></li>
<li style="font-weight: 400"><kbd>remote_read</kbd></li>
<li style="font-weight: 400"><kbd>remote_write</kbd></li>
</ul>
<p>Once again, the official Prometheus documentation includes the schema for this file, which is written in YAML format. In this chapter, we will introduce an example configuration for us to analyze, but only go into detail on the <kbd>global</kbd> and <kbd>scrape_configs</kbd> sections. The alerting and <kbd>rule_files</kbd> are covered in <a href="9aa1e3da-13cf-4051-845d-1d1c924ef47b.xhtml">Chapter 9</a><em>, Defining Alerting and Recording Rules</em>, while <kbd>remote_read</kbd> and <kbd>remote_write</kbd> are explained in <a href="1fb2aaff-5fe3-44ed-9df8-1cd27f383906.xhtml">Chapter 14</a>, <em>Integrating Long-Term Storage with Prometheus</em>.</p>
<div class="packt_tip">
<p>A configuration file with the most comprehensive list of options available can be found in the Prometheus project GitHub repository, located in the following address: <a href="https://github.com/prometheus/prometheus/blob/v2.9.2/config/testdata/conf.good.yml">https://github.com/prometheus/prometheus/blob/v2.9.2/config/testdata/conf.good.yml</a>.</p>
</div>
<p>Our example configuration looks as follows:</p>
<pre>global:<br/>  scrape_interval: 1m<br/>...<br/>scrape_configs:<br/>  - job_name: 'prometheus'<br/>    scrape_interval: 15s<br/>    scrape_timeout: 5s<br/>    sample_limit: 1000<br/>    static_configs:<br/>      - targets: ['localhost:9090']<br/>    metric_relabel_configs:<br/>      - source_labels: [ __name__ ]<br/>        regex: expensive_metric_.+<br/>        action: drop</pre>
<p>At first glance, it may seem a bit dense, but <span>for clarity's sake, </span>we're making some configurations whose defaults values don't usually need to be touched <span>explicit</span>.</p>
<p>Let's examine each section in detail.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Global configuration</h1>
                </header>
            
            <article>
                
<p>The <kbd>global</kbd> configuration defines the default parameters for every other configuration section, as well as outlining what labels should be added to metrics going to external systems, as shown in the following code block:</p>
<pre>global:<br/>  scrape_interval: 1m<br/>  scrape_timeout: 10s<br/>  evaluation_interval: 1m<br/>  external_labels:<br/>    dc: dc1<br/>    prom: prom1</pre>
<div class="packt_infobox">Duration can only be integer values and can only have one unit. This means that trying to use 0.5 minutes instead of 30 seconds or one <span>minute </span>30 seconds instead of 90 seconds will be considered a configuration error.</div>
<p><kbd>scrape_interval</kbd> sets the default frequency targets that should be scraped. This is usually between 10 seconds and one minute, and the default <kbd>1m</kbd> is a good conservative value to start. Longer intervals are not advisable as the lost granularity (especially in gauges) starts to impact the ability to properly alert on issues and makes querying finicky as you need to be aware that some shorter intervals might not return data. Additionally, considering the default loopback delta of five minutes (mentioned in the command-line flags), any <kbd>scrape_interval</kbd> longer than 150 seconds (2 minutes 30 seconds) will mean every time series for a given target will be considered stale if a single scrape fails.</p>
<p><kbd>scrape_timeout</kbd> defines how long Prometheus should wait by default for a response from a target before closing the connection and marking the scrape as failed (10 seconds if not declared). Bear in mind that even though it is expected that targets respond to scrapes fairly quickly, the guidelines for metrics exposition mandate that collection should happen at scrape time and not cached, which means there can be some exporters that take a bit longer to respond.</p>
<p>Similar to <kbd>scrape_interval</kbd>, <kbd>evaluation_interval</kbd> sets the default frequency recording and alerting rules are evaluated. For sanity, both should have the same. This is going to be discussed in more detail in <a href="9aa1e3da-13cf-4051-845d-1d1c924ef47b.xhtml">Chapter 9</a>, <em>Defining Alerting and Recording Rules</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5423912c-df8a-4cab-9a10-8e20e264621d.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5.1: Representation of scrape intervals and evaluation intervals inside Prometheus</div>
<p>Lastly, <kbd>external_labels</kbd> allows you to set label name/value pairs that are added to time series or alerts going to external systems, such as Alertmanager, remote read and write infrastructure, or even other Prometheis through federation. This functionality is usually employed to uniquely identify the source of a given alert or time series; therefore, it is common to identify the region, datacenter, shard, or even the instance identifier of a Prometheus server.</p>
<div class="packt_infobox">As per the official documentation, the plural form of <em>Prometheus</em> is <em>Prometheis</em>: <a href="https://prometheus.io/docs/introduction/faq/#what-is-the-plural-of-prometheus">https://prometheus.io/docs/introduction/faq/#what-is-the-plural-of-prometheus</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scrape configuration</h1>
                </header>
            
            <article>
                
<p>Even though Prometheus accepts an empty file as a valid configuration file, the absolute minimum useful configuration needs a <kbd>scrape_configs</kbd> section. This is where we define the targets for metrics collection, and if some post-scrape processing is needed before actual ingestion.</p>
<p>In the configuration example we introduced previously, we defined two scrape jobs: <kbd>prometheus</kbd> and <kbd>blackbox</kbd>. In Prometheus terms, a scrape is the action of collecting metrics through an HTTP request from a targeted instance, parsing the response, and ingesting the collected samples to storage. The default HTTP endpoint used in the Prometheus ecosystem for metrics collection is aptly named <kbd>/metrics</kbd>.</p>
<p>A collection of such instances is called a <strong>job</strong>. The instances in a job are usually all running copies of the same service, and so there is usually a job definition for each kind of monitored software, though this can be a bit different when using service discovery, as we'll see in <a href="5360e790-3884-4eeb-aaa1-8aad21dc6c1e.xhtml">Chapter 12</a>, <em>Choosing the Right Service Discovery</em>. The combination of instance and job identify the source of the collected samples, and so these are automatically added as labels to the ingested data, <span>as shown in the following code block</span>:</p>
<pre>scrape_configs:<br/> - job_name: 'prometheus'<br/>    static_configs:<br/>      - targets: ['localhost:9090']<br/>...<br/><br/>  - job_name: 'blackbox'<br/>    static_configs:<br/>      - targets:<br/>        - http://example.com<br/>        - https://example.com:443<br/>...</pre>
<p>A scrape job definition needs at least a <kbd>job_name</kbd> and a set of targets. In this example, <kbd>static_configs</kbd> was used to declare the list of targets for both scrape jobs. While Prometheus supports a lot of ways to dynamically define this list, <kbd>static_configs</kbd> is the simplest and most straightforward method:</p>
<pre>scrape_configs:<br/> - job_name: 'prometheus'<br/>    scrape_interval: 15s<br/>    scrape_timeout: 5s<br/>    sample_limit: 1000<br/>    static_configs:<br/>      - targets: ['localhost:9090']<br/>    metric_relabel_configs:<br/>      - source_labels: [ __name__ ]<br/>        regex: expensive_metric_.+<br/>        action: drop</pre>
<p>Analyzing the <kbd>prometheus</kbd> scrape job in detail, we can see that both <kbd>scrape_interval</kbd> and <kbd>scrape_timeout</kbd> can be redeclared at the job level, thus overriding the global values. As stated before, having varying intervals is discouraged, so only use this when absolutely necessary.</p>
<p>By setting <kbd>sample_limit</kbd>, Prometheus will ensure that whatever value was set, it will be collected per scrape by not ingesting those samples when their number goes over the limit and marking the scrape as failed. This is a great safety net for preventing a cardinality explosion from a target outside of your control impacting the monitoring system.</p>
<p>The last relevant configuration here is <kbd>metric_relabel_configs</kbd>. This is a powerful rewrite engine that allows a collected metrics' identity to be transformed, or even dropped, before being saved to storage. The most common use cases for this feature is to blacklist a set of misbehaving metrics, dropping labels without compromising a metric's identity, or changing labels to better match Prometheus semantics. Ideally, <kbd>metric_relabel_configs</kbd> should be used as a stopgap while the problems aren't fixed at the source and so using it often can be a red flag. The preceding example is using <kbd>metric_relabel_configs</kbd> to drop every metric that starts with <kbd>expensive_metric_</kbd>:</p>
<pre>  - job_name: 'blackbox'<br/>    metrics_path: /probe<br/>    scheme: http<br/>    params:<br/>      module: [http_2xx]<br/>    static_configs:<br/>      - targets:<br/>        - http://example.com<br/>    relabel_configs:<br/>      - source_labels: [__address__]<br/>        target_label: __param_target<br/>      - source_labels: [__param_target]<br/>        target_label: instance<br/>      - target_label: __address__<br/>        replacement: 127.0.0.1:9115</pre>
<p>While we are going to explore blackbox exporter in depth in the next chapter, its configuration is used here to help explain the following important configurations:</p>
<ul>
<li style="font-weight: 400"><kbd>metrics_path</kbd> is used to change which endpoint Prometheus should scrape</li>
<li style="font-weight: 400"><kbd>scheme</kbd> defines whether HTTP or HTTPS is going to be used when connecting to targets</li>
<li><span><kbd>params</kbd> allows you to define a set of optional HTTP parameters</span></li>
</ul>
<p>However, the most important and most useful configuration is <kbd>relabel_configs</kbd>. It provides the same powerful semantics as <kbd>metric_relabel_configs</kbd>, but it has a very different function; <kbd>relabel_configs</kbd> is used to manipulate the scrape job's list of targets. The relabel actions are performed in sequence, and so it is possible to create or modify labels and then use those in the next action. By default, a target will have a couple of labels that have been generated automatically and that will be available for relabeling: the <kbd>job</kbd> label will be set to the <kbd>job_name</kbd> configuration,  <kbd>__address__</kbd> label will be created with the target's host and port, <kbd>__scheme__</kbd> and <kbd>__metrics_path__</kbd> labels will be set to their respective configurations (<kbd>scheme</kbd> and <kbd>metrics_path</kbd>), and a <kbd>__param_&lt;name&gt;</kbd> label will be created for each of the parameters defined in the <kbd>params</kbd> configuration. Additionally, <kbd>__meta_</kbd> labels will be available when using a service discovery mechanism, as we'll see in <a href="5360e790-3884-4eeb-aaa1-8aad21dc6c1e.xhtml">Chapter 12</a>, <em>Choosing the Right Service Discovery</em>. If the <kbd>instance</kbd> label is not set by the end of the relabeling phase, <kbd>__address__</kbd> will be used to set it. Labels that start with two underscores (<kbd>__</kbd>) will be removed when the relabeling phase ends. As a final note, if you need temporary labels during the relabeling process, always use the <kbd>__tmp</kbd> prefix, as it is guaranteed to not overlap with Prometheus internal labels.</p>
<p class="mce-root">In the case of the blackbox exporter, this functionality is very useful as we need to send the probe requests to the exporter, which will then use the <kbd>target</kbd> <kbd>GET</kbd> parameter to perform its job. So, going through the example, for each target specified in <kbd>static_configs</kbd>, this configuration does the following:</p>
<ul>
<li style="font-weight: 400">Copies the target's address into a <kbd>__param_target</kbd> label, which will be used to set the <kbd>target</kbd> <kbd>GET</kbd> parameter in the scrape</li>
<li style="font-weight: 400">Copies the content of this newly created label into the <kbd>instance</kbd> label so that it is explicitly set, bypassing the automatic generation based on the <kbd>__address__</kbd></li>
<li style="font-weight: 400">Replaces the <kbd>__address__</kbd> label with the address of the blackbox exporter so that scrapes are done to the exporter and not directly to the target we specified in <kbd><kbd>static_configs</kbd></kbd></li>
</ul>
<div class="packt_tip">While <kbd>relabel_configs</kbd> is used to rewrite the target list (it runs before the scrape is performed), <kbd>metric_relabel_configs</kbd> is used to rewrite labels or drop samples (it runs after the scrape is performed).</div>
<div class="packt_infobox">The example configuration used in this section is for demonstration purposes only. For example, there should be no need to set <kbd>sample_limit</kbd> on Prometheus itself, or to drop metrics without a concrete reason.</div>
<p>A very useful metric that must be introduced is the <kbd>up</kbd> metric. This metric exposes the status of a scrape job. It includes, at least, a label with the correspondent job name and another with the targeted instance. In its sample, we can have the value <kbd>1</kbd> for a successful scrape or <kbd>0</kbd> for a failed one.</p>
<p>Next, we're going to start managing Prometheus in different deployment environments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Managing Prometheus in a standalone server</h1>
                </header>
            
            <article>
                
<p>As we previously went through several configuration definitions, we're now ready to put them to practice by managing a standalone instance of Prometheus. In these examples, we'll be exposing several configurations while providing an environment to validate them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Server deploy</h1>
                </header>
            
            <article>
                
<p>To create a new instance of Prometheus, move into the correct repository path, as shown here:</p>
<pre><strong>cd chapter05/</strong></pre>
<p>Ensure that no other test environments are running and spin up this chapter's environment, like so:</p>
<pre><strong>vagrant global-status</strong><br/><strong>vagrant up</strong></pre>
<p>After a few moments, the new instance will be available for inspection, and the Prometheus web interface will be accessible at <kbd>http://192.168.42.10:9090</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuration inspection</h1>
                </header>
            
            <article>
                
<p>With the newly created instance running, it's time to log in using the following command:</p>
<pre><strong>vagrant ssh prometheus</strong></pre>
<p>We can validate the startup configuration in use by looking into its <kbd>systemd</kbd> unit file by <span>using the following command</span>:</p>
<pre><strong>cat /etc/systemd/system/prometheus.service</strong></pre>
<p>The following excerpt shows the flags that are currently in place:</p>
<pre>ExecStart=/usr/bin/prometheus \<br/>    --config.file=/etc/prometheus/prometheus.yml \<br/>    --storage.tsdb.path=/var/lib/prometheus/data \<br/>    --web.console.templates=/usr/share/prometheus/consoles \<br/>    --web.console.libraries=/usr/share/prometheus/console_libraries</pre>
<p>The configuration file for Prometheus itself, as defined by the <kbd>--config.file</kbd> flag, can be reviewed as follows:</p>
<pre><strong>cat /etc/prometheus/prometheus.yml</strong></pre>
<p>As we can see, the configuration in use is similar to the one that was presented previously, in the Prometheus configuration file walkthrough. We can now validate a couple of concepts we mentioned previously.</p>
<p>Due to <kbd>metric_relabel_configs</kbd> in the <kbd>prometheus</kbd> job, we can use two of Prometheus' per-scrape metrics to determine the number of samples being dropped by our configuration as follows:</p>
<ul>
<li><kbd>scrape_samples_scraped</kbd>: This metric provides the total of samples collected</li>
<li><kbd>scrape_samples_post_metric_relabeling</kbd>: This metric provides the total of samples available after the metric relabeling takes place</li>
</ul>
<p>If we subtract these two metrics, we obtain the number of dropped samples (in our example, this is all of the metric names starting with <kbd>go_</kbd>):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b5186397-edfd-4ac5-a0b5-e7ee7282c009.png" style="width:52.00em;height:31.33em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 5.2: </span>Number of metrics being dropped</div>
<p>We can confirm the outcome of the configuration relabeling, which in our example, generates the instance labels under the <kbd>blackbox</kbd> job:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/81d9eaa7-bc18-414f-b81d-3a879252e4bd.png" style="width:48.83em;height:40.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 5.3: </span>Instance labels generated by <em>relabel_configs</em></div>
<p>You can validate the Prometheus configuration by using a provided utility called <kbd>promtool</kbd>, which will be thoroughly dissected on <a href="19357d8c-dfcf-4497-ae80-4761f6633d14.xhtml">Chapter 8</a>,<em> Troubleshooting and Validation</em>. When reloading Prometheus with a new configuration, you also have the option to look at the <kbd>prometheus_config_last_reload_successful</kbd> metric to assess whether the configuration was successfully parsed and applied.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cleanup</h1>
                </header>
            
            <article>
                
<p>When you've finish testing, just make sure you're inside the <kbd>chapter05/</kbd> path and execute the following:</p>
<pre><strong>vagrant destroy -f</strong></pre>
<p class="mce-root">Don't worry too much – you can easily spin up the environment again if you so require.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Managing Prometheus in Kubernetes</h1>
                </header>
            
            <article>
                
<p>Kubernetes is the first project to graduate from the CNCF and is currently the de facto standard for container orchestration. Early on, Heapster was widely used as a monitoring solution that came out-of-the-box with Kubernetes. It started out as a tool to send monitoring data to external systems but then grew to become a monitoring system itself. However, it didn't take long for Prometheus to become the de facto standard monitoring system for Kubernetes clusters. Nowadays, most of the components that make up a Kubernetes cluster have native Prometheus instrumentation.</p>
<p>In the following sections, we'll go into how to integrate Prometheus in a Kubernetes environment by, employing examples based on the Kubernetes project and the Prometheus Operator project.</p>
<div class="packt_infobox">You can find the complete source code of the Kubernetes project and the Prometheus Operator at the following addresses, respectively:<br/>
<a href="https://github.com/kubernetes/kubernetes">https://github.com/kubernetes/kubernetes</a> and <a href="https://github.com/coreos/prometheus-operator">https://github.com/coreos/prometheus-operator</a>.</div>
<p class="mce-root">Ensure that you have all the software requirements that were defined in <a href="8f575e82-4713-45c7-8eb3-0af3e0e61ed3.xhtml">Chapter 3</a>, <em>Setting Up a Test Environment</em>, available in their specific versions, particularly the following:</p>
<ul>
<li class="mce-root">Minikube</li>
<li class="mce-root">kubectl</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Static configuration</h1>
                </header>
            
            <article>
                
<p>Although this approach is quite far from being advised, it provides the foundations to better understand and troubleshoot a Prometheus server running in Kubernetes. In this example, we'll create a Prometheus deployment using a ConfigMap to define the server configuration.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kubernetes environment</h1>
                </header>
            
            <article>
                
<p>Ensure that there's no instance of <kbd>minikube</kbd> running the following commands:</p>
<pre><strong>minikube status</strong><br/><strong>minikube delete</strong></pre>
<div class="packt_infobox"><kbd>minikube delete</kbd> is a destructive instruction, so be sure you save your work before proceeding.</div>
<p>Start a new <kbd>minikube</kbd> instance with the following specifications:</p>
<pre><strong>minikube start \</strong><br/><strong>  --cpus=2 \</strong><br/><strong>  --memory=2048 \</strong><br/><strong>  --kubernetes-version="v1.14.0" \</strong><br/><strong>  --vm-driver=virtualbox</strong></pre>
<p>When the previous command finishes, a new Kubernetes environment should be ready to be used. You may access its dashboard by using the following command, which will <span>open the Kubernetes dashboard address in your default browser</span>:</p>
<pre><strong>minikube dashboard</strong></pre>
<p>To proceed with our example, ensure that you move into the correct repository path, like so:</p>
<pre><strong>cd chapter05/provision/kubernetes/static</strong></pre>
<p>For the sake of organization, we'll be creating a new namespace called <kbd>monitoring</kbd> using the following manifest with the help of <kbd>kubectl</kbd>:</p>
<pre><strong>apiVersion: v1</strong><br/><strong>kind: Namespace</strong><br/><strong>metadata:</strong><br/><strong>  name: monitoring</strong></pre>
<p>Apply the previous manifest using the following command:</p>
<pre><strong>kubectl apply -f monitoring-namespace.yaml</strong></pre>
<p>We can validate the successful namespace creation on the Kubernetes dashboard:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/29f1589b-449a-4300-9623-28fcd8d16f23.png" style="width:40.25em;height:23.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 5.4: </span>Kubernetes dashboard - monitoring namespace</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Prometheus server deployment</h1>
                </header>
            
            <article>
                
<p>With our new namespace available, it's time to create a very simple Prometheus configuration and save it on a ConfigMap using the following manifest:</p>
<pre>apiVersion: v1<br/>kind: ConfigMap<br/>metadata:<br/>  name: prometheus-config<br/>  namespace: monitoring <br/>data:<br/>  prometheus.yml: |<br/>    scrape_configs:<br/>    - job_name: prometheus<br/>      static_configs:<br/>      - targets:<br/>        - localhost:9090</pre>
<p><span>Apply the previous manifest using the following command:</span></p>
<pre><strong>kubectl apply -f prometheus-configmap.yaml</strong></pre>
<p>Now, it's time to start a new deployment of Prometheus, making sure we mount the previously configured ConfigMap into the pod we are deploying. The Deployment object is configured with the following metadata:</p>
<pre>apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: prometheus-deployment<br/>  namespace: monitoring<br/>  labels:<br/>    app: prometheus</pre>
<p>The Prometheus container will be started with its configuration file and data directory coming from volume mounts, shown as follows:</p>
<pre>        args:<br/>          - --config.file=/etc/config/prometheus.yml<br/>          - --storage.tsdb.path=/data<br/>        volumeMounts:<br/>          - name: config-volume<br/>            mountPath: /etc/config/prometheus.yml<br/>            subPath: prometheus.yml<br/>          - name: prometheus-data<br/>            mountPath: /data<br/>            subPath: ""</pre>
<p>The <kbd>config-volume</kbd> volume is created from a ConfigMap, while the <kbd>prometheus-data</kbd> volume is created with an empty directory. This can be seen in the following snippet:</p>
<pre>      volumes:<br/>        - name: config-volume<br/>          configMap:<br/>           name: prometheus-config<br/>        - name: prometheus-data<br/>          emptyDir: {}</pre>
<p><span>Apply the previous manifest using the following command:</span></p>
<pre><strong>kubectl apply -f prometheus-deployment.yaml</strong></pre>
<p>We can follow the deployment status using this snippet:</p>
<pre><strong>kubectl rollout status deployment/prometheus-deployment -n monitoring</strong></pre>
<p>We should look at the logs of our Prometheus instance using the following command:</p>
<pre><strong>kubectl logs --tail=20 -n monitoring -l app=prometheus</strong></pre>
<p>After a successful deployment, we're ready to assign a new service to our instance, choosing <kbd>NodePort</kbd> so we can access it without requiring port-forwarding, like so:</p>
<pre>kind: Service<br/>apiVersion: v1<br/>metadata:<br/>  name: prometheus-service<br/>  namespace: monitoring<br/>spec:<br/>  selector:<br/>    app: prometheus<br/>  type: NodePort<br/>  ports:<br/>  - name: prometheus-port<br/>    protocol: TCP<br/>    port: 9090<br/>    targetPort: 9090</pre>
<p><span>Apply the previous manifest using the following:</span></p>
<pre><strong>kubectl apply -f prometheus-service.yaml</strong></pre>
<p>And you're ready to check your new Prometheus service using the following code snippet:</p>
<pre><strong>minikube service prometheus-service -n monitoring</strong></pre>
<p>This will open your browser on the Prometheus service endpoint. <span>You can now check the running configuration and targets using the Prometheus web interface:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9d66fcb0-d114-413a-924c-5b1b5a193991.png" style="width:32.33em;height:17.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 5.5: </span>Prometheus initial configuration</div>
<p>Now that we have Prometheus running in Kubernetes, we can start adding targets for it to scrape. In the next section, we will have a look at how you can achieve this.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adding targets to Prometheus</h1>
                </header>
            
            <article>
                
<p>For the sake of this example, we'll deploy yet another service and add it to our Prometheus server, going step by step on how to do it. We'll use a small <em>Hello World</em> type of application called <em>Hey</em> for our setup.</p>
<div class="packt_infobox">The code for the <em>Hey</em> application can be inspected at <a href="https://github.com/kintoandar/hey">https://github.com/kintoandar/hey</a>.</div>
<p>These steps are quite similar to the deployment of the Prometheus server. Start by creating a new deployment for <em>Hey</em> using the following manifest:</p>
<pre>apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: hey-deployment<br/>  namespace: monitoring<br/>  labels:<br/>    app: hey<br/>...<br/>      - name: hey<br/>        image: kintoandar/hey:v1.0.1<br/>...<br/>        - name: http<br/>          containerPort: 8000<br/>...</pre>
<p><span>Apply the previous manifest using the following command:</span></p>
<pre><strong>kubectl apply -f hey-deployment.yaml</strong></pre>
<p>We can follow the deployment status using this code snippet:</p>
<pre><strong>kubectl rollout status deployment/hey-deployment -n monitoring</strong></pre>
<p>We can validate the logs of our <em>Hey</em> instance using the following command:</p>
<pre><strong>kubectl logs --tail=20 -n monitoring -l app=hey</strong></pre>
<p>After a successful deployment, we're ready to assign a new service to our instance choosing <kbd>NodePort</kbd> so that we can access it without requiring port-forwarding, like so:</p>
<pre>kind: Service<br/>apiVersion: v1<br/>metadata:<br/>  name: hey-service<br/>  namespace: monitoring<br/>spec:<br/>  selector:<br/>    app: hey<br/>  type: NodePort<br/>  ports:<br/>  - name: hey-port<br/>    protocol: TCP<br/>    port: 8000<br/>    targetPort: 8000</pre>
<p><span>Apply the previous manifest using the following command:</span></p>
<pre><strong>kubectl apply -f hey-service.yaml</strong></pre>
<p>Now, you're ready to check your new <em>Hey</em> service like so:</p>
<pre><strong>minikube service hey-service -n monitoring</strong></pre>
<p>Since Prometheus is statically managed in our example, we need to add the new <em>Hey</em> target for metric collection. This means that we need to change the Prometheus ConfigMap to reflect the newly added service, like so:</p>
<pre>kind: ConfigMap<br/>metadata:<br/>  name: prometheus-config<br/>  namespace: monitoring <br/>data:<br/>  prometheus.yml: |<br/>    scrape_configs:<br/>    - job_name: prometheus<br/>      static_configs:<br/>      - targets:<br/>        - localhost:9090<br/>    - job_name: hey<br/>      static_configs:<br/>      - targets:<br/>        - hey-service.monitoring.svc:8000</pre>
<p><span>Apply the previous manifest using the following command:</span></p>
<pre><strong>kubectl create -f prometheus-configmap-update.yaml -o yaml --dry-run | kubectl apply -f -</strong></pre>
<p>If you check the running Prometheus configuration, nothing has changed; this is because a new deployment wasn't triggered. For that to happen, something needs to change on the deployment definition, so we just change the version annotation and apply the new manifest like so:</p>
<pre><strong>kubectl apply -f prometheus-deployment-update.yaml</strong></pre>
<p>We can follow the deployment status using the following command:</p>
<pre><strong>kubectl rollout status deployment/prometheus-deployment -n monitoring</strong></pre>
<p>After a moment, a new deployment will take place, changing the Prometheus configuration and a new target will present itself, which you can validate in the Prometheus web user interface:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/06658f18-12d1-47fd-90e4-f35e00f2c945.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 5.6: </span>Prometheus targeting the Hey application</div>
<div class="packt_tip">You may have noticed that no <span><strong>role-based access control</strong> </span>(<strong>RBAC</strong>) configuration was required in this example. This is because all pods run in the same namespace and Prometheus didn't require access to the Kubernetes API yet. We strongly believe RBAC is fundamental in a secure Kubernetes setup.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dynamic configuration – the Prometheus Operator</h1>
                </header>
            
            <article>
                
<p>CoreOS was the pioneer in building a pattern called Operator, which abstracts the complexity of packaging, deployment, and the management of Kubernetes applications. The Operator synthesizes the knowledge required for the operation of an application (such as configuration and deploy logic) into Kubernetes custom resources and custom controllers.</p>
<div class="packt_infobox">A custom resource is an object that extends the Kubernetes API, allowing custom API definitions. Custom controller strives to achieve the user's required state for a resource, continuously working on maintaining such a state.</div>
<p>The combination of both a Kubernetes custom resource and custom controller into a pattern is what brings the Operator definition to life.</p>
<p>When implementing this type of pattern, instead of defining per example, the persistent storage for an application, as well as the specific configuration for their environment, the user would rather just request an instance of that application, and the Operator would abstract all the required dependencies and provide the final result automatically.</p>
<p>In our case, besides managing the deployment, including the number of pods and persistent volumes of the Prometheus server, the Prometheus Operator will also update the configuration dynamically using the concept of ServiceMonitor, which targets services with matching rules against the labels of running containers:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ed29b32a-94a0-4d12-b694-1265ce526576.png" style="width:33.92em;height:17.33em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 5.7: </span>Prometheus Operator logic diagram</div>
<p>Empowered with this knowledge, we'll provide an example on how to deploy and configure Prometheus using the Prometheus Operator, including collecting metrics from an application, this time running on a different namespace.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kubernetes environment</h1>
                </header>
            
            <article>
                
<p>Ensure that there's no instance of <kbd>minikube</kbd> running, like so:</p>
<pre><strong>minikube status</strong><br/><strong>minikube delete</strong></pre>
<p>Start a new <kbd>minikube</kbd> instance with the following specifications:</p>
<pre><strong>minikube start \</strong><br/><strong>  --cpus=2 \</strong><br/><strong>  --memory=2048 \</strong><br/><strong>  --kubernetes-version="v1.14.0" \</strong><br/><strong>  --vm-driver=virtualbox</strong></pre>
<p>When the previous command finishes, a new Kubernetes environment should be ready to be used. You may access its dashboard by using the following command, which will <span>open the Kubernetes dashboard address in your default browser</span>:</p>
<pre><strong>minikube dashboard</strong></pre>
<p>To proceed with the deployment of our example, ensure that you move into the correct repository path, as shown here:</p>
<pre><strong>cd chapter05/provision/kubernetes/operator</strong></pre>
<p>Like the previous example, we'll be creating a new namespace called <kbd>monitoring</kbd> with the help of <kbd>kubectl</kbd>, like so:</p>
<pre><strong>kubectl apply -f monitoring-namespace.yaml</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Prometheus Operator deployment</h1>
                </header>
            
            <article>
                
<p>With the new namespace available, it's time to ensure that all access permissions are in place for the Prometheus Operator, as shown in the next few configuration snippets. The first one defines the <kbd>ClusterRole</kbd>:</p>
<pre>apiVersion: rbac.authorization.k8s.io/v1<br/>kind: <strong>ClusterRole</strong><br/>metadata:<br/>  name: <strong>prometheus-operator</strong><br/>rules:<br/>- apiGroups: [apiextensions.k8s.io]<br/>  verbs: ['*']<br/>  resources: [customresourcedefinitions]<br/>- apiGroups: [monitoring.coreos.com]<br/>  verbs: ['*']<br/>  resources: <br/>  - alertmanagers<br/>  - prometheuses<br/>  - servicemonitors<br/>...</pre>
<p>Then, we apply the <kbd>ClusterRole</kbd> to a <kbd>ClusterRoleBinding</kbd>:</p>
<pre>apiVersion: rbac.authorization.k8s.io/v1<br/>kind: <strong>ClusterRoleBinding</strong><br/>metadata:<br/>  name: <strong>prometheus-operator</strong><br/>roleRef:<br/>  apiGroup: rbac.authorization.k8s.io<br/>  kind: <strong>ClusterRole</strong><br/>  name: <strong>prometheus-operator</strong><br/>subjects:<br/>- kind: <strong>ServiceAccount</strong><br/>  name: <strong>prometheus-operator</strong><br/>  namespace: monitoring</pre>
<p>Finally, we create a <kbd>ServiceAccount</kbd> for the <kbd>ClusterRoleBinding</kbd>:</p>
<pre>apiVersion: v1<br/>kind: <strong>ServiceAccount</strong><br/>metadata:<br/>  name: <strong>prometheus-operator</strong><br/>  namespace: monitoring</pre>
<p><span>Apply the manifest containing the previous snippets using the following command:</span></p>
<pre><strong>kubectl apply -f prometheus-operator-rbac.yaml</strong></pre>
<p>Having the <span>new service account configured,</span> we're ready to deploy the Operator itself, like so:</p>
<pre>apiVersion: apps/v1beta2<br/>kind: Deployment<br/>metadata:<br/>  labels:<br/>    k8s-app: prometheus-operator<br/>  name: prometheus-operator<br/>  namespace: monitoring<br/>spec:<br/>  replicas: 1<br/>  selector:<br/>    matchLabels:<br/>      k8s-app: prometheus-operator<br/>...<br/>      <strong>serviceAccountName</strong>: <strong>prometheus-operator</strong></pre>
<p><span>Apply the previous manifest using the following command:</span></p>
<pre><strong>kubectl apply -f prometheus-operator-deployment.yaml</strong></pre>
<p>We can follow the deployment status using the following code snippet:</p>
<pre><strong>kubectl rollout status deployment/prometheus-operator -n monitoring</strong></pre>
<p><span>With the Operator deployed, we can now use it to </span>deploy<span> and manage Prometheus instances.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Prometheus server deployment</h1>
                </header>
            
            <article>
                
<p>Before proceeding with the setup of Prometheus, we'll need to grant its instances with the right access control permissions.  The following snippets from the Prometheus RBAC manifest do just that. First we need to create a <kbd>ClusterRole</kbd> that allows Prometheus access to <kbd>/metrics</kbd> through GET requests:</p>
<pre>apiVersion: rbac.authorization.k8s.io/v1<br/>kind: <strong>ClusterRole</strong><br/>metadata:<br/>  name: <strong>prometheus-k8s</strong><br/>rules:<br/>...<br/>- nonResourceURLs:<br/>  - <strong>/metrics</strong><br/>  verbs:<br/>  - <strong>get</strong></pre>
<p>Next, we create a <kbd>ClusterRoleBinding</kbd> to grant the permissions from the aforementioned <kbd>ClusterRole</kbd> to a user, which in our case will be a <kbd>ServiceAccount</kbd>:</p>
<pre>apiVersion: rbac.authorization.k8s.io/v1<br/>kind: ClusterRoleBinding<br/>metadata:<br/>  name: prometheus-k8s<br/>roleRef:<br/>  apiGroup: rbac.authorization.k8s.io<br/>  kind: <strong>ClusterRole</strong><br/>  name: <strong>prometheus-k8s</strong><br/>subjects:<br/>- kind: <strong>ServiceAccount</strong><br/>  name: <strong>prometheus-k8s</strong><br/>  namespace: monitoring</pre>
<p>Finally, we create a <kbd>ServiceAccount</kbd> for Prometheus:</p>
<pre>apiVersion: v1<br/>kind: <strong>ServiceAccount</strong><br/>metadata:<br/>  name: <strong>prometheus-k8s</strong><br/>  namespace: monitoring</pre>
<p><span>Apply the manifest containing the previous snippets using the following command:</span></p>
<pre><strong>kubectl apply -f prometheus-rbac.yaml</strong></pre>
<p>Having the service account ready, we can now use the Prometheus Operator to deploy our Prometheus servers using the following manifest:</p>
<pre>apiVersion: monitoring.coreos.com/v1<br/><strong>kind</strong>: <strong>Prometheus</strong><br/>metadata:<br/>  labels:<br/>    prometheus: k8s<br/>  name: k8s<br/>  namespace: monitoring<br/>spec:<br/>  baseImage: quay.io/prometheus/prometheus<br/>  version: v2.9.2<br/>  replicas: 2<br/>...<br/>  <strong>serviceAccountName</strong>: <strong>prometheus-k8s</strong><br/>  serviceMonitorNamespaceSelector: {}<br/>  serviceMonitorSelector: {}</pre>
<p><span>Apply the previous manifest using the following command:</span></p>
<pre><strong>kubectl apply -f prometheus-server.yaml</strong></pre>
<p>We can follow the deployment progress using the following command:</p>
<pre><strong>kubectl rollout status statefulset/prometheus-k8s -n monitoring</strong></pre>
<p>When the deployment is finished, we'll be ready to create a new service for our Prometheus servers and launch the web interface to validate the current settings, like so:</p>
<pre><strong>kubectl apply -f prometheus-service.yaml</strong><br/><br/><strong>minikube service prometheus-service -n monitoring<br/></strong></pre>
<p>Following is the <span>Prometheus </span><span>default configuration created by the Prometheus Operator:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/378fe1fa-94fd-43c0-93af-072535466a0e.png" style="width:32.83em;height:25.58em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 5.8: </span>Prometheus default configuration created by the Prometheus Operator</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adding targets to Prometheus</h1>
                </header>
            
            <article>
                
<p>So far, we've deployed the Operator and used it to deploy Prometheus itself. Now, we're ready to add targets and go over the logic of how to generate them.</p>
<p>Before proceeding, we'll also deploy an application to increase the number of available targets. For this, we'll be using the <em>Hey</em> application once again, this time using the default namespace:</p>
<pre>apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: hey-deployment<br/>  <strong>namespace: default</strong><br/>spec:<br/>  replicas: 3<br/>  selector:<br/>    matchLabels:<br/>      <strong>app: hey</strong></pre>
<p><span>Pay close attention to the labels and the port name, </span><span>as shown in the following code block</span><span>; they'll be used by the service monitor:</span></p>
<pre>  template:<br/>    metadata:<br/>      labels:<br/>        <strong>app: hey</strong><br/>    spec:<br/>      containers:<br/>      - name: hey<br/>        image: kintoandar/hey:v1.0.1<br/>        ports:<br/>        - <strong>name: hey-port</strong><br/>          containerPort: 8000<br/>...</pre>
<p><span>Apply the manifest containing the previous snippets using the following command:</span></p>
<pre><strong>kubectl apply -f hey-deployment.yaml</strong></pre>
<p>We can follow the status of the deployment using the following command:</p>
<pre><strong>kubectl rollout status deployment/hey-deployment -n default</strong></pre>
<p>After the deployment finishes, we'll create a new service, as shown in the following code block. Pay close attention to the labels that will be used by the service monitor to target this service:</p>
<pre>kind: Service<br/>metadata:<br/>  labels:<br/>    squad: frontend<br/>  name: hey-service<br/>  namespace: default<br/>spec:<br/>  selector:<br/>    <strong>app: hey</strong><br/>  type: NodePort<br/>  ports:<br/>  - name: hey-port<br/>    protocol: TCP<br/>    port: 8000<br/>    targetPort: <strong>hey-port</strong></pre>
<p><span>Apply the previous manifest using the following command:</span></p>
<pre><strong>kubectl apply -f hey-service.yaml<br/><br/></strong><strong>minikube service hey-service -n default</strong></pre>
<p>Finally, we are going to create service monitors for both the Prometheus instances and the <em>Hey</em> application, which will instruct the Operator to configure Prometheus, adding the required targets. Pay close attention to the selector configuration – it will be used to match the services we created previously.</p>
<p>The following is the service monitor for Prometheus:</p>
<pre>apiVersion: monitoring.coreos.com/v1<br/>kind: ServiceMonitor<br/>metadata:<br/>  labels:<br/>    k8s-app: prometheus<br/>  name: prometheus<br/>  namespace: monitoring<br/>spec:<br/>  endpoints:<br/>  - interval: 30s<br/>    port: web<br/>  selector:<br/>    matchLabels:<br/>      <strong>prometheus: k8s</strong></pre>
<p>The service monitor for the <em>Hey</em> application is as follows:</p>
<pre>apiVersion: monitoring.coreos.com/v1<br/>kind: ServiceMonitor<br/>metadata:<br/>  labels:<br/>    app: hey<br/>  name: hey-metrics<br/>  namespace: default<br/>spec:<br/>  endpoints:<br/>  - interval: 30s<br/>    <strong>port: hey-port</strong><br/>  selector:<br/>    matchLabels:<br/>      squad: frontend</pre>
<p><span>Apply the previous manifests using the following command:</span></p>
<pre><strong>kubectl apply -f prometheus-servicemonitor.yaml</strong><br/><br/><strong>kubectl apply -f hey-servicemonitor.yaml</strong></pre>
<p>You can validate the successful deployment of the service monitors using the following command:</p>
<pre><strong>kubectl get servicemonitors --all-namespaces</strong></pre>
<p>After the Operator reconfigures Prometheus, which might take a few seconds, the added targets should be available on your Prometheus web interface:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4cc495c2-9f2d-437c-9666-316902f42895.png"/></p>
<p class="mce-root"/>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 5.9: </span>P<span>rometheus targets after the service monitors' configuration</span></div>
<p>ServiceMonitors are the main building block when using the Prometheus Operator. You can configure anything that goes into a scrape job, such as scrape and timeout intervals, metrics endpoint to scrape, HTTP query parameters, and so on. You can find the documentation for these configurations at <a href="https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint">https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we were introduced to some of the most important configuration concepts for setting up a Prometheus server. This knowledge is fundamental for tailoring Prometheus for your specific scenario. From startup flags to the configuration file, we also spun up an instance to experiment and validate the knowledge we obtained.</p>
<p>As more and more workloads are transitioning to containers, and specifically to Kubernetes, we dived into how to set up and manage Prometheus on such an environment. We began experimenting with static configurations as a stepping stone to understand a more robust approach, the Prometheus Operator.</p>
<p>In the next chapter, we'll go into the most common exporters and build upon what we've learned so that we can successfully collect data from various different sources on Prometheus.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What happens if <kbd>scrape_timeout</kbd> is not declared explicitly?</li>
<li>How can Prometheus be made to reload its configuration file?</li>
<li>How far back does Prometheus look for data before considering a time series stale?</li>
<li>What is the difference between <kbd>relabel_configs</kbd> and <kbd>metric_relabel_configs</kbd>?</li>
<li>On the static deployment example, we added a Kubernetes service for the Hey application as a target in Prometheus. What problems will arise if we increase the number of <em>Hey</em> pods?</li>
<li>Does static Prometheus static configuration make sense on a Kubernetes environment? Why?</li>
<li>In which Kubernetes facilities does the Prometheus Operator rely upon to achieve its goals?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><strong>Prometheus configuration</strong>: <a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/">https://prometheus.io/docs/prometheus/latest/configuration/configuration/</a></li>
<li><strong>Prometheus TSDB APIs</strong>: <a href="https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-admin-apis">https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-admin-apis</a></li>
<li><strong>Prometheus security</strong>: <a href="https://prometheus.io/docs/operating/security/">https://prometheus.io/docs/operating/security/</a></li>
<li><strong>Kubernetes custom controllers</strong>: <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#custom-controllers">https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#custom-controllers</a></li>
<li><strong>Kubernetes custom resources</strong>: <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#customresourcedefinitions">https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#customresourcedefinitions</a></li>
<li><strong>Prometheus Operator</strong>: <a href="https://github.com/coreos/prometheus-operator/blob/master/Documentation/design.md">https://github.com/coreos/prometheus-operator/blob/master/Documentation/design.md</a></li>
</ul>


            </article>

            
        </section>
    </body></html>