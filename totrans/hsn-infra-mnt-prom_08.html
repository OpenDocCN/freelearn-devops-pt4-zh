<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Exporters and Integrations</h1>
                </header>
            
            <article>
                
<p class="mce-root">Even though first-party exporters cover the basics pretty well, the Prometheus ecosystem provides a wide variety of third-party exporters that cover everything else. In this chapter, we will be introduced to some of the most useful exporters available—from <strong>operating system</strong> (<strong>OS</strong>) metrics and <strong>Internet Control Message Protocol</strong> (<strong>ICMP</strong>) probing to generating metrics from logs, or how to collect information from short-lived processes, such as batch jobs.</p>
<p class="mce-root">In brief, the following topics will be covered in this chapter:</p>
<ul>
<li class="mce-root">Test environments for this chapter</li>
<li class="mce-root">Operating system exporter</li>
<li class="mce-root">Container exporter</li>
<li class="mce-root">From logs to metrics</li>
<li class="mce-root">Blackbox monitoring</li>
<li class="mce-root">Pushing metrics</li>
<li class="mce-root">More exporters</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Test environments for this chapter</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll be using two test environments: one based on <strong>virtual machines</strong> (<strong>VMs</strong>) that mimic traditional static infrastructure and one based on Kubernetes for modern workflows. The following topics will guide you through the automated setup procedure for both of them, but will gloss over the details from each exporter—these will be explained in depth in their own sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Static infrastructure test environment</h1>
                </header>
            
            <article>
                
<p>This method will abstract all the deployment and configuration details, allowing you to have a fully provisioned test environment with a couple of commands. You'll still be able to connect to each of the guest instances and tinker with the example configurations.</p>
<p>To launch a new test environment, move into this chapter path, relative to the repository root:</p>
<pre><strong>cd ./chapter06/</strong></pre>
<p>Ensure no other test environments are running and spin up this chapter’s environment:</p>
<pre><strong>vagrant global-status<br/>vagrant up</strong></pre>
<p>You can validate the successful deploy of the test environment using:</p>
<pre><strong>vagrant status</strong></pre>
<p>Which will output the following:</p>
<pre><strong>Current machine states:</strong><br/><br/><strong>prometheus                running (virtualbox)</strong><br/><strong><span>target01        </span><span>          running (virtualbox)<br/><br/></span>This environment represents multiple VMs. The VMs are all listed</strong><br/><strong>above with their current state. For more information about a specific</strong><br/><strong>VM, run `vagrant status NAME`.</strong></pre>
<p>The end result will be an environment like the one depicted in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0c7250b5-02ab-47dd-97c8-78102e3f0217.png" style="width:38.25em;height:25.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 6.1: Diagram of the static infrastructure test environment</div>
<p>To connect to the <kbd>target01</kbd> instance, just run the following:</p>
<pre><strong>vagrant ssh target01</strong></pre>
<p>To connect to the Prometheus instance, use the following code snippet:</p>
<pre><strong>vagrant ssh prometheus</strong></pre>
<p>When you're finished with this environment, move into this chapter path, relative to the repository root as follows:</p>
<pre><strong>cd ./chapter06/</strong></pre>
<p>And execute the following instruction:</p>
<pre><strong>vagrant destroy -f</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kubernetes test environment</h1>
                </header>
            
            <article>
                
<p>To start the Kubernetes test environment, we first must ensure there's no instance of <kbd>minikube</kbd> running as follows:</p>
<pre><strong>minikube status</strong><br/><strong>minikube delete</strong></pre>
<p>Start a new <kbd>minikube</kbd> instance with the following specifications:</p>
<pre><strong>minikube start \</strong><br/><strong>  --cpus=2 \</strong><br/><strong>  --memory=3072 \</strong><br/><strong>  --kubernetes-version="v1.14.0" \</strong><br/><strong>  --vm-driver=virtualbox</strong></pre>
<p>When the previous command finishes, a new Kubernetes environment should be ready to be used.</p>
<p>For our Kubernetes test environment, we'll be building upon the lessons learned in <a href="12e775c2-bee9-4ebe-ad73-2f9313eeeeee.xhtml">Chapter 5</a>, <em>Running a Prometheus Server</em>, and using Prometheus Operator in our workflow. Since we already covered the Prometheus Operator setup, we'll deploy all the required components without going over each one of them.</p>
<p>Step into the following chapter number:</p>
<pre><strong>cd ./chapter06/</strong></pre>
<p>Deploy the Prometheus Operator and validate the successful deploy as follows:</p>
<pre><strong>kubectl apply -f ./provision/kubernetes/operator/bootstrap/</strong><br/><br/><strong>kubectl rollout status deployment/prometheus-operator -n monitoring</strong></pre>
<p>Use the Prometheus Operator to deploy Prometheus and ensure the deploy was successful like so:</p>
<pre><strong>kubectl apply -f ./provision/kubernetes/operator/deploy/</strong><br/><br/><strong>kubectl rollout status statefulset/prometheus-k8s -n monitoring</strong></pre>
<p>Add ServiceMonitors as shown in the following code, which will configure Prometheus jobs:</p>
<pre><strong>kubectl apply -f ./provision/kubernetes/operator/monitor/</strong><br/><br/><strong>kubectl get servicemonitors --all-namespaces</strong></pre>
<p>After a moment, you should have Prometheus available and ready; the following instruction will provide its web interface:</p>
<pre><strong>minikube service prometheus-service -n monitoring</strong></pre>
<p>You can validate the Kubernetes StatefulSet for Prometheus using the following instruction, which will open the Kubernetes dashboard:</p>
<pre><strong>minikube dashboard</strong></pre>
<div class="packt_tip">More information regarding the Kubernetes objects, including the StatefulSet controller, is available at <a href="https://kubernetes.io/docs/concepts/">https://kubernetes.io/docs/concepts/</a>.</div>
<p>The following screenshot illustrates the correct deployment of the Prometheus StatefulSet:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8a6e82f8-87c5-48ef-8a16-cde67a650f61.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 6.2 - Kubernetes dashboard depicting Prometheus StatefulSet</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Operating system exporter</h1>
                </header>
            
            <article>
                
<p>When monitoring infrastructure, the most common place to start looking is at the OS level. Metrics for resources such as CPU, memory, and storage devices, as well as kernel operating counters and statistics provide valuable insight to assess a system's performance characteristics. For a Prometheus server to collect these types of metrics, an OS-level exporter is needed on the target hosts to expose them in an HTTP endpoint. The Prometheus project provides such an exporter that supports Unix-like systems called the Node Exporter, and the community also maintains an equivalent exporter for Microsoft Windows systems called the WMI exporter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Node Exporter</h1>
                </header>
            
            <article>
                
<p>The Node Exporter is the most well-known Prometheus exporter, for good reason. It provides over 40 collectors for different areas of the OS, as well as a way of exposing local metrics for cron jobs and static information about the host. Like the rest of the Prometheus ecosystem, the Node Exporter comes with a sane default configuration and some smarts to identify what can be collected, so it's perfectly reasonable to run it without much tweaking.</p>
<div class="packt_infobox">In this context, <em>node</em> refers to a computer node or host and is not related to Node.js in any way.</div>
<p>Although this exporter was designed to be run as a non-privileged user, it does need to access kernel and process statistics, which aren't normally available when running inside a container. This is not to say that it doesn't work in containers—every Prometheus component can be run in containers—but that additional configuration is required for it to work. It is, therefore, recommended that the Node Exporter be run as a system daemon directly on the host whenever possible.</p>
<p><span>Node Exporter</span> collectors might gather different metrics depending on the system being run, as OS kernels vary in the way they expose internal state and what details they make available. As an example, metrics exposed by <kbd>node_exporter</kbd> on macOS will be substantially different from the ones on Linux. This means that, even though the <span>Node Exporter</span> supports Linux, Darwin (macOS), FreeBSD, OpenBSD, NetBSD, DragonFly BSD, and Solaris, each collector within the <span>Node Exporter</span> will have their own compatibility matrix, with Linux being the kernel with the most support.</p>
<p class="mce-root"/>
<div class="packt_tip">Metric names exposed by the <span>Node Exporter</span> changed in version 0.16.0 due to a standardization effort across the Prometheus project. This was a breaking change, which means that dashboards and tutorials made for earlier versions of this exporter won't work out of the box. An upgrade guide (<a href="https://github.com/prometheus/node_exporter/blob/v0.17.0/docs/V0_16_UPGRADE_GUIDE.md">https://github.com/prometheus/node_exporter/blob/v0.17.0/docs/V0_16_UPGRADE_GUIDE.md</a>) can be found in the <span>Node Exporter</span>'s repository.</div>
<div class="packt_infobox">The source code and installation files for the <span>Node Exporter</span> are available at <a href="https://github.com/prometheus/node_exporter">https://github.com/prometheus/node_exporter</a>.</div>
<p>By design, this exporter only produces aggregated metrics about processes (such as how many are running, and so on) and not individual metrics per process. In the Prometheus model, each process of relevance needs to expose its own metrics, or have a companion exporter to do that job for it. This is one of the reasons why it is ill-advised in most cases to run a generic process exporter without an explicit whitelist.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuration</h1>
                </header>
            
            <article>
                
<p>Exporters in the Prometheus ecosystem usually collect a specific set of metrics from a given process. The <span>Node Exporter</span> differs from most other exporters as machine-level metrics span a wide range of subsystems, and so it is architected to provide individual collectors, which can be turned on and off, depending on the instrumentation needs. Enabling collectors that are turned off by default can be done with the <kbd>--collector.&lt;name&gt;</kbd> set of flags; enabled collectors can be disabled by using the <kbd>--no-collector.&lt;name&gt;</kbd> flag variant.</p>
<p class="mce-root"/>
<p>From all the collectors enabled by default, one needs to be singled out due to its usefulness as well as its need of configuration to properly work. The <kbd>textfile</kbd> collector enables the exposition of custom metrics by watching a directory for files with the <kbd>.prom</kbd> extension that contain metrics in the Prometheus exposition format. The <kbd>--collector.textfile.directory</kbd> flag is empty by default and so needs to be set to a directory path for the collector to do its job. It is expected that only instance-specific metrics be exported through this method, for example:</p>
<ul>
<li>Local cron jobs can report their exit status through a metric (finish timestamp is not useful to record, as the metrics file modification timestamp is already exported as a metric)</li>
<li>Informational metrics (that only exist for the labels they provide), such as VM flavor, size, or assigned role</li>
<li>How many package upgrades are pending, if a restart is required</li>
<li>Anything else not covered by the built-in collectors</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment</h1>
                </header>
            
            <article>
                
<p>The test environment for static infrastructure for this chapter should already have <kbd>node_exporter</kbd> up and running through the automatic provisioning. Nevertheless, we can inspect it by connecting, for example, to the <kbd>target01</kbd> VM as follows:</p>
<pre><strong>cd ./chapter06/</strong><br/><strong>vagrant ssh target01</strong></pre>
<p>Then check the configuration of the provided <kbd>systemd</kbd> unit file like so:</p>
<pre><strong>vagrant@target01:~$ systemctl cat node-exporter</strong></pre>
<p>In this snippet, we can see the <kbd>textfile</kbd> collector directory being set so that custom metrics can be exported:</p>
<pre><strong>...</strong><br/><strong>ExecStart=/usr/bin/node_exporter --collector.textfile.directory=/var/lib/node_exporter</strong><br/><strong>...</strong></pre>
<p>Let's try creating a custom metric. To do that, we only need to write the metric to a file inside the <kbd>textfile</kbd> collector directory with a <kbd>.prom</kbd> extension as follows:</p>
<pre><strong>vagrant@target01:~$ echo test_metric 1 | sudo tee /var/lib/node_exporter/test.prom</strong></pre>
<div class="packt_tip">In a real-world scenario, you would need to make sure the file was written atomically so that <kbd>node_exporter</kbd> wouldn't see a half-written (thus corrupted) file. You could either write it out to a temporary file and then <kbd>mv</kbd> it into place (taking care to not cross mount point boundaries), or use the <kbd>sponge</kbd> utility, which is usually found in the <kbd>moreutils</kbd> package.</div>
<p>We can then request the <kbd>/metrics</kbd> endpoint and search for our test metric like so:</p>
<pre><strong>vagrant@target01:~$ curl -qs 0:9100/metrics | grep test_metric</strong></pre>
<p>The output should be something like the following:</p>
<pre><strong># HELP test_metric Metric read from /var/lib/node_exporter/test.prom</strong><br/><strong># TYPE test_metric untyped</strong><br/><strong>test_metric 1</strong></pre>
<p>This exporter produces a large number of metrics, depending on which collectors are enabled. Some of the more interesting metrics available from <kbd>node_exporter</kbd> are the following:</p>
<ul>
<li><kbd>node_cpu_seconds_total</kbd>, which provides the number of seconds cumulatively used per core for all the available CPU modes, is very useful for understanding the CPU utilization</li>
<li><kbd>node_memory_MemTotal_bytes</kbd> and <kbd>node_memory_MemAvailable_bytes</kbd>, which allow calculating the ratio of memory available</li>
<li><kbd>node_filesystem_size_bytes</kbd> and <kbd>node_filesystem_avail_bytes</kbd>, which enable the calculation of filesystem utilization</li>
<li><kbd>node_textfile_scrape_error</kbd>, which tells you if the textfile collector couldn't parse any of the metrics files in the textfile directory (when this collector is enabled)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Container exporter</h1>
                </header>
            
            <article>
                
<p>In the constant pursuit for workload isolation and resource optimization, we witnessed the move from physical to virtualized machines using hypervisors. Using virtualization implies a certain degree of resource usage inefficiency, as the storage, CPU, and memory need to be allocated to each running VM whether it uses them or not. A lot of work has been done in this area to mitigate such inefficiencies but, in the end, fully taking advantage of system resources is still a difficult problem.</p>
<p>With the rise of operating-system-level virtualization on Linux (that is, the use of containers), the mindset changed. We no longer want a full copy of an OS for each workload, but instead, only properly isolated processes to do the desired work. To achieve this, and focusing specifically on Linux containers, a set of kernel features responsible for isolating hardware resources (named cgroups or control groups) and kernel resources (named namespaces) were made available. Resources managed by cgroups are as follows:</p>
<ul>
<li>CPU</li>
<li>Memory</li>
<li>Disk I/O</li>
<li>Network</li>
</ul>
<p>These kernel features allow the user to have fine control over what resources a given workload has available, thus optimizing resource usage. Cgroups metrics are invaluable to any modern monitoring system. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">cAdvisor</h1>
                </header>
            
            <article>
                
<p><strong>Container Advisor</strong> (<strong>cAdvisor</strong>) is a project developed by Google that collects, aggregates, analyzes, and exposes data from running containers. The data available covers pretty much anything you might require, from memory limits to GPU metrics, all available and segregated by container and/or host.</p>
<p>cAdvisor isn't tied to Docker containers but it's usually deployed as one. Data is collected from the container daemon and Linux cgroups, making the discovery of containers transparent and completely automatic. It also exposes process limits and throttling events whenever these limits are reached, which is important information to keep an eye on to maximize infrastructure resource usage without negatively impacting workloads.</p>
<p>Besides exposing metrics in the Prometheus format, cAdvisor also ships with a useful web interface, allowing the instant visualization of the status of hosts and their containers.</p>
<div class="packt_infobox">The source code and installation files for cAdvisor are available at <a href="https://github.com/google/cadvisor">https://github.com/google/cadvisor</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuration</h1>
                </header>
            
            <article>
                
<p>When launching cAdvisor as a container, some host paths are required to be available in read-only mode. This will allow, for example, the collection of kernels, processes, and container data.</p>
<p>There are quite a few runtime flags, so we'll feature some of the most relevant for our test case in the following table:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 26.4596%">
<p><strong>Flag</strong></p>
</td>
<td style="width: 71.5404%">
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td style="width: 26.4596%">
<p><kbd>--docker</kbd></p>
</td>
<td style="width: 71.5404%">
<p>Docker endpoint, defaults to <kbd>unix:///var/run/docker.sock</kbd></p>
</td>
</tr>
<tr>
<td style="width: 26.4596%">
<p><kbd>--docker_only</kbd></p>
</td>
<td style="width: 71.5404%">
<p>Only report containers in addition to root stats</p>
</td>
</tr>
<tr>
<td style="width: 26.4596%">
<p><kbd>--listen_ip</kbd></p>
</td>
<td style="width: 71.5404%">
<p>IP to bind on, default to <kbd>0.0.0.0</kbd></p>
</td>
</tr>
<tr>
<td style="width: 26.4596%">
<p><kbd>--port</kbd></p>
</td>
<td style="width: 71.5404%">
<p>Port to listen on, defaults to <kbd>8080</kbd></p>
</td>
</tr>
<tr>
<td style="width: 26.4596%">
<p><kbd>--storage_duration</kbd></p>
</td>
<td style="width: 71.5404%">
<p>How long to store data, defaults to <kbd>2m0s</kbd></p>
</td>
</tr>
</tbody>
</table>
<div class="packt_tip">You can inspect the available runtime configurations using the following address: <a href="https://github.com/google/cadvisor/blob/release-v0.33/docs/runtime_options.md">https://github.com/google/cadvisor/blob/release-v0.33/docs/runtime_options.md</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment</h1>
                </header>
            
            <article>
                
<p>Although historically the cAdvisor code was embedded in the Kubelet binary, it is currently scheduled to be deprecated there. Therefore, we'll be launching cAdvisor as a DaemonSet to future proof this example and to expose its configurations, while also enabling its web interface, as a Kubernetes service, to be explored.</p>
<p>Ensure you move into the correct repository path as shown here:</p>
<pre><strong>cd ./chapter06/provision/kubernetes/</strong></pre>
<p>Next, we must create a DaemonSet, because we want cAdvisor running in every single node:</p>
<pre>apiVersion: apps/v1<br/>kind: DaemonSet<br/>metadata:<br/>  name: cadvisor<br/>  namespace: monitoring<br/>...</pre>
<p><span>Notice all the volume mounts allowing the collection of data from the Docker daemon and various Linux resources as shown here:</span></p>
<pre>...<br/>    spec:<br/>     containers:<br/>      - name: cadvisor<br/>        volumeMounts:<br/>        - {name: rootfs, mountPath: /rootfs, readOnly: true}<br/>        - {name: var-run, mountPath: /var/run, readOnly: true}<br/>        - {name: sys, mountPath: /sys, readOnly: true}<br/>        - {name: docker, mountPath: /var/lib/docker, readOnly: true}<br/>        - {name: disk, mountPath: /dev/disk, readOnly: true}<br/>...</pre>
<p>Apply the previous manifest using the following instruction:</p>
<pre><strong>kubectl apply -f ./cadvisor/cadvisor-daemonset.yaml</strong></pre>
<p>We can follow the deployment status using the following:</p>
<pre><strong>kubectl rollout status daemonset/cadvisor -n monitoring</strong></pre>
<p>When the deployment finishes, it's time to add a new service. Notice the port name that will be used in the ServiceMonitor. Here's the manifest we'll be using:</p>
<pre>apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  labels:<br/>    p8s-app: cadvisor<br/>  name: cadvisor-service<br/>  namespace: monitoring<br/>spec:<br/>  selector:<br/>    p8s-app: cadvisor<br/>  type: NodePort<br/>  ports:<br/>  - {<strong>name: http</strong>, protocol: TCP, port: 8080, targetPort: http}</pre>
<p>The manifest can be applied using the following:</p>
<pre><strong>kubectl apply -f ./cadvisor/cadvisor-service.yaml</strong></pre>
<p>We can now connect to the cAdvisor web interface using the following instruction:</p>
<pre><strong>minikube service cadvisor-service -n monitoring</strong></pre>
<p>This will open a browser window with an interface similar to the following figure:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/21261d68-ce9a-41af-a132-ea1ae8e77535.png" style="width:36.25em;height:25.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 6.3:</span> cAdvisor web interface</div>
<p>It's time to add cAdvisor exporters as new targets for Prometheus. For that, we'll be using the next <kbd>ServiceMonitor</kbd> manifest as shown here:</p>
<pre>apiVersion: monitoring.coreos.com/v1<br/>kind: ServiceMonitor<br/>metadata:<br/>  labels:<br/>    p8s-app: cadvisor<br/>  name: cadvisor-metrics<br/>  namespace: monitoring<br/>spec:<br/>  endpoints:<br/>  - interval: 30s<br/>    port: <strong>http</strong><br/>  selector:<br/>    matchLabels:<br/>      p8s-app: cadvisor</pre>
<p class="mce-root"/>
<p>The previous manifest can be applied using:</p>
<pre><strong>kubectl apply -f ./cadvisor/cadvisor-servicemonitor.yaml</strong></pre>
<p>After a few moments, you can inspect the newly added targets in the Prometheus web interface, using the following instruction to open its web interface:</p>
<pre><strong>minikube service prometheus-service -n monitoring</strong></pre>
<p>The following figure illustrates the <span>Prometheus <kbd>/targets</kbd> endpoint showing cAdvisor target:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f901508a-146e-4557-bc0a-b73f384cc5b7.png" style="width:40.83em;height:14.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 6.4:</span> Prometheus <em>/targets</em> endpoint showing cAdvisor target</div>
<p>With this, we now have container-level metrics. Do note that cAdvisor exports a large amount of samples per container, which can easily balloon exported metrics to multiple thousands of samples per scrape, possibly causing cardinality-related issues on the scraping Prometheus.</p>
<div class="packt_infobox">You can find every metric exposed by cAdvisor at their Prometheus documentation: <a href="https://github.com/google/cadvisor/blob/release-v0.33/docs/storage/prometheus.md">https://github.com/google/cadvisor/blob/release-v0.33/docs/storage/prometheus.md</a>.</div>
<p>From the thousands of metrics exported by cAdvisor, these are generally useful for keeping an eye out for problems:</p>
<ul>
<li><kbd>container_last_seen</kbd>,  which keeps track of the timestamp the container was last seen as running</li>
<li><kbd>container_cpu_usage_seconds_total</kbd>, which gives you a counter of the number of CPU seconds per core each container has used</li>
<li><kbd>container_memory_usage_bytes</kbd> and <kbd>container_memory_working_set_bytes</kbd>, which keep track of container memory usage (including cache and buffers) and just container active memory, respectively</li>
<li><kbd>container_network_receive_bytes_total</kbd> and <kbd>container_network_transmit_bytes_total</kbd>, which let you know how much traffic in the container receiving and transmitting, respectively</li>
</ul>
<p>When running on Kubernetes, cAdvisor doesn't provide you with insight into how the cluster is running—application-level metrics from Kubernetes itself. For this, we need another exporter: kube-state-metrics.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">kube-state-metrics</h1>
                </header>
            
            <article>
                
<p>kube-state-metrics does not export container-level data, as that's not its function. It operates at a higher level, exposing the Kubernetes state, providing metrics regarding the API internal objects such as pods, services, or deployments. The object metric groups currently available when using this exporter are the following:</p>
<ul>
<li>CronJob metrics</li>
<li>DaemonSet metrics</li>
<li>Deployment metrics</li>
<li>Job metrics</li>
<li>LimitRange metrics</li>
<li>Node metrics</li>
<li>PersistentVolume metrics</li>
<li>PersistentVolumeClaim metrics</li>
<li>Pod metrics</li>
<li>Pod Disruption Budget metrics</li>
<li>ReplicaSet metrics</li>
<li>ReplicationController metrics</li>
<li>Resource quota metrics</li>
<li>Service metrics</li>
<li>StatefulSet metrics</li>
<li>Namespace metrics</li>
<li>Horizontal Pod Autoscaler metrics</li>
<li>Endpoint metrics</li>
<li>Secret metrics</li>
<li>ConfigMap metrics</li>
</ul>
<p>There are two endpoints exposed by kube-state-metrics: one provides the API objects metrics and the other presents the internal metrics from the exporter itself.</p>
<div class="packt_infobox">The source code and installation files for kube-state-metrics are available at <a href="https://github.com/kubernetes/kube-state-metrics">https://github.com/kubernetes/kube-state-metrics</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuration</h1>
                </header>
            
            <article>
                
<p>When configuring kube-state-metrics, other than all the required RBAC permissions, there are also several runtime flags to be aware of. We will provide an overview of the more relevant ones for our test case in the following table:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 29.7765%">
<p><strong>Flag</strong></p>
</td>
<td style="width: 66.2235%">
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td style="width: 29.7765%">
<p><kbd>--host</kbd></p>
</td>
<td style="width: 66.2235%">
<p>IP to bind and <span>expose Kubernetes metrics on</span>, defaults to <kbd>0.0.0.0</kbd></p>
</td>
</tr>
<tr>
<td style="width: 29.7765%">
<p><kbd>--port</kbd></p>
</td>
<td style="width: 66.2235%">
<p>Port to expose Kubernetes metrics, defaults to <kbd>80</kbd></p>
</td>
</tr>
<tr>
<td style="width: 29.7765%">
<p><kbd>--telemetry-host</kbd></p>
</td>
<td style="width: 66.2235%">
<p>IP to expose internal metrics, defaults to <kbd>0.0.0.0</kbd></p>
</td>
</tr>
<tr>
<td style="width: 29.7765%">
<p><kbd>--telemetry-port</kbd></p>
</td>
<td style="width: 66.2235%">
<p>Port to expose internal metrics, defaults to <kbd>80</kbd></p>
</td>
</tr>
<tr>
<td style="width: 29.7765%">
<p><kbd>--collectors</kbd></p>
</td>
<td style="width: 66.2235%">
<p>Comma-separated list of metrics groups to enable, defaults to <span>ConfigMap</span>, CronJobs, DaemonSets, Deployments, endpoints, horizontalpodautoscalers, Jobs, LimitRanges, namespaces, Nodes, PersistentVolumeClaims, PersistentVolumes, PodDisruptionBudgets, pods, ReplicaSets, ReplicationControllers, resource quotas, Secrets, services, StatefulSets</p>
</td>
</tr>
<tr>
<td style="width: 29.7765%">
<p><kbd>--metric-blacklist</kbd></p>
</td>
<td style="width: 66.2235%">
<p>Comma-separated list of metrics to disable, mutually exclusive with the whitelist</p>
</td>
</tr>
<tr>
<td style="width: 29.7765%">
<p><kbd>--metric-whitelist</kbd></p>
</td>
<td style="width: 66.2235%">
<p>Comma-separated list of metrics to enable, mutually exclusive with the blacklist</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<div class="packt_infobox">Due to the unpredictable amount of objects required to be exported, which are directly proportional to the size of the cluster, a common pattern when deploying kube-state-metrics is to use a special container called <strong>addon-resizer</strong>, which can vertically resize the exporter pod dynamically. Information regarding <em>addon-resizer</em> can be found at <a href="https://github.com/kubernetes/autoscaler/tree/addon-resizer-release-1.8">https://github.com/kubernetes/autoscaler/tree/addon-resizer-release-1.8</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment</h1>
                </header>
            
            <article>
                
<p>We'll be building upon the Kubernetes test environment started previously. To begin the deployment, ensure you move into the correct repository path, relative to the repository root as follows:</p>
<pre><strong>cd ./chapter06/provision/kubernetes/</strong></pre>
<p>As access to the Kubernetes API is required, the <strong>role-based access control</strong> (<strong>RBAC</strong>) configuration for this deploy is quite extensive, which includes a Role, a RoleBinding, a ClusterRole, a ClusterRoleBinding, and a ServiceAccount. This manifest is available at <kbd>./kube-state-metrics/kube-state-metrics-rbac.yaml</kbd>.</p>
<p>It should be applied using the following command:</p>
<pre><strong>kubectl apply -f ./kube-state-metrics/kube-state-metrics-rbac.yaml</strong></pre>
<p>We'll be creating a deployment for kube-state-metrics with just one instance, as, in this case, no clustering or special deployment requirements are necessary:</p>
<pre>apiVersion: apps/v1<br/>kind: <strong>Deployment</strong><br/>metadata:<br/>  name: kube-state-metrics<br/>  namespace: monitoring <br/>spec:<br/>  selector:<br/>    matchLabels:<br/>      k8s-app: kube-state-metrics<br/>  replicas: <strong>1</strong><br/>...</pre>
<p>This deployment will run an instance of the <kbd>kube-state-metrics</kbd> exporter, along with <kbd>addon-resizer</kbd> to scale the exporter dynamically:</p>
<pre>...<br/>  template:<br/>    spec:<br/>      serviceAccountName: kube-state-metrics<br/>      containers:<br/>      - name: <strong>kube-state-metrics</strong><br/>...<br/>      - name: <strong>addon-resizer</strong><br/>...</pre>
<p>This can be applied using the following instruction:</p>
<pre><strong>kubectl apply -f ./kube-state-metrics/kube-state-metrics-deployment.yaml</strong></pre>
<p>We can follow the deployment status using the following:</p>
<pre><strong>kubectl rollout status deployment/kube-state-metrics -n monitoring</strong></pre>
<p>After a successful deployment, we'll be creating a service for this exporter, this time with two ports: one for the Kubernetes API object metrics and another for the exporter's internal metrics themselves:</p>
<pre>apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  name: kube-state-metrics<br/>  namespace: monitoring<br/>  labels:<br/>    k8s-app: kube-state-metrics<br/>  annotations:<br/>    prometheus.io/scrape: 'true'<br/>spec:<br/>  type: NodePort<br/>  ports:<br/>  - {name: <strong>http-metrics</strong>, port: 8080, targetPort: http-metrics, protocol: TCP}<br/>  - {name: <strong>telemetry</strong>, port: 8081, targetPort: telemetry, protocol: TCP}<br/>  selector:<br/>    k8s-app: kube-state-metrics</pre>
<p>The previous manifest can be applied as follows:</p>
<pre><strong>kubectl apply -f ./kube-state-metrics/kube-state-metrics-service.yaml</strong></pre>
<p>With the service in place, we are able to validate both metrics endpoints using the following command:</p>
<pre><strong>minikube service kube-state-metrics -n monitoring</strong></pre>
<p>This will open two different browser tabs, one for each metrics endpoint:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/41899a12-c246-4f9c-99ad-f2b051013453.png" style="width:20.58em;height:5.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 6.5: </span>The kube-state-metrics web interface</div>
<p>Finally, it is time to configure Prometheus to scrape both endpoints using the <kbd>ServiceMonitor</kbd> manifest as shown here:</p>
<pre>apiVersion: monitoring.coreos.com/v1<br/>kind: ServiceMonitor<br/>metadata:<br/>  labels:<br/>    k8s-app: kube-state-metrics<br/>  name: kube-state-metrics<br/>  namespace: monitoring<br/>spec:<br/>  endpoints:<br/>  - interval: 30s<br/>    port: <strong>http-metrics</strong><br/>  - interval: 30s<br/>    port: <strong>telemetry</strong><br/>  selector:<br/>    matchLabels:<br/>      k8s-app: kube-state-metrics</pre>
<p>And it can now be applied using the following command:</p>
<pre><strong>kubectl apply -f ./kube-state-metrics/kube-state-metrics-servicemonitor.yaml</strong></pre>
<p>We can now validate the correct configuration of scrape targets in Prometheus, using the following instruction to open its web interface:</p>
<pre><strong>minikube service prometheus-service -n monitoring</strong></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cb45ae0f-8e9f-481e-b075-ab635a27c1cf.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 6.6:</span> Prometheus /targets endpoint showing kube-state-metrics targets for metrics and telemetry</div>
<p class="mce-root">Some interesting metrics from kube-state-metrics that can be used to keep an eye on your Kubernetes clusters are:</p>
<ul>
<li class="mce-root"> <kbd>kube_pod_container_status_restarts_total</kbd>, which can tell you if a given pod is restarting on a loop;</li>
<li><kbd>kube_pod_status_phase</kbd>, which can be used to alert on pods that are in a non-ready state for a long time;</li>
<li>Comparing <kbd>kube_&lt;object&gt;_status_observed_generation</kbd> with <kbd>kube_&lt;object&gt;_metadata_generation</kbd> can give you a sense when a given object has failed but hasn't been rolled back</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">From logs to metrics</h1>
                </header>
            
            <article>
                
<p>In a perfect world, all applications and services would have been properly instrumented and we would only be required to collect metrics to gain visibility. External exporters are a stop-gap approach that simplifies our work, but not every service exposes its internal state through a neat API. Older daemon software, such as Postfix or ntpd, makes use of logging to relay their inner workings. For these cases, we're left with two options: either instrument the service ourselves (which isn't possible for closed source software) or rely on logs to gather the metrics we require. The next topics go over the available options for extracting metrics from logs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">mtail</h1>
                </header>
            
            <article>
                
<p>Developed by Google, mtail is a very light log processor that is capable of running programs with pattern matching logic, allowing the extraction of metrics from said logs. It supports multiple export formats, such as Prometheus, StatsD, Graphite, and more.</p>
<p>Besides the <kbd>/metrics</kbd> endpoint, the <kbd>/</kbd> endpoint for the mtail service exposes valuable debug information. This endpoint is available in the static infrastructure test environment at <kbd>http://192.168.42.11:9197</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/80ad4e51-8be9-4473-8202-3ed0462d3b6f.png" style="width:33.17em;height:26.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 6.7:</span> mtail web interface</div>
<div class="packt_infobox">The source code and installation files for mtail are available at <a href="https://github.com/google/mtail">https://github.com/google/mtail</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuration</h1>
                </header>
            
            <article>
                
<p class="mce-root">To configure mtail, we require a program with the pattern matching logic. Let's look at a very straightforward example available in the official repository:</p>
<pre># simple line counter<br/>counter line_count<br/>/$/ {<br/>  line_count++<br/>}</pre>
<p class="mce-root">This program defines the <kbd>line_count</kbd> metric of the <kbd>counter</kbd> type, an RE2-compatible expression <kbd>/$/</kbd> matching the end of a line, and finally, an action between <kbd>{ }</kbd>, which, in this case, increments the <kbd>line_count</kbd> counter.</p>
<p class="mce-root">To run this program, we are only required to start mtail with command line flags to point it to our program and to the log we want to monitor. Here are some of the most useful flags for our test case:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 19.8098%">
<p><kbd>-address</kbd></p>
</td>
<td style="width: 79.2393%">
<p>Host or IP to bind</p>
</td>
</tr>
<tr>
<td style="width: 19.8098%">
<p><kbd>-port</kbd></p>
</td>
<td style="width: 79.2393%">
<p>Listener port, defaults to <kbd>3903</kbd></p>
</td>
</tr>
<tr>
<td style="width: 19.8098%">
<p><kbd>-progs</kbd></p>
</td>
<td style="width: 79.2393%">
<p>Path to the programs</p>
</td>
</tr>
<tr>
<td style="width: 19.8098%">
<p><kbd>-logs</kbd></p>
</td>
<td style="width: 79.2393%">
<p>Comma-separated list of files to monitor (this flag can be set multiple times)</p>
</td>
</tr>
</tbody>
</table>
<div class="packt_tip">You can find the mtail programming guide at <a href="https://github.com/google/mtail/blob/master/docs/Programming-Guide.md">https://github.com/google/mtail/blob/master/docs/Programming-Guide.md</a> and the RE2 syntax at <a href="https://github.com/google/re2/wiki/Syntax">https://github.com/google/re2/wiki/Syntax</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment</h1>
                </header>
            
            <article>
                
<p class="mce-root">In our static infrastructure test environment, we can validate the configuration of mtail by connecting to the <kbd>target01</kbd> instance as shown here:</p>
<pre><strong>cd ./chapter06/</strong><br/><br/><strong>vagrant ssh target01</strong></pre>
<p>Then checking the configuration of the provided <kbd>systemd</kbd> unit file as shown in the following command:</p>
<pre><strong>vagrant@target01:~$ systemctl cat mtail-exporter</strong></pre>
<p>As in this example, <kbd>mtail</kbd> is counting the lines of the <kbd>syslog</kbd> file so it needs to have proper permissions to access system logs, and thus, we run the <kbd>mtail</kbd> daemon with <kbd>Group=adm</kbd> to make this work. We can see all the required arguments for the <kbd>mtail</kbd> service in the following snippet from the unit file, including the path to the line count program:</p>
<pre><strong>...</strong><br/><strong>Group=adm</strong><br/><strong>ExecStart=/usr/bin/mtail -address 0.0.0.0 -port 9197 -progs /etc/mtail_exporter/line_count.mtail -logs /var/log/syslog</strong><br/><strong>...</strong></pre>
<p>On the Prometheus instance, we added the following job:</p>
<pre>  - job_name: 'mtail'<br/>    scrape_interval: 15s<br/>    scrape_timeout: 5s<br/>    static_configs:<br/>      - targets: ['target01:9197']</pre>
<div class="packt_infobox">In a real-world scenario, you would name the scrape job as the daemon whose logs mtail is monitoring, such as ntpd or Postfix.</div>
<p>Using the Prometheus expression browser, available at <kbd>http://192.168.42.10:9090</kbd>, we can validate, not only that the scrapes are being successful through the <kbd>up</kbd> metric, but also that our metric is available:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2f3bbc09-a32c-4f34-aa3a-609d72f408e1.png" style="width:58.83em;height:58.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 6.8:</span> mtail line_count metric</div>
<p>Some interesting metrics from mtail that can be used to keep an eye on this exporter are:</p>
<ul>
<li><kbd>mtail_log_watcher_error_count</kbd> , which counts the number of errors received from <kbd>fsnotify</kbd> (kernel-based notification system for filesystem events)</li>
<li><kbd>mtail_vm_line_processing_duration_milliseconds_bucket</kbd>, a histogram which provides the line processing duration distribution in milliseconds per mtail program</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Grok exporter</h1>
                </header>
            
            <article>
                
<p>Similarly to <kbd>mtail</kbd>, <kbd>grok_exporter</kbd> parses unstructured log data and generates metrics from it. However, as the name suggests, the main difference is the domain-specific language for this exporter being modeled after the Logstash pattern language (Grok), which enables the reuse of patterns you might already have built.</p>
<div class="packt_infobox">The source code and installation files for <kbd>grok_exporter</kbd> are available at <a href="https://github.com/fstab/grok_exporter">https://github.com/fstab/grok_exporter</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuration</h1>
                </header>
            
            <article>
                
<p>This exporter requires a conf<span>iguration file for its setup. There are five main sections in the configuration, which w</span>e can dissect in the following snippets from the exporter's configuration file deployed in our static infrastructure test environment. The <kbd>global</kbd> section sets the configuration format version.  Version 2 is currently the standard configuration version, and so we set it here:</p>
<pre>global:<br/>    config_version: 2</pre>
<p>The input section defines the location of the logs to be parsed. If <kbd>readall</kbd> is set to <kbd>true</kbd>, the file will be completely parsed before waiting for new lines; as we can see, we're not doing that in our example:</p>
<pre>input:<br/>    type: file<br/>    path: /var/log/syslog<br/>    readall: false</pre>
<p>The <kbd>grok</kbd> section loads the patterns to use for parsing. These are configured in a separate location, as can be seen here:</p>
<pre>grok:<br/>    patterns_dir: /etc/grok_exporter/patterns</pre>
<p>The <kbd>metrics</kbd> section is where the magic happens. It d<span>efines what metrics to extract from the parsed logs. Every Prometheus metric type is natively supported in this exporter. The configuration for each <kbd>type</kbd> can be slightly different, so you should check its documentation. However, we're going to provide an overview of the configuration that is common among them:</span></p>
<ul>
<li><span>The <kbd>match</kbd> configuration  defines the regular expression for data extraction; in our example, <kbd>LOGLEVEL</kbd> is a predefined pattern to match log levels.</span></li>
<li><span>The <kbd>labels</kbd> configuration is able to use Go's templating syntax to output whatever was extracted from the match definition; in this case, we used <kbd>level</kbd> as our variable in the match pattern and so it is available as <kbd>.level</kbd> in the template:</span></li>
</ul>
<pre style="padding-left: 60px">metrics:<br/>    - type: counter<br/>      name: grok_loglevel_example_total<br/>      help: Total log level events triggered.<br/>      match: '.*\(echo %{LOGLEVEL:level}\)$' <br/>      labels:<br/>          level: '{{.level}}'</pre>
<div class="packt_tip">The full configuration documentation is available at <a href="https://github.com/fstab/grok_exporter/blob/v0.2.7/CONFIG.md">https://github.com/fstab/grok_exporter/blob/v0.2.7/CONFIG.md</a></div>
<p>Finally, the <kbd>server</kbd> section is where the bind address and port for the exporter are defined:</p>
<pre>server:<br/>    host: 0.0.0.0<br/>    port: 9144</pre>
<p>Now that we have a better understanding of what goes into the configuration file, it is time for us to try this exporter out in our test environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment</h1>
                </header>
            
            <article>
                
<p>In our static infrastructure test environment, we can validate the configuration of <kbd>grok_exporter</kbd> by connecting to the <kbd>target01</kbd> instance as shown here:</p>
<pre><strong>cd ./chapter06/</strong><br/><br/><strong>vagrant ssh target01</strong></pre>
<p>Outputting the configuration of the provided <kbd>systemd</kbd> unit file is shown in the following code snippet:</p>
<pre><strong>vagrant@target01:~$ systemctl cat grok-exporter</strong></pre>
<p>Just as with the <kbd>mtail</kbd> exporter, we need to run <kbd>grok_exporter</kbd> with <kbd>Group=adm</kbd> so that it has access to <kbd>syslog</kbd> without requiring being run as a privileged user. We can see all the required arguments for the <kbd>grok_exporter</kbd> service in the following snippet from the unit file, including the path to the configuration file mentioned before:</p>
<pre><strong>...</strong><br/><strong>Group=adm</strong><br/><strong>ExecStart=/usr/bin/grok_exporter -config /etc/grok_exporter/config.yml</strong><br/><strong>...</strong></pre>
<p>On the Prometheus instance, we added the following job:</p>
<pre>  - job_name: 'grok'<br/>    scrape_interval: 15s<br/>    scrape_timeout: 5s<br/>    static_configs:<br/>      - targets: ['target01:9144']</pre>
<p>Using the Prometheus expression browser, available at <kbd>http://192.168.42.10:9090</kbd>, we can validate not only whether the scrapes are successful but also that our metric is available:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e2f553af-8c1b-4021-9dec-dfeb67e9a58d.png" style="width:51.42em;height:51.50em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 6.9:</span> <em>grok_exporter</em> example metric</div>
<p>Some interesting metrics from <kbd>grok_exporter</kbd> that can be used to keep an eye on this exporter are:</p>
<ul>
<li><kbd>grok_exporter_line_buffer_peak_load</kbd>, a summary which provides the number of lines that are read from the log file and waiting to be processed</li>
<li><kbd>grok_exporter_line_processing_errors_total</kbd>, which exposes the total number of processing errors for each defined metric</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Blackbox monitoring</h1>
                </header>
            
            <article>
                
<p>Introspection is invaluable to gather data about a system, but sometimes we're required to measure from the point of view of a user of that system. In such cases, probing is a good option to simulate user interaction. As probing is made from the outside and without knowledge regarding the inner workings of the system, this is classified as blackbox monitoring, as discussed in <a href="4214ddff-8289-4dc6-b0ef-240510a22192.xhtml">Chapter 1</a>, <em>Monitoring Fundamentals</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Blackbox exporter</h1>
                </header>
            
            <article>
                
<p><kbd>blackbox_exporter</kbd> is one of the most peculiar of all the currently available exporters in the Prometheus ecosystem. Its usage pattern is ingenious and usually, newcomers are puzzled by it. We'll be going to dive into this exporter with the hope of making its use as straightforward as possible.</p>
<p>The <kbd>blackbox_exporter</kbd> service exposes two main endpoints:</p>
<ul>
<li><kbd>/metrics</kbd>: Where its own metrics are exposed</li>
<li><kbd>/probe</kbd>: It is the query endpoint that enables blackbox probes, returning their results in Prometheus exposition format</li>
</ul>
<p>Besides the two previous endpoints, the <kbd>/</kbd> of the service also provides valuable information, including logs for the probes performed. This endpoint is available in the static infrastructure test environment at <kbd>http://192.168.42.11:9115</kbd>.</p>
<p>The blackbox exporter supports probing endpoints through a wide variety of protocols natively, such as TCP, ICMP, DNS, HTTP (versions 1 and 2), as well as TLS on most probes. Additionally, it also supports scripting text-based protocols such as IRC, IMAP, or SMTP by connecting through TCP and configuring what messages should be sent and what responses are expected; even plain HTTP would be possible to script but, as HTTP probing is such a common use case, it's already built in.</p>
<p>Having said that, this exporter doesn't cover all the blackbox-style monitoring needs. For those cases, writing your own exporter might be needed. As an example, you can't use <kbd>blackbox_exporter</kbd> to test a Kafka topic end to end, so you might need to look for an exporter able to produce a message to Kafka and then consume it back:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/93515ee5-4572-42e6-984b-a8520d7d16e8.png" style="width:16.83em;height:14.83em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 6.10:</span> blackbox_exporter web interface</div>
<p>The <kbd>/probe</kbd> endpoint, when hit with an HTTP GET request with the parameters module and target, it executes the specified <kbd>prober</kbd> module against the defined target, and the result is then exposed as Prometheus metrics:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/487c05fd-69a4-41c3-9272-f917c67ea8fe.png" style="width:40.00em;height:22.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 6.11: </span>blackbox_exporter high-level workflow</div>
<p>For example, a request such as <kbd>http://192.168.42.11:9115/probe?module=http_2xx&amp;target=example.com</kbd> will return something like the following snippet (a couple of metrics were discarded for briefness):</p>
<pre># HELP probe_duration_seconds Returns how long the probe took to complete in seconds<br/># TYPE probe_duration_seconds gauge<br/>probe_duration_seconds 0.454460181<br/># HELP probe_http_ssl Indicates if SSL was used for the final redirect<br/># TYPE probe_http_ssl gauge<br/>probe_http_ssl 0<br/># HELP probe_http_status_code Response HTTP status code<br/># TYPE probe_http_status_code gauge<br/>probe_http_status_code 200<br/># HELP probe_ip_protocol Specifies whether probe ip protocol is IP4 or IP6<br/># TYPE probe_ip_protocol gauge<br/>probe_ip_protocol 4<br/># HELP probe_success Displays whether or not the probe was a success<br/># TYPE probe_success gauge<br/>probe_success 1</pre>
<div class="packt_tip">When debugging probes, you can append <kbd>&amp;debug=true</kbd> to the HTTP GET URL to enable debug information.</div>
<div class="packt_infobox">The source code and installation files for <kbd>blackbox_exporter</kbd> are available at <a href="https://github.com/prometheus/blackbox_exporter">https://github.com/prometheus/blackbox_exporter</a>.</div>
<p>A quirk to be aware of when using <kbd>blackbox_exporter</kbd> is that the <kbd>up</kbd> metric does not reflect the status of the probe, but merely that Prometheus can reach the exporter. As can be seen in the previous metrics output, there is a <kbd>probe_success</kbd> metric that represents the status of the probe itself. This means that it is common for the <kbd>up</kbd> metric to appear healthy, but the probe might be failing, which is a common source for confusion.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuration</h1>
                </header>
            
            <article>
                
<p>The scrape job configuration for blackbox probes is unusual, in the sense that both the <kbd>prober</kbd> module and the list of targets, whether static or discovered, need to be relayed to the exporter as HTTP GET parameters to the <kbd>/probe</kbd> endpoint. To make this work, a bit of <kbd>relabel_configs</kbd> magic is required, as seen in <a href="12e775c2-bee9-4ebe-ad73-2f9313eeeeee.xhtml">Chapter 5</a>, <em>Running a Prometheus Server</em>.</p>
<p>Using the following Prometheus configuration snippet as an example, we're setting up an ICMP probe against the Prometheus instance, while <kbd>blackbox_exporter</kbd> is running on <kbd>target01</kbd>:</p>
<pre>  - job_name: 'blackbox-icmp'<br/>    metrics_path: /probe<br/>    params:<br/>      module: [icmp]<br/>    static_configs:<br/>      - targets:<br/>       - prometheus.prom.inet<br/>    relabel_configs:<br/>      - source_labels: [__address__]<br/>        target_label: __param_target<br/>      - source_labels: [__param_target]<br/>        target_label: instance<br/>      - target_label: __address__<br/>        replacement: target01:9115</pre>
<div class="packt_infobox">Due to the nature of the ICMP probe, it requires elevated privileges to be run. In our environment, we're setting the capability to use raw sockets (<kbd>setcap cap_net_raw+ep /usr/bin/blackbox_exporter</kbd>) to guarantee such privileges.</div>
<p>The goal is to replace the address of the target with the address of <kbd>blackbox_exporter</kbd>, ensuring the internal <kbd>__param_target</kbd> keeps the address of the target. Focusing on how the <kbd>relabel_configs</kbd> is processed, the following happens:</p>
<ul>
<li>The <kbd>__address__</kbd> value (which contains the address of the target) is stored into <kbd>__param_target</kbd>.</li>
<li><kbd>__param_target</kbd> value is then stored into the instance label.</li>
<li>The <kbd>blackbox_exporter</kbd> host is then applied to <kbd>__address__</kbd>.</li>
</ul>
<p>This enables to Prometheus to query <kbd>blackbox_exporter</kbd> (using <kbd>__address__</kbd>), keep the instance label with the target definition, and pass the parameters module and target (using the internal <kbd>__param_target</kbd>) to the <kbd>/probe</kbd> endpoint, which returns the metrics data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment</h1>
                </header>
            
            <article>
                
<p>In our static infrastructure test environment, we can validate the configuration of <kbd>blackbox_exporter</kbd> by connecting to the <kbd>target01</kbd> instance as shown here:</p>
<pre><strong>cd ./chapter06/</strong><br/><br/><strong>vagrant ssh target01</strong></pre>
<p>Then checking the configuration of the provided <kbd>systemd</kbd> unit file as shown in the following command:</p>
<pre><strong>vagrant@target01:~$ systemctl cat blackbox-exporter</strong></pre>
<div class="packt_tip">The configuration can be reloaded in runtime by sending an HTTP POST to the <kbd>/-/reload</kbd> endpoint or a <kbd>SIGHUP</kbd> to the <kbd>blackbox_exporter</kbd> process. If there are configuration errors, it will not be applied.</div>
<p>We can see all the required arguments for the <kbd>blackbox_exporter</kbd> service in the following snippet from the unit file, including the path to the configuration file:</p>
<pre><strong>...</strong><br/><strong>ExecStart=/usr/bin/blackbox_exporter --config.file=/etc/blackbox_exporter/blackbox.yml</strong><br/><strong>...</strong></pre>
<p>The configuration we tailored for our example can be found in the following snippet:</p>
<pre>modules:<br/>  http_2xx:<br/>    prober: http<br/>    http:<br/>      preferred_ip_protocol: ip4<br/><br/>  icmp:<br/>    prober: icmp<br/>    icmp:<br/>      preferred_ip_protocol: ip4</pre>
<div class="packt_infobox">Notice <kbd>preferred_ip_protocol: ip4</kbd> is used, as <kbd>blackbox_exporter</kbd> prefers <kbd>ipv6</kbd>, but we're forcing <kbd>ipv4</kbd> in our probes.</div>
<p>On the Prometheus instance, we added the following jobs:</p>
<pre>- job_name: 'blackbox-http'<br/>    metrics_path: /probe<br/>    params:<br/>      module: [http_2xx]<br/>    static_configs:<br/>      - targets: [ 'http://example.com', 'https://example.com:443' ]<br/>...<br/>  - job_name: 'blackbox-icmp'<br/>    metrics_path: /probe<br/>    params:<br/>      module: [icmp]<br/>    static_configs:<br/>      - targets:<br/>         - prometheus<br/>...</pre>
<p>Using the Prometheus web interface, available at <kbd>http://192.168.42.10:9090/targets</kbd>, we can validate whether the scrapes are successful (independently of the return status of the probes):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d2c5b2de-f1a8-4d04-a4e1-172c2cd6276d.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 6.12:</span> Prometheus /targets endpoint showing the blackbox_exporter targets</div>
<p>As mentioned before, the <kbd>/targets</kbd> page doesn't tell you whether a probe was successful or not. This needs to be validated in the expression browser by querying the <kbd>probe_success</kbd> metric:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/67800f60-c4c3-47dd-a5de-04f90153a2ac.png" style="width:27.25em;height:19.58em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 6.13: Prometheus expression browser showing the probe_success query results</div>
<p>Some interesting metrics that can be collected from <kbd>blackbox_exporter</kbd> (both about the exporter itself and from probes) are:</p>
<ul>
<li><kbd>blackbox_exporter_config_last_reload_successful</kbd>, which exposes if the exporter's configuration file was reloaded successfully after a <kbd>SIGHUP</kbd></li>
<li><kbd>probe_http_status_code</kbd>, which allows you to understand what HTTP status code is being returned when using the HTTP <kbd>prober</kbd> module</li>
<li><kbd>probe_ssl_earliest_cert_expiry</kbd>, which returns the timestamp for when the certificate chain from a SSL probe becomes invalid due to one of the certificates in the chain expiring</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pushing metrics</h1>
                </header>
            
            <article>
                
<p>Despite the intense debate regarding push versus pull and the deliberate decision of using pull in the Prometheus server design, there are some legitimate situations where push is more appropriate.</p>
<p>One of those situations is batch jobs, though, for this statement to truly make sense, we need to clearly define what is considered a batch job. In this scope, a service-level batch job is a processing workload not tied to a particular instance, executed infrequently or on a schedule, and as such is not always running. This kind of job makes it very hard to generate successful scrapes if instrumented, which, as discussed previously in <a href="12e775c2-bee9-4ebe-ad73-2f9313eeeeee.xhtml">Chapter 5</a>, <em>Running a Prometheus Server</em>, results in metric staleness, even if running for long enough to be scraped occasionally.</p>
<p>There are alternatives to relying on pushing metrics; for example, by using the textfile collector from <kbd>node_exporter</kbd> as described previously. Nevertheless, this option does not come without downsides. If the workload is not specific to a particular instance, you'll end up with multiple time series plus the cleanup logic of the textfile collector files, unless the lifetime of the metric matches the lifetime of the instance, which can then work out well in practice.</p>
<p>As a last resort, you have Pushgateway, which we'll be covering next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pushgateway</h1>
                </header>
            
            <article>
                
<p>This exporter should only be employed in very specific use cases, as stated previously, and we should be aware of some common pitfalls. One possible problem is the lack of high availability, making it a single point of failure. This also impacts scalability as the only way to accommodate more metrics/clients is to either scale the instance vertically (adding more resources) or sharding (having different Pushgateway instances for different logical groups). By using Pushgateway, Prometheus does not scrape an application instance directly, which prevents having the <kbd>up</kbd> metric as a proxy for health monitoring. Additionally, and similarly to the textfile collector from <kbd>node_exporter</kbd>, metrics need to be manually deleted from Pushgateway via its API, or they will forever be exposed to Prometheus.</p>
<p>To push a metric, you need to send an HTTP POST request to the Pushgateway endpoint, using the following URL path definition. This will be demonstrated in the following deployment section:</p>
<pre><strong>http://&lt;pushgateway_address&gt;:&lt;push_port&gt;/metrics/job/&lt;job_name&gt;/[&lt;label_name1&gt;/&lt;label_value1&gt;]/[&lt;label_nameN&gt;/&lt;label_valueN&gt;]</strong></pre>
<p>Here, <kbd>&lt;job_name&gt;</kbd> will become the value of the label job for the metrics pushed and the <kbd>&lt;label_name&gt;/&lt;label_value&gt;</kbd> pairs will become additional label/value pairs. Keep in mind that metrics will be available until manually deleted, or in the case of a restart, when persistence is not configured.</p>
<div class="packt_infobox">The source code and installation files for Pushgateway are available at <a href="https://github.com/prometheus/pushgateway">https://github.com/prometheus/pushgateway</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuration</h1>
                </header>
            
            <article>
                
<p>As Pushgateway is a centralized point where instances push their metrics, when a scrape is performed by Prometheus, the label instance will be automatically set to the Pushgateway server address/port for every single metric it exposes, and the label job to whatever name was set in the Prometheus scrape job definition. On label collision, Prometheus renames the original labels to <kbd>exported_instance</kbd> and <kbd>exported_job</kbd>, respectively. To avoid this behavior, <kbd>honor_labels: true</kbd> should be used in the scrape job definition to guarantee the labels that prevail are the ones coming from Pushgateway.</p>
<p>The noteworthy runtime configuration for our test case is as follows:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 35.1823%">
<p><strong>Flag</strong></p>
</td>
<td style="width: 63.8669%">
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td style="width: 35.1823%">
<p><kbd>--web.listen-address</kbd></p>
</td>
<td style="width: 63.8669%">
<p>Bind address, defaults to <kbd>0.0.0.0:9091</kbd></p>
</td>
</tr>
<tr>
<td style="width: 35.1823%">
<p><kbd>--persistence.file</kbd></p>
</td>
<td style="width: 63.8669%">
<p>Persistence file location, if empty metrics are only kept in memory</p>
</td>
</tr>
<tr>
<td style="width: 35.1823%">
<p><kbd>--persistence.interval</kbd></p>
</td>
<td style="width: 63.8669%">
<p>Interval to write to the persistence file, defaults to 5m</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment</h1>
                </header>
            
            <article>
                
<p>We'll be building upon the Kubernetes test environment started previously. In this particular scenario, we'll deploy an instance of Pushgateway and we'll be adding it as a target in Prometheus. To validate the correctness of our setup, we'll create a Kubernetes CronJob to emulate a batch job style of workload, and push its metrics to the Pushgateway service to ensure Prometheus collects our data.</p>
<p>To begin the deployment, ensure you move into the correct repository path, relative to the code repository root:</p>
<pre><strong>cd ./chapter06/provision/kubernetes/</strong></pre>
<p>To deploy an instance of Pushgateway, you can use the following manifest. Keep in mind that this service does not support high availability or clustering:</p>
<pre>apiVersion: apps/v1<br/>kind: Deployment<br/>metadata: {name: pushgateway, namespace: monitoring}<br/>spec:<br/>  selector:<br/>    matchLabels: {p8s-app: pushgateway}<br/>  replicas: 1<br/>  template:<br/>    metadata:<br/>      labels: {p8s-app: pushgateway}<br/>    spec:<br/>      containers:<br/>      - name: pushgateway<br/>...</pre>
<p>Apply the manifest by executing the following command:</p>
<pre><strong>kubectl apply -f ./pushgateway/pushgateway-deployment.yaml</strong></pre>
<p>And follow the deployment using the following:</p>
<pre><strong>kubectl rollout status deployment/pushgateway -n monitoring</strong></pre>
<p>After a successful deployment, it's time to provide a <kbd>Service</kbd> to our new instance, using the following manifest:</p>
<pre>apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  name: pushgateway-service<br/>  namespace: monitoring<br/>  labels:<br/>    p8s-app: pushgateway<br/>spec:<br/>  type: NodePort<br/>  ports:<br/>  - {name: push-port, port: 9091, targetPort: push-port, protocol: TCP}<br/>  selector:<br/>    p8s-app: pushgateway</pre>
<p>The following instruction applies to the previous manifest:</p>
<pre><strong>kubectl apply -f ./pushgateway/pushgateway-service.yaml </strong></pre>
<p>You may now validate the web interface for Pushgateway using the following command:</p>
<pre><strong>minikube service pushgateway-service -n monitoring</strong></pre>
<p><span>This will open a new browser tab pointing to the newly created Pushgateway instance web interface, which should look like the following figure:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/034cb26b-a5b7-4f17-9887-3fa1d16a51a1.png" style="width:17.92em;height:8.17em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 6.14:</span> Pushgateway web interface without any metric being pushed</div>
<p>Now, we need to instruct Prometheus to scrape Pushgateway. This can be accomplished via a new <kbd>ServiceMonitor</kbd> manifest as follows:</p>
<pre>apiVersion: monitoring.coreos.com/v1<br/>kind: ServiceMonitor<br/>metadata:<br/>  labels:<br/>    p8s-app: pushgateway<br/>  name: pushgateway<br/>  namespace: monitoring<br/>spec:<br/>  endpoints:<br/>  - interval: 30s<br/>    port: push-port<br/>    honorLabels: true<br/>  selector:<br/>    matchLabels:<br/>      p8s-app: pushgateway</pre>
<p>To apply this ServiceMonitor, we just type the following command:</p>
<pre><strong>kubectl apply -f ./pushgateway/pushgateway-servicemonitor.yaml</strong></pre>
<p>Now that we have our monitoring infrastructure in place, we need to simulate a batch job to validate our setup.</p>
<p>We can rely on the following manifest, which pushes a dummy <kbd>batchjob_example</kbd> metric with several labels to the Pushgateway service endpoint using a handcrafted <kbd>curl</kbd> payload:</p>
<pre>apiVersion: batch/v1beta1<br/>kind: CronJob<br/>metadata:<br/>  name: batchjob<br/>spec:<br/>  schedule: "*/1 * * * *"<br/>  jobTemplate:<br/>    spec:<br/>      template:<br/>        spec:<br/>          containers:<br/>          - name: batchjob<br/>            image: kintoandar/curl:7.61.1<br/>            args:<br/>            - -c<br/>            - 'echo "batchjob_example $(date +%s)" | curl -s --data-binary @- http://pushgateway-service.monitoring.svc.cluster.local:9091/metrics/job/batchjob/app/example/squad/yellow'<br/>          restartPolicy: OnFailure</pre>
<p>To apply the previous manifest, use the following command:</p>
<pre><strong>kubectl apply -f ./pushgateway/batchjob-cronjob.yaml</strong></pre>
<p class="mce-root">After a minute, the web interface for Pushgateway will look similar to this screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/535397ed-e81b-4f4c-b4bb-4c660cfe4ce7.png" style="width:34.50em;height:18.83em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 6.15: Pushgateway web interface presenting the batchjob_example metric</div>
<p>We can now use the Prometheus expression browser to validate the metric is being scraped from Pushgateway:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/614c64be-455d-4140-af56-45b359c91112.png" style="width:34.42em;height:21.75em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 6.16:</span> Prometheus expression browser showing the batchjob_example metric</div>
<p>As Pushgateway's job is to proxy metrics from other sources, it provides very little metrics of its own - just the standard Go runtime metrics, process metrics, HTTP handler metrics and build info. However, there is one application metric to note, which is <kbd>push_time_seconds</kbd>. This will tell you the last time a specific group (combination of labels used in the HTTP API when pushing) was seen. This can be used to detect missing or delayed jobs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">More exporters</h1>
                </header>
            
            <article>
                
<p>The Prometheus community has produced a great number of exporters for just about anything you might need. However, making an intentional choice to deploy a new piece of software in your infrastructure has an indirect price to pay upfront. That price translates into the deployment automation code to be written, the packaging, the metrics to be collected and alerting to be created, the logging configuration, the security concerns, the upgrades, and other things we sometimes take for granted. When choosing an open source exporter, or any other open source project for that matter, there are a few indicators to keep in mind.</p>
<p>We should validate the community behind the project, the general health of contributions, if issues are being addressed, pull requests are being timely managed, and whether the maintainers are open to discuss and interact with the community. Technically, we should also check whether the official Prometheus client libraries are being used by the particular project. With that said, we'll be covering a few noteworthy exporters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">JMX exporter</h1>
                </header>
            
            <article>
                
<p>The <strong>Java Virtual Machine</strong> (<strong>JVM</strong>) is a popular choice for core infrastructure services, such as Kafka, ZooKeeper, and Cassandra, among others. These services, like many others, do not natively offer metrics in the Prometheus exposition format and instrumenting such applications is far from being a trivial task. In these scenarios, we can rely on the <strong>Java Management Extensions</strong> (<strong>JMX</strong>) to expose the application<span>'</span>s internal state through the <strong>Managed Beans</strong> (<strong>MBeans</strong>). The JMX exporter extracts numeric data from the exposed MBeans and converts it into Prometheus metrics, exposing them on an HTTP endpoint for ingestion.</p>
<p>The exporter is available in the following two forms:</p>
<ul>
<li><strong>Java agent</strong>: In this mode, the exporter is loaded inside the local JVM where the target application is running and exposes a new HTTP endpoint.</li>
<li><strong>Standalone HTTP server</strong>: In this mode, a separate JVM instance is used to run the exporter that connects via JMX to the target JVM and exposes collected metrics on its own HTTP server.</li>
</ul>
<p>The documentation strongly advises deploying the exporter using the Java agent, for good reason; the agent produces richer sets of metrics as compared with the standalone exporter, as it has access to the full JVM being instrumented. However, both have trade-offs that are important to be aware of so that the right tool for the job is chosen.</p>
<p>Although the standalone server does not have access to the JVM specific metrics, such as garbage collector statistics or process memory/CPU usage, it easier to deploy and manage on static infrastructure when Java applications already have JMX enabled and are long-running processes that might not be convenient to touch. Adding to that, the exporter upgrade cycle becomes decoupled with the application life cycle, even though new releases are infrequent.</p>
<p>On the other hand, the Java agent provides the full range of available metrics in the JVM, but needs to be loaded into the target application at startup. This might be simpler to do on regularly deployed applications or when those applications run in containers.</p>
<p>Another benefit of running the agent is that the target JVM is also responsible to serve its own metrics, so the <kbd>up</kbd> metric from the scrape job can represent the process status without ambiguity.</p>
<p>Both options require a configuration file that can whitelist, blacklist, and/or relabel metrics from MBeans into the Prometheus format. An important performance consideration is the use of whitelists whenever possible. Some applications expose a very large amount of MBeans (such as Kafka or Cassandra) and frequent scrapes do have a significant performance impact.</p>
<div class="packt_tip">You can find useful examples of configuration files for the most used applications at <a href="https://github.com/prometheus/jmx_exporter/tree/master/example_configs">https://github.com/prometheus/jmx_exporter/tree/master/example_configs</a>.</div>
<div class="packt_infobox">The source code for <kbd>jmx_exporter</kbd> is available at <a href="https://github.com/prometheus/jmx_exporter">https://github.com/prometheus/jmx_exporter</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">HAProxy exporter</h1>
                </header>
            
            <article>
                
<p>HAProxy, a well-known load balancing solution, at the time of writing does not expose Prometheus metrics natively. Fortunately, it has an exporter, made by the Prometheus maintainers, to ensure its metrics can be collected, which is the <kbd>haproxy_exporter</kbd>. HAProxy natively exposes its metrics in <strong>comma-separated value</strong> (<strong>CSV</strong>) format via a configurable HTTP endpoint by using the <kbd>stats enable</kbd> configuration. The <kbd>haproxy_exporter</kbd>, which runs as a separate daemon, is able to connect to the HAProxy stats endpoint, consume the CSV, and convert its contents to the Prometheus metric format, exposing it in a synchronous manner when triggered by a scrape.</p>
<p>Instrumenting the load-balancer layer can be quite useful when the applications in the backend pools aren't properly instrumented and thus don't expose access metrics. For example, dashboards and alerts can be created for HTTP error rates or backend availability without any development effort from the application side. This is not meant to be a long-term solution, but can help in transitioning from legacy monitoring systems to Prometheus.</p>
<div class="packt_infobox">You can find the source code and installation files for the <kbd>haproxy_exporter</kbd> at <a href="https://github.com/prometheus/haproxy_exporter">https://github.com/prometheus/haproxy_exporter</a></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we had the opportunity to discover some of the most used Prometheus exporters available. Using test environments, we were able to interact with operating-system-level exporters running on VMs and container-specific exporters running on Kubernetes. We found that sometimes we need to rely on logs to obtain metrics and went through the current best options to achieve this. Then, we explored blackbox probing with the help of <kbd>blackbox_exporter</kbd> and validated its unique workflow. We also experimented with pushing metrics instead of using the standard pull approach from Prometheus, while making clear why sometimes this method does indeed make sense.</p>
<p>All these exporters enable you to gain visibility without having to natively instrument code, which sometimes is much more costly than relying on community-driven exporters.</p>
<p>With so many sources of metrics, now is the time to understand how to extract useful information from their data. In the next chapter, we'll go over PromQL and how best to leverage it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>How would you collect custom metrics with the Node Exporter?</li>
<li>What resources does cAdvisor consult to generate metrics?</li>
<li>kube-state-metrics expose numerous API objects. Is there a way to restrict that number?</li>
<li>How could you debug a <kbd>blackbox_exporter</kbd> probe?</li>
<li>If an application does not expose metrics, in Prometheus format or otherwise, what could an option to monitor it be?</li>
<li>What are the downsides of using Pushgateway?</li>
<li>If a particular batch job is host specific, is there any alternative to the use of Pushgateway?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root"><strong>Prometheus exporters</strong><span>:</span> <a href="https://prometheus.io/docs/instrumenting/exporters/">https://prometheus.io/docs/instrumenting/exporters/</a></li>
<li><strong>Prometheus port allocations</strong>: <a href="https://github.com/prometheus/prometheus/wiki/Default-port-allocations">https://github.com/prometheus/prometheus/wiki/Default-port-allocations</a></li>
<li><strong>Manpages cgroups</strong>: <a href="http://man7.org/linux/man-pages/man7/cgroups.7.html">http://man7.org/linux/man-pages/man7/cgroups.7.html</a></li>
<li><strong>Manpages namespaces</strong>: <a href="http://man7.org/linux/man-pages/man7/namespaces.7.html">http://man7.org/linux/man-pages/man7/namespaces.7.html</a></li>
<li><strong>Kubernetes resource usage monitoring</strong>: <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/">https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/</a></li>
</ul>


            </article>

            
        </section>
    </body></html>