- en: Exporters and Integrations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导出器和集成
- en: Even though first-party exporters cover the basics pretty well, the Prometheus
    ecosystem provides a wide variety of third-party exporters that cover everything
    else. In this chapter, we will be introduced to some of the most useful exporters
    available—from **operating system** (**OS**) metrics and **Internet Control Message
    Protocol** (**ICMP**) probing to generating metrics from logs, or how to collect
    information from short-lived processes, such as batch jobs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 即使第一方导出器已经很好地覆盖了基本内容，Prometheus生态系统仍然提供了多种第三方导出器，涵盖了其他所有内容。在本章中，我们将介绍一些最有用的导出器——从**操作系统**（**OS**）指标和**互联网控制消息协议**（**ICMP**）探测，到从日志生成指标，或者如何从短生命周期的进程（如批处理作业）中收集信息。
- en: 'In brief, the following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 简要来说，本章节将涵盖以下主题：
- en: Test environments for this chapter
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章的测试环境
- en: Operating system exporter
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作系统导出器
- en: Container exporter
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器导出器
- en: From logs to metrics
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从日志到指标
- en: Blackbox monitoring
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黑盒监控
- en: Pushing metrics
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推送指标
- en: More exporters
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多导出器
- en: Test environments for this chapter
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本章的测试环境
- en: 'In this chapter, we''ll be using two test environments: one based on **virtual
    machines** (**VMs**) that mimic traditional static infrastructure and one based
    on Kubernetes for modern workflows. The following topics will guide you through
    the automated setup procedure for both of them, but will gloss over the details
    from each exporter—these will be explained in depth in their own sections.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用两个测试环境：一个基于**虚拟机**（**VMs**）模拟传统的静态基础设施，另一个基于Kubernetes，用于现代工作流。以下主题将指导你完成这两种环境的自动化设置过程，但会略过每个导出器的详细内容——这些将在各自的章节中详细讲解。
- en: Static infrastructure test environment
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 静态基础设施测试环境
- en: This method will abstract all the deployment and configuration details, allowing
    you to have a fully provisioned test environment with a couple of commands. You'll
    still be able to connect to each of the guest instances and tinker with the example
    configurations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法将抽象化所有部署和配置细节，只需几条命令，你就能拥有一个完全配置的测试环境。你仍然可以连接到每个访客实例，并调整示例配置。
- en: 'To launch a new test environment, move into this chapter path, relative to
    the repository root:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动新的测试环境，请进入相对于仓库根目录的以下章节路径：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Ensure no other test environments are running and spin up this chapter’s environment:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 确保没有其他测试环境正在运行，并启动本章的环境：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can validate the successful deploy of the test environment using:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下命令验证测试环境是否成功部署：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Which will output the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 该指令将输出以下内容：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The end result will be an environment like the one depicted in the following
    diagram:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果将是如下图所示的环境：
- en: '![](img/0c7250b5-02ab-47dd-97c8-78102e3f0217.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c7250b5-02ab-47dd-97c8-78102e3f0217.png)'
- en: 'Figure 6.1: Diagram of the static infrastructure test environment'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：静态基础设施测试环境示意图
- en: 'To connect to the `target01` instance, just run the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接到`target01`实例，只需运行以下命令：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To connect to the Prometheus instance, use the following code snippet:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接到Prometheus实例，使用以下代码片段：
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'When you''re finished with this environment, move into this chapter path, relative
    to the repository root as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当你完成当前环境后，请进入相对于仓库根目录的以下章节路径：
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'And execute the following instruction:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后执行以下指令：
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Kubernetes test environment
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes测试环境
- en: 'To start the Kubernetes test environment, we first must ensure there''s no
    instance of `minikube` running as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动Kubernetes测试环境，首先必须确保没有正在运行的`minikube`实例，如下所示：
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Start a new `minikube` instance with the following specifications:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下规格启动一个新的`minikube`实例：
- en: '[PRE9]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: When the previous command finishes, a new Kubernetes environment should be ready
    to be used.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当前面的命令执行完毕后，一个新的Kubernetes环境应该已经准备好可以使用。
- en: For our Kubernetes test environment, we'll be building upon the lessons learned
    in [Chapter 5](12e775c2-bee9-4ebe-ad73-2f9313eeeeee.xhtml), *Running a Prometheus
    Server*, and using Prometheus Operator in our workflow. Since we already covered
    the Prometheus Operator setup, we'll deploy all the required components without
    going over each one of them.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的Kubernetes测试环境，我们将基于在[第5章](12e775c2-bee9-4ebe-ad73-2f9313eeeeee.xhtml)中学到的经验，*运行Prometheus服务器*，并在我们的工作流中使用Prometheus
    Operator。由于我们已经讲解了Prometheus Operator的设置，我们将直接部署所有必要的组件，而不逐一讲解。
- en: 'Step into the following chapter number:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 进入以下章节编号：
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Deploy the Prometheus Operator and validate the successful deploy as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 部署Prometheus Operator并验证成功部署，方法如下：
- en: '[PRE11]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Use the Prometheus Operator to deploy Prometheus and ensure the deploy was
    successful like so:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Prometheus Operator 部署 Prometheus 并确保部署成功，如下所示：
- en: '[PRE12]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Add ServiceMonitors as shown in the following code, which will configure Prometheus
    jobs:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下代码添加 ServiceMonitors，这将配置 Prometheus 作业：
- en: '[PRE13]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After a moment, you should have Prometheus available and ready; the following
    instruction will provide its web interface:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 稍等片刻后，您应该能够访问 Prometheus 并准备就绪；以下指令将提供其 Web 界面：
- en: '[PRE14]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You can validate the Kubernetes StatefulSet for Prometheus using the following
    instruction, which will open the Kubernetes dashboard:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下指令验证 Kubernetes 的 Prometheus StatefulSet，这将打开 Kubernetes 仪表盘：
- en: '[PRE15]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: More information regarding the Kubernetes objects, including the StatefulSet
    controller, is available at [https://kubernetes.io/docs/concepts/](https://kubernetes.io/docs/concepts/).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 Kubernetes 对象（包括 StatefulSet 控制器）的更多信息，请访问 [https://kubernetes.io/docs/concepts/](https://kubernetes.io/docs/concepts/)。
- en: 'The following screenshot illustrates the correct deployment of the Prometheus
    StatefulSet:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了 Prometheus StatefulSet 的正确部署：
- en: '![](img/8a6e82f8-87c5-48ef-8a16-cde67a650f61.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a6e82f8-87c5-48ef-8a16-cde67a650f61.png)'
- en: Figure 6.2 - Kubernetes dashboard depicting Prometheus StatefulSet
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 - Kubernetes 仪表盘展示 Prometheus StatefulSet
- en: Operating system exporter
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作系统 exporter
- en: When monitoring infrastructure, the most common place to start looking is at
    the OS level. Metrics for resources such as CPU, memory, and storage devices,
    as well as kernel operating counters and statistics provide valuable insight to
    assess a system's performance characteristics. For a Prometheus server to collect
    these types of metrics, an OS-level exporter is needed on the target hosts to
    expose them in an HTTP endpoint. The Prometheus project provides such an exporter
    that supports Unix-like systems called the Node Exporter, and the community also
    maintains an equivalent exporter for Microsoft Windows systems called the WMI
    exporter.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在监控基础设施时，最常见的起点是操作系统层面。有关资源的指标，如 CPU、内存和存储设备，以及内核操作计数器和统计信息，为评估系统性能特征提供了宝贵的见解。为了让
    Prometheus 服务器收集这些类型的指标，目标主机上需要有一个操作系统级别的 exporter 来将这些数据暴露在 HTTP 端点上。Prometheus
    项目提供了一个支持类 Unix 系统的 exporter，叫做 Node Exporter，社区也维护了一个等效的 exporter，专门用于 Microsoft
    Windows 系统，称为 WMI exporter。
- en: The Node Exporter
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Node Exporter
- en: The Node Exporter is the most well-known Prometheus exporter, for good reason.
    It provides over 40 collectors for different areas of the OS, as well as a way
    of exposing local metrics for cron jobs and static information about the host.
    Like the rest of the Prometheus ecosystem, the Node Exporter comes with a sane
    default configuration and some smarts to identify what can be collected, so it's
    perfectly reasonable to run it without much tweaking.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Node Exporter 是最著名的 Prometheus exporter，且有充分的理由。它为操作系统的不同领域提供了超过 40 个收集器，并且可以暴露关于
    cron 作业的本地指标和主机的静态信息。与其他 Prometheus 生态系统组件一样，Node Exporter 提供了一个合理的默认配置，并具备智能识别可以收集哪些数据的功能，因此无需过多调整，直接运行是完全合理的。
- en: In this context, *node* refers to a computer node or host and is not related
    to Node.js in any way.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在此上下文中，*node* 指的是计算机节点或主机，与 Node.js 无关。
- en: Although this exporter was designed to be run as a non-privileged user, it does
    need to access kernel and process statistics, which aren't normally available
    when running inside a container. This is not to say that it doesn't work in containers—every
    Prometheus component can be run in containers—but that additional configuration
    is required for it to work. It is, therefore, recommended that the Node Exporter
    be run as a system daemon directly on the host whenever possible.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管该 exporter 被设计为以非特权用户身份运行，但它确实需要访问内核和进程统计信息，这些信息在容器内通常无法获取。这并不意味着它在容器中无法工作——每个
    Prometheus 组件都可以在容器中运行——只是需要额外的配置才能使其正常工作。因此，推荐尽可能直接在主机上以系统守护进程的方式运行 Node Exporter。
- en: Node Exporter collectors might gather different metrics depending on the system
    being run, as OS kernels vary in the way they expose internal state and what details
    they make available. As an example, metrics exposed by `node_exporter` on macOS
    will be substantially different from the ones on Linux. This means that, even
    though the Node Exporter supports Linux, Darwin (macOS), FreeBSD, OpenBSD, NetBSD,
    DragonFly BSD, and Solaris, each collector within the Node Exporter will have
    their own compatibility matrix, with Linux being the kernel with the most support.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Metric names exposed by the Node Exporter changed in version 0.16.0 due to a
    standardization effort across the Prometheus project. This was a breaking change,
    which means that dashboards and tutorials made for earlier versions of this exporter
    won't work out of the box. An upgrade guide ([https://github.com/prometheus/node_exporter/blob/v0.17.0/docs/V0_16_UPGRADE_GUIDE.md](https://github.com/prometheus/node_exporter/blob/v0.17.0/docs/V0_16_UPGRADE_GUIDE.md))
    can be found in the Node Exporter's repository.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: The source code and installation files for the Node Exporter are available at
    [https://github.com/prometheus/node_exporter](https://github.com/prometheus/node_exporter).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: By design, this exporter only produces aggregated metrics about processes (such
    as how many are running, and so on) and not individual metrics per process. In
    the Prometheus model, each process of relevance needs to expose its own metrics,
    or have a companion exporter to do that job for it. This is one of the reasons
    why it is ill-advised in most cases to run a generic process exporter without
    an explicit whitelist.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Configuration
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exporters in the Prometheus ecosystem usually collect a specific set of metrics
    from a given process. The Node Exporter differs from most other exporters as machine-level
    metrics span a wide range of subsystems, and so it is architected to provide individual
    collectors, which can be turned on and off, depending on the instrumentation needs.
    Enabling collectors that are turned off by default can be done with the `--collector.<name>`
    set of flags; enabled collectors can be disabled by using the `--no-collector.<name>`
    flag variant.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'From all the collectors enabled by default, one needs to be singled out due
    to its usefulness as well as its need of configuration to properly work. The `textfile`
    collector enables the exposition of custom metrics by watching a directory for
    files with the `.prom` extension that contain metrics in the Prometheus exposition
    format. The `--collector.textfile.directory` flag is empty by default and so needs
    to be set to a directory path for the collector to do its job. It is expected
    that only instance-specific metrics be exported through this method, for example:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Local cron jobs can report their exit status through a metric (finish timestamp
    is not useful to record, as the metrics file modification timestamp is already
    exported as a metric)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Informational metrics (that only exist for the labels they provide), such as
    VM flavor, size, or assigned role
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息性指标（仅存在于它们提供的标签中），如虚拟机类型、大小或分配的角色
- en: How many package upgrades are pending, if a restart is required
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有多少个软件包升级待处理，是否需要重启
- en: Anything else not covered by the built-in collectors
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何其他未由内建收集器覆盖的内容
- en: Deployment
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署
- en: 'The test environment for static infrastructure for this chapter should already
    have `node_exporter` up and running through the automatic provisioning. Nevertheless,
    we can inspect it by connecting, for example, to the `target01` VM as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的静态基础设施测试环境应该已经通过自动化配置使`node_exporter`启动并运行。不过，我们可以通过连接到`target01`虚拟机来检查它，例如如下所示：
- en: '[PRE16]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then check the configuration of the provided `systemd` unit file like so:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然后检查提供的`systemd`单元文件配置，如下所示：
- en: '[PRE17]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In this snippet, we can see the `textfile` collector directory being set so
    that custom metrics can be exported:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码片段中，我们可以看到设置了`textfile`收集器目录，以便可以导出自定义指标：
- en: '[PRE18]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s try creating a custom metric. To do that, we only need to write the
    metric to a file inside the `textfile` collector directory with a `.prom` extension
    as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试创建一个自定义指标。为此，我们只需要将指标写入`textfile`收集器目录中的一个`.prom`扩展名的文件，如下所示：
- en: '[PRE19]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In a real-world scenario, you would need to make sure the file was written atomically
    so that `node_exporter` wouldn't see a half-written (thus corrupted) file. You
    could either write it out to a temporary file and then `mv` it into place (taking
    care to not cross mount point boundaries), or use the `sponge` utility, which
    is usually found in the `moreutils` package.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际场景中，你需要确保文件是原子性地写入的，以防`node_exporter`看到一个半写的（因此损坏的）文件。你可以将文件先写入一个临时文件，然后使用`mv`将其移到正确位置（小心不要跨越挂载点边界），或者使用`
    sponge`工具，通常在`moreutils`包中可以找到。
- en: 'We can then request the `/metrics` endpoint and search for our test metric
    like so:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以请求`/metrics`端点并搜索我们的测试指标，如下所示：
- en: '[PRE20]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output should be something like the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该类似于以下内容：
- en: '[PRE21]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This exporter produces a large number of metrics, depending on which collectors
    are enabled. Some of the more interesting metrics available from `node_exporter`
    are the following:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这个导出程序会生成大量的指标，具体取决于启用了哪些收集器。`node_exporter`提供的一些更有趣的指标如下：
- en: '`node_cpu_seconds_total`, which provides the number of seconds cumulatively
    used per core for all the available CPU modes, is very useful for understanding
    the CPU utilization'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`node_cpu_seconds_total`，它提供每个核心累计使用的秒数，适用于所有可用的CPU模式，非常有助于理解CPU的利用率'
- en: '`node_memory_MemTotal_bytes` and `node_memory_MemAvailable_bytes`, which allow
    calculating the ratio of memory available'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`node_memory_MemTotal_bytes`和`node_memory_MemAvailable_bytes`，这可以用来计算可用内存的比率'
- en: '`node_filesystem_size_bytes` and `node_filesystem_avail_bytes`, which enable
    the calculation of filesystem utilization'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`node_filesystem_size_bytes`和`node_filesystem_avail_bytes`，这可以用来计算文件系统的利用率'
- en: '`node_textfile_scrape_error`, which tells you if the textfile collector couldn''t
    parse any of the metrics files in the textfile directory (when this collector
    is enabled)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`node_textfile_scrape_error`，它会告诉你在启用此收集器时，`textfile`收集器是否无法解析`textfile`目录中的任何指标文件'
- en: Container exporter
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器导出程序
- en: In the constant pursuit for workload isolation and resource optimization, we
    witnessed the move from physical to virtualized machines using hypervisors. Using
    virtualization implies a certain degree of resource usage inefficiency, as the
    storage, CPU, and memory need to be allocated to each running VM whether it uses
    them or not. A lot of work has been done in this area to mitigate such inefficiencies
    but, in the end, fully taking advantage of system resources is still a difficult
    problem.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在追求工作负载隔离和资源优化的过程中，我们见证了从物理机到虚拟机的迁移，使用了虚拟化技术。使用虚拟化意味着存在一定程度的资源使用低效，因为无论虚拟机是否使用它们，存储、CPU和内存都需要分配给每个正在运行的虚拟机。虽然在这个领域已经做了很多工作来缓解这些低效问题，但最终，完全利用系统资源仍然是一个难题。
- en: 'With the rise of operating-system-level virtualization on Linux (that is, the
    use of containers), the mindset changed. We no longer want a full copy of an OS
    for each workload, but instead, only properly isolated processes to do the desired
    work. To achieve this, and focusing specifically on Linux containers, a set of
    kernel features responsible for isolating hardware resources (named cgroups or
    control groups) and kernel resources (named namespaces) were made available. Resources
    managed by cgroups are as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Linux 操作系统级虚拟化（即使用容器）的兴起，思维方式发生了变化。我们不再需要为每个工作负载提供操作系统的完整副本，而只需要正确隔离的进程来完成所需的工作。为了实现这一点，特别是在
    Linux 容器方面，一组负责隔离硬件资源（称为 cgroups 或控制组）和内核资源（称为命名空间）的内核特性被引入。cgroups 管理的资源如下：
- en: CPU
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU
- en: Memory
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存
- en: Disk I/O
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 磁盘 I/O
- en: Network
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络
- en: These kernel features allow the user to have fine control over what resources
    a given workload has available, thus optimizing resource usage. Cgroups metrics
    are invaluable to any modern monitoring system.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这些内核特性允许用户对给定工作负载可用的资源进行精细控制，从而优化资源的使用。Cgroups 指标对于任何现代监控系统都是无价的。
- en: cAdvisor
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: cAdvisor
- en: '**Container Advisor** (**cAdvisor**) is a project developed by Google that
    collects, aggregates, analyzes, and exposes data from running containers. The
    data available covers pretty much anything you might require, from memory limits
    to GPU metrics, all available and segregated by container and/or host.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**容器顾问** (**cAdvisor**) 是一个由 Google 开发的项目，负责收集、汇总、分析并暴露运行中的容器的数据。可用的数据几乎涵盖了你可能需要的任何内容，从内存限制到
    GPU 指标，所有这些都可以按容器和/或主机进行分类展示。'
- en: cAdvisor isn't tied to Docker containers but it's usually deployed as one. Data
    is collected from the container daemon and Linux cgroups, making the discovery
    of containers transparent and completely automatic. It also exposes process limits
    and throttling events whenever these limits are reached, which is important information
    to keep an eye on to maximize infrastructure resource usage without negatively
    impacting workloads.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: cAdvisor 并不依赖于 Docker 容器，但它通常作为容器进行部署。数据通过容器守护进程和 Linux cgroups 收集，使得容器的发现变得透明且完全自动化。它还会在达到限制时暴露进程限制和节流事件，这是最大化基础设施资源使用而不对工作负载产生负面影响的重要信息。
- en: Besides exposing metrics in the Prometheus format, cAdvisor also ships with
    a useful web interface, allowing the instant visualization of the status of hosts
    and their containers.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 除了以 Prometheus 格式暴露指标外，cAdvisor 还提供了一个有用的 Web 界面，允许即时可视化主机及其容器的状态。
- en: The source code and installation files for cAdvisor are available at [https://github.com/google/cadvisor](https://github.com/google/cadvisor).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: cAdvisor 的源代码和安装文件可以在 [https://github.com/google/cadvisor](https://github.com/google/cadvisor)
    找到。
- en: Configuration
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置
- en: When launching cAdvisor as a container, some host paths are required to be available
    in read-only mode. This will allow, for example, the collection of kernels, processes,
    and container data.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当以容器形式启动 cAdvisor 时，某些主机路径需要以只读模式可用。这将允许，例如，收集内核、进程和容器的数据。
- en: 'There are quite a few runtime flags, so we''ll feature some of the most relevant
    for our test case in the following table:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多运行时标志，我们将以下表格中列出一些与我们测试案例最相关的标志：
- en: '| **Flag** | **Description** |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| **标志** | **描述** |'
- en: '| `--docker` | Docker endpoint, defaults to `unix:///var/run/docker.sock` |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| `--docker` | Docker 端点，默认为 `unix:///var/run/docker.sock` |'
- en: '| `--docker_only` | Only report containers in addition to root stats |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| `--docker_only` | 仅报告容器信息，外加根统计数据 |'
- en: '| `--listen_ip` | IP to bind on, default to `0.0.0.0` |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| `--listen_ip` | 要绑定的 IP，默认为 `0.0.0.0` |'
- en: '| `--port` | Port to listen on, defaults to `8080` |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| `--port` | 监听端口，默认为 `8080` |'
- en: '| `--storage_duration` | How long to store data, defaults to `2m0s` |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| `--storage_duration` | 数据存储时长，默认为 `2m0s` |'
- en: 'You can inspect the available runtime configurations using the following address:
    [https://github.com/google/cadvisor/blob/release-v0.33/docs/runtime_options.md](https://github.com/google/cadvisor/blob/release-v0.33/docs/runtime_options.md).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下地址查看可用的运行时配置：[https://github.com/google/cadvisor/blob/release-v0.33/docs/runtime_options.md](https://github.com/google/cadvisor/blob/release-v0.33/docs/runtime_options.md)。
- en: Deployment
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署
- en: Although historically the cAdvisor code was embedded in the Kubelet binary,
    it is currently scheduled to be deprecated there. Therefore, we'll be launching
    cAdvisor as a DaemonSet to future proof this example and to expose its configurations,
    while also enabling its web interface, as a Kubernetes service, to be explored.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管历史上 cAdvisor 代码被嵌入在 Kubelet 二进制文件中，但它目前计划在该位置被弃用。因此，我们将以 DaemonSet 的方式启动 cAdvisor，以便为这个示例做好未来兼容性，同时公开其配置，并将其
    Web 界面作为 Kubernetes 服务进行探索。
- en: 'Ensure you move into the correct repository path as shown here:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你进入正确的仓库路径，如下所示：
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we must create a DaemonSet, because we want cAdvisor running in every
    single node:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须创建一个 DaemonSet，因为我们希望在每个节点上都运行 cAdvisor：
- en: '[PRE23]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Notice all the volume mounts allowing the collection of data from the Docker
    daemon and various Linux resources as shown here:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，所有的卷挂载允许从 Docker 守护进程和各种 Linux 资源收集数据，如下所示：
- en: '[PRE24]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Apply the previous manifest using the following instruction:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下指令应用之前的清单：
- en: '[PRE25]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can follow the deployment status using the following:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式跟踪部署状态：
- en: '[PRE26]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'When the deployment finishes, it''s time to add a new service. Notice the port
    name that will be used in the ServiceMonitor. Here''s the manifest we''ll be using:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 部署完成后，是时候添加一个新的服务了。注意将在 ServiceMonitor 中使用的端口名称。以下是我们将使用的清单：
- en: '[PRE27]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The manifest can be applied using the following:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下命令应用清单：
- en: '[PRE28]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can now connect to the cAdvisor web interface using the following instruction:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用以下指令连接到 cAdvisor Web 界面：
- en: '[PRE29]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This will open a browser window with an interface similar to the following
    figure:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打开一个浏览器窗口，界面与下图类似：
- en: '![](img/21261d68-ce9a-41af-a132-ea1ae8e77535.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21261d68-ce9a-41af-a132-ea1ae8e77535.png)'
- en: 'Figure 6.3: cAdvisor web interface'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3：cAdvisor 网络界面
- en: 'It''s time to add cAdvisor exporters as new targets for Prometheus. For that,
    we''ll be using the next `ServiceMonitor` manifest as shown here:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候将 cAdvisor 导出器添加为 Prometheus 的新目标了。为此，我们将使用下一个 `ServiceMonitor` 清单，如下所示：
- en: '[PRE30]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The previous manifest can be applied using:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下命令应用之前的清单：
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'After a few moments, you can inspect the newly added targets in the Prometheus
    web interface, using the following instruction to open its web interface:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 稍等片刻后，你可以通过以下指令打开 Prometheus Web 界面，检查新添加的目标：
- en: '[PRE32]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The following figure illustrates the Prometheus `/targets` endpoint showing
    cAdvisor target:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了 Prometheus `/targets` 端点，显示 cAdvisor 目标：
- en: '![](img/f901508a-146e-4557-bc0a-b73f384cc5b7.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f901508a-146e-4557-bc0a-b73f384cc5b7.png)'
- en: 'Figure 6.4: Prometheus */targets* endpoint showing cAdvisor target'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4：Prometheus */targets* 端点显示 cAdvisor 目标
- en: With this, we now have container-level metrics. Do note that cAdvisor exports
    a large amount of samples per container, which can easily balloon exported metrics
    to multiple thousands of samples per scrape, possibly causing cardinality-related
    issues on the scraping Prometheus.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个，我们现在拥有了容器级别的指标。请注意，cAdvisor 会为每个容器导出大量样本，这可能会导致每次抓取时导出的指标数量达到几千个样本，进而可能引发
    Prometheus 在抓取时的基数问题。
- en: You can find every metric exposed by cAdvisor at their Prometheus documentation: [https://github.com/google/cadvisor/blob/release-v0.33/docs/storage/prometheus.md](https://github.com/google/cadvisor/blob/release-v0.33/docs/storage/prometheus.md).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 cAdvisor 的 Prometheus 文档中找到每个导出的指标：[https://github.com/google/cadvisor/blob/release-v0.33/docs/storage/prometheus.md](https://github.com/google/cadvisor/blob/release-v0.33/docs/storage/prometheus.md)。
- en: 'From the thousands of metrics exported by cAdvisor, these are generally useful
    for keeping an eye out for problems:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 从 cAdvisor 导出的成千上万的指标中，这些通常是用来监控问题的有用指标：
- en: '`container_last_seen`,  which keeps track of the timestamp the container was
    last seen as running'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`container_last_seen`，它跟踪容器最后一次被看到作为正在运行的时间戳'
- en: '`container_cpu_usage_seconds_total`, which gives you a counter of the number
    of CPU seconds per core each container has used'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`container_cpu_usage_seconds_total`，它提供了每个容器每个核心使用的 CPU 秒数的计数器'
- en: '`container_memory_usage_bytes` and `container_memory_working_set_bytes`, which
    keep track of container memory usage (including cache and buffers) and just container
    active memory, respectively'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`container_memory_usage_bytes` 和 `container_memory_working_set_bytes`，分别用于跟踪容器内存使用情况（包括缓存和缓冲区）和仅容器的活跃内存'
- en: '`container_network_receive_bytes_total` and `container_network_transmit_bytes_total`,
    which let you know how much traffic in the container receiving and transmitting,
    respectively'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`container_network_receive_bytes_total` 和 `container_network_transmit_bytes_total`，它们分别显示容器接收和发送的流量'
- en: 'When running on Kubernetes, cAdvisor doesn''t provide you with insight into
    how the cluster is running—application-level metrics from Kubernetes itself. For
    this, we need another exporter: kube-state-metrics.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: kube-state-metrics
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'kube-state-metrics does not export container-level data, as that''s not its
    function. It operates at a higher level, exposing the Kubernetes state, providing
    metrics regarding the API internal objects such as pods, services, or deployments.
    The object metric groups currently available when using this exporter are the
    following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: CronJob metrics
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DaemonSet metrics
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment metrics
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Job metrics
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LimitRange metrics
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node metrics
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PersistentVolume metrics
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PersistentVolumeClaim metrics
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pod metrics
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pod Disruption Budget metrics
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReplicaSet metrics
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReplicationController metrics
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource quota metrics
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service metrics
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StatefulSet metrics
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Namespace metrics
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal Pod Autoscaler metrics
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Endpoint metrics
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secret metrics
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ConfigMap metrics
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two endpoints exposed by kube-state-metrics: one provides the API
    objects metrics and the other presents the internal metrics from the exporter
    itself.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: The source code and installation files for kube-state-metrics are available
    at [https://github.com/kubernetes/kube-state-metrics](https://github.com/kubernetes/kube-state-metrics).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Configuration
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When configuring kube-state-metrics, other than all the required RBAC permissions,
    there are also several runtime flags to be aware of. We will provide an overview
    of the more relevant ones for our test case in the following table:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '| **Flag** | **Description** |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| `--host` | IP to bind and expose Kubernetes metrics on, defaults to `0.0.0.0`
    |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| `--port` | Port to expose Kubernetes metrics, defaults to `80` |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| `--telemetry-host` | IP to expose internal metrics, defaults to `0.0.0.0`
    |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| `--telemetry-port` | Port to expose internal metrics, defaults to `80` |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| `--collectors` | Comma-separated list of metrics groups to enable, defaults
    to ConfigMap, CronJobs, DaemonSets, Deployments, endpoints, horizontalpodautoscalers,
    Jobs, LimitRanges, namespaces, Nodes, PersistentVolumeClaims, PersistentVolumes,
    PodDisruptionBudgets, pods, ReplicaSets, ReplicationControllers, resource quotas,
    Secrets, services, StatefulSets |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: '| `--metric-blacklist` | Comma-separated list of metrics to disable, mutually
    exclusive with the whitelist |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| `--metric-whitelist` | Comma-separated list of metrics to enable, mutually
    exclusive with the blacklist |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: Due to the unpredictable amount of objects required to be exported, which are
    directly proportional to the size of the cluster, a common pattern when deploying
    kube-state-metrics is to use a special container called **addon-resizer**, which
    can vertically resize the exporter pod dynamically. Information regarding *addon-resizer*
    can be found at [https://github.com/kubernetes/autoscaler/tree/addon-resizer-release-1.8](https://github.com/kubernetes/autoscaler/tree/addon-resizer-release-1.8).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll be building upon the Kubernetes test environment started previously.
    To begin the deployment, ensure you move into the correct repository path, relative
    to the repository root as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在之前启动的Kubernetes测试环境基础上进行构建。要开始部署，请确保切换到正确的仓库路径，相对于仓库根目录，如下所示：
- en: '[PRE33]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As access to the Kubernetes API is required, the **role-based access control**
    (**RBAC**) configuration for this deploy is quite extensive, which includes a
    Role, a RoleBinding, a ClusterRole, a ClusterRoleBinding, and a ServiceAccount.
    This manifest is available at `./kube-state-metrics/kube-state-metrics-rbac.yaml`.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 由于需要访问Kubernetes API，这次部署的**基于角色的访问控制**（**RBAC**）配置相当广泛，包括Role、RoleBinding、ClusterRole、ClusterRoleBinding和ServiceAccount。该清单位于`./kube-state-metrics/kube-state-metrics-rbac.yaml`。
- en: 'It should be applied using the following command:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 应该使用以下命令来应用：
- en: '[PRE34]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We''ll be creating a deployment for kube-state-metrics with just one instance,
    as, in this case, no clustering or special deployment requirements are necessary:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为kube-state-metrics创建一个仅有一个实例的部署，因为在这种情况下，不需要集群或特殊的部署要求：
- en: '[PRE35]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This deployment will run an instance of the `kube-state-metrics` exporter,
    along with `addon-resizer` to scale the exporter dynamically:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这个部署将运行一个`kube-state-metrics`导出器实例，并搭配`addon-resizer`动态扩展导出器：
- en: '[PRE36]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This can be applied using the following instruction:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下指令应用此操作：
- en: '[PRE37]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can follow the deployment status using the following:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令来跟踪部署状态：
- en: '[PRE38]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'After a successful deployment, we''ll be creating a service for this exporter,
    this time with two ports: one for the Kubernetes API object metrics and another
    for the exporter''s internal metrics themselves:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 部署成功后，我们将为此导出器创建一个服务，这次使用两个端口：一个用于Kubernetes API对象的度量，另一个用于导出器的内部度量：
- en: '[PRE39]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The previous manifest can be applied as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 上述清单可以按照以下方式应用：
- en: '[PRE40]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'With the service in place, we are able to validate both metrics endpoints using
    the following command:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 有了服务后，我们可以使用以下命令验证两个度量端点：
- en: '[PRE41]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This will open two different browser tabs, one for each metrics endpoint:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打开两个不同的浏览器标签页，每个标签页对应一个度量端点：
- en: '![](img/41899a12-c246-4f9c-99ad-f2b051013453.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41899a12-c246-4f9c-99ad-f2b051013453.png)'
- en: Figure 6.5: The kube-state-metrics web interface
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5：kube-state-metrics的Web界面
- en: 'Finally, it is time to configure Prometheus to scrape both endpoints using
    the `ServiceMonitor` manifest as shown here:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，是时候配置Prometheus来抓取这两个端点，使用如下所示的`ServiceMonitor`清单：
- en: '[PRE42]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'And it can now be applied using the following command:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以使用以下命令应用它：
- en: '[PRE43]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We can now validate the correct configuration of scrape targets in Prometheus,
    using the following instruction to open its web interface:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过以下指令验证Prometheus中抓取目标的正确配置，打开其Web界面：
- en: '[PRE44]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![](img/cb45ae0f-8e9f-481e-b075-ab635a27c1cf.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb45ae0f-8e9f-481e-b075-ab635a27c1cf.png)'
- en: 'Figure 6.6: Prometheus /targets endpoint showing kube-state-metrics targets
    for metrics and telemetry'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6：Prometheus /targets端点，显示kube-state-metrics的度量和遥测目标
- en: 'Some interesting metrics from kube-state-metrics that can be used to keep an
    eye on your Kubernetes clusters are:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 来自kube-state-metrics的一些有趣度量，可以用来监控你的Kubernetes集群：
- en: '`kube_pod_container_status_restarts_total`, which can tell you if a given pod
    is restarting on a loop;'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube_pod_container_status_restarts_total`，可以告诉你某个特定的Pod是否处于重启循环中；'
- en: '`kube_pod_status_phase`, which can be used to alert on pods that are in a non-ready
    state for a long time;'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube_pod_status_phase`，可以用来告警那些长时间处于非就绪状态的Pods；'
- en: Comparing `kube_<object>_status_observed_generation` with `kube_<object>_metadata_generation`
    can give you a sense when a given object has failed but hasn't been rolled back
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`kube_<object>_status_observed_generation`与`kube_<object>_metadata_generation`进行比较，可以帮助你了解何时某个对象失败但还没有回滚
- en: From logs to metrics
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从日志到度量
- en: 'In a perfect world, all applications and services would have been properly
    instrumented and we would only be required to collect metrics to gain visibility.
    External exporters are a stop-gap approach that simplifies our work, but not every
    service exposes its internal state through a neat API. Older daemon software,
    such as Postfix or ntpd, makes use of logging to relay their inner workings. For
    these cases, we''re left with two options: either instrument the service ourselves
    (which isn''t possible for closed source software) or rely on logs to gather the
    metrics we require. The next topics go over the available options for extracting
    metrics from logs.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: mtail
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developed by Google, mtail is a very light log processor that is capable of
    running programs with pattern matching logic, allowing the extraction of metrics
    from said logs. It supports multiple export formats, such as Prometheus, StatsD,
    Graphite, and more.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the `/metrics` endpoint, the `/` endpoint for the mtail service exposes
    valuable debug information. This endpoint is available in the static infrastructure
    test environment at `http://192.168.42.11:9197`:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/80ad4e51-8be9-4473-8202-3ed0462d3b6f.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.7: mtail web interface'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: The source code and installation files for mtail are available at [https://github.com/google/mtail](https://github.com/google/mtail).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Configuration
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To configure mtail, we require a program with the pattern matching logic. Let''s
    look at a very straightforward example available in the official repository:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This program defines the `line_count` metric of the `counter` type, an RE2-compatible
    expression `/$/` matching the end of a line, and finally, an action between `{
    }`, which, in this case, increments the `line_count` counter.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'To run this program, we are only required to start mtail with command line
    flags to point it to our program and to the log we want to monitor. Here are some
    of the most useful flags for our test case:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '| `-address` | Host or IP to bind |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| `-port` | Listener port, defaults to `3903` |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| `-progs` | Path to the programs |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| `-logs` | Comma-separated list of files to monitor (this flag can be set
    multiple times) |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: You can find the mtail programming guide at [https://github.com/google/mtail/blob/master/docs/Programming-Guide.md](https://github.com/google/mtail/blob/master/docs/Programming-Guide.md)
    and the RE2 syntax at [https://github.com/google/re2/wiki/Syntax](https://github.com/google/re2/wiki/Syntax).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our static infrastructure test environment, we can validate the configuration
    of mtail by connecting to the `target01` instance as shown here:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Then checking the configuration of the provided `systemd` unit file as shown
    in the following command:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'As in this example, `mtail` is counting the lines of the `syslog` file so it
    needs to have proper permissions to access system logs, and thus, we run the `mtail`
    daemon with `Group=adm` to make this work. We can see all the required arguments
    for the `mtail` service in the following snippet from the unit file, including
    the path to the line count program:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'On the Prometheus instance, we added the following job:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: In a real-world scenario, you would name the scrape job as the daemon whose
    logs mtail is monitoring, such as ntpd or Postfix.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the Prometheus expression browser, available at `http://192.168.42.10:9090`,
    we can validate, not only that the scrapes are being successful through the `up`
    metric, but also that our metric is available:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f3bbc09-a32c-4f34-aa3a-609d72f408e1.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.8: mtail line_count metric'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'Some interesting metrics from mtail that can be used to keep an eye on this
    exporter are:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '`mtail_log_watcher_error_count` , which counts the number of errors received
    from `fsnotify` (kernel-based notification system for filesystem events)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mtail_vm_line_processing_duration_milliseconds_bucket`, a histogram which
    provides the line processing duration distribution in milliseconds per mtail program'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grok exporter
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similarly to `mtail`, `grok_exporter` parses unstructured log data and generates
    metrics from it. However, as the name suggests, the main difference is the domain-specific
    language for this exporter being modeled after the Logstash pattern language (Grok),
    which enables the reuse of patterns you might already have built.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: The source code and installation files for `grok_exporter` are available at
    [https://github.com/fstab/grok_exporter](https://github.com/fstab/grok_exporter).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Configuration
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This exporter requires a configuration file for its setup. There are five main
    sections in the configuration, which we can dissect in the following snippets
    from the exporter''s configuration file deployed in our static infrastructure
    test environment. The `global` section sets the configuration format version.  Version
    2 is currently the standard configuration version, and so we set it here:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The input section defines the location of the logs to be parsed. If `readall`
    is set to `true`, the file will be completely parsed before waiting for new lines;
    as we can see, we''re not doing that in our example:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The `grok` section loads the patterns to use for parsing. These are configured
    in a separate location, as can be seen here:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The `metrics` section is where the magic happens. It defines what metrics to
    extract from the parsed logs. Every Prometheus metric type is natively supported
    in this exporter. The configuration for each `type` can be slightly different,
    so you should check its documentation. However, we''re going to provide an overview
    of the configuration that is common among them:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: The `match` configuration  defines the regular expression for data extraction;
    in our example, `LOGLEVEL` is a predefined pattern to match log levels.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `labels` configuration is able to use Go''s templating syntax to output
    whatever was extracted from the match definition; in this case, we used `level`
    as our variable in the match pattern and so it is available as `.level` in the
    template:'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The full configuration documentation is available at [https://github.com/fstab/grok_exporter/blob/v0.2.7/CONFIG.md](https://github.com/fstab/grok_exporter/blob/v0.2.7/CONFIG.md)
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the `server` section is where the bind address and port for the exporter
    are defined:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Now that we have a better understanding of what goes into the configuration
    file, it is time for us to try this exporter out in our test environment.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our static infrastructure test environment, we can validate the configuration
    of `grok_exporter` by connecting to the `target01` instance as shown here:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Outputting the configuration of the provided `systemd` unit file is shown in
    the following code snippet:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Just as with the `mtail` exporter, we need to run `grok_exporter` with `Group=adm`
    so that it has access to `syslog` without requiring being run as a privileged
    user. We can see all the required arguments for the `grok_exporter` service in
    the following snippet from the unit file, including the path to the configuration
    file mentioned before:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'On the Prometheus instance, we added the following job:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Using the Prometheus expression browser, available at `http://192.168.42.10:9090`,
    we can validate not only whether the scrapes are successful but also that our
    metric is available:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2f553af-8c1b-4021-9dec-dfeb67e9a58d.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.9: *grok_exporter* example metric'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'Some interesting metrics from `grok_exporter` that can be used to keep an eye
    on this exporter are:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '`grok_exporter_line_buffer_peak_load`, a summary which provides the number
    of lines that are read from the log file and waiting to be processed'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`grok_exporter_line_processing_errors_total`, which exposes the total number
    of processing errors for each defined metric'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blackbox monitoring
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introspection is invaluable to gather data about a system, but sometimes we're
    required to measure from the point of view of a user of that system. In such cases,
    probing is a good option to simulate user interaction. As probing is made from
    the outside and without knowledge regarding the inner workings of the system,
    this is classified as blackbox monitoring, as discussed in [Chapter 1](4214ddff-8289-4dc6-b0ef-240510a22192.xhtml),
    *Monitoring Fundamentals*.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Blackbox exporter
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`blackbox_exporter` is one of the most peculiar of all the currently available
    exporters in the Prometheus ecosystem. Its usage pattern is ingenious and usually,
    newcomers are puzzled by it. We''ll be going to dive into this exporter with the
    hope of making its use as straightforward as possible.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'The `blackbox_exporter` service exposes two main endpoints:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '`/metrics`: Where its own metrics are exposed'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/probe`: It is the query endpoint that enables blackbox probes, returning
    their results in Prometheus exposition format'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Besides the two previous endpoints, the `/` of the service also provides valuable
    information, including logs for the probes performed. This endpoint is available
    in the static infrastructure test environment at `http://192.168.42.11:9115`.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: The blackbox exporter supports probing endpoints through a wide variety of protocols
    natively, such as TCP, ICMP, DNS, HTTP (versions 1 and 2), as well as TLS on most
    probes. Additionally, it also supports scripting text-based protocols such as
    IRC, IMAP, or SMTP by connecting through TCP and configuring what messages should
    be sent and what responses are expected; even plain HTTP would be possible to
    script but, as HTTP probing is such a common use case, it's already built in.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'Having said that, this exporter doesn''t cover all the blackbox-style monitoring
    needs. For those cases, writing your own exporter might be needed. As an example,
    you can''t use `blackbox_exporter` to test a Kafka topic end to end, so you might
    need to look for an exporter able to produce a message to Kafka and then consume
    it back:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93515ee5-4572-42e6-984b-a8520d7d16e8.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.10: blackbox_exporter web interface'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 'The `/probe` endpoint, when hit with an HTTP GET request with the parameters
    module and target, it executes the specified `prober` module against the defined
    target, and the result is then exposed as Prometheus metrics:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/487c05fd-69a4-41c3-9272-f917c67ea8fe.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11: blackbox_exporter high-level workflow
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a request such as `http://192.168.42.11:9115/probe?module=http_2xx&target=example.com`
    will return something like the following snippet (a couple of metrics were discarded
    for briefness):'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: When debugging probes, you can append `&debug=true` to the HTTP GET URL to enable
    debug information.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: The source code and installation files for `blackbox_exporter` are available
    at [https://github.com/prometheus/blackbox_exporter](https://github.com/prometheus/blackbox_exporter).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: A quirk to be aware of when using `blackbox_exporter` is that the `up` metric
    does not reflect the status of the probe, but merely that Prometheus can reach
    the exporter. As can be seen in the previous metrics output, there is a `probe_success`
    metric that represents the status of the probe itself. This means that it is common
    for the `up` metric to appear healthy, but the probe might be failing, which is
    a common source for confusion.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Configuration
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The scrape job configuration for blackbox probes is unusual, in the sense that
    both the `prober` module and the list of targets, whether static or discovered,
    need to be relayed to the exporter as HTTP GET parameters to the `/probe` endpoint.
    To make this work, a bit of `relabel_configs` magic is required, as seen in [Chapter
    5](12e775c2-bee9-4ebe-ad73-2f9313eeeeee.xhtml), *Running a Prometheus Server*.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the following Prometheus configuration snippet as an example, we''re
    setting up an ICMP probe against the Prometheus instance, while `blackbox_exporter`
    is running on `target01`:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Due to the nature of the ICMP probe, it requires elevated privileges to be run.
    In our environment, we're setting the capability to use raw sockets (`setcap cap_net_raw+ep
    /usr/bin/blackbox_exporter`) to guarantee such privileges.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is to replace the address of the target with the address of `blackbox_exporter`,
    ensuring the internal `__param_target` keeps the address of the target. Focusing
    on how the `relabel_configs` is processed, the following happens:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: The `__address__` value (which contains the address of the target) is stored
    into `__param_target`.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`__param_target` value is then stored into the instance label.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `blackbox_exporter` host is then applied to `__address__`.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This enables to Prometheus to query `blackbox_exporter` (using `__address__`),
    keep the instance label with the target definition, and pass the parameters module
    and target (using the internal `__param_target`) to the `/probe` endpoint, which
    returns the metrics data.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our static infrastructure test environment, we can validate the configuration
    of `blackbox_exporter` by connecting to the `target01` instance as shown here:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Then checking the configuration of the provided `systemd` unit file as shown
    in the following command:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The configuration can be reloaded in runtime by sending an HTTP POST to the
    `/-/reload` endpoint or a `SIGHUP` to the `blackbox_exporter` process. If there
    are configuration errors, it will not be applied.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see all the required arguments for the `blackbox_exporter` service in
    the following snippet from the unit file, including the path to the configuration
    file:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The configuration we tailored for our example can be found in the following
    snippet:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Notice `preferred_ip_protocol: ip4` is used, as `blackbox_exporter` prefers
    `ipv6`, but we''re forcing `ipv4` in our probes.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'On the Prometheus instance, we added the following jobs:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Using the Prometheus web interface, available at `http://192.168.42.10:9090/targets`,
    we can validate whether the scrapes are successful (independently of the return
    status of the probes):'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d2c5b2de-f1a8-4d04-a4e1-172c2cd6276d.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.12: Prometheus /targets endpoint showing the blackbox_exporter targets'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned before, the `/targets` page doesn''t tell you whether a probe
    was successful or not. This needs to be validated in the expression browser by
    querying the `probe_success` metric:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/67800f60-c4c3-47dd-a5de-04f90153a2ac.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13: Prometheus expression browser showing the probe_success query results
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: 'Some interesting metrics that can be collected from `blackbox_exporter` (both
    about the exporter itself and from probes) are:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '`blackbox_exporter_config_last_reload_successful`, which exposes if the exporter''s
    configuration file was reloaded successfully after a `SIGHUP`'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`probe_http_status_code`, which allows you to understand what HTTP status code
    is being returned when using the HTTP `prober` module'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`probe_ssl_earliest_cert_expiry`, which returns the timestamp for when the
    certificate chain from a SSL probe becomes invalid due to one of the certificates
    in the chain expiring'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pushing metrics
  id: totrans-343
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Despite the intense debate regarding push versus pull and the deliberate decision
    of using pull in the Prometheus server design, there are some legitimate situations
    where push is more appropriate.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: One of those situations is batch jobs, though, for this statement to truly make
    sense, we need to clearly define what is considered a batch job. In this scope,
    a service-level batch job is a processing workload not tied to a particular instance,
    executed infrequently or on a schedule, and as such is not always running. This
    kind of job makes it very hard to generate successful scrapes if instrumented,
    which, as discussed previously in [Chapter 5](12e775c2-bee9-4ebe-ad73-2f9313eeeeee.xhtml),
    *Running a Prometheus Server*, results in metric staleness, even if running for
    long enough to be scraped occasionally.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: There are alternatives to relying on pushing metrics; for example, by using
    the textfile collector from `node_exporter` as described previously. Nevertheless,
    this option does not come without downsides. If the workload is not specific to
    a particular instance, you'll end up with multiple time series plus the cleanup
    logic of the textfile collector files, unless the lifetime of the metric matches
    the lifetime of the instance, which can then work out well in practice.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: As a last resort, you have Pushgateway, which we'll be covering next.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Pushgateway
  id: totrans-348
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This exporter should only be employed in very specific use cases, as stated
    previously, and we should be aware of some common pitfalls. One possible problem
    is the lack of high availability, making it a single point of failure. This also
    impacts scalability as the only way to accommodate more metrics/clients is to
    either scale the instance vertically (adding more resources) or sharding (having
    different Pushgateway instances for different logical groups). By using Pushgateway,
    Prometheus does not scrape an application instance directly, which prevents having
    the `up` metric as a proxy for health monitoring. Additionally, and similarly
    to the textfile collector from `node_exporter`, metrics need to be manually deleted
    from Pushgateway via its API, or they will forever be exposed to Prometheus.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 'To push a metric, you need to send an HTTP POST request to the Pushgateway
    endpoint, using the following URL path definition. This will be demonstrated in
    the following deployment section:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Here, `<job_name>` will become the value of the label job for the metrics pushed
    and the `<label_name>/<label_value>` pairs will become additional label/value
    pairs. Keep in mind that metrics will be available until manually deleted, or
    in the case of a restart, when persistence is not configured.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: The source code and installation files for Pushgateway are available at [https://github.com/prometheus/pushgateway](https://github.com/prometheus/pushgateway).
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Configuration
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As Pushgateway is a centralized point where instances push their metrics, when
    a scrape is performed by Prometheus, the label instance will be automatically
    set to the Pushgateway server address/port for every single metric it exposes,
    and the label job to whatever name was set in the Prometheus scrape job definition.
    On label collision, Prometheus renames the original labels to `exported_instance`
    and `exported_job`, respectively. To avoid this behavior, `honor_labels: true`
    should be used in the scrape job definition to guarantee the labels that prevail
    are the ones coming from Pushgateway.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'The noteworthy runtime configuration for our test case is as follows:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '| **Flag** | **Description** |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
- en: '| `--web.listen-address` | Bind address, defaults to `0.0.0.0:9091` |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
- en: '| `--persistence.file` | Persistence file location, if empty metrics are only
    kept in memory |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
- en: '| `--persistence.interval` | Interval to write to the persistence file, defaults
    to 5m |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
- en: Deployment
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll be building upon the Kubernetes test environment started previously. In
    this particular scenario, we'll deploy an instance of Pushgateway and we'll be
    adding it as a target in Prometheus. To validate the correctness of our setup,
    we'll create a Kubernetes CronJob to emulate a batch job style of workload, and
    push its metrics to the Pushgateway service to ensure Prometheus collects our
    data.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin the deployment, ensure you move into the correct repository path,
    relative to the code repository root:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'To deploy an instance of Pushgateway, you can use the following manifest. Keep
    in mind that this service does not support high availability or clustering:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Apply the manifest by executing the following command:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'And follow the deployment using the following:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'After a successful deployment, it''s time to provide a `Service` to our new
    instance, using the following manifest:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The following instruction applies to the previous manifest:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'You may now validate the web interface for Pushgateway using the following
    command:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'This will open a new browser tab pointing to the newly created Pushgateway
    instance web interface, which should look like the following figure:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/034cb26b-a5b7-4f17-9887-3fa1d16a51a1.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.14: Pushgateway web interface without any metric being pushed'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to instruct Prometheus to scrape Pushgateway. This can be accomplished
    via a new `ServiceMonitor` manifest as follows:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'To apply this ServiceMonitor, we just type the following command:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Now that we have our monitoring infrastructure in place, we need to simulate
    a batch job to validate our setup.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: 'We can rely on the following manifest, which pushes a dummy `batchjob_example`
    metric with several labels to the Pushgateway service endpoint using a handcrafted
    `curl` payload:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'To apply the previous manifest, use the following command:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'After a minute, the web interface for Pushgateway will look similar to this
    screenshot:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/535397ed-e81b-4f4c-b4bb-4c660cfe4ce7.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.15: Pushgateway web interface presenting the batchjob_example metric'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now use the Prometheus expression browser to validate the metric is
    being scraped from Pushgateway:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/614c64be-455d-4140-af56-45b359c91112.png)'
  id: totrans-393
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.16: Prometheus expression browser showing the batchjob_example metric'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: As Pushgateway's job is to proxy metrics from other sources, it provides very
    little metrics of its own - just the standard Go runtime metrics, process metrics,
    HTTP handler metrics and build info. However, there is one application metric
    to note, which is `push_time_seconds`. This will tell you the last time a specific
    group (combination of labels used in the HTTP API when pushing) was seen. This
    can be used to detect missing or delayed jobs.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: More exporters
  id: totrans-396
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Prometheus community has produced a great number of exporters for just about
    anything you might need. However, making an intentional choice to deploy a new
    piece of software in your infrastructure has an indirect price to pay upfront.
    That price translates into the deployment automation code to be written, the packaging,
    the metrics to be collected and alerting to be created, the logging configuration,
    the security concerns, the upgrades, and other things we sometimes take for granted.
    When choosing an open source exporter, or any other open source project for that
    matter, there are a few indicators to keep in mind.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: We should validate the community behind the project, the general health of contributions,
    if issues are being addressed, pull requests are being timely managed, and whether
    the maintainers are open to discuss and interact with the community. Technically,
    we should also check whether the official Prometheus client libraries are being
    used by the particular project. With that said, we'll be covering a few noteworthy
    exporters.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: JMX exporter
  id: totrans-399
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Java Virtual Machine** (**JVM**) is a popular choice for core infrastructure
    services, such as Kafka, ZooKeeper, and Cassandra, among others. These services,
    like many others, do not natively offer metrics in the Prometheus exposition format
    and instrumenting such applications is far from being a trivial task. In these
    scenarios, we can rely on the **Java Management Extensions** (**JMX**) to expose
    the application's internal state through the **Managed Beans** (**MBeans**). The
    JMX exporter extracts numeric data from the exposed MBeans and converts it into
    Prometheus metrics, exposing them on an HTTP endpoint for ingestion.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'The exporter is available in the following two forms:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '**Java agent**: In this mode, the exporter is loaded inside the local JVM where
    the target application is running and exposes a new HTTP endpoint.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standalone HTTP server**: In this mode, a separate JVM instance is used to
    run the exporter that connects via JMX to the target JVM and exposes collected
    metrics on its own HTTP server.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The documentation strongly advises deploying the exporter using the Java agent,
    for good reason; the agent produces richer sets of metrics as compared with the
    standalone exporter, as it has access to the full JVM being instrumented. However,
    both have trade-offs that are important to be aware of so that the right tool
    for the job is chosen.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: Although the standalone server does not have access to the JVM specific metrics,
    such as garbage collector statistics or process memory/CPU usage, it easier to
    deploy and manage on static infrastructure when Java applications already have
    JMX enabled and are long-running processes that might not be convenient to touch.
    Adding to that, the exporter upgrade cycle becomes decoupled with the application
    life cycle, even though new releases are infrequent.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the Java agent provides the full range of available metrics
    in the JVM, but needs to be loaded into the target application at startup. This
    might be simpler to do on regularly deployed applications or when those applications
    run in containers.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit of running the agent is that the target JVM is also responsible
    to serve its own metrics, so the `up` metric from the scrape job can represent
    the process status without ambiguity.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: Both options require a configuration file that can whitelist, blacklist, and/or
    relabel metrics from MBeans into the Prometheus format. An important performance
    consideration is the use of whitelists whenever possible. Some applications expose
    a very large amount of MBeans (such as Kafka or Cassandra) and frequent scrapes
    do have a significant performance impact.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: You can find useful examples of configuration files for the most used applications
    at [https://github.com/prometheus/jmx_exporter/tree/master/example_configs](https://github.com/prometheus/jmx_exporter/tree/master/example_configs).
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: The source code for `jmx_exporter` is available at [https://github.com/prometheus/jmx_exporter](https://github.com/prometheus/jmx_exporter).
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: HAProxy exporter
  id: totrans-411
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HAProxy, a well-known load balancing solution, at the time of writing does not
    expose Prometheus metrics natively. Fortunately, it has an exporter, made by the
    Prometheus maintainers, to ensure its metrics can be collected, which is the `haproxy_exporter`.
    HAProxy natively exposes its metrics in **comma-separated value** (**CSV**) format
    via a configurable HTTP endpoint by using the `stats enable` configuration. The
    `haproxy_exporter`, which runs as a separate daemon, is able to connect to the
    HAProxy stats endpoint, consume the CSV, and convert its contents to the Prometheus
    metric format, exposing it in a synchronous manner when triggered by a scrape.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: Instrumenting the load-balancer layer can be quite useful when the applications
    in the backend pools aren't properly instrumented and thus don't expose access
    metrics. For example, dashboards and alerts can be created for HTTP error rates
    or backend availability without any development effort from the application side.
    This is not meant to be a long-term solution, but can help in transitioning from
    legacy monitoring systems to Prometheus.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: You can find the source code and installation files for the `haproxy_exporter`
    at [https://github.com/prometheus/haproxy_exporter](https://github.com/prometheus/haproxy_exporter)
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-415
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we had the opportunity to discover some of the most used Prometheus
    exporters available. Using test environments, we were able to interact with operating-system-level
    exporters running on VMs and container-specific exporters running on Kubernetes.
    We found that sometimes we need to rely on logs to obtain metrics and went through
    the current best options to achieve this. Then, we explored blackbox probing with
    the help of `blackbox_exporter` and validated its unique workflow. We also experimented
    with pushing metrics instead of using the standard pull approach from Prometheus,
    while making clear why sometimes this method does indeed make sense.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: All these exporters enable you to gain visibility without having to natively
    instrument code, which sometimes is much more costly than relying on community-driven
    exporters.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: With so many sources of metrics, now is the time to understand how to extract
    useful information from their data. In the next chapter, we'll go over PromQL
    and how best to leverage it.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-419
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How would you collect custom metrics with the Node Exporter?
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What resources does cAdvisor consult to generate metrics?
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: kube-state-metrics expose numerous API objects. Is there a way to restrict that
    number?
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How could you debug a `blackbox_exporter` probe?
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If an application does not expose metrics, in Prometheus format or otherwise,
    what could an option to monitor it be?
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the downsides of using Pushgateway?
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If a particular batch job is host specific, is there any alternative to the
    use of Pushgateway?
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-427
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Prometheus exporters**: [https://prometheus.io/docs/instrumenting/exporters/](https://prometheus.io/docs/instrumenting/exporters/)'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prometheus port allocations**: [https://github.com/prometheus/prometheus/wiki/Default-port-allocations](https://github.com/prometheus/prometheus/wiki/Default-port-allocations)'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manpages cgroups**: [http://man7.org/linux/man-pages/man7/cgroups.7.html](http://man7.org/linux/man-pages/man7/cgroups.7.html)'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manpages namespaces**: [http://man7.org/linux/man-pages/man7/namespaces.7.html](http://man7.org/linux/man-pages/man7/namespaces.7.html)'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes resource usage monitoring**: [https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/](https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/)'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
