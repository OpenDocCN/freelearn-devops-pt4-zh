<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Troubleshooting and Validation</h1>
                </header>
            
            <article>
                
<p>Troubleshooting is, in itself, an art and, in this chapter, we will provide some useful guidelines on how to quickly detect and fix problems. You will discover useful endpoints that expose critical information, learn about promtool, Prometheus' command-line interface and validation tool, and how to integrate it into your daily workflow. Finally, we'll look into the Prometheus database and collect insightful information regarding its usage.</p>
<p>In brief, the following topics will be covered in this chapter:</p>
<ul>
<li style="font-weight: 400">The test environment for this chapter</li>
<li style="font-weight: 400">Exploring promtool</li>
<li style="font-weight: 400">Logs and endpoint validation</li>
<li style="font-weight: 400">Analyzing the time series database</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The test environment for this chapter</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll be focusing on the Prometheus server and will be deploying a new instance so that we can apply the concepts covered in this chapter using a new test environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment</h1>
                </header>
            
            <article>
                
<p>To create a new instance of Prometheus, move into the correct repository path:</p>
<pre><strong>cd chapter08/</strong></pre>
<p>Ensure that no other test environments are running and spin up this chapter's environment:</p>
<pre><strong>vagrant global-status</strong><br/><strong>vagrant up</strong></pre>
<p>You can validate the successful deployment of the test environment using the following:</p>
<pre><strong>vagrant status</strong></pre>
<p>This should output the following:</p>
<pre><strong>Current machine states:</strong><br/><br/><strong>prometheus running (virtualbox)</strong><br/><br/><strong>The VM is running. To stop this VM, you can run `vagrant halt` to </strong><strong>shut it down forcefully, or you can run `vagrant suspend` to simply </strong><strong>suspend the virtual machine. In either case, to restart it again, </strong><strong>simply run `vagrant up`.</strong></pre>
<p>The new instance will be available for inspection and the Prometheus web interface is accessible at <kbd>http://192.168.42.10:9090</kbd>.</p>
<p>You can now access the Prometheus instance by executing the following command:</p>
<pre><strong>vagrant ssh prometheus</strong></pre>
<p>Now that you're connected to the Prometheus instance, you can validate the instructions described in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cleanup</h1>
                </header>
            
            <article>
                
<p>When you finish testing, just make sure you're inside <kbd>chapter08/</kbd> and execute the following:</p>
<pre><strong>vagrant destroy -f</strong></pre>
<p>Don't worry too much – you can easily spin up the environment again if you so require.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring promtool</h1>
                </header>
            
            <article>
                
<p>Prometheus ships with a very useful supporting command-line tool called <kbd>promtool</kbd>. This small Golang binary can be used to quickly perform several troubleshooting actions and is packed with helpful subcommands.</p>
<p>The features available can be divided into four categories, which we'll be covering next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Checks</h1>
                </header>
            
            <article>
                
<p><span>The subcommands that belong to this category provide the user with the ability to check and validate several configuration aspects of the Prometheus server and metric standards compliance. The following sections depict their usage.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">check config</h1>
                </header>
            
            <article>
                
<p>There are several types of checks presented by <kbd>promtool</kbd>. One of the most valuable is the check for the main configuration file for the Prometheus server.</p>
<p><kbd>check config</kbd> expects a path to the Prometheus main configuration file and outputs its assessment of the validity of the configuration. When something is amiss, this subcommand can tell the user what the problem is: if it's a non-breaking issue, such as an empty discovery file, it will output a warning but permit <kbd>promtool</kbd> to exit with success; when it encounters breaking errors, such as incorrect syntax, it will output an error and will mark the check as a failure. Using the exit code returned by the tool – <kbd>0</kbd> for success and <kbd>1</kbd> for failure – is a great way to ensure configuration changes won't break Prometheus upon restart and should be used as a pre-flight check. Besides the main configuration file, this option also recursively checks any referenced file, such as rules files.</p>
<p>The following example illustrates its usage:</p>
<pre><strong>vagrant@prometheus:~$ promtool check config /etc/prometheus/prometheus.yml </strong><br/><strong>Checking /etc/prometheus/prometheus.yml</strong><br/><strong>  SUCCESS: 1 rule files found</strong><br/><br/><strong>Checking /etc/prometheus/first_rules.yml</strong><br/><strong>  SUCCESS: 1 rules found</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">check rules</h1>
                </header>
            
            <article>
                
<p><kbd>check rules</kbd> analyzes and pinpoints misconfigurations in rule configuration files. It allows the targeting of specific rules files directly, which lets you test files that aren't yet referenced in the main Prometheus configuration. This ability can be handy in both the development cycle of rule files and for validating automatic changes in said files when using configuration management. We'll be covering these concepts in depth in <a href="9aa1e3da-13cf-4051-845d-1d1c924ef47b.xhtml">Chapter 9</a>, <em>Defining Alerting and Recording Rules</em>.</p>
<p>Here's the expected output when checking a rule file:</p>
<pre><strong>vagrant@prometheus:~$ promtool check rules /etc/prometheus/first_rules.yml </strong><br/><strong>Checking /etc/prometheus/first_rules.yml</strong><br/><strong>  SUCCESS: 1 rules found</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">check metrics</h1>
                </header>
            
            <article>
                
<p>The <kbd>check metrics</kbd> subcommand validates whether the metrics passed to it follow the Prometheus guidelines in terms of consistency and correctness. This can be useful in the development cycle to make sure new instrumentation conforms to the standard, as well as using it in automation if you have some control over whether new jobs follow the same rules. It expects the metrics payload as input via <kbd>STDIN</kbd>, so you can pipe either a file or, the output of <kbd>curl</kbd> directly into it. <span>For the sake of this example, we are exposing an issue that occurred in Prometheus </span><span>prior to</span><span> version </span><kbd>2.8.0</kbd><span>:</span> </p>
<pre><strong>~$ curl -s http://prometheus:9090/metrics | promtool check metrics</strong><br/><strong>prometheus_tsdb_storage_blocks_bytes_total non-counter metrics should not have "_total" suffix</strong></pre>
<p>As you can see, there appears to be an issue with the <kbd>prometheus_tsdb_storage_blocks_bytes_total</kbd> metric. Let's have a look at this particular metric to troubleshoot the error:</p>
<pre>~$ curl -s http://prometheus:9090/metrics | grep prometheus_tsdb_storage_blocks_bytes_total<br/># HELP prometheus_tsdb_storage_blocks_bytes_total The number of bytes that are currently used for local storage by all blocks.<br/># TYPE prometheus_tsdb_storage_blocks_bytes_total <strong>gauge</strong><br/>prometheus_tsdb_storage_blocks_bytes<strong>_total</strong> 0</pre>
<p>In these older versions of Prometheus, it appears the metric is declared as a gauge but has the <kbd>_total</kbd> suffix, which should only be used in counters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Queries</h1>
                </header>
            
            <article>
                
<p><span>The subcommands that belong to this category enable the execution of PromQL expressions directly from the command line. These queries rely on the Prometheus public HTTP API. The following topics demonstrate how to use them.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">query instant</h1>
                </header>
            
            <article>
                
<p>The <kbd>query instant</kbd> subcommand allows the querying of the Prometheus server directly via the command line based on the current time. For it to work, a Prometheus server URL must be supplied as an argument, as well as the query to execute, like so:</p>
<pre><strong>vagrant@prometheus:~$ promtool query instant 'http://prometheus:9090' 'up == 1'</strong><br/><strong>up{instance="prometheus:9090", job="prometheus"} =&gt; 1 @[1550609854.042]</strong><br/><strong>up{instance="prometheus:9100", job="node"} =&gt; 1 @[1550609854.042]</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">query range</h1>
                </header>
            
            <article>
                
<p>Similar to the previous subcommand, <kbd>query range</kbd> enables the results to be displayed for a specified time range. As such, we must provide the start and end Unix-formatted timestamps, as well as the query and Prometheus server endpoint.</p>
<p>As an example, we'll be using the <kbd>date</kbd> command to define the start and end timestamps, generating a Unix-formatted timestamp for five minutes ago and another for now. We may also specify the resolution of our query using the <kbd>--step</kbd> flag, which in our example is one minute. Finally, we place the PromQL expression to be executed, ending up with an instruction similar to the following:</p>
<pre><strong>vagrant@prometheus:~$ promtool query range --start=$(date -d '5 minutes ago' +'%s') --end=$(date -d 'now' +'%s') --step=1m 'http://prometheus:9090' 'node_network_transmit_bytes_total{device="eth0",instance="prometheus:9100",job="node"}'</strong><br/><strong>node_network_transmit_bytes_total{device="eth0", instance="prometheus:9100", job="node"} =&gt;</strong><br/><strong>139109 @[1551019990]</strong><br/><strong>139251 @[1551020050]</strong><br/><strong>139401 @[1551020110]</strong><br/><strong>139543 @[1551020170]</strong><br/><strong>139693 @[1551020230]</strong><br/><strong>140571 @[1551020290]</strong></pre>
<div class="packt_tip">The <kbd>date</kbd> command available inside our test environment is from GNU <kbd>coreutils</kbd>, which differs from the BSD-based one available on macOS. The syntax might not be directly compatible between the two.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">query series</h1>
                </header>
            
            <article>
                
<p>With the <kbd>query series</kbd> subcommand, you can search all of the time series that match a set of metric names and labels. Here's how to use it:</p>
<pre><strong>vagrant@prometheus:~$ promtool query series 'http://prometheus:9090' --match='up' --match='go_info{job="prometheus"}'</strong><br/><strong>{__name__="go_info", instance="prometheus:9090", job="prometheus", version="go1.11.5"}</strong><br/><strong>{__name__="up", instance="prometheus:9090", job="prometheus"}</strong><br/><strong>{__name__="up", instance="prometheus:9100", job="node"}</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">query labels</h1>
                </header>
            
            <article>
                
<p>Using <kbd>query labels</kbd>, you can search for a specific label across all the available metrics and return all the possible values attached to it; for example:</p>
<pre><strong>vagrant@prometheus:~$ promtool query labels 'http://prometheus:9090' 'mountpoint'</strong><br/><strong>/</strong><br/><strong>/run</strong><br/><strong>/run/lock</strong><br/><strong>/run/user/1000</strong><br/><strong>/vagrant</strong><br/><strong>/var/lib/lxcfs</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Debug</h1>
                </header>
            
            <article>
                
<p><span>The subcommands that belong to this category allow the extraction of debug data from the running Prometheus server so it can be analyzed. We'll be demonstrating how to use them next.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">debug pprof</h1>
                </header>
            
            <article>
                
<p>The Prometheus server, like most serious software written in Go, is instrumented using a package from the standard library named <kbd>pprof</kbd>, which provides runtime profiling information using a specific format. The files produced in this format can then be read by a command-line tool with the same name (<kbd>pprof</kbd>), which uses them to generate reports and visualizations of the profiling data. <kbd>promtool</kbd> offers the <kbd>debug pprof</kbd> subcommand, which we can see in action in the following snippet:</p>
<pre><strong>vagrant@prometheus:~$ promtool debug pprof 'http://prometheus:9090'</strong><br/><strong>collecting: http://prometheus:9090/debug/pprof/profile?seconds=30</strong><br/><strong>collecting: http://prometheus:9090/debug/pprof/block</strong><br/><strong>collecting: http://prometheus:9090/debug/pprof/goroutine</strong><br/><strong>collecting: http://prometheus:9090/debug/pprof/heap</strong><br/><strong>collecting: http://prometheus:9090/debug/pprof/mutex</strong><br/><strong>collecting: http://prometheus:9090/debug/pprof/threadcreate</strong><br/><strong>collecting: http://prometheus:9090/debug/pprof/trace?seconds=30</strong><br/><strong>Compiling debug information complete, all files written in "debug.tar.gz".</strong></pre>
<p>When we extract the archive generated by the previous command, we can see a few files:</p>
<pre><strong>vagrant@prometheus:~$ tar xzvf debug.tar.gz </strong><br/><strong>cpu.pb</strong><br/><strong>block.pb</strong><br/><strong>goroutine.pb</strong><br/><strong>heap.pb</strong><br/><strong>mutex.pb</strong><br/><strong>threadcreate.pb</strong><br/><strong>trace.pb</strong></pre>
<p>Using <kbd>pprof</kbd>, we can generate an image of the dump, as we can observe in the next snippet: </p>
<pre><strong>vagrant@prometheus:~$ pprof -svg heap.pb &gt; /vagrant/cache/heap.svg</strong></pre>
<div class="packt_infobox"><span>The test environment comes with the </span><kbd>pprof</kbd><span> command-line tool ready to be used. More information on how to build and deploy it is available at </span><a href="https://github.com/google/pprof">https://github.com/google/pprof</a>.</div>
<p>On your host machine, inside the code repository under the <kbd>./cache/</kbd> path (relative to the repository root), you should now have a scalable vector graphics file named <kbd>heap.svg</kbd>, which can be opened by your browser for inspection. The following screenshot shows what you might see when looking at the file produced by the preceding example:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4f459c31-a4a3-4664-8d7d-fab050e008cf.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 8.1: Example of a heap map generated by pprof</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">debug metrics</h1>
                </header>
            
            <article>
                
<p>This subcommand downloads the metrics exposed by the supplied Prometheus instance in a compressed archive. <kbd>debug metrics</kbd> is not commonly used as the <kbd>/metrics</kbd> Prometheus endpoint is available to anyone able to run this command; it exists so that it is easier to provide the current state of a Prometheus instance to external assistance (such as the Prometheus maintainers) when asked to. This subcommand can be used as follows:</p>
<pre><strong>vagrant@prometheus:~$ promtool debug metrics 'http://prometheus:9090'</strong><br/><strong>collecting: http://prometheus:9090/metrics</strong><br/><strong>Compiling debug information complete, all files written in "debug.tar.gz".</strong><br/><br/><strong>vagrant@prometheus:~$ tar xzvf debug.tar.gz </strong><br/><strong>metrics.txt</strong><br/><br/><strong>vagrant@prometheus:~$ tail -n 5 metrics.txt </strong><br/><strong># HELP promhttp_metric_handler_requests_total Total number of scrapes by HTTP status code.</strong><br/><strong># TYPE promhttp_metric_handler_requests_total counter</strong><br/><strong>promhttp_metric_handler_requests_total{code="200"} 284</strong><br/><strong>promhttp_metric_handler_requests_total{code="500"} 0</strong><br/><strong>promhttp_metric_handler_requests_total{code="503"} 0</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">debug all</h1>
                </header>
            
            <article>
                
<p>This option aggregates the previous debug subcommands into a single instruction, as we can see in the following example:</p>
<pre><strong>vagrant@prometheus:~$ promtool debug all 'http://prometheus:9090'</strong><br/><strong>collecting: http://prometheus:9090/debug/pprof/threadcreate</strong><br/><strong>collecting: http://prometheus:9090/debug/pprof/profile?seconds=30</strong><br/><strong>collecting: http://prometheus:9090/debug/pprof/block</strong><br/><strong>collecting: http://prometheus:9090/debug/pprof/goroutine</strong><br/><strong>collecting: http://prometheus:9090/debug/pprof/heap</strong><br/><strong>collecting: http://prometheus:9090/debug/pprof/mutex</strong><br/><strong>collecting: http://prometheus:9090/debug/pprof/trace?seconds=30</strong><br/><strong>collecting: http://prometheus:9090/metrics</strong><br/><strong>Compiling debug information complete, all files written in "debug.tar.gz".</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tests</h1>
                </header>
            
            <article>
                
<p><kbd>promtool</kbd> has recently gained the ability to run unit tests against defined recording and alerting rules. This feature is very useful in situations where you might need to check whether an expression matches certain conditions that have never happened before, thus making it difficult to be sure they will work when the time comes. This subcommand is called <kbd>test rules</kbd> and takes one or more test files as arguments. We're going to provide a deep dive of this feature later on, when we tackle how to best take advantage of rules in <a href="9aa1e3da-13cf-4051-845d-1d1c924ef47b.xhtml">Chapter 9</a>, <em>Defining Alerting and Recording Rules</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logs and endpoint validation</h1>
                </header>
            
            <article>
                
<p>In the next sections, we go through several useful HTTP endpoints and service logs that can be fundamental to troubleshoot issues with a Prometheus instance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Endpoints</h1>
                </header>
            
            <article>
                
<p>Checking whether Prometheus is up and running is usually very simple, as it follows the conventions most cloud-native applications use for service health: one endpoint to check whether the service is healthy and another to check whether it is ready to start handling incoming requests. For those who use or have used Kubernetes in the past, these might sound familiar; in fact, Kubernetes also uses these conventions to assess whether a container needs to be restarted (for example, if the application deadlocks and stops responding to health probes) and whether it can start sending traffic to the container. In Prometheus, these are the <kbd>/-/healthy</kbd> and <kbd>/-/ready</kbd> endpoints.</p>
<p>You can try these endpoints yourself by running the following commands in the test environment and checking their output, as well as their HTTP status code:</p>
<pre><strong>vagrant@prometheus:~$ curl -w "%{http_code}\n" http://localhost:9090/-/healthy</strong><br/><strong>Prometheus is Healthy.</strong><br/><strong>200</strong><br/><br/><strong>vagrant@prometheus:~$ curl -w "%{http_code}\n" http://localhost:9090/-/ready</strong><br/><strong>Prometheus is Ready.</strong><br/><strong>200</strong></pre>
<div class="packt_tip">In traditional infrastructure, it is usual to use the readiness endpoint as the backend health probe when using load balancers in front of a set of Prometheus instances, as only one health check can be configured. By using the readiness endpoint, traffic is only routed to an instance ready to accept it.</div>
<p>Additionally, Prometheus exposes a <kbd>/debug/pprof/</kbd> endpoint, which is used by the <kbd>promtool debug pprof</kbd> command, as was shown in the previous section. This endpoint also exposes a navigable web UI where <kbd>pprof</kbd> debug information can be consulted, such as current goroutines and their stack traces, heap allocations, memory allocations, and more:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1b086a77-cbde-4025-b506-eb3680ceeba3.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 8.2: Information available on the Prometheus server/debug/pprof endpoint</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logs</h1>
                </header>
            
            <article>
                
<p>Prometheus logging is very terse when compared with most current software. This is a very conscious effort from the part of the Prometheus maintainers, as extraneous logging can lead to performance issues. Additionally, supporting different log streams (such as application logs, access logs, and slow query logs) without just writing it all to standard output - and thus spamming the application log with other types of logs - would force Prometheus to explicitly support writing to files, which is undesirable in cloud-native environments. Having said that, you can configure Prometheus to increase application log verbosity by setting the <kbd>--log.level</kbd> flag. As an example, failed scrapes are considered normal operating behavior and, as such, do not show up in the logs; however, they can be recorded by increasing the log verbosity to the <kbd>debug</kbd> log level.</p>
<p>The Prometheus instance in the test environment for this chapter is already configured with the log level set to debug. You can confirm this by running the following:</p>
<pre><strong>vagrant@prometheus:~$ sudo systemctl cat prometheus.service</strong></pre>
<p>The relevant section should have the following flags set:</p>
<pre>ExecStart=/usr/bin/prometheus \<br/>    --log.level=debug \<br/>    --config.file=/etc/prometheus/prometheus.yml \<br/>    --storage.tsdb.path=/var/lib/prometheus/data \<br/>    --web.console.templates=/usr/share/prometheus/consoles \<br/>    --web.console.libraries=/usr/share/prometheus/console_libraries</pre>
<p>So, now we can see what happens when a scrape fails. To make this happen, we can stop the <kbd>node_exporter</kbd> service on the test environment and have a look at the Prometheus logs:</p>
<pre><strong>vagrant@prometheus:~$ sudo service node-exporter stop</strong><br/><strong>vagrant@prometheus:~$ sudo journalctl -fu prometheus | grep debug</strong><br/><strong>Feb 23 15:28:14 prometheus prometheus[1438]: level=debug ts=2019-02-23T15:28:14.44856006Z caller=scrape.go:825 component="scrape manager" scrape_pool=node target=http://prometheus:9100/metrics msg="Scrape failed" err="Get http://prometheus:9100/metrics: dial tcp 192.168.42.10:9100: connect: connection refused"</strong><br/><strong>Feb 23 15:28:29 prometheus prometheus[1438]: level=debug ts=2019-02-23T15:28:29.448826505Z caller=scrape.go:825 component="scrape manager" scrape_pool=node target=http://prometheus:9100/metrics msg="Scrape failed" err="Get http://prometheus:9100/metrics: dial tcp 192.168.42.10:9100: connect: connection refused"</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzing the time series database</h1>
                </header>
            
            <article>
                
<p>A critical component of the Prometheus server is its time series database. Being able to analyze the usage of this database is essential to detect series churn and cardinality problems. Churn, in this context, refers to time series that become stale (for example, from the origin target stop being collected or the series disappearing from one scrape to the next), and a new series with slightly different identity starts being collected next. A usual example of churn is related to Kubernetes application deploys, where the pod instance IP address changes making the previous time series obsolete, and replacing it with a new one. This impacts performance when querying, as samples with – possibly – no relevance are returned.</p>
<p>Thankfully, there's an obscure tool within the source code for the Prometheus database that allows analyzing its data, and is appropriately named <kbd>tsdb</kbd>.</p>
<div class="packt_infobox">You can find the source code for the <kbd>tsdb</kbd> tool at <a href="https://github.com/prometheus/tsdb/">https://github.com/prometheus/tsdb/</a>. It can easily be built by running  <kbd>go get github.com/prometheus/tsdb/cmd/tsdb</kbd> on a system with the proper Go toolchain installed.     </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the tsdb tool</h1>
                </header>
            
            <article>
                
<p>The <kbd>tsdb</kbd> tool can run against Prometheus' entire database or just a particular block of data, and outputs useful information about the health of that data. To run this tool, we must ensure the Prometheus server is stopped:</p>
<pre><strong>vagrant@prometheus:~$ sudo systemctl stop prometheus</strong></pre>
<p>We'll be running the <kbd>tsdb</kbd> tool targeting the Prometheus database path. For the sake of brevity, we'll limit the output to three entries per section. If no block name is specified as an argument, the last available one will be used:</p>
<pre><strong>vagrant@prometheus:~$ sudo tsdb analyze --limit=3 /var/lib/prometheus/data/</strong></pre>
<p>The output is split into a couple of sections. In the heading, we can find a summary for the block, including the following:</p>
<ul>
<li style="font-weight: 400">Its full path location</li>
<li style="font-weight: 400">The block duration span, which, in standard Prometheus deployments defaults to two hours</li>
<li style="font-weight: 400">The number of series and label names contained in the block</li>
<li style="font-weight: 400">Statistics regarding the number of index entries</li>
</ul>
<p>Here, we can see the output generated by the previous instruction:</p>
<pre>Block path: /var/lib/prometheus/data/01D48RFXXF27F91FVNGZ107JCK<br/>Duration: 2h0m0s<br/>Series: 819<br/>Label names: 40<br/>Postings (unique label pairs): 592<br/>Postings entries (total label pairs): 3164</pre>
<p class="mce-root"/>
<p>While churn isn't really an issue in our test environment, we can see what label pairs were detected with the highest involvement in producing churn:</p>
<pre>Label pairs most involved in churning:<br/>112 job=node<br/>112 instance=prometheus:9100<br/>111 instance=prometheus:9090</pre>
<p>Next, we can find the label names with the highest churn:</p>
<pre>Label names most involved in churning:<br/>224 instance<br/>224 __name__<br/>224 job</pre>
<p>Right after the label names churn, we are presented with the most common label pairs:</p>
<pre>Most common label pairs:<br/>413 job=node<br/>413 instance=prometheus:9100<br/>406 instance=prometheus:9090</pre>
<p>Finally, we reach the high cardinality sections, starting with labels:</p>
<pre>Highest cardinality labels:<br/>394 __name__<br/>66 le<br/>30 collector</pre>
<div class="packt_infobox"><kbd>__name__</kbd> is the internal label that stores the metric name, so it's normal in a healthy Prometheus system for it to be considered the label with the highest cardinality. Keep in mind that this doesn't mean a metric name can't be wrongly used as a label (for example, suffixing metric names with IDs), so it's important to keep an eye out for sudden increases in cardinality.</div>
<p>Finally, we find statistics regarding metric names:</p>
<pre>Highest cardinality metric names:<br/>30 node_scrape_collector_duration_seconds<br/>30 node_scrape_collector_success<br/>20 prometheus_http_request_duration_seconds_bucket</pre>
<div class="packt_tip">The preceding statistics are collected from a two hour block. This, however, can also be queried for a given moment via the expression browser using a query similar to <kbd>topk(3, count({__name__=~".+"}) by (__name__))</kbd>.</div>
<p>As stated before, you can choose a different block to analyze:</p>
<pre>vagrant@prometheus:~$ ls -l /var/lib/prometheus/data/<br/>total 28<br/>drwxr-xr-x 3 prometheus prometheus 4096 Feb 21 21:15 01D45Z3QCP8D6135QNS4MEPJEK/<br/>drwxr-xr-x 3 prometheus prometheus 4096 Feb 21 21:15 <strong>01D486GRJTNYJH1RM0F2F4Q9TR</strong>/<br/>drwxr-xr-x 4 prometheus prometheus 4096 Feb 21 21:15 01D48942G83N129W5FKQ5B3XCH/<br/>drwxr-xr-x 4 prometheus prometheus 4096 Feb 21 21:15 01D48G04625Y6AKQ3Z63YJVNTQ/<br/>drwxr-xr-x 3 prometheus prometheus 4096 Feb 21 21:15 01D48G048ECAR9GZ7QY1Q8SQ6Z/<br/>drwxr-xr-x 4 prometheus prometheus 4096 Feb 21 21:15 01D48RFXXF27F91FVNGZ107JCK/<br/>-rw-r--r-- 1 prometheus prometheus 0 Feb 19 23:16 lock<br/>drwxr-xr-x 2 prometheus prometheus 4096 Feb 19 23:16 wal/<br/><br/>vagrant@prometheus:~$ sudo tsdb analyze /var/lib/prometheus/data <strong>01D486GRJTNYJH1RM0F2F4Q9TR</strong></pre>
<p>The benefit of the <kbd>tsdb</kbd> report is that it provides a deeper understanding of how metrics and labels are being used, and pinpoints good candidates to be explored and validated against their targets.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we had the opportunity to experiment with a couple of useful tools to troubleshoot and analyze Prometheus configuration issues and performance. We started with <kbd>promtool</kbd> and went through all its available options; then, we used several endpoints and logs to ensure everything was working as expected. Finally, we described the <kbd>tsdb</kbd> tool and how it can be used to troubleshoot and pinpoint problems with cardinality and the churn of metrics and labels in our Prometheus database.</p>
<p>We can now step into recording and alerting rules, which will be covered in the next chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li style="font-weight: 400">How can you validate whether the main Prometheus configuration file has an issue?</li>
<li style="font-weight: 400">How can you assess whether metrics exposed by a target are up to Prometheus standards?</li>
<li style="font-weight: 400">Using <kbd>promtool</kbd>, how would you perform an instant query?</li>
<li style="font-weight: 400">How can you find all the label values being used?</li>
<li style="font-weight: 400">How do you enable debug logs on the Prometheus server?</li>
<li style="font-weight: 400">What's the difference between ready and healthy endpoints?</li>
<li style="font-weight: 400">How can you find the churn of labels on an old block of Prometheus data?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li style="font-weight: 400"><strong>Golang pprof</strong>: <a href="https://golang.org/pkg/runtime/pprof/">https://golang.org/pkg/runtime/pprof/</a></li>
</ul>


            </article>

            
        </section>
    </body></html>