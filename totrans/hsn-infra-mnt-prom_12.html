<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Defining Alerting and Recording Rules</h1>
                </header>
            
            <article>
                
<p class="mce-root">Recording rules are a useful concept of Prometheus. They allow you to speed up heavy queries and enable subqueries in PromQL that otherwise would be very expensive. Alerting rules are similar to recording rules, but with alert-specific semantics. As testing is a fundamental part of any system, you'll have the opportunity in this chapter to learn how to ensure that recording and alerting rules behave as expected before being deployed. <span>Understanding these constructs will help make Prometheus faster and more robust, as well as enabling its alerting capabilities.</span></p>
<p class="mce-root">The following topics will be covered in this chapter:</p>
<ul>
<li>Creating the test environment</li>
<li>How does rule evaluation work?</li>
<li>Setting up alerting in Prometheus</li>
<li>Testing your rules</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the test environment</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll be focusing on the Prometheus server and we'll be deploying a new instance so that we can apply the concepts covered.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment</h1>
                </header>
            
            <article>
                
<p>Let's begin by creating a new instance of Prometheus and deploying it to the server:</p>
<p class="mce-root"/>
<ol>
<li>To create a new instance of Prometheus, move into the correct repository path:</li>
</ol>
<pre style="padding-left: 60px"><strong>cd chapter09/</strong></pre>
<ol start="2">
<li class="mce-root">Ensure that no other test environments are running and spin up this chapter's environment:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant global-status</strong><br/><strong>vagrant up</strong></pre>
<ol start="3">
<li>Validate the successful deployment of the test environment using the following code:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant status</strong></pre>
<p style="padding-left: 60px">This will output the following:</p>
<pre style="padding-left: 60px">Current machine states:<br/><br/>prometheus                   running (virtualbox)<br/><br/>The VM is running. To stop this VM, you can run `vagrant halt` to<br/>shut it down forcefully, or you can run `vagrant suspend` to simply<br/>suspend the virtual machine. In either case, to restart it again,<br/>simply run `vagrant up`.</pre>
<p>The new instance will be available for inspection and the Prometheus web interface will be accessible at <kbd>http://192.168.42.10:9090</kbd>.</p>
<p>You can now access the <kbd>prometheus</kbd> instance by executing the following command:</p>
<pre><strong>vagrant ssh prometheus</strong></pre>
<p>Now that you're connected to the <kbd>prometheus</kbd> instance, you can validate the instructions described in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cleanup</h1>
                </header>
            
            <article>
                
<p>When you've finished testing, make sure that you're inside <kbd>chapter09/</kbd> and execute the following command:</p>
<pre><strong>vagrant destroy -f</strong></pre>
<p class="mce-root"/>
<p class="mce-root">Don't worry too much; you can easily spin up the environment again if you so require.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding how rule evaluation works</h1>
                </header>
            
            <article>
                
<p>Prometheus allows the periodic evaluation of PromQL expressions and the storage of the time series generated by them; these are called <strong>rules</strong>. There are two types of rules, as we'll see in this chapter. These rules are <em>recording</em> and <em>alerting</em> rules. They share the same evaluation engine, but have some variation in purpose, which we'll go into next.</p>
<p>The recording rules' evaluation results are saved into the Prometheus database as samples for the time series specified in the configuration. This type of rule can help take the load off of heavy dashboards by pre-computing expensive queries, aggregating raw data into a time series that can then be exported to external systems (such as higher-level Prometheus instances through federation, as described in <a href="3615e455-c55b-49af-8c9b-b5342d4859dd.xhtml">Chapter 13</a>, <em>Scaling and Federating Prometheus</em>), and can help to create compound range vector queries (while recording rules were the only way to do this in the past, the new subquery syntax enabled exploratory use cases for these).</p>
<p>Alerting rules trigger when an evaluated PromQL expression in a rule produces a non-empty result. They are the mechanism by which alerting over time series is done in Prometheus. Alerting rules also produce new time series when they trigger, but don't use the evaluation result as a sample; instead, they create an <kbd>ALERTS</kbd> metric with the alert name and state as labels, as well as any additional labels defined in the configuration. This will be further analyzed in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using recording rules</h1>
                </header>
            
            <article>
                
<p>Rules are defined separately from the main Prometheus configuration file, and are included by the latter through the <kbd>rule_files</kbd> top-level configuration key. They are evaluated periodically, and that interval can be defined globally with <kbd>evaluation_interval</kbd> inside <kbd>global</kbd> (defaults to one minute).</p>
<p class="mce-root"/>
<p>We can see this by looking at the configuration provided with the test environment:</p>
<pre><strong>vagrant@prometheus:~$ cat /etc/prometheus/prometheus.yml</strong><br/><strong>global:</strong><br/><strong>...</strong><br/><strong>  evaluation_interval: 1m</strong><br/><strong>...</strong><br/><br/><strong>rule_files:</strong><br/><strong>  - "recording_rules.yml"</strong><br/><strong>...</strong></pre>
<p><kbd>rule_files</kbd> takes a list of paths, which can be relative to the main Prometheus configuration or absolute paths. Additionally, globs can be used to match filenames (not directories); for example, <kbd>/etc/prometheus/rules/*.yml</kbd>. Changes in rule files are not automatically picked up by Prometheus, so a reload is needed (as described in <a href="12e775c2-bee9-4ebe-ad73-2f9313eeeeee.xhtml">Chapter 5</a>, <em>Running a Prometheus Server</em>). Prometheus will fail to reload if any error are found in rule files, and will keep running using the previous configuration. However, if the server is restarted, it will fail to start. To make sure that this does not happen, <kbd>promtool</kbd> can be used to test for errors in advance (as explained in <a href="19357d8c-dfcf-4497-ae80-4761f6633d14.xhtml">Chapter 8</a>, <em>Troubleshooting and Validation</em>) – this is strongly recommended when using automation to deploy rules.</p>
<p>Just like the <kbd>prometheus.yml</kbd> configuration file, the <kbd>rules</kbd> files are also defined in are YAML format. The actual format is very easy to understand:</p>
<pre>groups:<br/>- name: &lt;group_name_1&gt;<br/>  interval: &lt;evaluation_interval&gt;<br/>  rules:<br/>  - record: &lt;rule_name_1&gt;<br/>    expr: &lt;promql_expression_1&gt;<br/>    labels:<br/> &lt;label_name&gt;: &lt;label_value&gt;<br/>  ...<br/>  - record: &lt;rule_name_N&gt;<br/>    expr: &lt;promql_expression_N&gt;<br/>...<br/>- name: &lt;group_name_N&gt;<br/>  ...</pre>
<p>Each file defines one or more rule groups under the <kbd>groups</kbd> key. Each group has a <kbd>name</kbd>, an optional evaluation <kbd>interval</kbd> (which defaults to the global evaluation interval defined in the main Prometheus configuration file), and a list of <kbd>rules</kbd>. Each rule instructs Prometheus to record the result of evaluating the PromQL expression defined in <kbd>expr</kbd> <span>into the specified metric name, optionally adding or overriding the series label set before storing the result by setting them in <kbd>labels</kbd>.</span> The rules in each group are evaluated sequentially in the order they are declared, which means that a time series generated by a rule can safely be used in subsequent rules within the same group. Samples generated by rules will have the timestamp corresponding to the rule group evaluation time. The next figure illustrates the process previously mentioned:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/74b724c0-a869-455c-8a0d-1382e5fab039.png" style="width:39.58em;height:24.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 9.1: The Rule Manager is the Prometheus internal subsystem responsible for the periodic evaluation of rules according to their group's evaluation interval, as well as managing the alerting life cycle</div>
<p>Let's have a look at the recording rules available in the test environment for this chapter:</p>
<pre><strong>vagrant@prometheus:~$ cat /etc/prometheus/recording_rules.yml</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This file has two rule groups, named <kbd>recording_rules</kbd> and <kbd>different_eval_interval</kbd>:</p>
<pre class="mce-root">...<br/>- name: recording_rules<br/>  rules:<br/>  - record: instance:node_cpu:count<br/>    expr: count without (cpu) (count without (mode) (node_cpu_seconds_total))<br/>...</pre>
<p>The first rule group is composed of a single recording rule, which is using the global evaluation interval, taking the <kbd>node_cpu_seconds_total</kbd> metric from the Node Exporter to count the number of CPU cores available in the <strong>virtual machine</strong> (<strong>VM</strong>), and recording that result into a new time series named <kbd>instance:node_cpu:count</kbd>.</p>
<p>The second rule group is busier; it shows a custom evaluation interval for the group and a recording rule using the time series generated by previous rules in the group. We won't go into exactly what these rules are doing, as they will serve as examples for the following rule naming conventions section, but the evaluation interval can be seen here:</p>
<pre>...<br/>- name: different_eval_interval<br/>  interval: 5s<br/>...</pre>
<p>By declaring an evaluation interval in this second rule group, we are overriding the configuration set in the <kbd>global</kbd> section of <kbd>prometheus.yml</kbd>—the rules in this group will produce samples at the specified frequency. This was done for demonstration purposes only; setting different intervals is usually discouraged, for the same reasons as in scrape jobs: queries might produce erroneous results when using a series with different sampling rates, and having to <span>periodically</span> <span>keep track of what series have what becomes unmanageable.</span></p>
<p> </p>
<p>Prometheus provides a status page in the web <strong>user interface</strong> (<strong>UI</strong>) where a user can check the loaded rule groups along with their enclosed recording rules, their recording state, how long the last evaluation took for each, and how long ago they were run. You can find this page by going into <span class="packt_screen">Status | Rules</span> on the top bar:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6d4c3893-2aff-4b18-9bb2-5656f04eed6d.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.2: Prometheus web interface showing the /rules endpoint</div>
<p>With this information, we have now the fundamentals on how to create recording rules. Next, we'll explore the naming convention that the Prometheus community has agreed upon for recording rules.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Naming convention for recording rules</h1>
                </header>
            
            <article>
                
<p class="mce-root">Recording rule name validation abides by the same regular expression as metric names, and so rules can technically be named the same as any other metric. However, having clear standards when naming recording rules can make it easier to identify them among scraped metrics, know from which metrics they were derived, and understand what aggregations were applied to them.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span>The Prometheus community has gravitated toward a well-defined naming convention for recording rules. This is based on years of experience running Prometheus at scale. This enables all of the aforementioned advantages when used correctly.</span></p>
<p>The recommended convention for recording rule naming is composed of three sections, separated by colons, and takes the following form: <kbd>level:metric:operations</kbd>. The first section represents the aggregation level of the rule, which means that it will list the labels/dimensions that are present and relevant (usually separated by underscores); the second section is the metric name that was the basis for the rule; and the third section lists the aggregation operations that were applied to the metric.</p>
<p>The recording rules presented in this chapter all follow this convention, so let's have a look at the second rule group available in the test environment:</p>
<pre>- record: handler_instance:prometheus_http_request_duration_seconds_sum:rate5m<br/>  expr: &gt;<br/>    rate(prometheus_http_request_duration_seconds_sum[5m])<br/><br/>- record: handler_instance:prometheus_http_request_duration_seconds_count:rate5m<br/>  expr: &gt;<br/>    rate(prometheus_http_request_duration_seconds_count[5m])<br/><br/>- record: handler:prometheus_http_request_duration_seconds:mean5m<br/>  expr: &gt;<br/>    sum without (instance) (<br/>      handler_instance:prometheus_http_request_duration_seconds_sum:rate5m<br/>    )<br/>    /<br/>    sum without (instance) (<br/>      handler_instance:prometheus_http_request_duration_seconds_count:rate5m<br/>    )</pre>
<p>Looking at the naming of the first rule, we can easily understand that the rule is based on the <kbd>prometheus_http_request_duration_seconds_sum</kbd> metric; <kbd>rate5m</kbd> indicates that <kbd>rate()</kbd> is being applied to a range vector of five minutes, and the interesting labels present are <kbd>handler</kbd> and <kbd>instance</kbd>.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The same logic is applied to the second rule, but this time using the <kbd>prometheus_http_request_duration_seconds_count</kbd> metric. The third rule, however, is a bit more nuanced; as it is dividing <kbd>_sum</kbd> by the <kbd>_count</kbd> of latency events, it effectively represents the five-minute latency average of, in this case, HTTP requests served by Prometheus. As we aggregated the <kbd>instance</kbd> label away, the <kbd>level</kbd> section reflects this by only having <kbd>handler</kbd> as the relevant dimension. The last thing to note is that the metric name for this rule is now <kbd>prometheus_http_request_duration_seconds</kbd>, as it neither represents the sum nor the count, but it still makes it possible to clearly understand which metrics this rule is based on.</p>
<p>Naming recording rules can be a hard task, a balancing act between precisely representing all the factors at play and being concise enough for metric names to be manageable. When you find yourself in a situation where it isn't immediately clear how to name a recording rule given its expression, a good rule of thumb to follow is to make sure another person that is aware of this naming convention can tie the rule back to the metric used, what labels/dimensions should be present, and what transformations were applied.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up alerting in Prometheus</h1>
                </header>
            
            <article>
                
<p>So far, we have covered how PromQL can be invaluable in querying the collected data, but when we require an expression to be continuously evaluated so that an event is triggered when a defined condition is met, we're promptly stepping into alerting. We explained how alerting is one of the components of monitoring in <a href="4214ddff-8289-4dc6-b0ef-240510a22192.xhtml">Chapter 1</a>, <em>Monitoring Fundamentals</em>. To be clear, Prometheus is not responsible for issuing email, Slack, or any other forms of notification; that is the responsibility of another service. This service is typically Alertmanager, which we'll go over in <a href="db658650-14d2-4a7e-9ae0-1c003e63109c.xhtml">Chapter 11</a>, <em>Understanding and Extending Alertmanager</em>. Prometheus leverages the power of alerting rules to push alerts, which we'll be covering next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is an alerting rule?</h1>
                </header>
            
            <article>
                
<p>An alerting rule is much like a recording rule with some additional definitions; they can even share the same rule group without any issues. The biggest difference is that, when firing, they are sent to an external endpoint via an HTTP POST with a JSON payload for further processing. Expanding on the term active, in this context, we are talking about when the current state differs from the desired state, which boils down to when an expression returns one or more samples.</p>
<p class="mce-root"/>
<p>Alerting rules, such as recording rules, rely on PromQL expressions that are evaluated on a defined interval. This interval can be a globally configured one or can be local to the specific rule group. In each of the interval iterations, triggered alerts are validated to ensure that they're still active; if not, they're considered to be resolved.</p>
<p>For every sample returned by our expression, it will trigger an alert. This is important to keep in mind, because a relaxed PromQL expression can generate a deluge of alerts, so keep them as aggregated as possible.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring alerting rules</h1>
                </header>
            
            <article>
                
<p>To demonstrate how to create and understand alerting rules, we'll guide you through the entire process. This will touch not only on the main Prometheus configuration file, but also on rule files and how the web interface of the server behaves with regard to alerts.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Prometheus server configuration file</h1>
                </header>
            
            <article>
                
<p>Inside this chapter's test environment, we can find the following Prometheus configuration:</p>
<pre class="mce-root"><strong>vagrant@prometheus:~$ cat /etc/prometheus/prometheus.yml </strong><br/><strong>global:</strong><br/><strong>...</strong><br/><strong> evaluation_interval: 1m</strong><br/><strong>...</strong><br/><strong>rule_files:</strong><br/><strong> - "recording_rules.yml"</strong><br/><strong> - "alerting_rules.yml"</strong><br/><strong>alerting:</strong><br/><strong> alertmanagers:</strong><br/><strong> - static_configs:</strong><br/><strong> - targets:</strong><br/><strong> - “prometheus:5001”</strong><br/><strong>...</strong></pre>
<p>In this configuration, there are three components to be aware of:</p>
<ul>
<li><kbd>evaluation_interval</kbd>: This is responsible for defining the global evaluation interval for recording and alerting rules, which can be overridden at the rule group level using the <kbd>interval</kbd> keyword.</li>
<li><kbd>rule_files</kbd>: This is the file location where Prometheus can read the configured recording and/or alerting rules.</li>
<li><kbd>alerting</kbd>: This is the endpoint(s) where Prometheus sends alerts for further processing.</li>
</ul>
<p>In the alerting section, we've configured <kbd>“prometheus:5001”</kbd>. Behind this endpoint, there is nothing more than a small service, called <strong>alertdump</strong>, that is listening on port <kbd>5001</kbd> for the HTTP POST requests and is simply dumping their payload onto a log file. This will help dissect what Prometheus sends when an alert is firing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Rule file configuration</h1>
                </header>
            
            <article>
                
<p>Previously, we took a look at the Prometheus configuration file; we'll now move onto the provided alerting rules example, which we can see in the following snippet:</p>
<pre><strong>vagrant@prometheus:~$ cat /etc/prometheus/alerting_rules.yml </strong><br/><strong>groups:</strong><br/><strong>- name: alerting_rules</strong><br/><strong> rules:</strong><br/><strong> - alert: NodeExporterDown</strong><br/><strong> expr: up{job="node"} != 1</strong><br/><strong> for: 1m</strong><br/><strong> labels:</strong><br/><strong> severity: "critical"</strong><br/><strong> annotations:</strong><br/><strong> description: "Node exporter {{ $labels.instance }} is down."</strong><br/><strong> link: "https://example.com"</strong></pre>
<p>Let's look at the <kbd>NodeExporterDown</kbd> alert definition more closely. We can split the configuration into five distinct sections: <kbd>alert</kbd>, <kbd>expr</kbd>, <kbd>for</kbd>, <kbd>labels</kbd>, and <kbd>annotations</kbd>. We'll now go over each one of these in the next table:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Section</strong></p>
</td>
<td>
<p><strong>Description</strong></p>
</td>
<td>
<p><strong>Mandatory</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>alert</kbd></p>
</td>
<td>
<p>The alert name to use</p>
</td>
<td>
<p>Yes</p>
</td>
</tr>
<tr>
<td>
<p><kbd>expr</kbd></p>
</td>
<td>
<p>The PromQL expression to evaluate</p>
</td>
<td>
<p>Yes</p>
</td>
</tr>
<tr>
<td>
<p><kbd>for</kbd></p>
</td>
<td>
<p>The time to ensure that the alert is being triggered before sending the alert, defaults to 0</p>
</td>
<td>
<p>No</p>
</td>
</tr>
<tr>
<td>
<p><kbd>labels</kbd></p>
</td>
<td>
<p>User-defined key-value pairs</p>
</td>
<td>
<p>No</p>
</td>
</tr>
<tr>
<td>
<p><kbd>annotations</kbd></p>
</td>
<td>
<p>User-defined key-value pairs</p>
</td>
<td>
<p>No</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<div class="packt_tip">The Prometheus community typically uses CamelCase for alert naming.</div>
<div class="packt_infobox">Prometheus does not carry out validation to check whether an alert name is already in use, so it is possible for two or more alerts to share the same name but evaluate different expressions. This might cause issues, such as tracking which specific alert is triggering, or writing tests for alerts.</div>
<p>The <kbd>NodeExporterDown</kbd> rule will only trigger when the <kbd>up</kbd> metric with the <kbd>job=”node”</kbd> selector is not <kbd>1</kbd> for more than one minute, which we'll now test by stopping the Node Exporter service:</p>
<pre><strong>vagrant@prometheus:~$ sudo systemctl stop node-exporter</strong><br/><strong>vagrant@prometheus:~$ sudo systemctl status node-exporter</strong><br/><br/><strong>...</strong><br/><strong>Mar 05 20:49:40 prometheus systemd[1]: Stopping Node Exporter...</strong><br/><strong>Mar 05 20:49:40 prometheus systemd[1]: Stopped Node Exporter.</strong></pre>
<p>We're now forcing an alert to become active. This will force the alert to go through three different states:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Order</strong></p>
</td>
<td>
<p><strong>State</strong></p>
</td>
<td>
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>Inactive</p>
</td>
<td>
<p>Not yet pending or firing</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>Pending</p>
</td>
<td>
<p>Not yet active long enough to become firing</p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>Firing</p>
</td>
<td>
<p>Active <kbd>for</kbd> more than the defined <kbd>for</kbd> clause threshold</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Going to the <kbd>/alerts</kbd> endpoint on the Prometheus server web interface, we can visualize the three different states for the <kbd>NodeExporterDown</kbd> alert. First, the alert is inactive, as we can see in the following figure:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f1780ac8-5699-40ba-ac51-fe19d917fb04.png" style="width:23.67em;height:17.58em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.3: The NodeExporterDown alert is inactive</div>
<p>Then, we can see the alert in a pending state. This means that, while the alert condition has been triggered, Prometheus will continue to check whether that condition keeps being triggered for each evaluation cycle until the <kbd>for</kbd> duration has passed. The next figure illustrates the pending state; <span>notice that the</span> <strong>Show annotations</strong> <span>tick box is selected, which expands the alert annotations:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b30ea059-23e0-4025-95e2-19505112ae71.png" style="width:38.42em;height:34.25em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.4: The NodeExporterDown alert is pending</div>
<p>Finally, we can see the alert turn to firing. This means that the alert is active for more than the duration defined by the <kbd>for</kbd> clause – in this case, 1 minute:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0e0284b0-8f88-4c81-a54b-f19bb7618cb7.png" style="width:30.33em;height:26.83em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.5: The NodeExporterDown alert is firing</div>
<p>When an alert becomes firing, Prometheus sends a JSON payload to the configured alerting service endpoint, which, in our case, is the alertdump service, which is configured to log to the <kbd>/vagrant/cache/alerting.log</kbd> file. This makes it very easy to understand what kind of information is being sent and can be validated as follows:</p>
<pre><strong>vagrant@prometheus:~$ cat /vagrant/cache/alerting.log</strong></pre>
<pre>[<br/>   {<br/>       "labels": {<br/>           "alertname": "NodeExporterDown",<br/>           "dc": "dc1",<br/>           "instance": "prometheus:9100",<br/>           "job": "node",<br/>           "prom": "prom1",<br/>           "severity": "critical"<br/>       },<br/>       "annotations": {<br/>           "description": "Node exporter prometheus:9100 is down.",<br/>           "link": "https://example.com"<br/>       },<br/>       "startsAt": "2019-03-04T21:51:15.04754979Z",<br/>       "endsAt": "2019-03-04T21:58:15.04754979Z",<br/>       "generatorURL": "http://prometheus:9090/graph?g0.expr=up%7Bjob%3D%22node%22%7D+%21%3D+1&amp;g0.tab=1"<br/>   }<br/>]</pre>
<p>Now that we've seen how to configure some alerting rules and validated what Prometheus is sending to the configured alerting system, let's explore how to enrich those alerts with contextual information by using labels and annotations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Labels and annotations</h1>
                </header>
            
            <article>
                
<p>In the alert rule definition, there were two optional sections: labels and annotations. Labels define the identity of an alert and they can change according to the evaluation cycle they're in; if they do this, it will alter the alert identity. To demonstrate this point, we'll introduce the <kbd>ALERTS</kbd> metric, which tracks all active alerts and their labels. As we can see in the following figure, we have a label called <kbd>alertstate</kbd>, which tracks the alert state and transitions from <kbd>pending</kbd> to <kbd>firing</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bb8a64c2-1844-4c5d-8e47-a29cba583986.png" style="width:32.75em;height:17.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.6: The ALERTS metric</div>
<p class="mce-root"/>
<p>Something to keep in mind is the issue of using a sample value in a label. Although it is technically possible, it's also a very bad idea. Doing so will change the alert identity every time the value changes, and, as such, will always be restarting the defined <kbd>for</kbd> countdown, resulting in an alert that will never enter the <kbd>firing</kbd> state.</p>
<p>On the other hand, annotations do not belong to the identity of an alert, and, as such, are not stored in the <kbd>ALERTS</kbd> metric. These are useful to enrich the alert with more context and information. Annotations are also templated using the Go templating language, as we saw in our example. By using the <kbd>{{ .Labels.instance }}</kbd> <span>template syntax,</span> we are accessing the available alert labels, selecting the <kbd>instance</kbd> label, and using its value in the annotation <kbd>description</kbd> field. The value of the firing sample is available by using <kbd>{{ .Value }}</kbd>, if required.</p>
<div class="packt_tip">The Golang template <kbd>.Labels</kbd> and <kbd>.Value</kbd> <span>variables</span> are also available as <kbd>$labels</kbd> and <kbd>$value</kbd> for convenience.</div>
<p><span>The following snippet shows the alert rule in our example</span>:</p>
<pre>   annotations:<br/>     description: "Node exporter {{ .Labels.instance }} is down."<br/>     link: "https://example.com"</pre>
<p>This will produce the following rendered result when <kbd>firing</kbd>:</p>
<pre>       "annotations": {<br/>           "description": "Node exporter prometheus:9100 is down.",<br/>           "link": "https://example.com"<br/>       },</pre>
<div class="packt_tip">You can find more information regarding Golang templating at <a href="https://golang.org/pkg/text/template/">https://golang.org/pkg/text/template/</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Delays on alerting</h1>
                </header>
            
            <article>
                
<p>In the previous topics, we talked about the three states that an alert goes through; but there's more to it when calculating the total time required for an alert to become firing. First, there's the scrape interval (which, in our example, is 30 seconds, although generally the scrape and evaluation intervals should be the same for clarity), we then have the rule evaluation interval (in our case, it was globally defined as 1 minute), and, finally, there's the 1 minute defined in the alert rule's <kbd>for</kbd> clause. If we put all of these variables together, the time for this alert to be considered as <kbd>firing</kbd> can take up to 2 minutes and 30 seconds in the worst-case scenario. The next figure illustrates this example situation:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8973ea05-760e-4131-8a83-38b670f8c308.png" style="width:40.17em;height:34.17em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.7: Alert delay visualized</div>
<p>All these delays are just on the Prometheus side. The external service processing the alert sent may have other constraints, which can make the global delay until a notification is sent even longer.</p>
<div class="packt_infobox">Before Prometheus 2.4.0, the <kbd>pending</kbd> and <kbd>firing</kbd> states were not persistent across restarts, which could extend the delay for alerting even further. This was solved by implementing a new metric, called <kbd>ALERTS_FOR_STATE</kbd>, which stores the alert states. You can find the release notes for Prometheus 2.4.0 at <a href="https://github.com/prometheus/prometheus/releases/tag/v2.4.0">https://github.com/prometheus/prometheus/releases/tag/v2.4.0</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing your rules</h1>
                </header>
            
            <article>
                
<p>In <a href="19357d8c-dfcf-4497-ae80-4761f6633d14.xhtml">Chapter 8</a>, <em>Troubleshooting and Validation</em>, we went through the features that <kbd>promtool</kbd> has to offer, with the exception of testing. The <kbd>test rules</kbd> subcommand can simulate the periodic ingestion of samples for several time series, use those series to evaluate recording and alerting rules, and then test whether the recorded series match what was configured as the expected results. Now that we understand recording and alerting rules, we'll look at how to ensure that they behave as expected, by creating unit tests and using <kbd>promtool</kbd> to validate our rules.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recording rules tests</h1>
                </header>
            
            <article>
                
<p>The <kbd>promtool</kbd> tool included in the Prometheus binary distribution allows us to define test cases to validate that the rules we write behave as expected. The test environment for this chapter also comes with a suite of pre-built tests for the rules we've explored so far. You can have a look at the configuration here:</p>
<pre><strong>vagrant@prometheus:~$ cat /etc/prometheus/tests.yml</strong></pre>
<p>This file has tests for all recording and alerting rules presented in this chapter. Although you don't need to define every test in a single file (it is, in fact, tidier to do a test file per rule group to keep things organized), this was done in this case for simplicity. For now, let's analyze only the recording rules, as they are simpler to grasp. The top-level configuration keys for the test file defines which rule files to load and the default evaluation interval for the tests, which governs the periodicity of recording and alerting rule evaluation when they don't explicitly state their own:</p>
<pre>rule_files:<br/>  - /etc/prometheus/recording_rules.yml<br/>...<br/>evaluation_interval: 1m</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<div class="packt_infobox">While the <kbd>rule_files</kbd> configuration key in the test files might look the same as in the main Prometheus configuration file, it does not support globing (using filename wildcards).</div>
<p>Following these global configurations comes the definition of the test cases, under the <kbd>tests</kbd> key. You can define multiple test packs, each with their own simulated scrape intervals, collected series, and rules under test. Let's have a look at the first test defined in the file, to which we added some comment to make it easier to understand:</p>
<pre>tests:</pre>
<p><span><kbd>interval</kbd> sets the time that interval samples are generated in our simulated time series:</span></p>
<pre>- interval: 15s</pre>
<p>The list of <kbd>input_series</kbd> define what time series to generate and what values to produce in each iteration of the simulated collection interval:</p>
<pre>    input_series:<br/>      - series: 'node_cpu_seconds_total{cpu="0",instance="prometheus:9100",job="node",mode="user"}'<br/>        values: '1 1 1 1 1 1 1 1 1 1'<br/>      - series: 'node_cpu_seconds_total{cpu="1",instance="prometheus:9100",job="node",mode="user"}'<br/>        values: '1 1 1 1 1 1 1 1 1 1'<br/>      - series: 'node_cpu_seconds_total{cpu="0",instance="example:9100",job="node",mode="idle"}'<br/>        values: '1 1 1 1 1 1 1 1 1 1'<br/>      - series: 'node_cpu_seconds_total{cpu="0",instance="example:9100",job="node",mode="system"}'<br/>        values: '1 1 1 1 1 1 1 1 1 1'</pre>
<p><span>The list of PromQL expressions to test is defined as <kbd>promql_expr_test</kbd>:</span></p>
<pre>promql_expr_test:</pre>
<p>Each <kbd>expr</kbd> defines a particular expression:</p>
<pre>- expr: instance:node_cpu:count</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The point in time at which this expression will be run is set by setting <kbd>eval_time</kbd>, and the expected samples should be returned by running that expression as <kbd>exp_samples</kbd>:</p>
<pre>        eval_time: 1m<br/>        exp_samples:<br/>          - labels: 'instance:node_cpu:count{instance="prometheus:9100", job="node"}'<br/>            value: 2<br/>          - labels: 'instance:node_cpu:count{instance="example:9100", job="node"}'<br/>            value: 1</pre>
<p>In this test pack, we can see four time series being generated every 15 seconds for the same metric, <kbd>node_cpu_seconds_total</kbd>. As the actual value of these series isn't relevant for this recording rule (it only counts the number of CPUs per instance), a value of <kbd>1</kbd> was set for every sample. Do note the variation of labels present, namely that the <kbd>prometheus:9100</kbd> instance is reporting metrics for two CPUs and <kbd>example:9100</kbd> for one. The actual test is just validating that, when the <kbd>instance:node_cpu:count</kbd> expression is evaluated at <kbd>t=1m</kbd> (as if 1 minute had passed after the generated collection started), the returned samples should show the correct count of CPUs for each instance.</p>
<p>We are now ready to execute the tests using the following instruction:</p>
<pre><strong>vagrant@prometheus:/etc/prometheus$ promtool test rules tests.yml </strong><br/><strong>Unit Testing: tests.yml</strong><br/><strong>  SUCCESS</strong></pre>
<p class="mce-root">This ensures that the recording rule configured behaves the way we were expecting. You can try breaking the test by removing one of the input series from the <kbd>prometheus:9100</kbd> instance in the <kbd>instance:node_cpu:count</kbd> test pack. When you run the tests again, the following will be displayed, as one of the tests is now failing:</p>
<pre>vagrant@prometheus:/etc/prometheus$ promtool test rules tests.yml <br/>Unit Testing: tests.yml<br/>  FAILED:<br/>    expr:'instance:node_cpu:count', time:1m0s, <br/>        exp:"{__name__=\"instance:node_cpu:count\", instance=\"example:9100\", job=\"node\"} 1E+00, {__name__=\"instance:node_cpu:count\", instance=\"example:9100\", job=\"node\"} 1E+00, {__name__=\"instance:node_cpu:count\", instance=\"<strong>prometheus:9100</strong>\", job=\"node\"} <strong>2E+00</strong>", <br/>        got:"{__name__=\"instance:node_cpu:count\", instance=\"example:9100\", job=\"node\"} 1E+00, {__name__=\"instance:node_cpu:count\", instance=\"example:9100\", job=\"node\"} 1E+00, {__name__=\"instance:node_cpu:count\", instance=\"<strong>prometheus:9100</strong>\", job=\"node\"} <strong>1E+00</strong>"</pre>
<p class="mce-root"/>
<p>What this output tells us is that <kbd>promtool</kbd> was expecting the defined set of samples, but a different set of samples was returned. You can see that, just as we configured, the recording rule now only reports one CPU for the <kbd>prometheus:9100</kbd> instance. This gives us confidence that the rule is behaving exactly as we wanted.</p>
<p>The tests for the second recording rule group are mostly the same, but they showcase a powerful notation for generating a richer input series:</p>
<pre>  - interval: 5s<br/>    input_series:<br/>      - series: 'prometheus_http_request_duration_seconds_count{handler="/",instance="localhost:9090",job="prometheus"}'<br/>        values: '0+5x60'<br/>      - series: 'prometheus_http_request_duration_seconds_sum{handler="/",instance="localhost:9090",job="prometheus"}'<br/>        values: '0+1x60'</pre>
<p>This is called an <strong>expanding notation</strong>. This is a compact way of declaring a formula for the generation of time series values over time. It takes the form of either <kbd>A+BxC</kbd> or <kbd>A-BxC</kbd>, where <kbd>A</kbd> is the starting value, <kbd>B</kbd> is the amount of increase (when preceded by <kbd>+</kbd>) or decrease (when preceded by <kbd>-</kbd>) the series value should have in each iteration, and <kbd>C</kbd> is how many iterations this increase or decrease should be applied for.</p>
<p>Coming back to our example, <kbd>0+5x60</kbd> will expand to the following series:</p>
<pre>0 5 10 15 20 … 290 295 300</pre>
<p>You can mix and match literal values with expanding notation when declaring the values for an input time series. This allows you to create complex behavior with ease. Take the following example:</p>
<pre>0 1 1 0 1+0x3 0 1 1 0 1 1 0 0 0 1 1 0+0x3 1</pre>
<p>This will be expanded into the following:</p>
<pre>0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 0 0 0 1</pre>
<p>Testing is fundamental to avoid unforeseen problems and, with the information covered so far, you're now able to generate your own unit tests for recording rules. Next, we'll continue tackling unit tests, but this time specifically related to alerting rules.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Alerting rules tests</h1>
                </header>
            
            <article>
                
<p>Unit tests for alerting rules are quite similar to the ones used for recording rules. We'll be using the example alert provided earlier in this chapter to perform a walkthrough on how to configure alerting tests and how to validate them. As mentioned before, the test environment for this chapter comes with a suite of tests for the rules presented here, including the alert rule we're interested in. Once again, you can have a look at the test file using the following command:</p>
<pre><strong>vagrant@prometheus:~$ cat /etc/prometheus/tests.yml</strong></pre>
<p>Focusing solely on the alerting component, we can see that we first define where the alerting rules are located:</p>
<pre>rule_files:<br/>  - /etc/prometheus/alerting_rules.yml</pre>
<p>The default rule evaluation interval is shared between recording and alerting rules in the same file:</p>
<pre>evaluation_interval: 1m</pre>
<p>The alerting test is conveniently in its own test group, so let's have a look at its full definition:</p>
<pre>  - interval: 1m<br/>    input_series:<br/>      - series: 'up{job="node",instance="prometheus:9100"}'<br/>        values: '1 1 1 0 0 0 0 0 0 0'<br/>      - series: 'up{job="prometheus",instance="prometheus:9090"}'<br/>        values: '1 0 1 1 1 0 1 1 1 1'</pre>
<p><span>The test group definition is the same as explained previously, with the exception of the</span> <kbd>alert_rule_test</kbd> <span>section, which is where we define alerting tests. A thing to note in this example is that the second input series should never be picked up by our testing rule, as the defined alert is specifically matching</span> <kbd>job="node"</kbd><span>:</span></p>
<pre>    alert_rule_test:<br/>      - alertname: NodeExporterDown<br/>        eval_time: 3m<br/>      - alertname: NodeExporterDown<br/>        eval_time: 4m<br/>        exp_alerts:<br/>          - exp_labels:<br/>              instance: "prometheus:9100"<br/>              job: "node"<br/>              severity: "critical"<br/>            exp_annotations:<br/>              description: "Node exporter prometheus:9100 is down."<br/>              link: "https://example.com"</pre>
<div class="packt_tip">It's not mandatory to have <kbd>alert_rule_test</kbd> and <kbd>promql_expr_test</kbd> in separate test blocks; you may have both in the same test group when you have recording and alerting rules using the same input time series and with the same evaluation interval.</div>
<p>The <kbd>alert_rule_test</kbd> section lists what alerts should be evaluated (<kbd>alertname</kbd>) at what time relative to the simulated start of the test run (<kbd>eval_time</kbd>). If the alert is expected to be firing at that time, an additional <kbd>exp_alerts</kbd> section should be defined listing what set of expected labels (<kbd>exp_labels</kbd>) and annotations (<kbd>exp_annotations</kbd>) should be present for each instance of the alert. Leaving the <kbd>exp_alerts</kbd> section empty means that the alert is not expected to be firing at the given time.</p>
<p>The first alerting test will be executed at the third minute, and, as the matching series we provided previously returns the value <kbd>1</kbd> at that moment, the alert expression defined at <kbd>alerting_rules.yml</kbd> will not trigger – this means that no data is returned by the expression defined in the alert.</p>
<p>The second alerting rule will be executed at <span>the fourth minute</span> <span>and will return data, as the matching series we provided has the sample value</span> <kbd>0</kbd> <span>at that specific moment. All the labels returned by the alerting rule needs to be explicitly checked. The test must also check all the descriptions returned by the alert, with any templated variable fully expanded.</span></p>
<p>We can now run the test using the following instruction:</p>
<pre><strong>vagrant@prometheus:~$ promtool test rules /etc/prometheus/tests.yml </strong><br/><strong>Unit Testing: /etc/prometheus/tests.yml</strong><br/><strong>  SUCCESS</strong></pre>
<p>As an extra step, try changing the description of the second alert from <kbd>prometheus:9100</kbd> to something like <kbd>prometheus:9999</kbd> and run the test again. You should get the following output:</p>
<pre>vagrant@prometheus:~$ promtool test rules /etc/prometheus/tests.yml <br/>Unit Testing: /etc/prometheus/tests.yml<br/>  FAILED:<br/>    alertname:NodeExporterDown, time:4m0s, <br/>        exp:"[Labels:{alertname=\"NodeExporterDown\", instance=\"prometheus:9100\", job=\"node\", severity=\"critical\"} Annotations:{description=\"Node exporter <strong>prometheus:9999</strong> is down.\", link=\"https://example.com\"}]", <br/>        got:"[Labels:{alertname=\"NodeExporterDown\", instance=\"prometheus:9100\", job=\"node\", severity=\"critical\"} Annotations:{description=\"Node exporter <strong>prometheus:9100</strong> is down.\", link=\"https://example.com\"}]"</pre>
<p>While this alert is very simple and easy for determining in which conditions it will fire, tests for alerting rules provide you with the assurance that alerts will trigger when conditions that you can't reasonably reproduce in your environment happen.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we had the opportunity to observe a different way to produce a derivative time series. Recording rules help improve monitoring system stability and performance when recurrent heavy queries are required by pre-computing them into new time series that are comparatively cheap to consult. Alerting rules bring the power and flexibility of PromQL to alerts; they enable triggering alerts for complex and dynamic thresholds as well as targeting multiple instances or even different applications using a single alert rule. Having a good grasp on how delays are introduced in alerts will now help you tailor them to your needs, but remember, a little delay is better than noisy alerts. Finally, we explored how to create unit tests for our rules and validate them even before a Prometheus server is running.</p>
<p>The next chapter will step into another component of monitoring: visualization. We'll be diving into Grafana, the community-preferred choice for Prometheus-powered dashboards.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li style="font-weight: 400">What are the primary uses for recording rules?</li>
<li style="font-weight: 400">Why should you avoid setting different evaluation intervals in rule groups?</li>
<li style="font-weight: 400">If you were presented with the <kbd>instance_job:latency_seconds_bucket:rate30s</kbd> metric, what labels would you expect to find and what would be the expression used to record it?</li>
<li style="font-weight: 400">Why is using the sample value of an alert in the alert labels a bad idea?</li>
<li style="font-weight: 400">What is the pending state of an alert?</li>
<li style="font-weight: 400">How long would an alert wait between being triggered and transitioning to the <kbd>firing</kbd> state when the <kbd>for</kbd> clause is not specified?</li>
<li style="font-weight: 400">How can you test your rules without using Prometheus?</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root"><strong>Prometheus recording rules</strong>: <a href="https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/">https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/</a></li>
<li class="mce-root"><strong>Rule naming best practices</strong>: <a href="https://prometheus.io/docs/practices/rules/">https://prometheus.io/docs/practices/rules/</a></li>
<li class="mce-root"><strong>Prometheus alerting rules</strong>: <a href="https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/">https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/</a></li>
<li class="mce-root"><strong>Prometheus unit testing</strong>: <a href="https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/">https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/</a></li>
</ul>


            </article>

            
        </section>
    </body></html>