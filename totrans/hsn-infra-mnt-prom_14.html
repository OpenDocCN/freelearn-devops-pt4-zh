<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Understanding and Extending Alertmanager</h1>
                </header>
            
            <article>
                
<p class="mce-root">Alerting is a critical component in any monitoring stack. In the Prometheus ecosystem, alerts and their subsequent notifications are decoupled. Alertmanager is the component that handles these alerts. In this chapter, we'll be focusing on converting alerts into useful notifications using Alertmanager. From reliability to customization, we'll delve into the inner workings of the Alertmanager service, providing the required knowledge to configure, troubleshoot, and customize all the options available. We'll make sure that concepts such as alert routing, silencing, and inhibition are clear so that you can decide how to implement them in your own stack.</p>
<p class="mce-root">Since <span>Alertmanager is </span><span>a critical component, high availability will also be explored, and we will also explain the relationship between Prometheus and Alertmanager. We will customize notifications and learn how to build and use reusable templates so that notifications are appropriate and carry accurate information when they reach their destination. We will finish this chapter by learning how to monitor the monitoring system and, more importantly, learning how to be alerted when the system is partially or completely down.</span></p>
<p>We will explore the following topics in this chapter:</p>
<ul>
<li>Test environment for this chapter.</li>
<li>Alertmanager fundamentals.</li>
<li>Alertmanager configuration.</li>
<li>Common Alertmanager notification integrations.</li>
<li>Customizing your alert notifications.</li>
<li>Who watches the Watchmen?</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the test environment</h1>
                </header>
            
            <article>
                
<p>To work with Alertmanager, we'll be three new instances to simulate a highly available setup. This approach will allow us to not only expose the required configurations, but also validate how everything works together.</p>
<p>The setup we'll be using resembles the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d40bc98a-eaf8-4512-b628-306b0872f0f0.png" style="width:30.67em;height:14.92em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.1: Test environment</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment</h1>
                </header>
            
            <article>
                
<p>Let's begin by deploying the Alertmanager test environment:</p>
<ol>
<li>To launch a new test environment, move into this chapter's path, relative to the repository root:</li>
</ol>
<pre style="padding-left: 60px"><strong>cd ./chapter11/</strong></pre>
<ol start="2">
<li>Ensure that no other test environments are running and spin up this chapter's environment:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant global-status</strong><br/><strong>vagrant up</strong></pre>
<ol start="3">
<li>You can validate the successful deployment of the test environment using the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant status</strong></pre>
<p style="padding-left: 60px">You will receive the following output:</p>
<pre style="padding-left: 60px"><strong>Current machine states:</strong><br/><br/><strong>prometheus running (virtualbox)</strong><br/><strong>alertmanager01 running (virtualbox)</strong><br/><strong>alertmanager02 running (virtualbox)</strong><br/><strong>alertmanager03 running (virtualbox)</strong><br/><br/><strong>This environment represents multiple VMs. The VMs are all listed</strong><br/><strong>above with their current state. For more information about a specific</strong><br/><strong>VM, run `vagrant status NAME`.</strong></pre>
<p>When the deployment tasks end, you'll be able to validate the following endpoints on your host machine using your favorite JavaScript-enabled web browser:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Service</strong></p>
</td>
<td>
<p><strong>Endpoint</strong></p>
</td>
</tr>
<tr>
<td>
<p>Prometheus</p>
</td>
<td>
<p><kbd>http://192.168.42.10:9090</kbd></p>
</td>
</tr>
<tr>
<td>
<p>Alertmanager01</p>
</td>
<td>
<p><kbd>http://192.168.42.11:9093</kbd></p>
</td>
</tr>
<tr>
<td>
<p>Alertmanager02</p>
</td>
<td>
<p><kbd>http://192.168.42.12:9093</kbd></p>
</td>
</tr>
<tr>
<td>
<p>Alertmanager03</p>
</td>
<td>
<p><kbd>http://192.168.42.13:9093</kbd></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>You should be able to access the desired instance by using one of the following commands:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Instance</strong></p>
</td>
<td>
<p><strong>Command</strong></p>
</td>
</tr>
<tr>
<td>
<p>Prometheus</p>
</td>
<td>
<p><kbd>vagrant ssh prometheus</kbd></p>
</td>
</tr>
<tr>
<td>
<p><span>Alertmanager01</span></p>
</td>
<td>
<p><kbd>vagrant ssh alertmanager01</kbd></p>
</td>
</tr>
<tr>
<td>
<p><span>Alertmanager02</span></p>
</td>
<td>
<p><kbd>vagrant ssh alertmanager02</kbd></p>
</td>
</tr>
<tr>
<td>
<p><span>Alertmanager03</span></p>
</td>
<td>
<p><kbd>vagrant ssh alertmanager03</kbd></p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cleanup</h1>
                </header>
            
            <article>
                
<p>When you've finished testing, just make sure you're inside <kbd>./chapter11/</kbd> and execute the following:</p>
<pre><strong>vagrant destroy -f</strong></pre>
<p>Don't worry too much <span>–</span> you can easily spin up the environment again if you need to.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Alertmanager fundamentals</h1>
                </header>
            
            <article>
                
<p>We covered how alerting rules work in Prometheus in <a href="9aa1e3da-13cf-4051-845d-1d1c924ef47b.xhtml">Chapter 9</a>, <em>Defining Alerting and Recording Rules</em>, but those, by themselves, aren't all that useful. As we mentioned previously, Prometheus delegates notification handling and routing to external systems through a Webhook-style HTTP interface. This is where Alertmanager comes in.</p>
<p class="mce-root">Alertmanager is responsible for accepting the alerts generated from Prometheus alerting rules and converting them into notifications. The latter can take any form, such as email messages, chat messages, pages, or even Webhooks that will then trigger custom actions, such as logging alerts to a data store or creating/updating tickets. Alertmanager is also the only component in the official stack that distributes its state across instances so that it can keep track of things such as which alerts were already sent and which ones are silenced.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The notification pipeline</h1>
                </header>
            
            <article>
                
<p>The following diagram, inspired by the architecture diagram of Alertmanager, provides an overview of the steps an alert goes through until it's successfully sent as a notification:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/18b36137-36bb-4725-9259-1abbdfc5528b.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.2: Notification pipeline overview</div>
<p>The preceding diagram has a lot to unpack, so we're going to go through each of these steps in the next few sections. Knowing how the alert pipeline works will help you understand the various configuration options, how to troubleshoot missing alerts, and generally take full advantage of everything Alertmanager has to offer.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dispatching alert groups to the notification pipeline</h1>
                </header>
            
            <article>
                
<p>Whenever an alerting rule is triggered, Prometheus will send an alert in the form of a JSON payload to the Alertmanager API, and it will keep sending updates at each evaluation interval of that rule or every minute (configurable through the <kbd>--rules.alert.resend-delay</kbd> flag), whichever is longer. When alerts are received by Alertmanager, they go through the dispatching step, where they will be grouped using one or more of the alert labels, such as <kbd>alertname</kbd>. We're going to discuss more about this in the <em>Alertmanager configuration</em> section, later in this chapter. This allows you to sort alerts into categories, which can reduce the number of notifications that are sent as multiple alerts in the same category and are grouped together in a single notification, which will then trigger the notification pipeline:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f2fb4c43-294d-41ac-85cf-6457f1cef663.png" style="width:42.67em;height:28.33em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.3: Alertmanager interface grouping alerts by alertname</div>
<p>When running multiple Prometheus instances with the same configuration (a common setup when pursuing high availability/redundancy), alerting rules for the same condition won't necessarily trigger at the exact same time. Alertmanager accounts for this situation by having a configurable time interval. It will wait before doing anything else so that similar alerts can be grouped together and thus avoid sending multiple notifications for a single type of problem.</p>
<p class="mce-root">This grouping is done in parallel across all the user-specified criteria. Each group will then trigger the notification pipeline, which we'll cover next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Inhibition</h1>
                </header>
            
            <article>
                
<p>A good example to help us understand what alert inhibition is is imagining a server rack and what happens if the top-of-rack switch fails. In this scenario, all the servers and services in that rack will start triggering alerts because they suddenly became unreachable. To avoid this problem, we can use the alert for the top-of-rack switch which, if triggered, will prevent the notifications for all the other alerts in that rack from going out. This helps the operator become more focused on the real problem and avoid a flood of unactionable alerts.</p>
<p class="mce-root">So, in a nutshell, inhibition allows you to map dependencies between alerts, and therefore prevents notifications for dependent alerts from going any further in the notification pipeline. This is set up in the Alertmanager configuration file, which means inhibitions require a service reload if changed.</p>
<p class="mce-root">If the alert is not matched in the inhibition phase, it will then step into the silencer step.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Silencing</h1>
                </header>
            
            <article>
                
<p>Silencing is a common concept in monitoring/alerting systems; it is how you can avoid alert notifications from going out in a time-capped way. It is often used to disable notifications during maintenance windows, or to temporarily suppress alerts of lower importance during incidents. Alertmanager takes this concept and supercharges it by taking advantage of the fact that alerts coming in usually have one or more differentiating labels: ones from the originating alerting rule expression, the alertname, the alert's label fields, from <kbd><span>alert_relabel_configs</span></kbd>, as well as the ones from the Prometheus <kbd>external_labels</kbd>. This means that any one of these labels (or a combination of them) can be used to temporarily disable notifications through either direct matching or through regular expression matching:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/34d02fda-3fe3-4365-a54a-c946cecab22e.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.4: Creating a silence matching alertname=NodeExporterDown</div>
<div class="packt_tip">You should be careful with regex matching as you can accidentally silence more than you expect. The Alertmanager web UI can help prevent this as it shows a preview of which firing alerts will be suppressed when creating new silences.</div>
<p>Silences are defined at runtime. They can be set using the Alertmanager web interface, <kbd>amtool</kbd> (the Alertmanager command-line interface, which is going to be presented shortly), or directly through the API. They can be set while an alert is firing, such as during an incident, or in advance so that planned maintenance doesn't spam the people doing on-call. It is not supposed to be a permanent solution for a firing alert, only a temporary measure; this is why creating a silence requires that you set an expiration date for it, and why the web UI only recognizes durations up to days.</p>
<p>Since the silencing step comes after inhibition, if you silence an alert that is triggering inhibition rules, it will continue to inhibit other alerts.</p>
<p>If the alert didn't match any of the silences, it will go through to the next step in the notification pipeline, which is the routing phase.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Routing</h1>
                </header>
            
            <article>
                
<p>When an alert batch reaches this phase, Alertmanager needs to decide where to send it. Since the most common use cases are to have different people interested in different alerts, different notification methods for different alert severities, or even a combination of both, this step enables just that by way of a routing tree. It is composed of routes (if any), each of which specifies a match criteria for one or more labels and a receiver, and a root node, which defines a catch-all receiver in case none of the sub-routes have a match for the alert groups passing through. Sub-routes can have their own routes, making it a multi-level tree. Matching is done in the order the routes are declared in, going into defined sub-routes first when a route matches, and the highest depth match will define which receiver will be used. This will become clearer when we put it in action using the Alertmanager configuration.</p>
<p>Receivers and notifiers work similar in concept to address book contacts. Receivers are named contacts that can have one or more notifiers, which are like contact information. Alertmanager supports a lot of different notifiers that generally fall into one of the following categories: email, chat (Slack, WeChat, HipChat), and page (PagerDuty, Opsgenie, VictorOps, Pushover). Additionally, it also supports the Webhook notifier, which is a generic integration point that can be used to support every other notification system that is not built into Alertmanager.</p>
<p>After this routing stage connects an alert batch with a receiver, Alertmanager will then run a notification job for each notifier specified in that receiver. This job takes care of deduplicating, sending, and retrying notifications. For deduplication, it first checks the notification log (which will be discussed later in this chapter) to make sure that this particular notification hasn't been sent yet; if is already there, no further action is taken. Next, it will try to send the notification and, if it succeeds, that will be recorded in the notification log. If a notification fails to go through (for example, API error, connection timeout, and so on), the job will try again.</p>
<p>Now that we know the basics of the notification pipeline, let's have a look at what happens when there are several Alertmanager instances and how the alert state is shared among them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Alertmanager clustering</h1>
                </header>
            
            <article>
                
<p>The overview of the notification pipeline does not touch on the high availability component of Alertmanager. The way high availability is achieved is by relying on gossip (based on the HashiCorp memberlist, <a href="https://github.com/hashicorp/memberlist">https://github.com/hashicorp/memberlist</a>), instead of using a consensus-based protocol; this means there's no real reason for choosing an odd number of instances in a cluster. Using gossip, the cluster shares the <strong>notification log</strong> (<strong>nflog</strong>) between all Alertmanager instances, which in turn will be aware of the collective state of the cluster regarding notifications. In the case of a network partition, there will be notifications being sent from each side of the partition, since logically it's better to receive more notifications than failing to notify altogether.</p>
<p>As we now know, inhibition is set at the configuration file level, so it should be the same across all Alertmanager instances. However, silences also need to be gossiped across the cluster as they are set at runtime on a single Alertmanager instance. This is a good way to validate if the clustering is working as expected <span>– </span>confirming whether the configured silences show up in all instances.</p>
<p>The Alertmanager <kbd>/#/status</kbd> page shows the status of the gossip cluster, along with the known peers. You can check out this endpoint in our test environment by opening up, for example, <kbd>http://192.168.42.11:9093/#/status</kbd>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fc9b2956-1d3e-4687-ae5a-a39c2d29aaa8.png" style="width:27.42em;height:25.25em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.5: Alertmanager cluster status</div>
<p>The way clustering works in Alertmanager is like so: every Prometheus instance sends alerts to all the Alertmanager instances they know about. Those instances, assuming they are all in the same HA cluster, order themselves, and the one that becomes the first will handle alert notifications. That instance will distribute the notification log through gossip, which will list the notifications that were successfully sent. Each of the remaining Alertmanager instances will have an increasing amount of delay, according to their respective position in the ordering, to wait for the notification log updates. Alerts in the notification log will not be sent again by these instances <span>–</span> if the notification log does not state that a given notification was taken care of by the time the gossip delay is done, then the second Alertmanager will take care of it, and so on:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/484c081e-4d9e-488e-9a22-7a584ff688b8.png" style="width:16.00em;height:22.83em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.6: Alertmanager clustering overview</div>
<div class="packt_tip">Prometheus instances talk directly to all Alertmanager instances, since the cluster members will take care of deduplication between themselves. This means that no load balancers should be placed between Prometheus and Alertmanager.</div>
<p>Alertmanager clustering assumes that every instance is running with the same configuration file. However, failing to do so should only impact its ability to deduplicate notifications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Alertmanager configuration</h1>
                </header>
            
            <article>
                
<p>In <a href="9aa1e3da-13cf-4051-845d-1d1c924ef47b.xhtml">Chapter 9</a>,<em> Defining Alerting and Recording Rules</em>, we discussed how Prometheus generates and pushes out alerts. Having also made clear the distinction between an alert and a notification, it's now time to use Alertmanager to handle the alerts that are sent by Prometheus and turn them into notifications.</p>
<p>Next, we'll go through the configuration required on Prometheus, along with the configuration options available in Alertmanager, so that we have notifications going out from our monitoring stack.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Prometheus configuration</h1>
                </header>
            
            <article>
                
<p>There are a couple of configurations that need to be done in Prometheus so that we can start using Alertmanager. The first thing to do is configure the external labels, which are labels that are added to time series data (if it doesn't already have them) when communicating with external systems, including but not limited to Alertmanager. These are labels that uniquely identify the origin of the metrics, such as <kbd>region</kbd>, <kbd>datacenter</kbd>, or <kbd>environment</kbd>. As a rule of thumb, if you feel tempted to add the same label name/value to every single scrape and recording rule, that label would probably make more sense as an external label as it introduces no dimension to metrics locally in your Prometheus instance, but would most likely be useful in higher-level systems (such as with federation or long-term metric storage), as we will also see in the next few chapters. As we'll see in the following example, external labels are configured inside the top-level <kbd>global</kbd> key in the Prometheus main configuration file.</p>
<p>The second thing to do is configure Prometheus so that it can send alerts to Alertmanager. As we discussed previously, in the <em>Alertmanager clustering</em> section, Prometheus instances are required so that you can find out about and send alerts to all the Alertmanager cluster members individually. This configuration is set on the Prometheus configuration file in a top-level section called <kbd>alerting</kbd>. An example of this configuration can be found in our test environment, as follows:</p>
<pre><strong>vagrant@prometheus:~$ cat /etc/prometheus/prometheus.yml</strong><br/><strong>global:</strong><br/><strong>  external_labels:</strong><br/><strong>    dc: dc1</strong><br/><strong>alerting:</strong><br/><strong>  alertmanagers:</strong><br/><strong>  - static_configs:</strong><br/><strong>    - targets:</strong><br/><strong>      - alertmanager01:9093</strong><br/><strong>      - alertmanager02:9093</strong><br/><strong>      - alertmanager03:9093</strong><br/><strong>...</strong></pre>
<p>In this <kbd>alerting</kbd> section, we can also use <kbd>alert_relabel_configs</kbd>, which has the same configuration syntax as <kbd>relabel_configs</kbd> and <kbd>metric_relabel_configs</kbd>, as explained in <a href="12e775c2-bee9-4ebe-ad73-2f9313eeeeee.xhtml">Chapter 5</a>, <em>Running a Prometheus Server</em>, but in this case, it applies only to alerts going out. Using relabeling here can be useful to prevent certain alerts from reaching Alertmanager altogether, altering or dropping labels to ease grouping, or even adding alert-specific labels that for, some reason, don't make sense in <kbd>external_labels</kbd>. Since <kbd>alert_relabel_configs</kbd> is run right before we send out alerts, external labels are present in those alerts and as such are available for manipulation. Here's an example of preventing alerts with a label called <kbd>environment</kbd> and matching value of <kbd>development</kbd> from being pushed to Alertmanager:</p>
<pre>  alert_relabel_configs:<br/>  - source_labels: [environment]<br/>    regex: development<br/>    action: drop</pre>
<p>While the preceding example illustrates how to drop alerts, it should not be used as a permanent solution as the better solution would probably be to not create these alerts at all.</p>
<p class="mce-root">Next, we're going to go through the Alertmanager configuration file, and its main areas, and point out some useful information that will help you get started with it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuration file overview</h1>
                </header>
            
            <article>
                
<p>Alertmanager is configured through a single configuration file, and can reload it at runtime without restarting the same way Prometheus does: either sending a <kbd>SIGHUP</kbd> to the process or sending an HTTP POST request to the <kbd>/-/reload</kbd> endpoint. Such as Prometheus, a malformed configuration will not be applied <span>–</span> an error message will be logged and the <kbd>alertmanager_config_last_reload_successful</kbd> metric that's found in its <kbd>/metrics</kbd> endpoint will be set to <kbd>0</kbd>.</p>
<p class="mce-root">The configuration file is divided into five top-level sections: <kbd>global</kbd>, <kbd>route</kbd>, <kbd>inhibit_rules</kbd>, <kbd>receivers</kbd>, and <kbd>templates</kbd>. In the following sections, we'll be exploring each one of them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">global</h1>
                </header>
            
            <article>
                
<p>The global section gathers all the configuration options that are valid in other sections of the file, and act as default settings for those options. Since those parameters may be overridden in other sections, using them is a good way to keep the configuration file as clean as possible, avoiding repetition. Among all the available parameters in this section, most of them are related to credentials and tokens as notifiers. There's a noteworthy one called <kbd>resolve_timeout</kbd>. Prometheus will send alerts that are produced by triggering alerting rules on every evaluation interval, updating the <kbd>EndTime</kbd> field in JSON payload. When those get resolved, it notify Alertmanager of those resolutions by updating the <kbd>EndTime</kbd>. If, for some reason, an alert stops getting updated periodically (for example, the Prometheus instance sending that alert crashed and is still in the recovery process), Alertmanager will use the last <span>received</span> <kbd>EndTime</kbd> to resolve the alert. The <kbd>resolve_timeout</kbd> configuration is used to resolve alerts created by non-Prometheus systems, that don't use <kbd>EndTime</kbd>. To be clear, this is not a setting you should be changing as it pertains to the Prometheus-Alertmanager alert protocol; it is being explained here for completeness.</p>
<p class="mce-root">As an example, the global section of the Alertmanager configuration in our test environment looks as follows:</p>
<pre>global:<br/>  smtp_smarthost: 'mail.example.com:25'<br/>  smtp_from: 'example@example.com'<br/>...</pre>
<p>This example configuration sets the default email <kbd>smarthost</kbd> and the <kbd>from</kbd> address for every receiver that uses the email (SMTP) notifier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">route</h1>
                </header>
            
            <article>
                
<p>This is ostensibly the most important configuration section of Alertmanager. In this section, we will define how to group alerts based on their labels (<kbd>group_by</kbd>), how long to wait for new alerts before sending additional notifications (<kbd>group_interval</kbd>), and how long to repeat them (<kbd>repeat_interval</kbd>), but most importantly, which receivers should be triggered for each alert batch (<kbd>receiver</kbd>). Since each route can have its own child routes, this forms a routing tree. The top-level route can't have any matching rules as it works like a catch-all for any alert that doesn't match any of its sub-routes. Each setting, except <kbd>continue</kbd>, made on a route is carried over to its child routes in a cascading fashion. Although the default behavior is to stop searching for a receiver when the most specific match possible is found, it is possible to set <kbd>continue</kbd> to <kbd>true</kbd>, making the matching process keep going, thereby allowing you to trigger multiple receivers.</p>
<p class="mce-root">You can find the following example route configuration in our test environment:</p>
<pre>route:<br/>  receiver: operations<br/>  group_by: ['alertname', 'job']<br/>  group_wait: 30s<br/>  group_interval: 5m<br/>  repeat_interval: 4h<br/><br/>  routes:<br/>  - match_re:<br/>      job: (checkoutService|paymentService)<br/>    receiver: yellow-squad-email<br/>    routes:<br/>    - match:<br/>        severity: pager<br/>      receiver: yellow-squad-pager<br/>...</pre>
<p>The main route in the preceding example does the following:</p>
<ul>
<li>Defines the <kbd>operations</kbd> receiver as the default route when no other sub-routes match</li>
<li>Groups incoming alerts by <kbd>alertname</kbd> and <kbd>job</kbd></li>
<li>Waits 30 seconds for more alerts to arrive before sending the first notification to reduce the number of notifications for the same problem</li>
<li>Waits five minutes before sending additional notifications when new alerts are added to a batch</li>
<li>Resends a notification every four hours for each alert batch with the currently firing alerts</li>
</ul>
<p>Additionally, it sets a sub-route for alerts whose <kbd>job</kbd> label matches either <kbd>checkoutService</kbd> or <kbd>paymentService</kbd> with its own receiver, <kbd>yellow-squad-email</kbd>. That sub-route, in turn, define its own child route that, if the severity label matches <kbd>pager</kbd>, should use the <kbd>yellow-squad-pager receiver</kbd> instead.</p>
<div class="packt_tip">The official Prometheus website offers a routing tree editor and visualizer at <a href="https://prometheus.io/webtools/alerting/routing-tree-editor/">https://prometheus.io/webtools/alerting/routing-tree-editor/</a>.</div>
<p>The <kbd>group_by</kbd> clause can also take the sole value of <kbd>...</kbd>, which will signal Alertmanager to not do any grouping for incoming alerts. This is very rarely used, as the purpose of grouping is to tune down the number of notifications so that the signal-to-noise ratio is high. One possible usage of this feature is to send every alert as-is to another system where alerts get processed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">inhibit_rules</h1>
                </header>
            
            <article>
                
<p>In this section, we will add rules to inhibit alerts. The way this works is by matching source alerts and muting target ones by using matchers on both. The only requirement is that the labels on the target and source match in terms of both label name and value, for example:</p>
<pre>inhibit_rules:<br/>  - source_match:<br/>      job: 'icmp'<br/>    target_match_re:<br/>      alertname: (AlertmanagerDown|NodeExporterDown)<br/>    equal: ['base_instance']</pre>
<p>In this example, we can read the following: if there's an alert with the <kbd>job</kbd> label set to <kbd>icmp</kbd>, mute all other alerts with an <kbd>alertname</kbd> that matches <kbd>AlertmanagerDown</kbd> or <kbd>NodeExporterDown</kbd> when <kbd>base_instance</kbd> is the same across all matched alerts. In other words, if the instance that is running Alertmanager and Node Exporter is down, skip sending alerts about those services and just send the one about the instance itself, allowing the operator to focus on the real problem.</p>
<div class="packt_tip">If any label in the equal clause does not exist in both source and target alerts, it will be considered matched and thus inhibition will be enabled.</div>
<div class="packt_infobox">If an alert matches both source and target in the inhibit rule definition, that alert will not be inhibited <span>– </span>this is to prevent an alert from inhibiting itself.</div>
<p>We can see the interface of Alertmanager when this occurs in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2e9ca144-f7bb-42bd-9887-47f351d211e7.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.7: Alertmanager interface showing only firing notifications</div>
<p>In the following screenshot, we can see all the inhibited alerts that weren't present in the preceding screenshot by selecting the <strong>Inhibited</strong> option in the top-right corner:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f3ac8d8a-c728-403e-a0e7-d75cd3448551.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 11.8: Alertmanager interface showing notifications, including the inhibited ones</span></div>
<p>The Alertmanager interface allows you to have a bird's-eye view of all alerts, not only the ones that are active but also the ones that are inhibited. The default is to not show inhibited alerts in order to reduce visual clutter; however, as we can see in the preceding screenshot, you can easily enable showing them by selecting the <strong>Inhibited</strong> checkmark in the top-right corner of the <strong>Filter</strong>/<strong>Group</strong> box.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">receiver</h1>
                </header>
            
            <article>
                
<p>When a route is matched, it will invoke a receiver. A receiver contains notifiers, which we'll be exploring more deeply in the following sections. Basically, the receiver is a named configuration for the available integrations.</p>
<p class="mce-root">In our test environment, we can find an example of a receiver by using the Webhook notifier, as follows:</p>
<pre>route:<br/>  receiver: operations<br/>...<br/>receivers:<br/>- name: 'operations'<br/>  webhook_configs:<br/>  - url: 'http://127.0.0.1:5001'<br/>...</pre>
<p>The top-level route, also known as the catch-all or fallback route, will trigger the receiver named <kbd>operations</kbd> when incoming alerts aren't matched in other sub-routes. The <kbd>operations</kbd> receiver is configured using a single notifier, which is the Webhook notifier. This means that alerts that go to this receiver are sent to the URL specified in the <kbd>url</kbd> configuration key. The Webhook notifier will be further dissected later in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">templates</h1>
                </header>
            
            <article>
                
<p>This section is where a list of paths that point to custom notification templates for the several notifiers that are available can be defined. Similar to other file path configurations in Prometheus, each path definition allows globing on the last component and can be defined as follows:</p>
<pre>templates:<br/>  - /etc/alertmanager/templates/*.tmpl</pre>
<p>We'll be using this section to define our custom templates in the <em><span>Customizing your </span>alert notifications</em> section.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The amtool command-line tool</h1>
                </header>
            
            <article>
                
<p>Similar to <kbd>promtool</kbd>, <kbd>amtool</kbd> is an easy-to-use command-line tool backed by the Alertmanager HTTP API. Besides being useful for validating the correctness of the Alertmanager configuration file, it also allows you to query the server for currently triggering alerts, and to execute actions such as silencing alerts or creating new ones. The <kbd>amtool</kbd> sub-commands are split into four groups <span>– </span><kbd>alert</kbd>, <kbd>silence</kbd>, <kbd>check-config</kbd>, and <kbd>config</kbd> <span>– </span>and we'll provide an overview of each one using the test environment.</p>
<p>To follow the examples in this section, make sure that you connect to one of the Alertmanager instances. Since they are clustered, any one of them will do, for example:</p>
<pre><strong>vagrant ssh alertmanager02</strong></pre>
<p>After logging in, you can run <kbd>amtool</kbd> as the default user, as no administrative permissions are required to interact with the Alertmanager API. Additionally, you can even use <kbd>amtool</kbd> to connect to any of the Alertmanager instances, not just the local instance, as most commands that interact with the HTTP API require that you specify the instance URL.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">alert</h1>
                </header>
            
            <article>
                
<p>This sub-command allows you to query the Alertmanager cluster for currently firing alerts, which can be achieved as follows:</p>
<pre>vagrant@alertmanager02:~$ <strong>amtool alert --alertmanager.url http://alertmanager02:9093</strong><br/>Alertname Starts At Summary <br/>deadmanswitch 2019-03-31 14:49:45 UTC <br/>InstanceDown 2019-03-31 15:48:30 UTC</pre>
<div class="packt_infobox">The <kbd>alert</kbd> sub-command default action is <kbd>query</kbd>. The equivalent command to the previous example would be <kbd>amtool alert query --alertmanager.url http://alertmanager02:9093</kbd>.</div>
<p>Another feature this sub-command has is the ability to create alerts on demand. This can come in useful for testing purposes. For example, let's create a new alert named <kbd>ExampleAlert</kbd> with the label <kbd>example="amtool"</kbd>:</p>
<pre>vagrant@alertmanager02:~$ <strong>amtool alert add alertname="ExampleAlert" example="amtool" --alertmanager.url http://alertmanager02:9093</strong></pre>
<p>The <kbd>add</kbd> action expects one label name/value pair per command argument, as we can see in the preceding code. This action will also consider the first argument since the alertname has no label name. An alert can be created without a name if we omit the <kbd>alertname</kbd> label, which can cause some weirdness in both <kbd>amtool</kbd> and the Alertmanager web UI, so some caution regarding this is advised.</p>
<p>We can check that it was added correctly by waiting a bit (the defined <kbd>group_wait</kbd> in the test environment is 30 seconds) and then querying the current alerts again:</p>
<pre>vagrant@alertmanager02:~$ <strong>amtool alert --alertmanager.url http://alertmanager02:9093</strong><br/>Alertname Starts At Summary <br/>deadmanswitch 2019-03-31 14:49:45 UTC <br/>InstanceDown 2019-03-31 15:48:30 UTC <br/>ExampleAlert 2019-03-31 15:55:00 UTC </pre>
<p>This action also allows you to specify other alert fields, such as end time, generator URL, and annotations. You can consult the <kbd>add</kbd> action command-line interface (arguments and options) by using the <kbd>help</kbd> flag:</p>
<pre><strong>vagrant@alertmanager01:~$ amtool alert add --help</strong></pre>
<p>Keep in mind that this newly created alert will be considered resolved after five minutes of inactivity (the default value for <kbd>resolve_timeout</kbd>), so be sure to add new instances of this alert (by running the <kbd>add</kbd> action) to keep it going if you need more time to test.</p>
<p class="mce-root">We'll be using this new alert next as a target for silencing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">silence</h1>
                </header>
            
            <article>
                
<p>With this sub-command, we can manage silences. First, we can try to query the available silences in the cluster by using the following instruction:</p>
<pre>vagrant@alertmanager02:~$ <strong>amtool silence --alertmanager.url http://alertmanager02:9093</strong><br/>ID Matchers Ends At Created By Comment</pre>
<div class="packt_infobox">The <kbd>silence</kbd> sub-command default action is <kbd>query</kbd>. The equivalent command to the previous example would be <kbd>amtool silence query --alertmanager.url http://alertmanager02:9093</kbd>.</div>
<p>As we can see, no silence is currently being enforced. Let's create a new one for the previous generated alert by matching its label, <kbd>example="amtool"</kbd>, and checking the silences again:</p>
<pre>vagrant@alertmanager02:~$ <strong>amtool silence add 'example="amtool"' --comment "ups" --alertmanager.url http://alertmanager02:9093</strong><br/>1afa55af-306a-408e-b85c-95b1af0d7169<br/><br/>vagrant@alertmanager02:~$ <strong>amtool silence --alertmanager.url http://alertmanager02:9093</strong><br/>ID Matchers Ends At Created By Comment <br/>1afa55af-306a-408e-b85c-95b1af0d7169 example=amtool 2019-03-31 16:58:08 UTC vagrant ups</pre>
<p>We can now see that the new silence has been added. To verify that it's already in effect, we can use the <kbd>alert</kbd> sub-command and check that the <kbd>ExampleAlert</kbd> has disappeared from the list of current alerts:</p>
<pre><strong>vagrant@alertmanager02:~$ amtool alert --alertmanager.url http://alertmanager02:9093</strong><br/><strong>Alertname Starts At Summary </strong><br/><strong>deadmanswitch 2019-03-31 14:49:45 UTC </strong><br/><strong>InstanceDown 2019-03-31 15:48:30 UTC</strong></pre>
<p class="mce-root">Let's remove the silence we just created by using the <kbd>expire</kbd> action. For this, we need the silence identifier, which can be seen in the <kbd>ID</kbd> column when we listed the current silences:</p>
<pre>vagrant@alertmanager02:~$ <strong>amtool silence expire 1afa55af-306a-408e-b85c-95b1af0d7169 --alertmanager.url http://alertmanager02:9093</strong><br/><br/>vagrant@alertmanager02:~$ <strong>amtool silence --alertmanager.url http://alertmanager02:9093</strong><br/>ID Matchers Ends At Created By Comment</pre>
<p>If we query the list of current alerts again, we will see our <kbd>ExampleAlert</kbd> there again.</p>
<p>These are the most common use cases for the silence feature. There are also other actions available, such as batch importing of silences (useful when migrating to a new cluster) or even updating an existing one if you ever so desire. As usual, the <kbd>--help</kbd> flag will provide you with guidance on how to use these actions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">check-config</h1>
                </header>
            
            <article>
                
<p>This is probably the most useful feature of <kbd>amtool</kbd>: the ability to validate the syntax and schema of our Alertmanager configuration file and referenced template files. You can test the <kbd>check-config</kbd> sub-command by following this example:</p>
<pre>vagrant@alertmanager02:~$ <strong>amtool check-config /etc/alertmanager/alertmanager.yml</strong> <br/>Checking '/etc/alertmanager/alertmanager.yml' SUCCESS<br/>Found:<br/> - global config<br/> - route<br/> - 1 inhibit rules<br/> - 8 receivers<br/> - 1 templates<br/>  SUCCESS</pre>
<p>This type of validation is quite easy to automate and should be done after any configuration change, but before reloading the Alertmanager instance, thereby preventing most types of configuration issues.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">config</h1>
                </header>
            
            <article>
                
<p>With the <kbd>config</kbd> sub-command, we can consult the internal configuration of a running Alertmanager instance, which includes all configurable fields, even ones not explicitly listed in the configuration file. You can check this by issuing the following command:</p>
<pre>vagrant@alertmanager02:~$ <strong>amtool config --alertmanager.url http://alertmanager02:9093</strong><br/>global:<br/>  resolve_timeout: 5m<br/>  http_config: {}<br/>  smtp_from: example@example.com<br/>  smtp_hello: localhost<br/>  smtp_smarthost: example.com:25<br/>  smtp_require_tls: true<br/>  slack_api_url: &lt;secret&gt;<br/>...</pre>
<p>Configuration fields that were not specified in the configuration file will show with their default values, and fields that deal with secrets (such as passwords and tokens) will be automatically redacted.</p>
<div class="packt_infobox">The <kbd>config</kbd> sub-command default action is shown. The equivalent command to the previous example would be <kbd>amtool config show --alertmanager.url http://alertmanager02:9093</kbd>.</div>
<p>The next sub-command action, <kbd>routes</kbd>, generates a text visualization of the configured routing tree. This command can be run against a running Alertmanager instance or a local configuration file. The syntax and output is as follows:</p>
<pre>vagrant@alertmanager02:~$ <strong>amtool config routes --alertmanager.url http://alertmanager02:9093</strong><br/>Routing tree:<br/>.<br/>└── default-route receiver: operations<br/>    ├── {job=~"^(?:^(?:(checkoutService|paymentService))$)$"} receiver: yellow-squad-email<br/>    │ └── {severity="pager"} receiver: yellow-squad-pager<br/>    ├── {job="firewall"} receiver: purple-squad-email<br/>    │ ├── {severity="slack"} receiver: purple-squad-slack<br/>    │ └── {severity="pager"} receiver: purple-squad-pager<br/>    └── {alertname=~"^(?:^(?:(AlertmanagerDown|NodeExporterDown))$)$"} receiver: violet-squad-slack<br/>        └── {severity="pager"} receiver: violet-squad-pager</pre>
<p>You can even validate the routing tree by providing labels to the <kbd>routes test</kbd> action and checking which route would be triggered. We can see this in action in the following example, where we're validating whether the triggered receiver is in fact <kbd>yellow-squad-email</kbd> when an alert comes in with the <kbd>job="checkoutService"</kbd> label:</p>
<pre>vagrant@alertmanager02:~$ <strong>amtool config routes test 'job="checkoutService"' --config.file /etc/alertmanager/alertmanager.yml</strong> <br/>yellow-squad-email</pre>
<p>Having this command-line tool around can help you streamline the development of complex routing rules and validate produced configurations without even needing an Alertmanager instance running locally.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kubernetes Prometheus Operator and Alertmanager</h1>
                </header>
            
            <article>
                
<p>In <a href="12e775c2-bee9-4ebe-ad73-2f9313eeeeee.xhtml">Chapter 5</a>, <em>Running a Prometheus Server</em>, we had the opportunity to experiment with the Prometheus Operator. Since Alertmanager is a fundamental component of the Prometheus stack, the Operator is also able to manage its instances. Besides taking care of an Alertmanager cluster, the Operator is also responsible for managing the configuration of recording and alerting rules.</p>
<p class="mce-root">To provide some insight into how to use the Operator to manage an Alertmanager cluster, we will provide, as an example, a full setup for you to try out. The Kubernetes manifests for getting Alertmanager and Prometheus up and running in our Kubernetes test environment can be found, relative to the repository root path, at the following path:</p>
<pre><strong>cd ./chapter11/provision/kubernetes/</strong></pre>
<p>The following steps will ensure that a new Kubernetes environment with all the required software has been provisioned so that we can then focus on the Alertmanager component:</p>
<ol>
<li>Validate that no other Kubernetes environment is running:</li>
</ol>
<pre style="padding-left: 60px"><strong>minikube status</strong><br/><strong>minikube delete</strong></pre>
<ol start="2">
<li>Start an empty Kubernetes environment:</li>
</ol>
<pre style="padding-left: 60px">minikube start \<br/>  --cpus=2 \<br/>  --memory=3072 \<br/>  --kubernetes-version="v1.14.0" \<br/>  --vm-driver=virtualbox</pre>
<ol start="3">
<li>Add the Prometheus Operator components and follow its deployment:</li>
</ol>
<pre style="padding-left: 60px"><strong>kubectl apply -f ./bootstrap/</strong><br/><br/><strong>kubectl rollout status deployment/prometheus-operator -n monitoring</strong></pre>
<ol start="4">
<li>Add the new Prometheus cluster, ensuring that it's successful:</li>
</ol>
<pre style="padding-left: 60px"><strong>kubectl apply -f ./prometheus/</strong><br/><br/><strong>kubectl rollout status statefulset/prometheus-k8s -n monitoring</strong></pre>
<ol start="5">
<li>Add all the targets to Prometheus and list them:</li>
</ol>
<pre style="padding-left: 60px"><strong>kubectl apply -f ./services/</strong><br/><br/><strong>kubectl get servicemonitors --all-namespaces</strong></pre>
<p>After the Kubernetes test environment is running, we can proceed with Alertmanager-specific configurations. Similar to the virtual machine-based test environment, we'll require the provisioning of not only Alertmanager itself, but also the alerting rules for Prometheus.</p>
<p>For the Alertmanager configuration, since we might want to add sensitive information such as email credentials or a pager token, we are going to use a Kubernetes secret. This also implies that there should be a ServiceAccount for accessing that secret.</p>
<p>We can create the ServiceAccount by applying the following manifest:</p>
<pre><strong>kubectl apply -f ./alertmanager/alertmanager-serviceaccount.yaml</strong></pre>
<p>Since we're using a secret, the Alertmanager configuration needs to be encoded into base64. A minimal configuration is provided and can be deployed by issuing the following command:</p>
<pre><strong>kubectl apply -f ./alertmanager/alertmanager-configuration.yaml</strong></pre>
<p>For reference, the minimal configuration that is encoded in the secret is the following:</p>
<pre>global:<br/><br/>route:<br/>  receiver: "null"<br/>  group_by:<br/>    - job<br/>  group_interval: 3m<br/>  repeat_interval: 3h<br/>  routes:<br/>    - match:<br/>        alertname: deadmanswitch<br/>      receiver: "null"<br/><br/>receivers:<br/>  - name: "null"</pre>
<p>Now, we can proceed with the deployment and get the Operator to do the heavy lifting for us. It will abstract the creation of a StatefulSet and get the cluster up and running. For this, we're required to apply the following manifest:</p>
<pre><strong>kubectl apply -f ./alertmanager/alertmanager-deploy.yaml</strong></pre>
<p>The important bit of the previous manifest can be seen in the following snippet:</p>
<pre>...<br/>kind: Alertmanager<br/>...<br/>spec:<br/>  baseImage: quay.io/prometheus/alertmanager<br/>...<br/>  replicas: 3<br/>...</pre>
<p>We can follow the state of the deployment by issuing the following instruction:</p>
<pre><strong>kubectl rollout status statefulset/alertmanager-k8s -n monitoring</strong></pre>
<p>To ensure that the Prometheus instances can collect metrics from the newly created Alertmanagers, we'll add a new Service and ServiceMonitor. For this, we need to apply the following manifests:</p>
<pre><strong>kubectl apply -f ./alertmanager/alertmanager-service.yaml</strong><br/><br/><strong>kubectl apply -f ./alertmanager/alertmanager-servicemonitor.yaml</strong></pre>
<p>It's now time to add the alerting rules. To do this, you're just required to apply the following manifest:</p>
<pre><strong>kubectl apply -f ./alertmanager/alerting-rules.yaml</strong></pre>
<p>If you open the previous manifest, you will see several rules. The following snippet illustrates the first one:</p>
<pre>...<br/>kind: PrometheusRule<br/>...<br/>spec:<br/>  groups:<br/>  - name: exporter-down<br/>    rules:<br/>    - alert: AlertmanagerDown<br/>      annotations:<br/>        description: Alertmanager is not being scraped.<br/>        troubleshooting: https://github.com/kubernetes-monitoring/kubernetes-mixin/blob/master/runbook.md<br/>      expr: |<br/>        absent(up{job="alertmanager-service",namespace="monitoring"} == 1)<br/>      for: 5m<br/>      labels:<br/>        severity: page<br/>...</pre>
<p>These rules will be added to the Prometheus instances, and the Operator will take care of reloading their configuration without causing any downtime of the service.</p>
<p>Finally, you can access the web interface of Prometheus and Alertmanager and validate all the configurations you've made so far by issuing the following instructions, which will open a couple of browser tabs:</p>
<pre><strong>minikube service alertmanager-service -n monitoring</strong><br/><br/><strong>minikube service prometheus-service -n monitoring</strong></pre>
<p>When you're finished testing, you can delete this Kubernetes-based environment by issuing the following command:</p>
<pre><strong>minikube delete</strong></pre>
<p>This setup gives you a quick overview of how to integrate Alertmanager with Prometheus on Kubernetes. Once again, the Prometheus Operator abstracts most of the complexity and allows you to focus on what matters most.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Common Alertmanager notification integrations</h1>
                </header>
            
            <article>
                
<p>Users and/or organizations have different requirements regarding notification methods; some might be using HipChat as a means of communication, while others rely on email, on-call usually demands a pager system such as PagerDuty or VictorOps, and so on. Thankfully, Alertmanager provides several integration options out of the box and covers most of the notification needs you might have. If not, there's always the Webhook notifier, which allows integration with custom notification methods. Next, we'll be exploring the most common integrations and how to configure them, as well as providing basic examples to get you started.</p>
<p>Something to keep in mind when considering integrating with chat systems is that they're designed for humans, and the use of a ticketing system is advised when thinking about low-priority alerting. When the process of creating alerts is easy and self-service, managing them can quickly get out of control. Tickets ensure accountability: Some of the major advantages of using tickets over alerting on chat channels is that they allow tracking, prioritization, and proper follow-up to ensure that the alerted problem does not happen again. This method also implicitly ensures ownership of notifications and stops the usual <em>who's the owner of this alert?</em> question from arising. Ownership empowers service maintainers to curate the alerts they receive and, as a side effect, also helps reduce alert fatigue.</p>
<div class="packt_tip">If you happen to be using JIRA for task tracking, there's a custom integration that relies on the Webhook notifier called <span>JIRAlert, </span>available at <a href="https://github.com/free/jiralert">https://github.com/free/jiralert</a>.</div>
<p>There is a configuration key that is common to all notifiers, and is called <kbd>send_resolved</kbd>. It takes a Boolean (true or false) and declares whether a notification should be sent when an alert is resolved. This is enabled by default for PagerDuty, Opsgenie, VictorOps, Pushover, and the Webhook integration, but disabled for the remaining notifiers, and is the main reason why you should prevent unnecessary spam.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Email</h1>
                </header>
            
            <article>
                
<p>Email is the standard communication method in most organizations, so it should be no surprise that it's supported by Alertmanager. Configuration-wise, it's quite straightforward to set up; however, since Alertmanager doesn't send emails directly, it needs to use an actual email relay. Let's use a real-world example that should be helpful for quick tests and low-budget setups, which is using the SMTP of an email provider (in this case, Google's Gmail):</p>
<pre>global:<br/>  smtp_smarthost: 'smtp.gmail.com:587'<br/>  smtp_from: 'alertmanager@example.com'<br/>  smtp_auth_username: 'alertmanager@example.com'<br/>  smtp_auth_identity: 'alertmanager@example.com'<br/>  smtp_auth_password: '&lt;generated_token&gt;'<br/><br/>route: <br/>  receiver: 'default'<br/><br/>receivers:<br/>- name: 'default'<br/>  email_configs:<br/>  - to: 'squad@example.com'</pre>
<p>In this particular example, since it's a bad idea in terms of online security to use your main Gmail password directly, you'll need an account with two-factor authentication enabled, and then generate an app password to use in the <kbd>smtp_auth_password</kbd> field.</p>
<div class="packt_tip">You can find out how to generate app passwords for a Gmail account on the <em>Sign in using App Passwords</em> support page, located at <a href="https://support.google.com/accounts/answer/185833">https://support.google.com/accounts/answer/185833</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chat</h1>
                </header>
            
            <article>
                
<p>At the time of writing, there are integrations for three chat services: Slack, WeChat, and HipChat. The following example represents the configuration for the Slack integration; later in this chapter, we'll provide a more in-depth customization overview for this kind of integration:</p>
<pre>global:<br/>  slack_api_url: 'https://hooks.slack.com/services/TOKEN'<br/><br/>route:<br/>  receiver: 'default'<br/><br/>receivers:<br/>- name: 'default'<br/>  slack_configs:<br/>  - channel: '#alerting'</pre>
<p>The <kbd>slack_api_url</kbd> should point to a <em>Slack Incoming Webhooks</em> URL. You can find out more by going to their documentation about this subject at <a href="https://api.slack.com/incoming-webhooks">https://api.slack.com/incoming-webhooks</a>. Since <kbd>slack_configs</kbd> is a list, you can specify multiple channels on a single receiver.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pager</h1>
                </header>
            
            <article>
                
<p>Being on-call is generally synonymous with carrying a pager,<span> </span>physical or otherwise. Alertmanager, at the time of writing, supports four pager-style service integrations: PagerDuty, Opsgenie, VictorOps, and Pushover. The configuration for each of these services is fairly simple to get started, as they mostly revolve around API URLs and authentication tokens. However, they also support deeper levels of customization, such as adding images, and links, and configuring service-specific fields, such as severity. These advanced configuration options are described in Alertmanager's official documentation, so they won't be replicated here. The following example demonstrates a basic configuration for PagerDuty:</p>
<pre>global:<br/>  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'<br/><br/>route:<br/>  receiver: 'default'<br/><br/>receivers:<br/>- name: 'default'<br/>  pagerduty_configs:<br/>  - service_key: 'PAGERDUTYSQUADTOKENEXAMPLE'</pre>
<p>Just like in the previous notifier, since the <kbd>pagerduty_configs</kbd> configuration is a list, you can trigger multiple service routes in a single receiver. You can find out more about PagerDuty's integration with Alertmanager here: <a href="https://www.pagerduty.com/docs/guides/prometheus-integration-guide/">https://www.pagerduty.com/docs/guides/prometheus-integration-guide/</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Webhook</h1>
                </header>
            
            <article>
                
<p>The Webhook integration opens up a world of possibilities for custom integrations. This feature allows Alertmanager to issue an HTTP POST request with the JSON payload of the notification to an endpoint of your choosing. Keep in mind that the URL is not templateable and the destination endpoint must be designed to handle the JSON payload. It can be used, for example, to push all notifications into a logging system such as Elasticsearch so that you can perform reporting and statistical analysis of the alert being generated. If your team uses IRC, this could also be a solution to integrate with it. One last example is the alertdump tool we've created for this book. It was previously used in <a href="9aa1e3da-13cf-4051-845d-1d1c924ef47b.xhtml">Chapter 9</a>, <em>Defining Alerting and Recording Rules</em>, to show what Prometheus sends out when alerting rules trigger, but it can also be used to demonstrate the notification payloads being sent by Alertmanager.</p>
<p>A simple configuration can be seen in the following code:</p>
<pre>global:<br/><br/>route:<br/>  receiver: 'default'<br/><br/>receivers:<br/>- name: 'default'<br/>  webhook_configs:<br/>  - url: 'http://127.0.0.1:5001'</pre>
<p>This configuration will send every alert that's received by Alertmanager as-is to alertdump, which in turn will then append the payload to a log file named after the host that alertdump is running on. This log file is located in a path that's accessible both inside each virtual machine from our test environment (<kbd>/vagrant/cache/alertmanager*.log</kbd>), and outside it (<kbd>./cache/alertmanager*.log</kbd>, relative to the repository root).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">null</h1>
                </header>
            
            <article>
                
<p>This is not a notifier per se, but a pattern commonly used to drop notifications. The way it's configured is by specifying a receiver without a notifier, which causes the notification to be dropped. The following example ensures that no notification will ever be sent:</p>
<pre>global:<br/><br/>route:<br/>  receiver: 'null'<br/><br/>receivers:<br/>- name: 'null'</pre>
<p>This is sometimes useful for demonstrative purposes, but not much else; alerts that aren't supposed to trigger notifications should be dropped at their source and not in Alertmanager, with the exception of alerts being used as source for inhibitions.</p>
<div class="packt_tip">Something to always keep an eye on is the <kbd>alertmanager_notifications_failed_total</kbd> Alertmanager metric, as it tracks all the failed attempts to deliver notifications per integration.</div>
<p>Now that we know the basics of Alertmanager notifiers, we can proceed to learn how to customize alert notifications so that the most important information is properly surfaced.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Customizing your alert notifications</h1>
                </header>
            
            <article>
                
<p>For each of the available integrations, Alertmanager already includes built-in templates for their notifications. However, these can be tailored to the specific needs of the user and/or organization. Similar to the alerting rule annotations we explored in <a href="9aa1e3da-13cf-4051-845d-1d1c924ef47b.xhtml">Chapter 9</a>, <em>Defining Alerting and Recording Rules</em>, alert notifications are templated using the Go templating language. Let's use the Slack integration as an example and understand how the messages are constructed so that they are tailored to your needs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Default message format</h1>
                </header>
            
            <article>
                
<p>To have an idea of what a notification without any customization looks like, we're going to use a very simple example. Take the following alerting rule, which we defined in our Prometheus instance:</p>
<pre>  - alert: deadmanswitch<br/>    expr: vector(42)</pre>
<p>As soon as this alert starts firing, an alert payload will be sent to Alertmanager. The following snippet demonstrates the payload being sent. Note the labels that are present, which include the <kbd>alertname</kbd> and the <kbd>external_labels</kbd> from the Prometheus instance:</p>
<pre>    {<br/>        "labels": {<br/>            "alertname": "deadmanswitch",<br/>            "dc": "dc1"<br/>        },<br/>        "annotations": {},<br/>        "startsAt": "2019-04-02T19:11:30.04754979Z",<br/>        "endsAt": "2019-04-02T19:14:30.04754979Z",<br/>        "generatorURL": "http://prometheus:9090/graph?g0.expr=vector%2842%29&amp;g0.tab=1"<br/>    }</pre>
<p>On the Alertmanager side, we will have this minimal configuration, just so that we can send a Slack notification (substituting <kbd>TOKEN</kbd> with an actual Slack token):</p>
<pre>global:<br/>  slack_api_url: 'https://hooks.slack.com/services/TOKEN'<br/><br/>route:<br/>  receiver: 'default'<br/><br/>receivers:<br/>- name: 'default'<br/>  slack_configs:<br/>  - channel: '#alerting'</pre>
<p>The end result would be a Slack message like the following:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6e072e2e-499d-4e06-a92c-52073b206923.png" style="width:26.17em;height:6.58em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.9: Slack default notification format</div>
<p>As we can see, the default notification format has a lot of information in it. But the question remains, <em>how was this generated</em>? To answer this question, we can have a look into the runtime configuration of the <kbd>default</kbd> receiver that was generated from our basic Alertmanager configuration, which the following snippet illustrates:</p>
<pre>...<br/>receivers:<br/>- name: default<br/>  slack_configs:<br/>  - send_resolved: false<br/>    http_config: {}<br/>    api_url: &lt;secret&gt;<br/>    channel: '#alerting'<br/>    <strong>username: '{{ template "slack.default.username" . }}'</strong><br/>    color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'<br/>    title: '{{ template "slack.default.title" . }}'<br/>    title_link: '{{ template "slack.default.titlelink" . }}'<br/>    pretext: '{{ template "slack.default.pretext" . }}'<br/>    text: '{{ template "slack.default.text" . }}'<br/>...</pre>
<div class="packt_tip">The Alertmanager runtime configuration can be inspected by using <kbd>amtool config</kbd> or by accessing the <kbd>/#/status</kbd> endpoint of an Alertmanager web interface.</div>
<p>As we can see, each of the customizable fields is configured using Go templating. We will take the <kbd>username</kbd> field to exemplify how the username in the Slack message gets generated, as it's a fairly straightforward template. All other templates being used follow the same logic, with varying levels of complexity.</p>
<p>The default templates used by Alertmanager can't be consulted locally on the test environment instances, as they are compiled and shipped within the Alertmanager binary. However, we can consult all the default templates for all the notification integrations provided by Alertmanager by looking at the <kbd>templates/default.tmpl</kbd> file in the Alertmanager code base. At the time of writing, the current version is 0.16.2 so, for convenience, we're linking to the referenced file here: <a href="https://github.com/prometheus/alertmanager/blob/v0.16.2/template/default.tmpl">https://github.com/prometheus/alertmanager/blob/v0.16.2/template/default.tmpl</a>.</p>
<p>If we have a look at the <kbd>default.tmpl</kbd> file, we will find the definition of the <kbd>slack.default.username</kbd> template:</p>
<pre>{{ define "slack.default.username" }}{{ template "__alertmanager" . }}{{ end }}</pre>
<p>As we can see, the template uses another template as its definition. So, if we look for the definition of the <kbd>__alertmanager</kbd> template, we'll find the following:</p>
<pre>{{ define "__alertmanager" }}<strong>AlertManager</strong>{{ end }}</pre>
<p>Now, you understand how the name <kbd>AlertManager</kbd> appears in the Slack notification. The tracing of each of the other templates is left as an exercise for you. In the next section, we're going to learn how to create our own templates, and then use them to customize our alert notifications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a new template</h1>
                </header>
            
            <article>
                
<p>Before we dive into creating a template, we must first realize what kind of data structures are sent to the notification templates. The following table depicts the available variables that can be used:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Variable</strong></p>
</td>
<td>
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>Alerts</kbd></p>
</td>
<td>
<p>A list of Alert structures, each one with its own Status, Labels, Annotations, StartsAt, EndsAt, and GeneratorURL</p>
</td>
</tr>
<tr>
<td>
<p><kbd>CommonAnnotations</kbd></p>
</td>
<td>
<p>The annotations that are common to all alerts</p>
</td>
</tr>
<tr>
<td>
<p><kbd>CommonLabels</kbd></p>
</td>
<td>
<p>The labels that are common to all alerts</p>
</td>
</tr>
<tr>
<td>
<p><kbd>ExternalURL</kbd></p>
</td>
<td>
<p>The URL for the Alertmanager that sent the alert</p>
</td>
</tr>
<tr>
<td>
<p><kbd>GroupLabels</kbd></p>
</td>
<td>
<p>The labels that are used in the grouping of alerts</p>
</td>
</tr>
<tr>
<td>
<p><kbd>receiver</kbd></p>
</td>
<td>
<p>The receiver that will handle the notification</p>
</td>
</tr>
<tr>
<td>
<p><kbd>status</kbd></p>
</td>
<td>
<p>This will be firing as long as there's an alert in that state, or it will become resolved</p>
</td>
</tr>
</tbody>
</table>
<div class="packt_tip">A comprehensive reference of all available data structures and functions is available at <a href="https://prometheus.io/docs/alerting/notifications/">https://prometheus.io/docs/alerting/notifications/</a>.</div>
<p>The best way to demonstrate how we can build a template is to provide an example. We've built the following template, which is available in the test environment, exactly with that intent. We will explain each part in detail while stripping the Alertmanager configurations down to only the important bits.</p>
<p>The end result we aim to create can be seen in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/23660965-21b3-44b9-a32c-eddadb0dd050.png" style="width:14.17em;height:13.83em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.10: Example Slack notification template</div>
<p>Now, we're going to dissect the example configuration, which will generate notifications that look like the ones shown in the preceding screenshot:</p>
<pre>route:<br/>  group_by: [alertname, job]<br/>  receiver: null<br/>  routes:<br/>  - match:<br/>      severity: slack<br/>    receiver: purple-squad-slack</pre>
<p>For the sake of this example, we're grouping alerts by <kbd>alertname</kbd> and <kbd>job</kbd>. This is important, because it will influence the <kbd>CommonAnnotations</kbd> and <kbd>CommonLabels</kbd>, as we'll see soon:</p>
<pre>receivers:<br/>- name: null<br/>- name: purple-squad-slack<br/>  slack_configs:<br/>  - api_url: 'https://hooks.slack.com/TOKEN'<br/>    channel: '#alerting'<br/>    title: &gt;<br/>      [{{ .Alerts | len }}x ALERT{{ if gt (len .Alerts.Firing) 1 }}S{{end}}] {{ .CommonLabels.alertname }}</pre>
<p>As we saw in the previous table, <kbd>.Alerts</kbd> is a list of all the alerts, so we want the length (<kbd>len</kbd>) of that list to create a title for the message, starting with the number of firing alerts. Note the <kbd>if</kbd> clause, which ensures the use of plurals if there is more than one alert. Finally, since we're grouping the alerts by <kbd>alertname</kbd>, we print the <kbd>alertname</kbd> after the square brackets:</p>
<pre>    text: &gt;<br/>      :link: &lt;{{ .CommonAnnotations.troubleshooting }}/{{ .CommonLabels.alertname }}|Troubleshooting Guide&gt;</pre>
<p>For the message body, we want to generate a link to the troubleshooting guide for this particular kind of alert. Our alerts are sending an annotation called <kbd>troubleshooting</kbd> with a base URL. If we rely on convention so that the guide name matches the alertname, we can easily generate the link using both fields.</p>
<p>To provide more context about the firing alerts, we'll add all the available alert labels to the message. To achieve this goal, we must go through every alert in the list:</p>
<pre>      {{ range .Alerts }}</pre>
<p>For every alert, we'll print the description that's available as an annotation of that alert:</p>
<pre>       *Description:* {{ .Annotations.description }}<br/>      *Details:*</pre>
<p>We'll print each alert label/value pair as well. To do that, we'll be ranging over the result of <kbd>SortedPairs</kbd>, which returns a sorted list of label/value pairs:</p>
<pre>        {{- range .Labels.SortedPairs }}</pre>
<div class="mce-root packt_tip"><span>The <kbd>{{-</kbd> code trims all trailing whitespace from the preceding text. More information on this is available at <a href="https://tip.golang.org/pkg/text/template/#hdr-Text_and_spaces">https://tip.golang.org/pkg/text/template/#hdr-Text_and_spaces</a>.</span></div>
<p class="mce-root">We're using the severity label as a routing key in order to choose the notifier (pager, email, or slack), so we don't want to expose it in the alert message. We can do that by adding an <kbd>if</kbd> clause so that we don't print that particular label/value:</p>
<pre>          {{- if ne .Name "severity"}}<br/>        • *{{ .Name }}:* `{{ .Value }}`<br/>          {{- end}}<br/>      {{- end }}<br/>      {{ end }}</pre>
<p>And that's it. You can even make this more manageable by getting this template out of the Alertmanager configuration and into its own template file. We've done this in the test environment, where the receiver configuration is just the following:</p>
<pre>- name: violet-squad-slack<br/>  slack_configs:<br/>  - channel: '#violet-squad-alerts'<br/>    title: '{{ template "slack.example.title" . }}'<br/>    text: '{{ template "slack.example.text" . }}'</pre>
<p>All the template definitions are available in the Alertmanager instances at the following path:</p>
<pre>/etc/alertmanager/templates/example.tmpl</pre>
<div class="packt_tip">Notification templating is quite hard to understand and build at first. Luckily, a tool was created by Julius Volz, one of Prometheus' co-founders and core maintainer, that allows you to quickly iterate on Slack notification templates. It's by far the best way to understand how they work and how to generate them. You can find it at <a href="https://juliusv.com/promslack/">https://juliusv.com/promslack/</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Who watches the Watchmen?</h1>
                </header>
            
            <article>
                
<p>The monitoring system is a critical component of any infrastructure. We rely on it to keep watch over everything <span>– </span>from servers and network devices to services and applications <span>– </span>and expect to be notified whenever there's a problem. However, when the problem is on the monitoring stack itself, or even on a notification provider so that alerts are generated but don't reach us, how will we, as operators, know?</p>
<p>Guaranteeing that the monitoring stack is up and running, and that notifications are able to reach recipients, is a commonly overlooked task. In this section, we will go into what can be done to mitigate risk factors and improve overall confidence in the monitoring system.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Meta-monitoring and cross-monitoring</h1>
                </header>
            
            <article>
                
<p>In broad terms, you can't have your monitoring system monitor itself; if the system suffers a serious failure, it won't be able to send a notification about it. Although it is common practice to have Prometheus scrape itself (you may see this in most tutorials), you obviously can't rely on it to alert on itself. This is where meta-monitoring comes in: it is the process by which the monitoring system is monitored.</p>
<p>The first option you should consider to mitigate this issue is to have a set of Prometheus instances that monitor every other Prometheus instance in their datacenter/zone. Since Prometheus generates relatively few metrics of its own, this would translate to a fairly light scrape job for the ones doing the meta-monitoring; they wouldn't even need to be solely dedicated to this:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7181b1e2-83d3-45f8-a939-c1c4c3096af1.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 11.11: Meta-monitoring – Prometheus group monitoring every other group</span></div>
<p>However, you may be wondering how this set of instances would be monitored. We could keep adding progressively higher-level instances to do meta-monitoring in a hierarchical fashion <span>– </span>at the datacenter level, then at the regional level, then at the global level <span>– </span>but we would still be left with a set of servers that aren't being monitored.</p>
<p>A complementary technique to mitigate this shortcoming is known as cross-monitoring. This method involves having Prometheus instances on the same responsibility level monitor as their peers. This way, every instance will have at least one other Prometheus watching over it and generating alerts if it fails:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8341ddba-98dc-4dff-832f-231728abb27a.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 11.12: Prometheus groups monitoring themselves</span></div>
<p class="mce-root">But what happens if the problem is in the Alertmanager cluster? Or if external connectivity prevents notifications from reaching the notification provider? Or even if the notification provider itself is suffering an outage? In the next section, we'll provide possible solutions to these questions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dead man's switch alerts</h1>
                </header>
            
            <article>
                
<p>Imagine that you have a set of Prometheus instances using an Alertmanager cluster for alerting. For some reason, a network partition happens between these two services. Even though each Prometheus instance detects that it can no longer reach any of the Alertmanager instances, they will have no means to send notifications.</p>
<p class="mce-root"/>
<p>In this situation, no alerts about the issue will ever be sent:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/998721d6-ecb3-4f19-b8ba-4186455fba9e.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.13: Network partition between Prometheus and Alertmanager</div>
<p>The original concept of a dead man's switch refers to a mechanism that activates if it stops being triggered/pressed. This concept has been adopted in the software world in several ways; for our purpose, we can achieve this by creating an alert that should always be firing <span>– </span>thereby constantly sending out notifications <span>– </span>and then checking if it ever stops. This way, we can exercise the full alerting path from Prometheus, through Alertmanager, to the notification provider, and ultimately to the recipient of the notifications so that we can ensure end-to-end connectivity and service availability. This, of course, goes against all we know about alert fatigue and we wouldn't want to be constantly receiving pages or emails about an always-firing alert. You can implement your own custom service implementing watchdog timers, but then you'll be in a situation where you need to monitor that as well. Ideally, you should leverage a third party so that you mitigate the risk of this service suffering from the same outage that is preventing notifications from going out.</p>
<p class="mce-root">For this, there's a service built around the dead man's switch type of alert, and it's curiously named <em>Dead Man's Snitch</em> (<a href="https://deadmanssnitch.com">deadmanssnitch.com</a>). This is a third-party provider, outside of your infrastructure, that's responsible for receiving your always-firing notification via email or Webhook and will, in turn, issue a page, Slack message, or Webhook if that notification stops being received for more than a configurable amount of time. This setup mitigates the problems we presented previously <span>–</span> even if the entire datacenter goes up in flames, you'll still be paged!</p>
<div class="mce-root packt_infobox">The full configuration guide for integrating Dead Man's Snitch with VictorOps and <span>PagerDuty </span>can be found at <a href="https://help.victorops.com/knowledge-base/victorops-dead-mans-snitch-integration/">https://help.victorops.com/knowledge-base/victorops-dead-mans-snitch-integration/</a> and <a href="https://www.pagerduty.com/docs/guides/dead-mans-snitch-integration-guide/">https://www.pagerduty.com/docs/guides/dead-mans-snitch-integration-guide/</a>, respectively.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we dived into the alerting component of the Prometheus stack, Alertmanager. This service was designed with availability in mind, and we had the opportunity to understand how it works, from generating better notifications to avoiding being flooded by useless ones. The notification pipeline is a very good starting point to grok the inner workings of Alertmanager, but we also went through its configuration, while providing examples to better solidify that knowledge. We were introduced to <kbd>amtool</kbd> and all the features it provides, such as adding, removing, and updating silences directly from the command line.</p>
<p>Alertmanager has several notification integrations available and we went through all of them, so you can pick and choose the ones you're interested in. Since we all want better notifications, we delved into how to customize the default notifications, using Slack as our example. A hard problem to solve is how to monitor the monitoring system; in this chapter, we learned how to make sure we are alerted when notifications aren't getting out.</p>
<p class="mce-root">In an always-changing infrastructure, it's not trivial to keep track of what's running and where it's running. In the next chapter, we'll be providing more insight into how Prometheus tackles service discovery and automates these tasks for you.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What happens to the notifications if there's a network partition between Alertmanager instances in the same cluster?</li>
<li>Can an alert trigger multiple receivers? What is required for that to happen?</li>
<li>What's the difference between <kbd>group_interval</kbd> and <kbd>repeat_interval</kbd>?</li>
<li>What happens if an alert does not match any of the configured routes?</li>
<li>If the notification provider you require is not supported natively by Alertmanager, how can you use it?</li>
<li>When writing custom notifications, how are <kbd>CommonLabels</kbd> and <kbd>CommonAnnotations</kbd> populated?</li>
<li>What can you do to ensure that the full alerting path is working from end to end?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><strong>Official Alertmanager page</strong>: <a href="https://prometheus.io/docs/alerting/alertmanager/">https://prometheus.io/docs/alerting/alertmanager/</a></li>
<li><strong>Alertmanager notification guide</strong>: <a href="https://prometheus.io/docs/alerting/notifications/">https://prometheus.io/docs/alerting/notifications/</a></li>
<li><strong>Alertmanager configuration specifics</strong>: <a href="https://prometheus.io/docs/alerting/configuration/">https://prometheus.io/docs/alerting/configuration/</a></li>
</ul>


            </article>

            
        </section>
    </body></html>