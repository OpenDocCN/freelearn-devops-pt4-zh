["```\ncd ./chapter12/\n```", "```\nvagrant global-status\nvagrant up\n```", "```\nvagrant status\n```", "```\nCurrent machine states:\n\nprometheus                running (virtualbox)\nconsul                    running (virtualbox)\n\nThis environment represents multiple VMs. The VMs are all listed above with their current state. For more information about a specific VM, run `vagrant status NAME`.\n```", "```\nvagrant destroy -f\n```", "```\nscrape_configs:\n - job_name: ec2_sd\n   ec2_sd_configs:\n    - region: eu-west-1\n      access_key: ACCESSKEYTOKEN\n      secret_key: 'SecREtKeySecREtKey+SecREtKey+SecREtKey'\n```", "```\nscrape_configs:\n  - job_name: kubernetes_sd\n    kubernetes_sd_configs:\n      - role: pod\n    relabel_configs:\n      - action: keep\n        regex: hey\n        source_labels:\n          - __meta_kubernetes_pod_label_app\n```", "```\nscrape_configs:\n  - job_name: 'consul_sd'\n    consul_sd_configs:\n      - server: http://consul.prom.inet:8500\n        datacenter: dc1\n    relabel_configs:\n      - source_labels: [__meta_consul_service]\n        target_label: job\n      - source_labels: [job, __address__]\n        regex: \"consul;([^:]+):.+\"\n        target_label: __address__\n        replacement: ${1}:9107\n```", "```\n~$ dig SRV hey.service.example.inet\n...\n;; QUESTION SECTION:\n;hey.service.example.inet. IN SRV\n\n;; ANSWER SECTION:\nhey.service.example.inet. 0 IN SRV 1 1 8080 server01.node.example.inet.\n\n;; ADDITIONAL SECTION:\nserver01.node.example.inet. 0 IN A 192.168.42.11\nserver01.node.example.inet. 0 IN TXT \"squad=purple\"\n...\n```", "```\nscrape_configs:\n  - job_name: 'dns_sd'\n    dns_sd_configs:\n      - names:\n        - hey.service.example.inet\n```", "```\n[\n    {\n        \"labels\": {\n            \"job\": \"node\"\n        },\n        \"targets\": [\n            \"192.168.42.11:9100\"\n        ]\n    }\n]\n```", "```\nscrape_configs:\n  - job_name: 'file_sd'\n    file_sd_configs:\n      - files:\n        - file_sd.json\n```", "```\nvagrant ssh consul\n```", "```\nvagrant@consul:~$ systemctl cat consul.service \n...\n[Service]\nUser=consul\nExecStart=/usr/bin/consul agent \\\n -dev \\\n -bind=192.168.42.11 \\\n -client=192.168.42.11 \\\n -advertise=192.168.42.11\n...\n```", "```\nvagrant@consul:~$ sudo /bin/ss -lnp | grep consul\nudp UNCONN 0 0 192.168.42.11:8301 0.0.0.0:* users:((\"consul\",pid=581,fd=8))\nudp UNCONN 0 0 192.168.42.11:8302 0.0.0.0:* users:((\"consul\",pid=581,fd=6))\nudp UNCONN 0 0 192.168.42.11:8600 0.0.0.0:* users:((\"consul\",pid=581,fd=9))\ntcp LISTEN 0 128 192.168.42.11:8300 0.0.0.0:* users:((\"consul\",pid=581,fd=3))\ntcp LISTEN 0 128 192.168.42.11:8301 0.0.0.0:* users:((\"consul\",pid=581,fd=7))\ntcp LISTEN 0 128 192.168.42.11:8302 0.0.0.0:* users:((\"consul\",pid=581,fd=5))\ntcp LISTEN 0 128 192.168.42.11:8500 0.0.0.0:* users:((\"consul\",pid=581,fd=11))\ntcp LISTEN 0 128 192.168.42.11:8502 0.0.0.0:* users:((\"consul\",pid=581,fd=12))\ntcp LISTEN 0 128 192.168.42.11:8600 0.0.0.0:* users:((\"consul\",pid=581,fd=10))\n```", "```\nvagrant@consul:~$ systemctl cat consul-exporter.service \n...\n[Service]\nUser=consul_exporter\nExecStart=/usr/bin/consul_exporter --consul.server=consul:8500\n...\n```", "```\nvagrant@consul:~$ curl -qs localhost:9107/metrics | grep \"^consul\"\nconsul_catalog_service_node_healthy{node=\"consul\",service_id=\"consul\",service_name=\"consul\"} 1\nconsul_catalog_services 1\nconsul_exporter_build_info{branch=\"HEAD\",goversion=\"go1.10.3\",revision=\"75f02d80bbe2191cd0af297bbf200a81cbe7aeb0\",version=\"0.4.0\"} 1\nconsul_health_node_status{check=\"serfHealth\",node=\"consul\",status=\"critical\"} 0\nconsul_health_node_status{check=\"serfHealth\",node=\"consul\",status=\"maintenance\"} 0\nconsul_health_node_status{check=\"serfHealth\",node=\"consul\",status=\"passing\"} 1\nconsul_health_node_status{check=\"serfHealth\",node=\"consul\",status=\"warning\"} 0\nconsul_raft_leader 1\nconsul_raft_peers 1\nconsul_serf_lan_members 1\nconsul_up 1\n```", "```\nexit\nvagrant ssh prometheus\n```", "```\nvagrant@prometheus:~$ cat /etc/prometheus/prometheus.yml \n...\n\n - job_name: 'consul_sd'\n consul_sd_configs:\n - server: http://consul:8500\n datacenter: dc1\n relabel_configs:\n - source_labels: [__meta_consul_service]\n target_label: job\n...\n```", "```\nvagrant@prometheus:~$ cat /vagrant/chapter12/configs/consul_exporter/payload.json \n{\n \"ID\": \"consul-exporter01\",\n \"Name\": \"consul-exporter\",\n \"Tags\": [\n \"consul\",\n \"exporter\",\n \"prometheus\"\n ],\n \"Address\": \"consul\",\n \"Port\": 9107\n}\n```", "```\nvagrant@prometheus:~$ curl --request PUT \\\n--data @/vagrant/chapter12/configs/consul_exporter/payload.json \\\nhttp://consul:8500/v1/agent/service/register\n```", "```\nvagrant@prometheus:~$ promtool query instant http://localhost:9090 'consul_catalog_services'\nconsul_catalog_services{instance=\"consul:9107\", job=\"consul-exporter\"} => 2 @[1555252393.681]\n```", "```\n...\n    relabel_configs: \n      - source_labels: [__meta_consul_tags]\n        regex: .*,exporter,.*\n        action: keep\n...\n```", "```\ncd ./chapter12/provision/kubernetes/\n```", "```\nminikube status\nminikube delete\n```", "```\nminikube start \\\n  --cpus=2 \\\n  --memory=3072 \\\n  --kubernetes-version=\"v1.14.0\" \\\n  --vm-driver=virtualbox \\\n  --extra-config=kubelet.authentication-token-webhook=true \\\n  --extra-config=kubelet.authorization-mode=Webhook\n```", "```\nkubectl apply -f ./bootstrap/\nkubectl rollout status deployment/prometheus-deployment -n monitoring\n```", "```\n$ cat bootstrap/03_prometheus-configmap.yaml\n...\ndata:\n prometheus.yml: |\n scrape_configs:\n...\n - job_name: kubernetes-pods\n kubernetes_sd_configs:\n - role: pod\n relabel_configs:\n - action: keep\n regex: hey\n source_labels:\n - __meta_kubernetes_pod_label_app\n...\n```", "```\nminikube service prometheus-service -n monitoring\n```", "```\nkubectl apply -f ./services/\nkubectl rollout status deployment/hey-deployment -n default\n```", "```\nminikube delete\n```", "```\n// Run starts a Discovery Manager and the custom service discovery implementation.\nfunc (a *Adapter) Run() {\n    go a.manager.Run()\n    a.manager.StartCustomProvider(a.ctx, a.name, a.disc)\n    go a.runCustomSD(a.ctx)\n}\n```", "```\ntype Discoverer interface {\n    Run(ctx context.Context, up chan<- []*targetgroup.Group)\n}\n```", "```\nfunc main() {\n...\n  sdAdapter := adapter.NewAdapter(ctx, *outputFile, \"exampleSD\", disc, logger)\n  sdAdapter.Run()\n...\n}\n```", "```\nvagrant ssh prometheus\n```", "```\nvagrant@prometheus:~$ sudo mv /etc/prometheus/prometheus_file_sd.yml /etc/prometheus/prometheus.yml\n```", "```\n- job_name: 'file_sd'\n    file_sd_configs:\n      - files:\n        - custom_file_sd.json\n```", "```\nvagrant@prometheus:~$ sudo systemctl reload prometheus\n```", "```\nvagrant@prometheus:~$ curl --request PUT \\\n--data @/vagrant/chapter12/configs/consul_exporter/payload.json \\\nhttp://consul:8500/v1/agent/service/register\n```", "```\nvagrant@prometheus:~$ sudo -u prometheus -- custom-sd --output.file=\"/etc/prometheus/custom_file_sd.json\" --listen.address=\"consul:8500\"\n```", "```\nvagrant@prometheus:~$ sudo cat /etc/prometheus/custom_file_sd.json \n[\n {\n \"targets\": [\"consul:9107\"],\n \"labels\": {\n \"__address__\": \"consul:9107\",\n \"__meta_consul_address\": \"192.168.42.11\",\n \"__meta_consul_network_segment\": \"\",\n \"__meta_consul_node\": \"consul\",\n \"__meta_consul_service_address\": \"consul\",\n \"__meta_consul_service_id\": \"consul-exporter01\",\n \"__meta_consul_service_port\": \"9107\",\n \"__meta_consul_tags\": \",consul,exporter,prometheus,\"\n }}]\n```"]