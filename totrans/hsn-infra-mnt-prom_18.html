<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Integrating Long-Term Storage with Prometheus</h1>
                </header>
            
            <article>
                
<p class="mce-root">The single-instance design of Prometheus makes it impractical to maintain large datasets of historical data, as it is limited by the amount of storage that's available locally. Having time series that span large periods allows seasonal trend analysis and capacity planning, and so, when the dataset doesn't fit into local storage, Prometheus provides this by pushing data to third-party clustered storage systems. In this chapter, we will look into remote read and write APIs, as well as shipping metrics for object storage with the help of Thanos. This will provide options on how to tackle this requirement, enabling several architecture choices.</p>
<p>In brief, the following topics will be covered in this chapter:</p>
<ul>
<li style="font-weight: 400">Test environment for this chapter</li>
<li style="font-weight: 400">Remote write and remote read</li>
<li style="font-weight: 400">Options for metrics storage</li>
<li style="font-weight: 400">Thanos remote storage and ecosystem</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Test environment for this chapter</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll be focusing on clustered storage. For this, we'll be deploying three instances to help simulate a scenario where Prometheus generates metrics and then we'll go through some options regarding how to store them on an object storage solution. This approach will allow us to not only explore the required configurations but also see how everything works together.</p>
<p>The setup we'll be using resembles the following diagram:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/cdf6ecf8-f54a-4bb0-9d8a-0f01dd3dc307.png" style="width:41.42em;height:23.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 14.1: Test environment for this chapter</div>
<p>In the next section, we will explain how to get the test environment up and running.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment</h1>
                </header>
            
            <article>
                
<p>To launch a new test environment, move into this path, relative to the repository root, shown as follows:</p>
<pre><strong>cd ./chapter14/</strong></pre>
<p>Ensure that no other test environments are running and spin up this chapter's environment<span>, shown as follows</span>:</p>
<pre><strong>vagrant global-status</strong><br/><strong>vagrant up</strong></pre>
<p>You can validate the successful deployment of the test environment using the following command:</p>
<pre><strong>vagrant status</strong></pre>
<p>This will output the following:</p>
<pre><strong>Current machine states:</strong><br/><br/><strong>prometheus                running (virtualbox)</strong><br/><strong>storage                   running (virtualbox)</strong><br/><strong>thanos                    running (virtualbox)</strong><br/><br/><strong>This environment represents multiple VMs. The VMs are all listed above with their current state. For more information about a specific VM, run `vagrant status NAME`.</strong></pre>
<p>When the deployment tasks end, you'll be able to validate the following endpoints on your host machine using your favorite JavaScript-enabled web browser:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Service</strong></p>
</td>
<td>
<p><strong>Endpoint</strong></p>
</td>
</tr>
<tr>
<td>
<p>Prometheus</p>
</td>
<td>
<p><kbd>http://192.168.42.10:9090</kbd></p>
</td>
</tr>
<tr>
<td>
<p>Thanos sidecar</p>
</td>
<td>
<p><kbd>http://192.168.42.10:10902</kbd></p>
</td>
</tr>
<tr>
<td>
<p>Object storage</p>
<p><kbd>Access key: strongACCESSkey</kbd></p>
<p><kbd>Secret key:  strongSECRETkey</kbd></p>
</td>
<td>
<p><kbd>http://192.168.42.11:9000</kbd></p>
</td>
</tr>
<tr>
<td>
<p>Thanos querier</p>
</td>
<td>
<p><kbd>http://192.168.42.12:10902</kbd></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>You should be able to access the desired instance by using one of the following commands:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Instance</strong></p>
</td>
<td>
<p><strong>Command</strong></p>
</td>
</tr>
<tr>
<td>
<p>Prometheus</p>
</td>
<td>
<p><kbd>vagrant ssh prometheus</kbd></p>
</td>
</tr>
<tr>
<td>
<p>Storage</p>
</td>
<td>
<p><kbd>vagrant ssh storage</kbd></p>
</td>
</tr>
<tr>
<td>
<p>Thanos</p>
</td>
<td>
<p><kbd>vagrant ssh thanos</kbd></p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cleanup</h1>
                </header>
            
            <article>
                
<p>When you've finished testing, just make sure that you're inside <kbd>./chapter14/</kbd> and execute the following command:</p>
<pre><strong>vagrant destroy -f</strong></pre>
<p>Don't worry too much; you can easily spin up the environment again if you need to.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Remote write and remote read</h1>
                </header>
            
            <article>
                
<p>Remote write and remote read allow Prometheus to push and pull samples, respectively: remote write is usually employed to implement remote storage strategies, while remote read allows PromQL queries to transparently target remote data. In the following topics, we'll go into each of these functionalities and present some examples of where they can be used.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Remote write</h1>
                </header>
            
            <article>
                
<p>Remote write was a very sought after feature for Prometheus. It was first implemented as native support for sending samples in the openTSDB, InfluxDB, and Graphite data formats. However, a decision was soon made to not support each possible remote system but instead provide a generic write mechanism that's suitable for building custom adapters. This enabled custom integrations decoupled from the Prometheus roadmap, while opening up the possibility of supporting the read path in those bridges as well. The system-specific implementations of remote write were removed from the Prometheus binary and converted into a standalone adapters as an example. The logic of relying on adapters and empowering the community so that it can build whatever integration is required follows the philosophy we discussed in <a href="5360e790-3884-4eeb-aaa1-8aad21dc6c1e.xhtml">Chapter 12</a>, <em>Choosing the Right Service Discovery</em>, for building custom service discovery integrations.</p>
<div class="packt_tip">Official examples of custom remote storage adapters can be found at <a href="https://github.com/prometheus/prometheus/tree/master/documentation/examples/remote_storage/remote_storage_adapter">https://github.com/prometheus/prometheus/tree/master/documentation/examples/remote_storage/remote_storage_adapter</a>.</div>
<p>Prometheus sends individual samples to remote write endpoints, using a very simple format which isn't tied to Prometheus internals. The system on the other end might not even be a storage system but a stream processor, such as Kafka or Riemann. This was a tough decision when defining the remote write design, as Prometheus already knew how to create efficient chunks and could just send those over the wire. Chunks would have made supporting streaming systems impractical, and sending samples is both easier to understand and easier to implement with regard to adapters.</p>
<p>Remote write was the target of a great enhancement with the release of Prometheus 2.8. Previously, when a metric failed to be delivered to a remote write endpoint (due to network or service issues) there was just a small buffer to store the data. If that buffer was filled, metrics would be dropped, and were permanently lost to those remote systems. Even worse, the buffer could create back-pressure and cause the Prometheus server to crash due to an <strong>Out of Memory</strong> (<strong>OOM</strong>) error. Since the remote write API started relying on the <strong>Write-Ahead Log</strong> (<strong>WAL</strong>) for bookkeeping, this doesn't happen anymore. Instead of using a buffer, the remote write now reads directly from the WAL, which has all transactions in flight and scraped samples. Using the WAL on the remote write subsystem makes Prometheus memory usage more predictable and allows it to resume from where it left off after a connectivity outage to the remote system.</p>
<p>Configuration-wise, the following snippet illustrates the minimal code required to set up a remote write endpoint in Prometheus:</p>
<pre>remote_write:<br/>  - url: http://example.com:8000/write</pre>
<p>Since remote write is another instance of interfacing with external systems, <kbd>external_labels</kbd> are also applied to samples before being sent. This can also prevent collision of metrics on the remote side when using more than one Prometheus server to push data to the same location. Remote write also supports <kbd>write_relabel_configs</kbd> to allow you to control which metrics are sent and which are dropped. This relabeling is run after external labels are applied.</p>
<p>Later in this chapter, we'll talk about a fairly new (and experimental) Thanos component called <strong>receiver</strong> as a practical example of remote write usage.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Remote read</h1>
                </header>
            
            <article>
                
<p>After the remote write feature was made available, requests for remote read started to flow. Imagine sending Prometheus data to a remote endpoint and then having to learn a new query language such as InfluxQL (the InfluxDB query language) to access the aforementioned data. The inclusion of this feature enables the transparent use of PromQL against data stored outside the Prometheus server as if it was locally available.</p>
<p>Queries run against remote data are centrally evaluated. This means that remote endpoints just send the data for the requested matchers and time ranges, and PromQL is applied in the Prometheus instance where the query originated. Once again, choosing centralized (as opposed to distributed) query evaluation was a critical decision at the time of designing the API. Distributed evaluation could have spread the load of each query, but would force the remote system to understand and evaluate PromQL and handle numerous corner cases when data is non-disjoint, greatly increasing the <span>implementation </span>complexity of <span>the aforementioned</span> systems. Centralized evaluation also allows the remote systems to  downsample the requested data, which greatly improves queries with very long time ranges.</p>
<p>One example where remote read can be useful is assisting migration between major versions of Prometheus, such as going from Prometheus <em>v1</em> to Prometheus <em>v2</em>. The latter can be configured to a remote read from the former, thus making the old instance read-only (no scrape jobs configured). This uses the <em>v1</em> instance as glorified remote storage until its metrics are no longer useful. A common gotcha that trips the implementer of this strategy is the fact that <kbd>external_labels</kbd> from the Prometheus instance configured with remote read need to have a match in the <kbd>external_labels</kbd> from the Prometheus instance being read.</p>
<p>On the flipside, an example of the remote read endpoint in Prometheus itself was seen in the previous chapter (<a href="3615e455-c55b-49af-8c9b-b5342d4859dd.xhtml">Chapter 13</a>, <em>Scaling and Federating Prometheus</em>): the Thanos sidecar uses the remote read API from the local Prometheus instance to get the time series data requested by the Thanos querier.</p>
<p>Configuration-wise, it is quite simple to set up Prometheus to do a remote read. The following snippet shows the required section of the configuration file:</p>
<pre>remote_read:<br/>  - url: http://example.com:8000/read</pre>
<p>The <kbd>remote_read</kbd> section also allows you to specify a list of matchers using <kbd>required_matchers</kbd> that need to be present in a selector to query a given endpoint. This is useful for remote systems that aren't storage or when you only write a subset of your metrics to remote storage, thus needing to restrict remote reads to <span>those metrics.</span> </p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Options for metrics storage</h1>
                </header>
            
            <article>
                
<p>By default, Prometheus does a great job of managing local storage of metrics using its own TSDB. But there are cases where this is not enough: local storage is limited by the amount of disk space available locally to the Prometheus instance, which isn't ideal for large retention periods, such as years, and large data volumes that go beyond the amount of disk space that is feasible to have attached to the instance. In the following sections, we'll be discussing the local storage approach, as well as the currently available options for remote storage.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Local storage</h1>
                </header>
            
            <article>
                
<p>Prometheus' out-of-the-box storage solution for time series data is simply local storage. It is simpler to understand and simpler to manage: the database lives in a single directory, which is easy to back up, restore, or destroy if needed. By avoiding clustering, Prometheus ensures sane behavior when facing network partitions; you don't want your monitoring system to fail when you need it the most. High availability is commonly achieved by simply running two Prometheus instances with the same configuration, each with its own database. This storage solution, however, does not cover all uses cases, and has a few shortcomings:</p>
<ul>
<li>It's not durable <span>– i</span>n a container orchestration deployment, the collected data will disappear when the container is rescheduled (as the previous data is destroyed and the current data is started afresh) if persistent volumes are not used, while in a VM deployment, the data will be as durable as the local disk.</li>
<li style="font-weight: 400">It's not horizontally scalable <span>– </span>using local storage means that your dataset can only be as big as the disk space you can make available to the instance.</li>
<li>It wasn't designed for long-term retention, even though, with the right metric criteria and cardinality control, commodity storage will go a long way.</li>
</ul>
<p>These shortcomings are the result of trade-offs that are made to ensure that small and medium deployments (which are by far the more common use cases) work great while also making advanced and large-scale use cases possible. Alerting and dashboards, in the context of day-to-day operational monitoring or troubleshooting ongoing incidents, only require a couple of weeks of data at most.</p>
<p class="mce-root"/>
<p>Before going all out for a remote metric storage system for long-term retention, we might consider managing local storage through the use of TSDB admin API endpoints, that is, <kbd>snapshot</kbd> and <kbd>delete_series</kbd>. These endpoints help keep local storage under control. As we mentioned in <a href="12e775c2-bee9-4ebe-ad73-2f9313eeeeee.xhtml">Chapter 5</a>, <em>Running a Prometheus Server</em>, the TSDB administration API is not available by default; Prometheus needs to be started with the <kbd>--web.enable-admin-api</kbd> flag so that the API is enabled.</p>
<p>In this chapter's test environment, you can try using these endpoints and evaluate what they aim to accomplish. By connecting to the <kbd>prometheus</kbd> instance, we can validate that the TSDB admin API has been enabled, and look up the local storage path by using the following command:</p>
<pre>vagrant@prometheus:~$ <strong>systemctl cat prometheus</strong><br/>...<br/>    --storage.tsdb.path=/var/lib/prometheus/data \<br/>    --web.enable-admin-api \<br/>...</pre>
<p>Issuing an HTTP <kbd>POST</kbd> to the <kbd>/api/v1/admin/tsdb/snapshot</kbd> endpoint will trigger a new snapshot that will store the available blocks in a snapshots directory. Snapshots are made using hard links, which makes them very space-efficient as long as Prometheus still has those blocks.  The following instructions illustrate how everything is processed:</p>
<pre>vagrant@prometheus:~$ <strong>curl -X POST http://localhost:9090/api/v1/admin/tsdb/snapshot</strong><br/>{"status":"success","data":{"name":"20190501T155805Z-55d3ca981623fa5b"}}<br/><br/>vagrant@prometheus:~$ <strong>ls /var/lib/prometheus/data/snapshots/</strong><br/>20190501T155805Z-55d3ca981623fa5b</pre>
<p>You can then back up the snapshot directory, which can be used as the TSDB storage path for another Prometheus instance through <kbd>--storage.tsdb.path</kbd> when that historical data is required for querying. Note that <kbd>--storage.tsdb.retention.time</kbd> might need to be adjusted to your data duration, as Prometheus might start deleting blocks outside the retention period.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This, of course, will not prevent the growth of the TSDB. To manage this aspect, we can employ the <kbd>/api/v1/admin/tsdb/delete_series</kbd> endpoint, which is useful for weekly or even daily maintenance. It operates by means of an HTTP POST request with a set of match selectors that mark all matching time series for deletion, optionally restricting the deletion to a given time window if a time range is also sent. The following table provides an overview of the URL parameters in question:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>URL parameters</strong></p>
</td>
<td>
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>match[]=&lt;selector&gt;</kbd></p>
</td>
<td>
<p>One or more match selectors, for example, <kbd>match[]={__name__=~"go_.*"}</kbd></p>
<p>(Deletes all metrics whose name starts with <kbd>go_</kbd>)</p>
</td>
</tr>
<tr>
<td>
<p><kbd>start=&lt;unix_timestamp&gt;</kbd></p>
</td>
<td>
<p>Start time for the deletion in the RFC 3339 or Unix format (optional, defaults to the earliest possible time)</p>
</td>
</tr>
<tr>
<td>
<p><kbd>end=&lt;unix_timestamp&gt;</kbd></p>
</td>
<td>
<p>End time for the deletion in RFC 3339 or Unix format (optional, defaults to the latest possible time)</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>After the <kbd>POST</kbd> request is performed, an HTTP <kbd>204</kbd> is returned. This will not immediately free up disk space, as it will have to wait until the next Prometheus compaction event. You can force this cleanup by requesting the <kbd>clean_tombstones</kbd> endpoint, as exemplified in the following instructions:</p>
<pre>vagrant@prometheus:~$ <strong>curl -X POST -w "%{http_code}\n" --globoff 'http://localhost:9090/api/v1/admin/tsdb/delete_series?match[]={__name__=~"go_.*"}'</strong><br/>204<br/><br/>vagrant@prometheus:~$ <strong>curl -X POST -w "%{http_code}\n" http://localhost:9090/api/v1/admin/tsdb/clean_tombstones</strong><br/>204</pre>
<p>This knowledge might help you keep local storage under control and avoid stepping into complex and time-consuming alternatives, when the concern is mostly around scalability.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Remote storage integrations</h1>
                </header>
            
            <article>
                
<p>Opting for remote metric storage shouldn't be taken lightly as it has several implications. Some factors to consider when choosing a remote storage solution, to name but a few, are as follows:</p>
<ul>
<li><strong>Maturity</strong>: Some storage solutions are more mature and better maintained than others.</li>
<li style="font-weight: 400"><strong>Control</strong>: There are some solutions where you run your own instances, while others are SaaS offerings.</li>
<li style="font-weight: 400"><strong>Availability, reliability and scalability</strong>: If you choose to manage the storage solution internally, you need to consider these aspects.</li>
<li style="font-weight: 400"><strong>Maintainability</strong>: Some options are truly complex to deploy and/or maintain.</li>
<li style="font-weight: 400"><strong>Remote read and write</strong>: Do you truly need both, or does write suffice for your use case?</li>
<li style="font-weight: 400"><strong>Cost</strong>: It might all come down to this; we define cost not only in the monetary sense, but also in terms of the time required to learn, test, and operate a solution.</li>
</ul>
<p>Another critical factor to consider relates to alerting. For reliability, rules should only query local data; this prevents transient failures in the network layer to negatively affect rule evaluation. As such, data in remote storage systems shouldn't be used for critical alerting, or at least you should be able to tolerate them being missing.</p>
<p>If the scale you're working with demands scalable storage or if historical data is crucial for your use case, for example, for capacity planning, then there are a few options available. The official Prometheus documentation has a comprehensive list of known remote storage integrations, available at <a href="https://prometheus.io/docs/operating/integrations/#remote-endpoints-and-storage">https://prometheus.io/docs/operating/integrations/#remote-endpoints-and-storage</a>. This list has integrations for several different use cases: SaaS offerings (such as SignalFX and Splunk); a stream processing system (Kafka); different time series databases <span>–</span> both paid (IRONdb) and open source (InfluxDB, Cortex, TimescaleDB, M3DB, to name a few); other monitoring systems (OpenTSDB, Graphite); and even generic datastores (such as Elasticsearch and TiKV). A good portion of them support both remote read and write. Some of them deserve an entire book of their own.</p>
<p>Curiously, the solution we are going to explore in depth isn't in the aforementioned list at the time of writing, as it uses an entirely different approach to the problem we're dealing with. In fact, Prometheus doesn't even need to know about it because it works like an overlay. We are going to focus on the most promising long-term storage solution, which beautifully balances complexity, cost, and the feature set: Thanos.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Thanos remote storage and ecosystem</h1>
                </header>
            
            <article>
                
<p>In <a href="3615e455-c55b-49af-8c9b-b5342d4859dd.xhtml">Chapter 13</a>, <em>Scaling and Federating Prometheus</em>, we were introduced to Thanos, an open source project that was created to improve upon some of the shortcomings of Prometheus at scale. Specifically, we went through how Thanos solves having a global view of several Prometheus instances using the Thanos sidecar and querier components. It's now time to meet other Thanos components and explore how they work together to enable cheap long-term retention using object storage. Keep in mind that complexity will increase when going down this path, so validate your requirements and whether the global view approach and local storage aren't enough for your particular use case.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Thanos ecosystem</h1>
                </header>
            
            <article>
                
<p>Besides the Thanos querier and sidecar, which we covered previously, there are a few other components in the Thanos ecosystem. All of these components coexist in the same binary and are run by invoking different sub-commands, which we'll enumerate later:</p>
<ul>
<li><kbd>query</kbd>: Commonly known as <em>querier</em>, it's a daemon that's responsible for fanning out queries and deduplicating results to configured StoreAPI endpoints</li>
<li><kbd>sidecar</kbd>: A daemon which exposes a StoreAPI endpoint for accessing the data from a local Prometheus instance and ships <span>the aforementioned</span> instance's TSDB blocks to object storage</li>
<li><kbd>store</kbd>: A daemon which acts as a gateway for remote storage, exposing a StoreAPI endpoint</li>
<li><kbd>compact</kbd>: Commonly known as <em>compactor,</em> this daemon is responsible for compacting blocks that are available in object storage and creating new downsampled time series</li>
<li><kbd>bucket</kbd>: A command line-tool that can verify, recover, and inspect data stored in object storage</li>
<li><kbd>receive</kbd>: Known as <em>receiver</em>, it's a daemon that accepts remote writes from Prometheus instances, exposing pushed data through a StoreAPI endpoint, and can ship blocks to object storage</li>
<li><kbd>rule</kbd>: Commonly known as <em>ruler</em>, it's a daemon that evaluates Prometheus rules (recording and alerting) against remote StoreAPI endpoints, exposes its own StoreAPI to make evaluation results available for querying, ships results to object storage, and connects to an Alertmanager cluster to send alerts</li>
</ul>
<div class="packt_infobox">You can find all the source code and installation files for Thanos at <a href="https://github.com/improbable-eng/thanos">https://github.com/improbable-eng/thanos</a>.</div>
<p>All of the following components work together to solve several challenges:</p>
<ul>
<li><strong>Global view</strong>: Querying every Prometheus instance from the same place, while aggregating and deduplicating the returned time series.</li>
<li><strong>Downsampling</strong>: Querying months or even years of data is a problem if samples come at full resolution; by automatically creating downsampled data, queries that span large time periods become feasible.</li>
<li><strong>Rules</strong>: Enables the creation of global alerts and recording rules that mix metrics from different Prometheus shards.</li>
<li><strong>Long-term retention</strong>: By leveraging object storage, it delegates durability, reliability, and scalability concerns of storage to outside the monitoring stack.</li>
</ul>
<p>While we'll be providing a glimpse of how these challenges are tackled with Thanos, our main focus will be on the long-term storage aspect of it.</p>
<p>Storage-wise, the Thanos project settled on object storage for long-term data. Most cloud providers provide this service, with the added benefit of also ensuring <strong>service-level agreements</strong> (<strong>SLA</strong>) for it. Object storage usually has 99.999999999% durability and 99.99% availability on any of the top cloud providers. If you have an on-premise infrastructure, there are also some options available: using Swift (the OpenStack component that provides object storage APIs), or even the MinIO project, which we use in this chapter's test environment. Most of these on-premise object storage solutions share the same characteristic:<span> </span>they provide APIs that are modeled to mimic the well-known AWS S3 due to so many tools supporting it. Moreover, object storage from cloud providers is typically a very cost-effective solution.</p>
<p>The following diagram provides a simple overview of the core components that are needed to achieve long-term retention of Prometheus time series data using Thanos:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/94916cc1-d386-4b83-8afb-3e7440cc68a5.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 14.2: High-level Thanos long-term storage architecture</div>
<p>As we can see in the previous diagram, we only need some Thanos components to tackle this challenge. In the following topics, we'll go over every component and expand on its role in the overall design.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Thanos components</h1>
                </header>
            
            <article>
                
<p>Now that we've seen an overview of the Thanos long-term storage architecture, it's time to meet all the available Thanos components. Besides providing an introduction to every single one of them, we'll be emphasizing those that are available in the test environment, expanding on their role in the ecosystem, and the configuration which is in place.</p>
<p class="mce-root"/>
<div class="packt_tip">You can find some community-driven alerts and dashboards at <a href="https://github.com/improbable-eng/thanos/tree/master/examples">https://github.com/improbable-eng/thanos/tree/master/examples</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Test environment specifics</h1>
                </header>
            
            <article>
                
<p>As stated at the beginning of this book, using the provided test environments won't incur any cost. As such, since we require an object storage bucket for our examples, we relied on a project called MinIO, which exposes an S3-compatible API;<span> </span>you can find more information about it at <a href="https://min.io/">https://min.io/</a>. Configuration-wise, this storage endpoint should be available in the storage instance with the following settings:</p>
<pre>vagrant@storage:~$ <strong>systemctl cat minio</strong><br/>...<br/>EnvironmentFile=<strong>/etc/default/minio</strong><br/>ExecStart=/usr/bin/minio server $MINIO_OPTS $MINIO_VOLUMES<br/>...</pre>
<p>The preceding <kbd>systemd</kbd> unit file loads the following environment variables:</p>
<pre>vagrant@storage:~$ sudo cat <strong>/etc/default/minio</strong> <br/>MINIO_VOLUMES='/var/lib/minio/data'<br/>MINIO_OPTS='--address :9000'<br/>MINIO_ACCESS_KEY='<strong>strongACCESSkey</strong>'<br/>MINIO_SECRET_KEY='<strong>strongSECRETkey</strong>'</pre>
<p>To ensure that you don't have to wait two hours to have blocks to ship into the object storage, the Prometheus server in our test environment has the following settings in place:</p>
<pre>vagrant@prometheus:~$ <strong>systemctl cat prometheus</strong><br/>...<br/>    --storage.tsdb.max-block-duration=10m \<br/>    --storage.tsdb.min-block-duration=10m<br/>...</pre>
<p>This configuration changes the block duration, setting the interval at which to flush them to disk, from the default two hours to ten minutes. Although setting such a low value is very useful for testing this particular feature, it is completely ill-advised for anything else. To make this crystal clear, there's no good reason to change these values to anything other than two hours, except for testing.</p>
<p>With the specifics for this chapter's test environment out of the way, we can now proceed to introduce each individual Thanos component.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Thanos query</h1>
                </header>
            
            <article>
                
<p>In <a href="3615e455-c55b-49af-8c9b-b5342d4859dd.xhtml">Chapter 13</a>,<em> Scaling and Federating Prometheus</em>, we had the chance to explore the Thanos querier and sidecar to solve the global view problem. For this component, the features we covered then will also be used in this chapter. We'll continue using <span><span>querier</span></span> to query several StoreAPI endpoints, taking advantage of the deduplication it provides, and using the query API through its web interface.</p>
<p>The configuration that's available in our test environment is quite simple, as we can see in the following snippet, which was taken from the <kbd>thanos</kbd> instance:</p>
<pre>vagrant@thanos:~$ <strong>systemctl cat thanos-query</strong><br/>...<br/>ExecStart=/usr/bin/thanos query \<br/>            --query.replica-label replica \<br/>            --store "prometheus:10901" \<br/>            --store "thanos:11901" \<br/>            --store "thanos:12901"<br/>...</pre>
<p>As you can see in the previous snippet, we specified several <kbd>--store</kbd> endpoints. To understand which is which, we can point our browser at the Thanos querier web interface, available at <a href="http://192.168.42.12:10902/stores">http://192.168.42.12:10902/stores</a>, and see the available stores, as depicted by following screenshot:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/19cc7e59-1495-4f5f-a41a-4b6d25479ee0.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 14.3: Thanos querier /stores endpoint</div>
<p>The previous screenshot illustrates all the available store APIs in our test environment. These will be the sources of data that will be used any time you execute a query in the Thanos querier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Thanos sidecar</h1>
                </header>
            
            <article>
                
<p>Besides the Store API that's exposed by the Thanos sidecar we covered previously, this component is also is able to gather TSDB blocks from disk and ship them to an object storage bucket. This allows you to decrease the retention of the Prometheus server by keeping historical data in a durable medium. In order to use the block upload feature in the sidecar, the <kbd>--storage.tsdb.min-block-duration</kbd> and <kbd>--storage.tsdb.max-block-duration</kbd> flags need to be set to the same value (two hours to match the default behavior), so that Prometheus local compaction is disabled.</p>
<p>The configuration in use is available on the <kbd>prometheus</kbd> instance and can be inspected by executing the following instruction:</p>
<pre>vagrant@prometheus:~$ <strong>systemctl cat thanos-sidecar</strong><br/>...<br/>ExecStart=/usr/bin/thanos sidecar \<br/>           --log.level debug \<br/>           --prometheus.url "http://localhost:9090" \<br/>           --tsdb.path "/var/lib/prometheus/data" \<br/>           --objstore.config-file "<strong>/etc/thanos/storage.yml</strong>"<br/>...</pre>
<p>As we can see, the <kbd>--objstore.config-file</kbd> flag loads all the required configuration from the file in order to ship the TSDB blocks to an object storage bucket, such as the bucket name (in our case, <kbd>thanos</kbd>), the storage endpoint, and access credentials. The following are the contents of that file:</p>
<pre>vagrant@prometheus:~$ sudo cat <strong>/etc/thanos/storage.yml</strong><br/>type: S3<br/>config:<br/>  bucket: '<strong>thanos</strong>'<br/>  endpoint: 'storage:9000'<br/>  insecure: true<br/>  signature_version2: true<br/>  access_key: 'strongACCESSkey'<br/>  secret_key: 'strongSECRETkey'</pre>
<p>In our Prometheus test instance, a new TSDB block will be generated every 10 minutes, and the Thanos sidecar will take care of shipping it to the object storage endpoint. We can review the available blocks in our bucket by using MinIO's web interface, available at <a href="http://192.168.42.11:9000/minio/thanos/">http://192.168.42.11:9000/minio/thanos/</a>. After logging in with the <kbd>access_key</kbd> and <kbd>secret_key</kbd> shown in the previous code snippet, you'll be greeted with something resembling the following screenshot:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/c3a97419-6648-445d-8fcc-3dd614274523.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 14.4: MinIO object storage web interface</div>
<p>We should now have some historical data available for testing. We'll need a way to query this data. That's where the Thanos store gateway comes into play.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Thanos store gateway</h1>
                </header>
            
            <article>
                
<p>The Thanos store gateway's main function is to provide access to the historical time series data in the blocks that are shipped to object storage through a StoreAPI endpoint. This means that it effectively acts as an API gateway. All major object store integrations in Thanos store (Google Cloud Storage, AWS S3, Azure Storage) are considered stable enough to run in production. It uses a relatively small local cache of all block metadata, and it keeps it in sync with the storage bucket.</p>
<p>A minimal configuration for this component is available in the Thanos instance in our test environment. The following is the snippet from it:</p>
<pre>vagrant@thanos:~$ <strong>systemctl cat thanos-store</strong><br/>...<br/>ExecStart=/usr/bin/thanos store \<br/>           --data-dir "/var/lib/thanos/store" \<br/>           --objstore.config-file "/etc/thanos/storage.yml" \<br/>           --grpc-address "0.0.0.0:11901" \<br/>           --http-address "0.0.0.0:11902"<br/>...</pre>
<p>As we can see, object storage configuration is done in its own configuration file. Like most of the other components, a store binds a StoreAPI GRPC port for receiving queries and an HTTP port so that its metrics can be collected by Prometheus.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Thanos compact</h1>
                </header>
            
            <article>
                
<p>Since Prometheus block compaction needs to be turned off for Thanos sidecar upload feature to work reliably, this work is delegated to a different component: Thanos compact. It was designed to use the same compaction strategy as the Prometheus storage engine itself, but for blocks in object storage instead. Since compaction cannot be done directly in object storage, this component requires a fair amount of available space (a few hundred GB, depending on the amount stored remotely) in local disks to process the blocks.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Another important function Thanos compact performs is creating downsampled samples. The biggest advantage of downsampling is querying large time ranges reliably, without needing to pull an overwhelming amount of data. The usage of <kbd>*_over_time</kbd> functions (as discussed in <a href="205ddb34-6ee8-4e22-b80f-39d5b2198c29.xhtml">Chapter 7</a>,<em> Prometheus Query Language <span>–</span> PromQL</em>) is also highly recommended when using downsampled data as the method that's used to downsample does not merely remove samples but also pre-aggregates them using five different aggregation functions. This means that five new time series for each raw series. Something very important to keep in mind is that full resolution data is only downsampled to a five minute resolution after 40 hours. Similarly, one hour's downsampled data is only created after 10 days by using the previously downsampled data with five minute resolution as the source. Keeping the raw data might be useful for zooming into a specific event in time, which you wouldn't be able to do with just downsampled data. There are three flags for managing the retention of data (that is, how long to keep it) in raw, five minute, and one hour form, as shown in the following table:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Flag</strong></p>
</td>
<td>
<p><strong>Duration</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>--retention.resolution-raw</kbd></p>
</td>
<td>
<p>The duration keeps data with a raw resolution in the object storage bucket, for example, 365d (it defaults to 0d, which means forever)</p>
</td>
</tr>
<tr>
<td>
<p><kbd>--retention.resolution-5m</kbd></p>
</td>
<td>
<p>The duration to keep data with 5-minute in the object storage bucket, for example, 365d (it defaults to 0d, which means forever)</p>
</td>
</tr>
<tr>
<td>
<p><kbd>--retention.resolution-1h</kbd></p>
</td>
<td>
<p>The duration to keep data with 1-hour in the object storage bucket, for example, 365d (it defaults to 0d, which means forever)</p>
</td>
</tr>
</tbody>
</table>
<div class="mce-root packt_infobox">Each storage bucket should only have one Thanos compactor associated with it, as it's not designed to run concurrently.</div>
<p>When considering retention policies, bear in mind that, as the first downsampling step aggregates five minutes-worth of data and the aggregation produces five new time series, you’d need to have a scrape interval lower than one minute to actually save space (the number of samples in the interval needs to be higher than the samples produced by the aggregation step).</p>
<p class="mce-root"/>
<p>The compactor can either be run as a daemon which springs to action whenever it's needed, or as single-shot job, exiting at the end of the run. In our test environment, we have a Thanos compactor running in the <kbd>thanos</kbd> instance to manage our object storage bucket. It's running as a service (using the <kbd>--wait</kbd> flag) to make the test environment simpler. The configuration being used is shown in the following snippet:</p>
<pre>vagrant@thanos:~$ <strong>systemctl cat thanos-compact</strong><br/>...<br/>ExecStart=/usr/bin/thanos compact \<br/>           --data-dir "/var/lib/thanos/compact" \<br/>           --objstore.config-file "/etc/thanos/storage.yml" \<br/>           --http-address "0.0.0.0:13902" \<br/>           <strong>--wait</strong> \<br/>           --retention.resolution-raw 0d \<br/>           --retention.resolution-5m 0d \<br/>           --retention.resolution-1h 0d<br/>...</pre>
<p>Just like the other components, the HTTP endpoint is useful for scraping metrics from it. As can be seen in the <kbd>retention.*</kbd> flags, we're keeping the data in all the available resolutions forever. We'll be discussing Thanos bucket next, a debugging tool that helps inspect Thanos-managed storage buckets.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Thanos bucket</h1>
                </header>
            
            <article>
                
<p>This component of the Thanos ecosystem is responsible for verifying, repairing, listing, and inspecting blocks in object storage. Unlike other components, this one behaves as a command-line tool instead of a daemon.</p>
<p>You can try out the following example of its usage: listing the available blocks in our object storage bucket:</p>
<pre>vagrant@thanos:~$ <strong>sudo thanos bucket ls -o wide --objstore.config-file=/etc/thanos/storage.yml</strong><br/>01D9SN3KEBNCB2MHASYXSDF1DE -- 2019-05-01 12:00 - 2019-05-01 12:10 Diff: 10m0s, Compaction: 1, Downsample: 0, Source: sidecar<br/>01D9SNNXCAXWKZ0EH6118FTHSS -- 2019-05-01 12:10 - 2019-05-01 12:20 Diff: 10m0s, Compaction: 1, Downsample: 0, Source: sidecar<br/>01D9SP87A9NZ9DE35TC2QNS7ZZ -- 2019-05-01 12:20 - 2019-05-01 12:30 Diff: 10m0s, Compaction: 1, Downsample: 0, Source: sidecar<br/>01D9SPTH88G9TR4503C4763TDN -- 2019-05-01 12:30 - 2019-05-01 12:40 Diff: 10m0s, Compaction: 1, Downsample: 0, Source: sidecar<br/>01D9SQCV68KVE7CXK4QDW9RWM1 -- 2019-05-01 12:40 - 2019-05-01 12:50 Diff: 10m0s, Compaction: 1, Downsample: 0, Source: sidecar<br/>01D9SQN6TVJ97NP4K82APW7YH9 -- 2019-05-01 10:51 - 2019-05-01 11:50 Diff: 58m41s, <strong>Compaction: 2</strong>, Downsample: 0, <strong>Source: compactor</strong></pre>
<p>This tool is very useful for troubleshooting issues and quickly understanding the state of blocks in the storage bucket.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Thanos receive</h1>
                </header>
            
            <article>
                
<p>This component, at the time of writing, is still very experimental. However, as promised in the <em>Remote write and read</em> section of this chapter, this component is an excellent example of what remote write is all about, so we decided to show you what it can do. It acts as a target for Prometheus remote write requests, and stores received samples locally. The receiver also implements a StoreAPI endpoint, so it has the ability to act as a store node. Finally, it can also ship blocks to object storage, just like sidecar.</p>
<p>To make more sense of what all of this means, let's explore two scenarios.</p>
<p>A Prometheus server will, by default, generate a block every two hours. Even using Thanos sidecar with block shipping, there's no way for Thanos querier to get to data that is still in the WAL without sidecar having to request it via a remote read API from the Prometheus server. When you get to a scale where Grafana, or another API client, generates a huge request rate against Prometheus, the odds are that it will impact Prometheus' performance and eventually affect alerting, even though Prometheus has some protections mechanisms, as we've seen previously. By using Thanos receiver, you can simply move the client queries to it, ensuring that the Prometheus server's main job is scraping and evaluating rules. In a nutshell, you will effectively be separating the reads from the writes of the Prometheus server. You could continue to use Thanos sidecar just to ship blocks and Thanos receiver to answer all Thanos querier inquiries, just like a fresh cache, protecting the Prometheus write path in the process.</p>
<div class="packt_tip">Thanos receiver generates blocks every two hours and, unlike Prometheus, this value is hardcoded by design.</div>
<p>Imagine another scenario where there are several tenants/teams using the infrastructure you're managing. If they're technically savvy enough, they'll eventually want to manage their own Prometheus servers. You could try to provide all the automation that's required to manage their servers, but you'll quickly encounter a bottleneck. One option would be to give them a Prometheus server to manage, a Thanos receiver endpoint, and a Thanos store endpoint. Thanos receiver would take care of shipping the blocks to object storage, and Thanos store would provide a way to access them, abstracting the complexity of remote storage from the tenant altogether. This would just be the first step in providing long-term storage as a service for your infrastructure.</p>
<p>In our test environment, in the <kbd>thanos</kbd> instance, we have a Thanos receiver running. In the following snippet, we can see its configuration:</p>
<pre>vagrant@thanos:~$ <strong>systemctl cat thanos-receive</strong><br/>...<br/>ExecStart=/usr/bin/thanos receive \<br/>           --tsdb.path "/var/lib/thanos/receive" \<br/>           --tsdb.retention 6h \<br/>           --labels "<strong>store=\"receiver\"</strong>" \<br/>           --remote-write.address "0.0.0.0:<strong>19291</strong>" \<br/>           --grpc-address "0.0.0.0:12901" \<br/>           --http-address "0.0.0.0:12902"<br/>...</pre>
<p>Since we already have a Thanos sidecar running alongside our Prometheus server, which is sending TSDB blocks to object storage, we disabled the shipping functionality of the receiver just by not adding the <kbd>--objstore.config-file</kbd> flag. Notice the <kbd>--labels</kbd> flag, which allows us to specify a label to add to all the time series exposed by this receiver's StoreAPI; this is effectively a way to configure external labels. Another noteworthy flag is <kbd>--remote-write.address</kbd>, which is used to provide the remote write endpoint. If we look into the <kbd>prometheus</kbd> instance, we will see the following configuration, which takes advantage of the aforementioned flag:</p>
<pre>vagrant@prometheus:~$ cat <strong>/etc/prometheus/prometheus.yml</strong> <br/>...<br/>remote_write:<br/>  - url: http://thanos:<strong>19291</strong>/api/v1/receive<br/>...</pre>
<p>To test all of this, we can simply stop the Thanos sidecar in the <kbd>prometheus</kbd> instance, as follows:</p>
<pre>vagrant@prometheus:~$ <strong>sudo systemctl stop thanos-sidecar</strong></pre>
<p>After doing this, Thanos querier will no longer be able to access Thanos sidecar and the recent block information won't be flushed to disk. This way, we can validate whether the receiver will provide this data. If we go over to the Thanos querier web interface at <a href="http://192.168.42.12:10902/graph">http://192.168.42.12:10902/graph</a> and run an instant query such as <kbd>up{instance=~"prometheus.+"}</kbd>, we are presented with the following output:</p>
<p class="mce-root"/>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/926c492e-582f-4f38-b16e-1c524b097a11.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 14.5: Thanos receiver showing metrics, even with the Thanos sidecar down</div>
<p>Note the <kbd>store</kbd> label, indicating that Thanos receiver is providing data while also informing us that Thanos sidecar is currently down. This proves that we can query recent data thanks to the Prometheus remote write API.</p>
<p>The decision to use this component shouldn't be taken lightly, as it has some disadvantages: besides the fact that it is clearly marked as experimental, it is effectively a push-based system, like Graphite. This means that all the downsides from push-based approaches apply, namely difficulty in managing abusive/rogue clients.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Thanos rule</h1>
                </header>
            
            <article>
                
<p>This component allows you to run Prometheus-compatible rules (recording/alerting) against remote query endpoints, such as Thanos querier or sidecar. It exposes a StoreAPI endpoint to make the results of rule evaluations available, which are stored in local blocks, and can also ship these TSDB blocks to object storage. It's tempting to imagine using this component as a solution for the centralized management of rules instead of spreading them across multiple Prometheus instances, but that's not its purpose and would be ill-advised. Throughout this book, we have stressed how critical rules are, especially for alerting. We also underlined the importance of running those rules locally in the Prometheus instance that has the required metrics. We've provided alternatives, such as a hierarchical or cross-service federation, for gathering metrics from different Prometheus instances. By using the Thanos ruler for alerting, you would be adding a number of points of failure to the critical path: other Thanos components, the network, and, in the worst case, object storage. Alerting needs to be predictable and reliable, so you can have a high level of confidence that it will work when you need it the most. Though Thanos ruler can have a legitimate set of use cases, it shouldn't be considered for most alerting needs. Nonetheless, it's important to acknowledge its existence.</p>
<div class="packt_tip">More information regarding the Thanos ruler can be found at <a href="https://thanos.io/components/rule.md/">https://thanos.io/components/rule.md</a>.</div>
<p>We now have a complete overview of the Thanos ecosystem and how it's configured on the test environment. We invite you to experiment with all the components while evaluating their behavior: for example, stopping all the store APIs except the Thanos store or using the Thanos bucket to understand what data is available in the object storage bucket.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we were introduced to remote read and remote write endpoints. We learned how the recent remote write strategy using WAL is so important for the global performance and availability of Prometheus. Then, we explored some alternatives for keeping Prometheus local storage under control, while explaining the implications of opting for a long-term storage solution. Finally, we delved into Thanos, exposing some of its design decisions and introducing the complete ecosystem of components, providing practical examples showing how all the different pieces work together. With this, we can now build a long-term storage solution for Prometheus if we need to.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What are the main advantages of a remote write based on WAL?</li>
<li>How can you perform a backup of a running Prometheus server?</li>
<li>Can the disk space of a Prometheus server be freed at runtime? If so, how?</li>
<li>What are the main advantages of Thanos using object storage?</li>
<li>Does it make sense to keep data in all available resolutions?</li>
<li>What is the role of Thanos store?</li>
<li>How can you inspect the data that's available in object storage using Thanos?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><strong>Prometheus HTTP API</strong>: <a href="https://prometheus.io/docs/prometheus/latest/querying/api/">https://prometheus.io/docs/prometheus/latest/querying/api/</a></li>
<li><strong>Thanos official website</strong>: <a href="https://thanos.io/">https://thanos.io</a></li>
</ul>


            </article>

            
        </section>
    </body></html>