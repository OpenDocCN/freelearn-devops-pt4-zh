<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Dealing with Databases in DevOps Scenarios</h1>
                </header>
            
            <article>
                
<p>In the previous chapters, you have learned about the continuous integration and continuous deployment of your software. You also learned how the same principles can be applied to the delivery of configuration in infrastructure. Once you have adopted these principles and start increasing the flow of value delivery, you might run into another challenge: managing your database schema changes.</p>
<p>Applying DevOps to databases can feel like trying to change the tires on a running car. You must find some way of coordinating changes between database schema and application code without taking the system down for maintenance.</p>
<p>In this chapter, you will learn about different approaches for doing just that: managing these schema changes while avoiding downtime. With proper planning and a disciplined approach, this can be achieved in a way that manages risks well. You will see how you can treat your database schema as code, and you will learn about the different approaches that are available to do so. You will also see another approach that avoids database schemas altogether, namely, going schema-less.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Managing a database schema as code</li>
<li>Applying database schema changes</li>
<li>Going schema-less</li>
<li>Other approaches and concerns</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>In order to practice the ideas that are laid out in this chapter, you will need to have the following tools installed:</p>
<ul>
<li>An application with the Entity Framework Core NuGet package installed</li>
<li>Visual Studio with SQL Server Data Tools</li>
<li>Access to Azure Pipelines</li>
<li>An Azure subscription, for accessing Cosmos DB</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Managing a database schema as code</h1>
                </header>
            
            <article>
                
<p>For those of you who are familiar with working with relational databases from application code, it is very likely they have been working with an <strong>object-relational mapper</strong> (<strong>ORM</strong>). ORMs were introduced to fill the impedance mismatch between object-oriented programming languages and the relational database schema, which works with tables. Well-known examples are Entity Framework and NHibernate.</p>
<p>ORMs provide a layer of abstraction that allows for the storage and retrieval of objects from a database, without worrying about the underlying table structure when doing so. To perform automated mapping of objects to tables, or the other way around, ORMs often have built-in capabilities for describing a database schema, the corresponding object model, and the mappings between them in a markup language. Most of the time, neither of these have to be written by hand. Often, they can be generated from an object model or an existing database, and the mappings between them are often, by convention, generated or drawn in a visual editor.</p>
<p>While all this allows for the current database schema to be defined as code, this alone does not help with coping with schema changes, yet. For handling schema changes as code, two common approaches are available. The first one describes every change in code; the other one describes only the latest version of the schema in code. These approaches are known as migration-based and state-based approaches. Both can rely on third-party tooling to use these for applying the changes to the database.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Migrations</h1>
                </header>
            
            <article>
                
<p>The first approach is based on keeping an ordered set of changes that have to be applied to the database. These changes are often called <em>migrations</em>, and they can be generated by tools such as Microsoft Entity Framework, or Redgate SQL Change Automation, or they can be written by hand.</p>
<p>Tools can automatically generate the migration scripts based on a comparison of the current schema of the database and the new schema definition in source control. This is called <strong>scaffolding</strong>. The scripts generated by tools are not always perfect, and they can be improved by applying the domain knowledge that the programmer has, but the tool does not. Once one or more new migrations are scaffolded or written, they can be applied to a database using the chosen tool. A diagram showing how that works is shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-937 image-border" src="assets/f7b921f8-9d80-4f1c-a2f4-18aabca2489b.png" style="width:29.08em;height:15.08em;"/></p>
<p>Here, we see how an ever-growing series of migrations, labeled m1 to m4, are generated to describe incremental changes to the database. To update the database to the latest version, the latest applied migration is determined and all migrations after that are added one after the other.</p>
<p>When editing migration scripts by hand, the following has to be kept in mind:</p>
<ul>
<li>The migration scripts should be ordered. Migrations describe the SQL statements that need to be executed in order to move the database from a version <em>x</em> to version <em>x+1</em>. Only once this is complete can the next migration be started.</li>
<li>A migration script should migrate not only the schema, but also the data. This can mean that in-between steps are needed. For example, moving two columns to another table often implies that the new columns are first created, then filled with the data from the old columns, and that only then are the old columns removed.</li>
<li>It is advisable to include all database objects in the migration scripts. Extra indexes and constraints should not be applied to the production database only, but also to test environments. With migrations, there is already a mechanism for delivering those from source control. Having these in the same migration scripts also ensures that indexes and constraints are applied in the same order, and cannot unexpectedly block migrations by existing only in production.</li>
<li>If possible, migration scripts should be made idempotent. If there is ever an issue or the suspicion of an issue, being able to just rerun the last migration is a great way to ensure that it is fully applied.</li>
</ul>
<p>One disadvantage of this approach is the strict ordering requirement that is imposed on generating and applying the generated migrations. This makes it hard to integrate this approach into a development workflow that relies heavily on the use of branches. Migrations created in different branches that are merged together only later might break the ordering of migrations or, even worse, merge a split in the migration path. For example, imagine the case where two migrations, <em>b</em> and <em>c</em>, in two different branches, have been created after an existing migration, <em>a</em>. How are these going to be merged? Neither order—<em>a, b, c</em> or <em>a, c, b—</em>is correct, since both <em>b</em> and <em>c</em> are created to be executed directly after <em>a.</em> The only way such an error can be fixed is by performing the following steps:</p>
<ol>
<li>Remove all migrations apart from the first new one, for example, <em>c</em> in this case.</li>
<li>Apply all other migrations to a database that has none of the new migrations applied; in this case, only <em>b</em> if <em>a</em> was already applied, or both <em>a</em> and <em>b</em>.</li>
<li>Generate a new migration for the other migrations; in this case, a replacement for <em>c.</em></li>
</ol>
<p>An advantage of this approach is that every individual schema change will be deployed against the database in the same fashion. Irrespective of whether one—or more than one—migration is applied to the production database at the same time, they will still run one by one in a predictable order and in the same way in which they ran against the test environment, even if they were applied there one by one.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">End state</h1>
                </header>
            
            <article>
                
<p>A different approach to managing schema changes is to not keep track of the individual changes (or migrations), but only store the latest version of the schema in source control. External tools are then used to compare the current schema in source control with the actual schema of the database, generate migration scripts, and apply these when running. The migration scripts are not stored, and are single-use only.</p>
<p>Unlike writing migrations, it is not feasible to execute a task like this by hand. While tracking the newest version of the schema by hand in source control can be managed, the same is not feasible for an end-state approach. Generating a migration script while comparing the existing schema and the new schema and applying this migration script can only be done using a tool. Examples of these tools are Redgate SQL Source Control and SQL Server Data Tools. How these tools work, is shown in the here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-936 image-border" src="assets/9bdb8efd-8b7a-43a1-9754-eeaa31b91458.png" style="width:22.92em;height:15.42em;"/></p>
<p>Here we see how the current actual database schema and the description of the desired database schema are compared to generate an upgrade and directly apply a script for making the changes needed to make the actual schema the same as the desired schema.</p>
<p>One advantage of this approach is that there is no series of scripts generated that have to be executed in a specific order. Therefore, this approach combines easily with extensive branching schemas, where changes are integrated more slowly over time. It also removes the need to write migrations by hand for simple scenarios, such as adding or deleting a column, table, or index.</p>
<p>The disadvantage of this approach is that it makes it harder to handle changes that need data operations as well. Again, imagine a scenario of moving two columns to another table. Since the tooling only enforces the new schema, this will lead to data loss if there is no further intervention.</p>
<p>One possible form of intervention to circumvent this is the addition of pre- and post-deployment scripts to the schema package. In the pre-deployment script, the current data is staged in a temporary table. Then, after applying the new schema, the data is copied from the temporary table to the new location in the post-deployment script.</p>
<p>This section was about managing database schema changes in a format that can be stored in source control. The next section discusses how these changes can be picked up at deploy time and then applied to a database.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applying database schema changes</h1>
                </header>
            
            <article>
                
<p>With the database schema, and optionally, a series of migrations defined in source control, it is time to start thinking about when to apply changes to the database schema. There are two methods to do so. Database schema changes can be applied prior to deployment of the new application version, or by the application code itself.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Upgrading as part of the release</h1>
                </header>
            
            <article>
                
<p>The first approach to applying databases changes is as part of the release pipeline. When this is the case, the tool that is responsible for reading and executing the migration scripts is invoked using a step in the pipeline.</p>
<p>This invocation can be done using a custom script in PowerShell or another scripting language. However, this is error-prone, and with every change of tool, there is a risk that the scripts need to be updated. Luckily, for most of the migration-based tools, there are Azure Pipelines tasks that are readily available for starting the migration from the release.</p>
<p>For example, there is an Azure Pipelines extension available for applying Entity Framework Core migrations to a database directly from the <kbd>dll</kbd> file where they are defined. This task can be added to the release pipeline for updating the database, before the new application code is deployed.</p>
<p>Another variation is a split between the build and the release phase of an application. In this case, the migration scripts are exported as a separate build artifact, either directly from source code—if written in SQL—or after executing a tool that generates the necessary SQL scripts as output. This artifact is then downloaded again in the release phase, where it is applied to the database using an Azure Pipelines task for executing SQL.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Upgrading by the application code</h1>
                </header>
            
            <article>
                
<p>Instead of applying schema changes from the release pipeline, they can also be applied by the application itself. Some of the ORMs, with migration support built in, have the capability to automatically detect whether the database schema matches the latest migration. If not, they can automatically migrate the schema to that latest version on the spot.</p>
<p>An example of an ORM that supports this is Entity Framework. The core version of Entity Framework does not have support for automatic migrations built in. In Entity Framework Core, a single line of application code can be used to initiate an upgrade at a time that is convenient from the perspective of the application. The code for doing so is shown in the following code snippet:</p>
<pre>using (var context = new MyContext(...))<br/>{<br/>    context.Database.Migrate();<br/>}</pre>
<p>The advantage of this approach is that it is very simple to enable. Just a Boolean switch in the configuration of, for example, Entity Framework can enable this workflow. However, the disadvantage is that most ORMs that support this will enforce a global lock on the database—stopping all database transactions while the migrations are running. For any migration or set of migrations that take more than a few seconds, this approach might be impractical.</p>
<p>This approach is normally only used for migration-based approaches. Approaches that use an end-state approach require an external third-party tool that is used to generate the necessary migration scripts and apply them. This is normally done from the release pipeline and is not wrapped in the application itself.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adding a process</h1>
                </header>
            
            <article>
                
<p>As the previous section illustrated, it is important to think about how and when changes to the database schema or the application (or applications!) that use that schema are applied. But no matter how the deployment of schema changes and code deployment are scheduled, there will always be a period at which one of the following is true:</p>
<ul>
<li>The new application code is already running while the schema changes are not applied yet or are in the process of being applied.</li>
<li>The old application code is still running while the schema changes are already applied or are in the process of being applied.</li>
<li>The application code is not running while the schema changes are being applied.</li>
</ul>
<p>The third situation is highly undesirable. This is true in general, but especially when practicing DevOps. If changes are shipped often and during working hours, it is unacceptable to take the application down for every schema change.</p>
<p>To prevent having to take the application down while schema changes are being applied, one of the following conditions has to be met:</p>
<ul>
<li>The schema changes are backward-compatible in such a way that the old version of the application code can run without errors against a database where the schema changes have already been applied, or are being applied.</li>
<li>The new application code is backward-compatible in such a way that it can run against both the old and the new versions of the schema.</li>
</ul>
<p>Meeting the first of these conditions ensures that the old application code can continue to run while the schema changes are being applied. Meeting the second of these conditions ensures that the new version of the application code can be deployed first, and once that is completed, the database can be upgraded while this code is running. While either will work, it is often desirable to aim for the first condition. The reason is that schema changes often support application code changes.</p>
<p>This means that the following is a safe process for deploying schema changes without downtime:</p>
<ol>
<li>Create a new database.</li>
<li>Apply the database changes.</li>
<li>Verify that the changes have been applied properly, or abort the deployment pipeline.</li>
<li>Deploy the new application code.</li>
</ol>
<p>It is important to realize that this process assumes failing forward. This means that if there ever is an issue with the deployment of schema changes, they should be resolved before going forward with the code changes.</p>
<p>Finally, meeting the condition of backward combability for schema changes can sometimes be impossible to fulfill for a schema change. If this is the case, the change can often be split into two partial changes that together have the same end result, while they both meet the condition of backward combability. For example, renaming a property, or changing the unit in which it stores a distance from feet to meters, can be executed as follows:</p>
<ol>
<li>Generate a migration that adds a new column to a database table, storing the distance in meters.</li>
<li>Add an application code that reads from the old column, but writes to both columns.</li>
<li>Deploy these changes to production.</li>
</ol>
<ol start="4">
<li>Add a new migration that migrates data from the old column to the new column for all cases where the new column is not yet filled, but the old column is.</li>
<li>Update the application code to read and write only the new column.</li>
<li>Deploy these changes to production.</li>
<li>Add a new migration that removes the old column.</li>
</ol>
<p>Using the correct tools and a proper process, it is possible to execute effective and safe deployments of schema changes. In the next section, another approach, using schema-less databases, is introduced.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Going schema-less</h1>
                </header>
            
            <article>
                
<p>In the previous sections, the focus was on relational databases, where strict schemas are applied to every table. A completely different approach to database schema management is to let go of having a database schema altogether. This can be done by using schema-less or document databases. A well-known example of a schema-less database is Azure Cosmos DB. These databases can store documents of different forms into the same table. Table is quoted here, since these types of databases often do not use the term "table", but call this a database, a container, or a collection.</p>
<p>Since these databases can store documents with a different schema in the same collection, schema changes no longer exist from a database's point of view. But of course, there will be changes to the structure of the corresponding objects in the application code over time. To see how to handle this, it is best to differentiate between storing objects in the database and reading them back.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing objects to the database</h1>
                </header>
            
            <article>
                
<p>The documents that are stored in a schema-less database are often serializations of objects in application code. When working with a relational database, these objects are often stored using an <strong>object-relational mapper</strong> (<strong>ORM</strong>), such as Entity Framework, Dapper, or NHibernate. When working with a document database, these objects are often serialized and stored in the database. This means that a change in the definition of that code object will also result in a different document structure when saving the object. Due to the nature of document databases, this will work fine.</p>
<p>As an example, take the following C# class and its JSON representation after serializing it to a document database:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<pre>public class Person<br/>{<br/>   [JsonConstructor]<br/>   private Person() {}<br/> <br/>   public Person(string name) {<br/>      Name = name ?? throw new ArgumentNullException();<br/>   }<br/> <br/>   [JsonProperty]<br/>   public string Name { get; private set; }<br/>}</pre></td>
<td>
<pre>{<br/>   “Name”: “Mark Anderson”<br/>}</pre></td>
</tr>
</tbody>
</table>
<p> </p>
<p>After this code has been running in a production environment for a while, and thousands of persons have been saved, a new requirement comes in. Next to the name of the person, the city where they live must also be recorded. For this reason, the <kbd>Person</kbd> class is extended to include another property. After performing this change and deploying the new code, whenever a person is saved, the following code is used, resulting in the JSON shown next to it:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<pre>public class Person<br/>{<br/>   [JsonConstructor]<br/>   private Person() {}<br/> <br/>   public Person(string name, string city) {<br/>      Name = name ?? throw new ArgumentNullException();<br/>      City = city ?? throw new ArgumentNullException();<br/>   }<br/> <br/>   [JsonProperty]<br/>   public string Name { get; private set; }<br/> <br/>   [JsonProperty]<br/>   public string City { get; private set; }<br/>}</pre></td>
<td>
<pre>{<br/>   “Name”: “Mark Anderson”,<br/>   “City”: “Amsterdam”<br/>}</pre></td>
</tr>
</tbody>
</table>
<p> </p>
<p>While the definition of the <kbd>Person</kbd> class has changed—and the corresponding JSON has as well—both document forms can be saved into the same collection.</p>
<p>This shows that from the viewpoint of writing information to the database, the schema-less approach is very convenient, since developers do not have to think about schema change management at all.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading objects from the database</h1>
                </header>
            
            <article>
                
<p>While schema-less databases make it extremely easy to write documents of different forms to the same collection, this can pose problems when reading documents back from that same collection and deserializing them. In reality, the problem of schema management is not removed, but deferred to a later point in time.</p>
<p>Continuing the example from the previous section, deserializing the first person that was saved on the new C# <kbd>Person</kbd> class definition will result in a null value for the city property. This can be unexpected, since the C# code guarantees that a person without a city can never be constructed. This is a clear example of the challenges that schema-less databases pose.</p>
<p>In this example, the issue can be circumvented by updating the <kbd>Person</kbd> class to the following:</p>
<pre>public class Person<br/>{<br/>   [JsonConstructor]<br/>   private Person() {}<br/> <br/>   public Person(string name, string city) {<br/>      Name = name ?? throw new ArgumentNullException();<br/>      City = city ?? throw new ArgumentNullException();<br/>   }<br/> <br/>   [JsonProperty]<br/>   public string Name { get; private set; }<br/> <br/>   [JsonIgnore]<br/>   private string _city;<br/> <br/>   [JsonProperty]<br/>   public string City { <br/>      get { return _city; }<br/>      private set { _city = value ?? _city = string.Empty}<br/>   }<br/>}</pre>
<p>Next to this scenario, where a property was added, there are many other scenarios that will require the C# class to be adapted in order to handle deserialization scenarios. Some examples are as follows:</p>
<ul>
<li>Adding a property of a primitive type</li>
<li>Adding a complex property, another object, or an array</li>
<li>Renaming a property</li>
<li>Replacing a property of a primitive type with a complex property</li>
<li>Making nullable properties non-nullable</li>
</ul>
<p>Adding code to objects to handle these situations increases the size and complexity of the code base, and pollutes the primary code base with the capabilities for coping with past situations. Especially when this happens often, this can lead to unwanted complications in a code base. To prevent this, a possible solution is to go through the following process whenever the schema of an object changes:</p>
<ol>
<li>Change the schema of the object, ensuring that there are only properties added. Even when the goal is to remove a property, at this stage, only a property with the new name is added.</li>
<li>Implement logic on the object to cope with the deserialization of old versions of the object.</li>
<li>Deploy the new version of the object.</li>
<li>Start a background process that loads all objects of the type from the database one by one, and saves them back to the database.</li>
<li>Once the background process has processed all existing entities, remove the code that is responsible for coping with the schema change during deserialization from the code base, along with any properties that are no longer used.</li>
</ol>
<p>Using this approach, all changes are propagated to all stored versions of the object over a period of time. The downside to this approach is that the change to the object's structure is spread over two changes that must be deployed separately. Also, deployment of the second change must wait until all objects in the database have been converted.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Other approaches and concerns </h1>
                </header>
            
            <article>
                
<p>Besides the more common approaches that were discussed previously, the following tips and approaches might help with reducing the amount of work in dealing with databases, or help reduce the risk associated with making database changes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Minimizing the influence of databases</h1>
                </header>
            
            <article>
                
<p>A first step in dealing with databases can be to reduce the chance that a database change has to be made. In many databases, it is possible to write stored procedures—or some other code or script—that executes within the database engine. While stored procedures come with some benefits, changing them can also count as a database schema change, or at the least result in changes that can be difficult to test.</p>
<p>One simple approach for this is to just replace stored procedures with application code that allows for easier side-by-side changes using feature toggles.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Full side-by-side deployment</h1>
                </header>
            
            <article>
                
<p>When working in a high-risk environment, or with a fragile database, there is also another approach to database schema changes that can be taken. This approach is based on applying feature toggles and the blue–green deployment pattern, and goes as follows:</p>
<ol>
<li>Change the application code in such a way that it writes any update to not just one, but to two databases.</li>
<li>In the production environment, create a complete, full copy of the existing database and configure the application code to write all changes to both databases. These databases will be called <em>old</em> and <em>new</em>, from now on.</li>
<li>Introduce the required changes to the new database schema and the application code <em>only</em> in the path that writes to the new database.</li>
<li>Introduce the necessary changes in all code paths that read data in such a way that all queries run against both databases.</li>
<li>Update the application code to detect differences in the query results between the new and the old databases, and log an error when it finds any discrepancy.</li>
<li>If the changes run without any issues, remove the old database, and the old read and write access paths, from the application code.</li>
<li>If the changes run with errors, fix the issue. Next, restart by restoring the backup of the intended new database, and resume at step five.</li>
</ol>
<p>The advantage of this approach is that it is very lightweight. The downside is that it is very involved, takes a lot of work, and is more expensive. Also, the extra database costs and duration of backup and restore operations should be taken into account.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing database changes</h1>
                </header>
            
            <article>
                
<p>Just as with application code, insights into the quality of database schema changes can be gathered through testing. Links to performing tests on database schemas can be found at the end of this chapter.</p>
<p>In most cases, in order to fully cover the risks introduced by database changes, system tests are needed that execute against a fully deployed stack of the application. This type of test can cover most of the risks that come from faulty schemas, invalid stored procedures, and database and application code mismatches.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you have learned how to manage your database schema and schema changes using source control. You know about both the migration-based and end state based storing of changes, and how to apply them to your production database in a safe manner.</p>
<p>Additionally, you have learned how schema-less databases can remove the burden of traditional schema management. However, this comes at the price of having to cope with schema differences when reading older versions of an object back from the database.</p>
<p>In the next chapter, you will learn about continuous testing. You will not only learn about testing techniques, but also about which to apply at what point, and how testing is a crucial part of DevOps and a critical enabler of a continuous flow of value to end users.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<p>As we conclude, here is a list of questions for you to test your knowledge regarding this chapter's material. You will find the answers in the <em>Assessments</em> section of the Appendix:</p>
<ol>
<li>True or false: When working with Entity Framework, schema management is  built in using migrations-based support.</li>
<li>True or false: When working with a migrations-based approach for schema management, you do not need extra tracking tables in your database schema.</li>
<li>True or false: When working with an end state-based approach for schema management, you do not need extra tracking tables in your database schema.</li>
<li>What are the benefits of a full side-by-side approach to database schema changes? (Choose multiple answers):<br/>
<ol>
<li>The risks are reduced to almost zero.</li>
<li>You can measure the actual performance impact of changes in a production-like environment.</li>
<li>Side-by-side migrations reduce cycle time.</li>
</ol>
</li>
<li>True or false: Schema-less databases remove the need for thinking about schema changes completely.</li>
<li>What is a possible technology choice that you can make to limit the impact of changes on your database schema?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li>More information about Entity Framework and Entity Framework Migrations can be found at <a href="https://docs.microsoft.com/nl-nl/ef/">https://docs.microsoft.com/nl-nl/ef/</a> and <a href="https://docs.microsoft.com/en-us/ef/ef6/modeling/code-first/migrations/">https://docs.microsoft.com/en-us/ef/ef6/modeling/code-first/migrations/</a>.</li>
<li>More information about Redgate and its database tooling can be found at <a href="https://www.red-gate.com/">https://www.red-gate.com/</a>.</li>
<li>More information on SQL Server Data Tools can be found at <a href="https://docs.microsoft.com/en-us/sql/ssdt/sql-server-data-tools?view=sql-server-2017">https://docs.microsoft.com/en-us/sql/ssdt/sql-server-data-tools?view=sql-server-2017</a>.</li>
<li>The Azure Pipelines extension for deploying Entity Framework Core migrations from a DLL can be found at <a href="https://marketplace.visualstudio.com/items?itemName=bendayconsulting.build-task">https://marketplace.visualstudio.com/items?itemName=bendayconsulting.build-task</a>.</li>
</ul>


            </article>

            
        </section>
    </body></html>