<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Continuous Testing</h1>
                </header>
            
            <article>
                
<p>In the previous chapters, you learned about the different types of techniques that are used to help to increase the rate at which you deliver changes to your production environment. If you are already using these techniques in your daily work, you will quickly notice that this is only possible if your work is of sufficient quality. If the quality of your work is not high enough, you will face many outages or issues and your end users will not be happy. To be successful, increasing the rate of change and increasing the quality of your work must go hand in hand. To recognize the quality of your work and to increase it, you first need to know what is meant by quality. This is where testing comes in. Testing is the discipline of reporting about the quality of software.</p>
<p>To introduce the topic of testing, this chapter will start by looking at how the quality of software development can be measured. Next, the topic of functional testing will be explored. First, the testing cone and pyramid will be introduced. These are models that can be used to determine which types of tests are needed and how many of each should be used. After this, the different types of tests will be discussed one by one. You will learn about how they work, what they test, and the benefits and downsides of the different types of tests. Finally, the last section will focus on how all metrics and test results, once generated and collected by your pipelines, can continuously report on the quality of the work of your team and even prevent changes of insufficient quality propagating to your users. All of this will help you to maintain the high quality of your software and enable you to confidently deliver that software quickly and frequently.</p>
<p>The following topics will be covered in this chapter:   </p>
<ul>
<li>Defining quality</li>
<li>Understanding test types</li>
<li>Executing functional tests</li>
<li>Executing nonfunctional tests</li>
<li>Maintaining quality</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>To experiment with the techniques described in this chapter, you might need one or more of the following:</p>
<ul>
<li>An Azure DevOps project with access to build and release pipelines and dashboards</li>
<li>Visual Studio 2019</li>
<li>A Basic + Test Plans license for Azure DevOps</li>
<li>A SonarCloud subscription</li>
</ul>
<p>All of these are available for free or can be obtained for free for a limited trial period.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining quality</h1>
                </header>
            
            <article>
                
<p>One of the primary goals of the DevOps mindset discussed in <a href="889f9224-f1b6-414d-bc80-16563f66e1e7.xhtml">Chapter 1</a>, <em>Introduction to DevOps</em>, is increasing the flow of value to end users. To do this, software must be deployed frequently, maybe even multiple times per day. To make frequent deployments possible, two things are important: automation and quality. Automation has been discussed extensively in the previous chapters, and so now it is time to move on to the topic of quality.</p>
<p>Once an automated build and release pipeline is in place and changes are starting to flow to production at an increasing speed, it is time to start measuring the quality of these changes. Even more importantly, this allows us to abort changes that are not of sufficient quality. What actually makes quality <em>sufficient</em> can differ from project to project. When creating games, a few bugs might be annoying for the user but nothing more. When creating software for airplanes or medical use, a single bug may cost lives. In software, higher quality is more expensive and/or takes more time. So, there is a trade-off between the number of features we can deliver and the quality that can be guaranteed. For every project, there is a different optimal trade-off between these.</p>
<p>Before quality can be measured, it is important that you first establish how to measure the quality of software. A common approach to monitoring the quality of software is to gather one or more metrics. For example, it could be decided to collect a set of five measurements <span>every week</span><span>. Graphing these metrics over time provides insight into how the quality of the software is evolving. An example of this might look something like the graph shown here:<br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1052 image-border" src="assets/3bb1f3cd-63f4-4466-8bb4-64abdbf00639.png" style="width:85.58em;height:36.25em;"/></p>
<p>The next sections discuss several examples of metrics.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Metrics for quality</h1>
                </header>
            
            <article>
                
<p>Metrics are a means of capturing something that is measured as a number. In software development, metrics are often used to represent a particular quality aspect that can be hard to quantify in itself. For example, the quality of a piece of software can be very hard to describe by itself. This holds even more for how quality changes. For this reason, we often capture numbers that, taken together, say something about the quality of software.</p>
<p>It is important to realize that metrics are a great tool, but should always be used with caution. For one thing, there might be more factors influencing the (perceived) quality of software than the metrics that are being measured. Also, once people know that a specific metric is recorded, they can optimize their work to increase or decrease the metric. While this might show the desired numbers in reports, this might not necessarily mean software quality is really improving. To combat this, often, more than one metric is recorded.</p>
<p>A well-known example is that of story point velocity in agile work environments. Recording the sprint velocity for a team to see whether it is becoming more efficient over time sounds effective; however, if the team size varies from sprint to sprint, then the metric might be useless since attendance is influencing velocity as well. Also, the metric can be easily falsified by a team agreeing on multiplying all estimations by a random number every sprint. While this would increase the numbers every sprint, this would not relate to an increase in team throughput anymore.</p>
<p>Moving on to metrics for measuring the quality of software, it can be difficult to objectively measure the quality of written code. Developers often have many opinions as to what constitutes <em>good code</em>, and the more the topic is discussed, the harder it can be to find consent in a team; however, when shifting attention to the results that come from using that code, it becomes easier to identify metrics that can help to provide insights into the quality of the code.</p>
<p>Some examples of this are as follows:</p>
<ul>
<li><strong>The percentage of integration builds that fails</strong>: If code does not compile or pass automated tests, then this is an indication that it is of insufficient quality. Since tests can be executed automatically by build pipelines whenever a new change is pushed, they are an excellent tool for determining the quality of code. Also, since they can be run and their results gathered before we deploy a change to production, the results can be used to cancel a change before deploying it to the next stage of a release pipeline. This way, only changes of sufficient quality propagate to the next stage.</li>
<li><strong>The percentage of code covered by automated tests</strong>: If a larger part of the code is being tested by unit tests, this increases the quality of the software.</li>
<li><strong>The change failure rate</strong>: This is the percentage of deployments of new versions of the code that lead to issues. An example of this is a situation where the web server runs out of memory after the deployment of a new version of the application.</li>
<li><strong>The amount of unplanned work</strong>: The amount of unplanned work that has to be performed in any period of time can be a great metric of quality. If the team is creating a SaaS offering that it is also operating, there will be time spent on operational duties. This is often referred to as unplanned work. The amount of unplanned work can be an indicator of the quality of the planned work. If the amount of unplanned work increases, then this may be because the quality has gone down. Examples of unplanned work can be live site incidents, following up on alerts, hotfixes, and patches.</li>
<li><strong>The number of defects that are being reported by users</strong>: If the number of bugs reported by users increases, this can be a sign that quality has been declining. Often, this is a lagging indicator, so once this number starts increasing, quality might have been going down for a while already. Of course, there can be many other reasons for this number increasing: new operating systems, an increase in the number of users, or changing expectations from users.</li>
</ul>
<ul>
<li><strong>The number of known issues</strong>: Even if there are very few new defects being found or reported, if defects are never fixed and the number of known issues keeps increasing slowly, then the quality of the software will slowly decline over time.</li>
<li><strong>The amount of technical debt</strong>: Technical debt is a term used to describe the consequences of sacrificing code quality for short-term gains, such as the quick delivery of code. Technical debt is discussed in detail in the next section.</li>
</ul>
<p>Testing is an activity that is performed to find and report on the quality of software. Test results (insights into quality) can be used to allow or cancel a change progressing to the next release stage.</p>
<p>In the next section, another dimension of quality is explored: the amount of technical debt in a code base.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical debt</h1>
                </header>
            
            <article>
                
<p>Technical debt is a term that describes the future costs of sacrificing code quality for something else. For example, to expedite the delivery of a new feature, a developer may choose to quickly expand an existing class with a few new methods to realize this feature. If the resulting class does not adhere to the principles of object-oriented design or grows to be too large, this can make for a class that is difficult to understand and maintain or change later. The term "debt" implies that something (time, quality, attention, or work) is owed to the solution. So long as this debt is not paid off, you have to pay interest in the form of all other work being slowed down a little bit.</p>
<p>Technical debt can take many forms, but some examples are as follows:</p>
<ul>
<li>Code that is not covered by any unit test where changes to the implementation of said code cannot be verified using the original tests that were used to create it</li>
<li>Code that is not written in a self-explanatory fashion using meaningful variable and method names</li>
<li>Code that does not adhere to coding principles, such as KISS, YAGNI, DRY, and/or SOLID</li>
<li>Classes that are too complex because they have too many variables and methods</li>
<li>Methods that are too complex because they have too many statements (flow-control statements specifically)</li>
<li>Classes or namespaces that have circular dependencies through different parts of the application</li>
<li>Classes that do not adhere to the architectural design for the application</li>
</ul>
<p>There are many forms of technical debt, and it can be daunting to oversee all of them. For this reason, there are many tools available that can measure the technical debt in a code base automatically and report on that. Tools for doing this will be discussed in the <em>Maintaining quality </em><span>section</span><span>.</span></p>
<p>While technical debt is often considered a bad thing, there might be good reasons for creating technical debt on purpose. Just as with a regular debt, it is important to manage the height of the debt and to ensure that interest can be paid and the debt itself can be paid off.</p>
<p><span>Companies</span><span> often </span><span>take on technical debt during the start-up phase, where it is often a conscious decision to quickly create a working solution. While this first version is used to validate the business proposition and attract funds, developers can pay off this debt by reimplementing or refactoring (parts</span><span> </span><span>of</span><span>) the application.</span></p>
<p>Another reason might be a market opportunity or an important business event that has been planned months in advance. Taking on some technical debt to make deadlines and deliver on time can be worth the cost.</p>
<p>However, never paying the debt and only taking on more debt over time will also increase the metaphorical interest to be paid every time a developer needs to make a change. The result will be that any change will take longer than the previous one. If this starts happening, it is unavoidable that at some point no change will be worthwhile anymore, since the cost always outweighs the benefits. At this point, a project or product has failed.</p>
<p><span>When talking about tests, it is important to understand which types of tests exist. The next section will go into this subject.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding test types</h1>
                </header>
            
            <article>
                
<p>In traditional software development, tests were often executed when <em>development was complete</em>, the <em>application was declared dev-done</em>, the <em>feature set was frozen</em>, or a similar statement. After declaring the development done, testing was performed, and often, a long period of going back and forth between testing and bug fixing started. The result was often that many bugs were still found after going live.</p>
<p>Shifting left is a testing principle that states that automated testing should be done earlier in the development process. If all activities involved with software development are drawn on a line from inception to release, then shifting left means moving automated testing activities closer to inception.</p>
<p>To do this, a wide selection of different types of tests are recognized—for example, unit tests, integration tests, and system tests. Different sources can suggest different types of tests, but these are some of the more well-known types. No matter the specific name of a type of test, when looking at tests with a high level of abstraction, they are often divided into the following two categories:</p>
<ul>
<li><strong>Functional tests</strong><span>: Functional tests </span><span>are in place to test whether the desired functionality is actually realized by the application.</span></li>
<li><strong>Non-functional tests</strong><span>: Non-functional tests </span><span>are used to verify whether the other desired properties of an application are realized and whether undesirable properties are not present.</span></li>
</ul>
<p><span>These types are further broken down into smaller subcategories, as shown in the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-494 image-border" src="assets/bbf7a7ed-dc9f-4f07-94e8-5891cd6dc942.png" style="width:40.00em;height:28.00em;"/></p>
<p>The following three sections contain brief recaps of the different types of functional and non-functional tests. This is to facilitate later discussions on which type of test to choose in which situation and how much of each type of test your project might need.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Types of automated functional tests</h1>
                </header>
            
            <article>
                
<p>When talking about automated functional tests, the three <span>most-used </span><span>types are unit tests, integration tests, and system tests. These types of test can be compared along several axes: the time it takes to create a test, the time it takes to execute a test, and the scope that they test:</span></p>
<ul>
<li><strong>Unit tests</strong>: Unit tests are the quickest to write, and they execute very quickly, often in less than a millisecond. They test the smallest possible scope in an application, often a single class or method. This means that, once written, it should virtually never be necessary to change a unit test. For many systems, it is more likely that a test will be deleted rather than changed.</li>
<li><strong>Integration tests</strong>: Integration tests take more time to write since they concern themselves with multiple units that have to be set up to work together. The execution of these tests should still be fast, averaging from below a second up to tens of seconds. Integration tests have a larger test scope, which means that, in return for this, they will cover a larger part of the code and are more likely to detect defects that are introduced with a change.</li>
<li><strong>System tests</strong>: System tests test a fully assembled and running application. Depending on the type of application, these are often API tests or automated UI tests. These tests take a lot of time to create since they rely on a deployed system to run and often require the setting up of an initial state in a database or another persistent store. The tests take a long time to execute, sometimes minutes per test. They are also less reliable and much more fragile than unit and integration tests. Even a minor change in an interface can cause a whole series of tests to fail. On the other hand, system tests can detect errors that both unit and integration tests cannot, since they actually test the running system.</li>
</ul>
<div class="packt_infobox">Please note that having a large test scope in a test has both an upside and a downside. The upside is that it can detect many errors. The downside is that a failing test with a very large test scope provides only a limited insight into what has gone wrong. Such a test failure will often require more investigation than a failing test with a smaller test scope.</div>
<div>
<p>The following sections explore each type of test in more detail.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unit tests</h1>
                </header>
            
            <article>
                
<p>Unit tests are used to test a single unit in isolation. When working in an object-oriented programming language, this will come down to having one test class for every class in an application. For full test coverage, the test class will then have one or more tests for every public method of the corresponding application class.</p>
<p>Unit tests should run extremely fast—on average, in a few milliseconds or less. To make this possible, each class is instantiated without its dependencies. This is enabled by the use of interfaces, where classes depend on interfaces instead of directly on other classes. For tests, the dependencies are then replaced with mock classes, as shown in the following diagram. On the left, the runtime configuration is shown; on the right, the configuration during tests is shown:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1053 image-border" src="assets/fd411400-b29b-4582-887e-6cdeb93cb918.png" style="width:33.17em;height:10.50em;"/></p>
<p>A mock class implements the same interface but has <span>no behavior associated</span><span> by default</span><span>. Specific behavior can be set up on a per-test basis. Mocks can also be used to verify that certain operations or functions on a dependency are</span> <span>called. As an example, take the following C#</span> class:</p>
<div>
<pre><span>public</span><span> </span><span>class</span><span> </span><span>WorkDivider<br/></span><span>{<br/></span><span>    </span><span>private</span><span> </span><span>readonly</span><span> </span><span>IMessageSender</span><span> </span><span>_messageSender</span><span>;<br/><br/></span><span>    </span><span>public</span><span> </span><span>WorkDivider</span><span>(</span><span>IMessageSender</span><span> </span><span>messageSender</span><span>)<br/></span><span>    {<br/></span><span>        </span><span>_messageSender</span><span> = </span><span>messageSender</span><span>;<br/></span><span>    }<br/><br/></span><span>    </span><span>public</span><span> </span><span>void</span><span> </span><span>DivideWork</span><span>(</span><span>IEnumerable</span><span>&lt;</span><span>WorkOrder</span><span>&gt; </span><span>workOrders</span><span>)<br/></span><span>    {<br/></span><span>        </span><span>foreach</span><span>(</span><span>var</span><span> </span><span>workOrder</span><span> </span><span>in</span><span> </span><span>workOrders</span><span>)<br/></span><span>        {<br/></span><span>            </span><span>_messageSender</span><span>.</span><span>SendMessage</span><span>(</span><span>workOrder</span><span>.</span><span>GetMessage</span><span>());<br/></span><span>        }<br/></span><span>    }<br/></span><span>}</span></pre></div>
<p>To instantiate this class in an automated test, an implementation of the <kbd>IMessageSender</kbd> interface is needed. To work around this dependency, a mocking framework such as Moq can be used to test <kbd>WorkDivider</kbd>, as follows. In these examples, <kbd>NUnit</kbd> is used as the testing framework:</p>
<pre>[TestFixture]<br/>public class WorkDividerTest<br/>{<br/>    private Mock&lt;IMessageSender&gt; _messageSender;<br/>    private WorkDivider _subject;<br/><br/>    [SetUp]<br/>    public void SetUp()<br/>    {<br/>        _messageSender = new Mock&lt;IMessageSender&gt;();<br/>        _subject = new WorkDivider(_messageSender.Object);<br/>    }<br/><br/>    [Test]<br/>    public void WhenSendingAnEnumerableOfWorkingOrders_EverOrderIsSendToTheMessageSender()<br/>    {<br/>        var workOrder = new WorkOrder();<br/><br/>        _subject.DivideWork(new[] { workOrder });<br/><br/>        _messageSender.Verify(x =&gt; x.SendMessage(workOrder), Times.Once);<br/>    }<br/>}</pre>
<p>This means that it is not possible to write unit tests for classes that interact with other systems, such as databases, caches, or service buses. To ensure that this does not make it impossible to cover large parts of the application with tests, it is common practice to isolate the integration with other systems in separate classes. These classes contain the interaction with a remote system, but no business logic and as little code as possible. It is then accepted that these classes are not covered by unit tests. The typical design patterns that are used to do this are the facade, adapter, and repository patterns.</p>
<div class="packt_infobox packt_tip"><span>Links to a more detailed guide on writing unit tests and how to mock classes are included a</span>t the end of this chapter. </div>
<p>Unit tests should be ready to run at the computer of every developer that clones the code base of an application. They should not require any special configuration or setup on the local computer and should be ready to go. This way, everyone who works with the code base can run the unit tests on their local computer. It is, therefore, a good practice for developers to run all unit tests on their own computers before pushing changes to the central repository.</p>
<p>Next to this local verification step, unit tests should also be a part of the continuous integration build. You will learn how to do this in the <em>Executing tests in a pipeline</em> <span>section</span><span> </span><span>later on. As long as there are failing unit tests in a pull request, it is better not to merge the changes to the master branch. This can even be made impossible using Git repo branch policies, which were discussed in</span> <a href="2be30fb3-5e71-4180-9830-f119e5a6cd76.xhtml">Chapter 2</a><span>,</span> <em>Everything Starts with Source Control</em><span>.</span></p>
<p>In the next section, the discussion of automated functional tests continues with integration tests.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Integration tests</h1>
                </header>
            
            <article>
                
<p>Integration tests are used to test whether a group of components works together correctly. These tests are used for two purposes:</p>
<ul>
<li>Increasing the test coverage for those parts of an application that are not covered by unit tests—for example, classes that interact with other systems</li>
<li>Addressing risks that are not addressed in unit tests and deal with classes interacting</li>
</ul>
<p>It can be hard to understand what integration risks are since it might seem clear that the whole will work as expected, as soon as all parts are working as expected. To understand this risk better, imagine that two components <span>working</span><span> </span><span>together are responsible for climate control. One is written measuring the temperature in degrees Celsius and the other is acting on that temperature, expecting its input in degrees Fahrenheit. It will quickly become clear that, while both components are working as intended, exchanging numbers and taking action based on those numbers, the combination will not produce the desired outcomes.</span></p>
<p>Integration tests, especially those that interact with other systems, will not only take longer to run than unit tests but often require more setup or configuration to run as well. This may even include secrets such as usernames, passwords, or certificates. To handle configuration such as this, a settings file can be created next to the tests from which settings are loaded before the tests are executed. Every developer can then create their own copy of that file and run the tests using their own configuration.</p>
<p>Continuing the example from the previous section, let's assume that the <kbd>MessageSender</kbd> class that implements the <kbd>IMessageSender</kbd> interface needs a connection string to do its work. A test class for <kbd>MessageSender</kbd> might then look as follows:</p>
<pre>[TestFixture]<br/>public class MessageSenderTest<br/>{<br/>    private MessageSender _messageSender;<br/><br/>    [SetUp]<br/>    public void SetUp()<br/>    {<br/>        var connectionString = TestContext.Parameters["MessageSenderConnectionString"];<br/>        _messageSender = new MessageSender(connectionString);<br/>    }<br/>}</pre>
<p><kbd>connectionString</kbd> needed for constructing the <kbd>MessageSender</kbd> class is received from the <kbd>Parameters</kbd> object on <kbd>TestContext.</kbd> This is the <kbd>NUnit</kbd> approach for making settings from a <kbd>.runsettings</kbd> file available. The exact approach can vary per test framework. An example <kbd>.runsettings</kbd> file would look as follows:</p>
<pre>&lt;?xml version="1.0" encoding="utf-8"?&gt;<br/>&lt;RunSettings&gt;<br/> &lt;TestRunParameters&gt;<br/> &lt;Parameter name="MessageSenderConnectionString" value="secret-value" /&gt;<br/> &lt;/TestRunParameters&gt;<br/>&lt;/RunSettings&gt;</pre>
<p>Moving the settings out to a separate file ensures that secrets are not checked into source control. In the <em>Executing tests in a pipeline</em> <span>section,</span><span> </span><span>you will learn how to build a</span> <kbd>.runsettings</kbd> <span>file for running tests in a pipeline.</span></p>
<p>This is because integration tests should also be part of the continuous integration build if possible. However, there is a risk that this will make a continuous integration build too slow. To counter this, one of the following solutions can be implemented:</p>
<ul>
<li>Integration tests are executed in a separate build that is triggered in parallel to the continuous integration build. This way, the duration of the continuous integration build stays low while the integration tests are still continuously executed, and developers get fast feedback on their work.</li>
<li>Integration tests are executed later in the pipeline, closer to the release of the software—for example, before or after the deployment to a test environment.</li>
</ul>
<p class="mce-root"/>
<p>The downside of the first approach is that executing integration tests this way will mean that the tests will no longer work as a quality gate before code is merged to the master. They will, of course, continue working as a quality-reporting mechanism. This means that, while errors might be merged, they will be detected and reported by the build.</p>
<p>The second approach does not have this risk since executing the tests is still part of the pipeline from source control to production; however, in this approach, the execution of the tests might be deferred to a later moment in time if not every build enters at least part of the release pipeline. This means that defects might become visible later on, extending the time between detecting and fixing an issue.</p>
<p>In either approach, failing integration tests will no longer block merging changes and you hence have to find another way to ensure that developers will take responsibility for fixing the defect that caused the tests to fail.</p>
<p>These trade-offs become even more evident with system tests, which often take so long that it is not possible to make them part of the continuous integration build.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">System tests</h1>
                </header>
            
            <article>
                
<p>The third and final type of automated functional tests is system tests. These tests are meant to run against a fully assembled and running application. System tests come in two flavors, depending on the type of application: an API test or a UI test. System tests can take a long time to execute, and it is not uncommon for long tests with an elaborate setup of test data to take well over a minute.</p>
<div class="packt_tip">You might come across something called coded UI tests. This is a now-deprecated Microsoft solution for writing UI tests. These tests could be executed from Azure Pipelines. Luckily, there are many alternatives, as referenced in Microsoft's deprecation message at <a href="https://devblogs.microsoft.com/devops/changes-to-coded-ui-test-in-visual-studio-2019">https://devblogs.microsoft.com/devops/changes-to-coded-ui-test-in-visual-studio-2019</a>.<a href="https://devblogs.microsoft.com/devops/changes-to-coded-ui-test-in-visual-studio-2019"/></div>
<p>System tests execute against a running application, which means that they will need configuration and setup before they can be run. The application needs to be running in a controlled environment and all of the integrations with data stores need to be fully operational. Integrations with other systems need to be either up and running or swapped out with a replacement mock to ensure that all operations that integrate with those systems will function properly.</p>
<p>These conditions make it less likely that developers will execute these tests on their local machines as they are making changes to the application. It is only when they are creating a new test or changing a test that they might do so. However, even then they may be executing these tests not against a locally run version of the application, but against a version that is already deployed to a test environment. This is not necessarily a good thing, but often just the reality in most teams.</p>
<div class="packt_infobox">An introduction to creating API or UI tests is unfortunately beyond the scope of this book. There are many products available on the market and which one is the best to use will differ from project to project.</div>
<p>When executing system tests as part of the pipeline, they are often run after the code has been deployed to at least one environment. This will often be the test environment. This implies that the system tests are on the critical path from a source code change to the deployment to production. If this path becomes too long, they can also be taken out of the pipeline. They are then run on a schedule—for example, every night. Just as with integration tests, this speeds up the pipeline, but it removes the opportunity to use system tests as a quality gate.</p>
<p>System tests, and UI tests in particular, are often fragile and can stop working unexpectedly after minor changes. For this reason, it is advised that you keep their number as low as possible; however, keep in mind that these are the tests that can catch particular errors, such as misconfiguration or other runtime errors, database-application mismatches, or series of operations that create error states.</p>
<p>Besides automated function tests, there are also manual functional tests that have value in many DevOps projects. These are discussed next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Types of manual functional tests</h1>
                </header>
            
            <article>
                
<p>While automated tests are a great tool for receiving feedback on development work quickly and often, there are still things that will be tested manually. While automating repetitive tests is the best way to continuously monitor quality, some things will require the human eye.</p>
<p>Manual testing is the tipping point for shifting left<em>.</em> Whenever any type of test or validation is shifted left, this means that it is executed before manual tests are performed. The benefit of this is that all of these automated activities add to the amount of confidence that we might have in the version of the application that is being tested, increasing the chances that the version will also pass manual testing. In other words, when manual testing starts, it should be very unlikely that any new issues will be uncovered.</p>
<p>There are two types of manual tests:</p>
<ul>
<li>Scripted tests</li>
<li>Exploratory tests</li>
</ul>
<p>Both types of tests will be discussed in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scripted testing</h1>
                </header>
            
            <article>
                
<p>Scripted testing is a technique that is used to minimize the amount of time spent on the test execution while still ensuring full coverage of all relevant test cases. This is done by splitting the testing into two distinct phases: test preparation and test execution. Test preparation is done in parallel to the development of the feature that is to be tested or even before development starts. During test preparation, the feature is analyzed and formal test cases are identified.</p>
<p>Once the test cases that must be executed are identified, manual test scripts are written that describe every step that is to be taken during the test execution phase later. These scripts are engineered in such a way that they are easy to follow and leave no room for questions or doubts. They are also written in such a way that the number of steps to execute is as low as possible. While this may cost more time to prepare, all of it is done to ensure that as little time as possible is spent during the test execution.</p>
<p>A deeper discussion of test analysis and how to identify test cases is beyond the scope of this book. While you are responsible for test case creation, Azure DevOps supports you in this. Using the Test Plans service, you can create test plans and record the test cases within them for quick execution later on.</p>
<p>To create a new test plan, perform the following steps:</p>
<ol>
<li>Open the Azure <span class="packt_screen">Test Plans</span> menu:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1054 image-border" src="assets/a6d9a160-c159-410c-95ed-bf3874089d27.png" style="width:55.50em;height:43.83em;"/></p>
<ol start="2">
<li>In this menu, click on <span class="packt_screen">Test Plans</span>. Here, you will be presented with an overview of all of the test plans you currently have.</li>
<li>Click the <span class="packt_screen">New Test Plan</span> button to start creating a new test plan. This will open a new dialog, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1055 image-border" src="assets/80d5b118-758f-4880-bb0c-c9b6cbd8783a.png" style="width:23.25em;height:18.33em;"/></p>
<ol start="4">
<li>Give a meaningful name to the test plan, for example, a name that illustrates what the test plan is for.</li>
<li>Link the test plan to the correct product area path.</li>
<li>Select the correct iteration, or sprint, that this test relates to.</li>
<li>Press <span class="packt_screen">Create</span> to finalize creating the test plan. This will automatically open this test plan, as shown here:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1056 image-border" src="assets/8c9a1059-2611-4782-99ee-43c9d3a88ca4.png" style="width:40.67em;height:25.58em;"/></p>
<p>A test plan can be split into multiple test suites, which in turn can be split into test suites again. In essence, test suites are for tests what folders are for files. Suites can be managed by pressing the ellipsis button that appears when hovering over the test suite. This is shown in the preceding screenshot.</p>
<p>After creating a test plan, it is time to add one or more test cases to the plan. To do this, ensure that the <span class="packt_screen">Define</span> tab is open for a test suite and click the <span class="packt_screen">New Test Case</span> button. A new popup will open:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1057 image-border" src="assets/c2669c64-0e59-4ded-8133-31bd11039dfb.png" style="width:59.67em;height:31.17em;"/></p>
<p>Here, the test steps and expected outcomes can be defined. To define a new test case, perform the following steps:</p>
<ol>
<li>Enter a title for the test case.</li>
<li>In the dialog, enter one or more actions and expected results that describe the test case in detail.</li>
<li>Once the test case is completely described, press the <span class="packt_screen">Save &amp; Clos</span><span class="packt_screen">e</span> button to save the test case and return to the previous screen where you can manage the test suites.</li>
</ol>
<p>Once the preparation is done and a feature is ready to be tested, all tests are executed. Since all tests are scripted in detail, this can be done quickly and effectively. There might even be developers, business analysts, or people from other parts of the company helping with the test execution. This means that the test execution itself will be really quick.</p>
<p>To start the execution of a test suite or plan, perform the following steps:</p>
<ol>
<li>Navigate to the <span class="packt_screen">Execute</span> tab:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1058 image-border" src="assets/c5b38e53-2506-453e-83f9-f32e6cfad57e.png" style="width:33.42em;height:17.08em;"/></p>
<ol start="2">
<li>Select one or more test cases.</li>
<li>Select one of the run options at the top-right.</li>
</ol>
<p>When choosing to run the tests against a web application, a new browser window with a <em>test runner</em> will open. This test runner can be used to go through all of the test cases and for every test case, through all of the steps, and keep track of all successes and errors as shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1059 image-border" src="assets/a9829353-3312-453f-977e-1995ee5ce661.png" style="width:23.25em;height:20.08em;"/></p>
<p>The tick or cross after every test step can be used to keep track of the outcomes for individual steps. If a step is marked as incorrect, a comment with the defect can be added. To mark a test case as passed or marked, the blue drop-down menu at the top-right can be used for marking the outcome. Once a test outcome is selected, the runner automatically progresses to the next test. Once all tests are performed, the results can be saved using the <span class="packt_screen">Save and close</span> button on the top-left.</p>
<p>To view the outcome of a test run, navigate to <span class="packt_screen">Test Plan</span><span class="packt_screen">s</span> and then <span class="packt_screen">Runs</span> to get the following dashboard:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1060 image-border" src="assets/19a4fdc6-e416-4905-bee5-b272bd762c41.png" style="width:62.67em;height:34.17em;"/></p>
<p>Here, you can select the run for which you want to see the outcomes to get a quick overview of the test outcomes. On the second tab, <span class="packt_screen">Test results</span>, it is possible to view a list of all test cases and whether they passed or not.</p>
<p>A major benefit of having detailed scripts is that the same tests can be performed more than once, reducing the cost per execution. If a test plan is executed multiple times, all run history is maintained and can be accessed using the view shown in the preceding screenshot. This is useful if manual tests are used as part of a regression test; however, once this becomes the case, it is often even more beneficial to automate the tests using system tests, if possible.</p>
<div class="packt_tip">It is possible to execute the same test multiple times, but for different <em>configurations</em>. When developing web applications, this is often done to test using different browsers. For desktop applications, this might be used to test for different operating systems. Working with configurations is detailed in the Microsoft documentation at <a href="https://docs.microsoft.com/en-us/azure/devops/test/mtm/test-configurations-specifying-test-platforms?view=azure-devops">https://docs.microsoft.com/en-us/azure/devops/test/mtm/test-configurations-specifying-test-platforms?view=azure-devops</a></div>
<p>The next section will discuss a final form of functional testing, namely, exploratory testing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploratory testing</h1>
                </header>
            
            <article>
                
<p>Writing and executing detailed test scripts can take a lot of time from both the test engineer and test executioner, so often these tests are automated. Once they are automated, they will fall into the category of system tests and automated UI tests in particular.</p>
<p>This does not necessarily mean that manual tests provide no value or no good return on investment at all. There are just some things that the human eye will catch that a computer will not, such as interfaces that are just not user friendly, misaligned interface elements, and text lines or images that are not fully displayed but get hidden behind other elements.</p>
<p>To catch these errors while not spending large amounts of time on detailed test scripting, exploratory testing might be a solution. In this approach, a tester opens the application and starts investigating those parts of the application that they feel contain the most risks with regard to the upcoming release. While exploring the application, the tester keeps track of which parts of the application they have visited and which test cases they have performed. Meanwhile, the tester also keeps track of new risks they identify or test cases they have not performed yet. In doing so, they are creating a list of covered and uncovered test cases while they are working. It also allows the tester to keep focusing on the most important risk and test cases all of the time. Once the exploratory test run is over, the tester can report on which application areas and test cases have been covered, which have not, and which risks are still not explored at all. This report can be valuable input for a product manager who must decide whether to move forward with a release or not.</p>
<p>A common misconception is that exploratory testing means that a tester is just clicking around to see whether the application is working okay. This is not the case, and the previous paragraphs have shown that exploratory testing is a highly structured activity that requires practice. If performed well, test preparation and test execution are interwoven during an exploratory testing session.</p>
<p>Exploratory testing is a great tool for when there is limited time or the amount of time available for testing is not known upfront. Exploratory testing may yield findings that need to be recorded as defects. How to do this is up next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reporting manual test results</h1>
                </header>
            
            <article>
                
<p>One of the activities that is also part of testing is the reporting of any defects or other issues found. This is often tedious and time-consuming work. You must try and reproduce the issue one more time, trying to remember how the issue manifested itself again, and write down all of these steps. Then, both the desired and undesired outcomes must be described, screenshots must be taken, and everything has to be inserted into a bug tracker or work management tool, such as Azure DevOps.</p>
<p>To make this easier, there is a <strong>Test &amp; Feedback</strong> extension for Azure DevOps available. The extension simply provides buttons for recording screenshots or videos and annotating them with text or drawings. Once an issue is found and documented by a recording or screenshot, it can be automatically submitted to Azure DevOps boards.</p>
<p>This extension is freely available from the Azure DevOps marketplace and runs in both Firefox and Chrome. Support for Edge is being worked on at the time of writing. A link to the extension is included at the end of this chapter.</p>
<div class="packt_infobox">The Test &amp; Feedback extension can be used when both executing scripted tests and when performing exploratory tests.</div>
<p>This concludes the discussion of different types of functional tests. The next section will help you to decide which type of test to use in your projects.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Strategies for deciding which types of functional tests you need</h1>
                </header>
            
            <article>
                
<p>With so many different types of tests, which type of test is the best for your project? Given the wide range of types of tests and their different properties, the answer is as you might expect: a mix of all of them, as they all have different properties.</p>
<p>The following diagram shows the relation between the time the different types of tests take to execute and the confidence in the quality of the software they provide. It shows that while manual tests that complete successfully have the highest likelihood of identifying any defects, they also take the longest to execute. For automated tests, the time taken for tens of thousands of unit tests can often be kept to a few minutes, while ten to a hundred system tests can take over 30 minutes:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1208 image-border" src="assets/55e1c0d2-a1ce-4c40-91d3-2fc8ae012b01.png" style="width:37.83em;height:25.83em;"/></p>
<p class="CDPAlignLeft CDPAlign"><span>Looking at this trade-off, it often makes sense to prefer</span> unit <span>tests over integration tests, integration tests over system tests, and any type of manual test over automated tests.</span></p>
<div class="packt_tip">
<p>If the quality of unit and integration tests increases, then this line will climb even more to the top-left. High-quality software architecture will also help to reduce the need for system and integration tests and increase the guarantees that unit tests bring. Both of these can make the positive effects of automated tests that run fast and often even stronger.</p>
</div>
<p>The understanding of this trade-off also helps to understand two models that can be used on deciding on your testing strategy: the testing pyramid and the testing trophy, which are discussed in the following two sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The testing pyramid</h1>
                </header>
            
            <article>
                
<p>In many older projects, there are not too many automated functional tests. Often, many of these tests are slow to run, have a large test scope, are hard to maintain, and fail regular without a clear cause. The value that these tests provide is often very limited. To counter the lack of good automated tests, there are then many manual tests that are used to do a full regression test of the application before a new version is deployed. These automated tests are very time consuming and rarely executed. There is no fast feedback to developers and defects are often detected late. It is hard to practice DevOps in such a situation since the focus in DevOps is on creating new versions often and at a high rate.</p>
<p>Such a group of tests for an application is often called an ice-cream cone of tests: many manual tests and few automated tests, of which only a few are unit tests. The ice-cream cone of tests is an anti-pattern, yet often found in older and/or long-running projects:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-955 image-border" src="assets/4b02dd29-7a62-410c-9acf-2c26d5aaa097.png" style="width:35.00em;height:24.58em;"/></p>
<p>To battle this, another, opposing, model was introduced: the test pyramid. This model advocates having many unit tests that give feedback on the quality of the application within minutes, quickly pointing out most of the errors. On top of this, other types of slower tests are layered to catch only those errors that previous layers cannot catch. Using this approach, there is a good trade-off between test coverage and test duration.</p>
<div class="packt_tip packt_infobox">Please note that the test pyramid does NOT advocate a layered approach. Do not first build a layer of unit tests and only proceed to integration tests when all unit tests are done. Instead, it advocates proportions: you should have a healthy ratio between unit tests, integration tests, and system tests.</div>
<p>General advice on the best ratio between different types of tests is very hard to give. But in most projects, a ratio of 1:5-15 for each step in the pyramid can be reasonable.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The testing trophy</h1>
                </header>
            
            <article>
                
<p>While the testing pyramid is a well-known and often-used approach for classifying tests and deciding on which types of tests to create, this approach has been criticized as well. While moving away from manual and system tests is widely accepted to be needed in DevOps teams, the focus on unit tests is not universally accepted. Some object to the fact that the testing pyramid hints at creating many more unit tests than integration tests.</p>
<p>These reasons for this objection are as follows:</p>
<ul>
<li><strong>Unit tests tend to be closely tied to the implementation that they test.</strong> Looking back at the test of <kbd>WorkDivider</kbd> in the section on unit tests<em>,</em> it can be seen that it relies on knowing how the <kbd>DivideWork</kbd> method is implemented. The test actually verifies the actual implementation: the call to <kbd>SendMessage()</kbd>. Many unit tests have this characteristic and, as a result, adding many unit tests increases the effort needed to change the implementation of the class-level design of a solution.</li>
<li><strong>Unit tests tend to have a higher rate of change than integration tests.</strong> Unit test classes are closely tied to the class they test. That means that if the class they test is replaced, the unit tests for this class also lose all value. For this reason, it is argued that integration tests might have a higher return on investment.</li>
<li><strong>Real value comes from integrating components, not from individual components.</strong> Even when all units are working in isolation, there might not be any value delivered by a system. The real value of software only comes once it is integrated and ready to run. Since testing should confirm value delivery, it is argued that the focus should be on writing integration tests over unit tests.</li>
</ul>
<p>To deal with these objections, the testing trophy was introduced by <em>Kent C. Dodds</em>. This model adopts the testing pyramid in the sense that it advocates as few manual and system tests as possible but differs in the fact that it does not emphasize unit tests over integration tests, but the other way around. The name testing trophy comes from the fact that if this was drawn, this would result in a figure that would resemble a trophy.</p>
<p>Unfortunately, there is no silver bullet and the best advice is to know about all three models and the reasoning behind them and apply the appropriate lines of reasoning to your current situation. When it comes to testing, there is no single best solution for all.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Types of non-functional tests</h1>
                </header>
            
            <article>
                
<p>Functional tests are mostly concerned with verifying whether the behavior displayed by an application is the behavior that is expected; however, there are more risks when it comes to application development: whether an application performs actions quickly enough, whether this performance degrades if more users use the system concurrently, and whether the system is easy for end users<span> </span><span>to use</span><span>. Tests that verify these properties of a system under test are called non-functional tests.</span></p>
<p>There are many types of non-functional tests, but three of them that are important in DevOps scenarios are as follows:</p>
<ul>
<li>Performance testing</li>
<li>Load testing</li>
<li>Usability testing</li>
</ul>
<p>Let's go over them one by one.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance testing</h1>
                </header>
            
            <article>
                
<p>Performance tests are executed to establish how quickly an application can perform an action, given a set of resources. Performance tests are often executed using specialized tools and run against a fully assembled system. If the tools used for automated API or UI tests record the duration of a test, the duration of these tests can be used as performance results as well.</p>
<p>To compare results over multiple test runs, it is important to ensure that all factors influencing performance are kept the same between tests. The setup of virtual machines for both test subjects and test runners should stay the same. The application configuration should remain constant and integration points should be in the same state as much as possible—for example, instead of reusing the same database, the same database should be restored from a backup before every performance test. This ensures that the results are comparable.</p>
<p>While performance and load tests are often mixed up, they are two different types of tests.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Load testing</h1>
                </header>
            
            <article>
                
<p>Load tests are performed to measure how much load the system can take before it breaks. These types of tests are sometimes also called stress tests. Unlike in a performance test, there are many requests executed in parallel. What is measured is the average performance of all requests, while slowly increasing the number of requests to the system. In most cases, this will identify a breaking point, a specific number of requests per second at which the performance will suddenly decrease. This is the number of requests per second that the system can maximally serve. When executing a load test, gathering the average performance while increasing the maximum number of requests will often result in a graph like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-957 image-border" src="assets/e8c4d209-1327-409d-951f-70ee08b4bd42.png" style="width:113.33em;height:57.67em;"/></p>
<p>This graph shows why it is important to know the breaking point of an application: too much load might crumble a system unexpectedly because of the sudden nature of the change in response times. Knowing where this point is allows operators to act before this point is reached in a production environment.</p>
<p>At the end of this chapter is a link to an online Microsoft lab for developers to practice load testing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Usability testing</h1>
                </header>
            
            <article>
                
<p>Another important type of testing is usability testing. While other types of tests focus on verifying whether the implementation has the behavior desired by the product team, usability tests focus on verifying whether the expectations of the user are actually met. This means that the test scope is even larger and these tests can identify user interfaces that are clumsy and help to find unclear text or user requests that were misinterpreted.</p>
<p>Usability tests are run by letting the user work with the final application on one or more tasks and observing or asking about how they interacted with the application. Results are often much more verbose than "passed" or "not passed,<span>"</span> and results are often given back to a product owner to write new user stories or change requirements.</p>
<p>A great technique for enabling usability testing is the use of feature flags. Feature flags enable us to gradually expose a new feature to more users. This capability can also be used to at first only expose a new feature to a select, limited set of users that are part of a usability study. This allows researchers or product owners to closely observe these users using the new feature, while other users cannot access it yet.</p>
<div class="packt_infobox">Feature flags were discussed in <a href="8ab4597a-becd-4855-9b45-89045982c14a.xhtml">Chapter 4</a>, <em>Continuous Deployment,</em> as a strategy for progressive exposure. Progressive exposure of new features is in itself a form of usability or user acceptance testing.</div>
<p>This approach can be extended to execute A/B tests. In these types of tests, half of the users are exposed to a new feature while the other half are not. Metrics are then gathered about all of the users to see whether the new feature brings users the benefits that were predicted for it—for example, if users use the application for more hours per day or not. This topic will be expanded upon in <a href="d32e245c-5a95-45a5-907f-b57ae17f60d3.xhtml">Chapter 11</a>, <em>Gathering User Feedback,</em> which looks at how to gather user feedback.</p>
<p>Doing this shifts usability testing closer to the right in the release process. It can also be shifted to the left by performing usability tests not with the final application, but with mockups.</p>
<p>This concludes the discussion of the different types of tests. In the next section, metrics and tests will be used to automatically measure quality and implement quality gates.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Executing tests in a pipeline</h1>
                </header>
            
            <article>
                
<p class="mce-root">Developers should execute tests on their local machine before opening a merge request for their code. That way, they can be confident that any of the changes they made did not break any of the previous behaviors of their code. In theory, this provides the guarantee that all code merged to the master branch compiles and has all tests passing. In practice, there are many reasons why this is not the case. Some can be as follows:</p>
<ul>
<li>Some tests might not be able to be run locally. They depend on confidential configuration values or are configured to run against a fully configured system. One or both of these are often the case for system tests. There are many situations where it is impossible to run system tests from the local system. Not all of these situations are necessarily desirable or insurmountable—but still, this is often the case.</li>
<li>Developers are only humans. They might forget to run the tests on their local machines after that one final tweak or are convinced that their changes did not break any existing behavior. Especially when delivering a bug fix under pressure, it can be tempting to skip running tests for the sake of speed.</li>
</ul>
<p>To prevent these situations from allowing code that is not fully tested to propagate through the pipeline, it is recommended to have all tests also execute from within the pipeline. The following sections will show how to do this for unit tests and integration tests and for tests that are being run using other systems. First up are unit tests.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running unit tests</h1>
                </header>
            
            <article>
                
<p>For many languages, support for running unit tests from the pipeline is built into Azure DevOps. Unit tests can be executed for C#, TypeScript, Python, Maven, C++, Go, and many more.</p>
<p>For some of these languages, a single ready-made task is available. One example of this are tests written in C#. During the execution of .NET tests<span>—</span>for example, in C#<span>—</span>test results are automatically stored in an XML format that is understood by the build agent.</p>
<p>This allows the pipeline agent to interpret the test results and visualize them in the build results, as shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1061 image-border" src="assets/073d95c2-9ccb-471c-a457-b2b84d0c98a9.png" style="width:105.92em;height:51.33em;"/></p>
<p>For some languages, more than one task has to be executed. For example, tests written in TypeScript are often executed via an NPM command. The following YAML can be used to do this:</p>
<pre>- task: Npm@0<br/>  displayName: 'Run unit tests - npm run tests'<br/>  inputs:<br/>    cwd: src<br/>    command: run<br/>    arguments: test</pre>
<p>This will execute a custom NPM command as specified in <kbd>package.json</kbd>. Unfortunately, this will not store the test results in a format that the pipeline agent understands. To translate the outcomes into the correct format, another task is needed:</p>
<pre>- task: PublishTestResults@2<br/>  displayName: 'Publish Test Results'<br/>  inputs:<br/>    testResultsFiles: '**\reportTests\TEST-*.xml'<br/>    mergeTestResults: true<br/>  condition: succeededOrFailed()</pre>
<p>Whether test results are available directly or have to be translated varies from programming language to programming language. Besides publishing test results, it is also recommended to gather test coverage results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recording unit test code coverage</h1>
                </header>
            
            <article>
                
<p>It is a best practice to not only run all unit tests during the build but to also determine the percentage of the code base that was executed during any of these tests. This is called <em>unit test code coverage</em> and is an indication of how thorough the tests are. The build can also be configured to publish the code coverage achieved by unit tests.</p>
<p>To configure the build to publish test coverage for .NET Core unit tests, the following steps must be performed:</p>
<ol>
<li>Install the NuGet package, <kbd>coverlet.msbuild</kbd>, into the unit test project.</li>
<li>Use the .NET Core task to execute the test and add two parameters to also generate coverage reports, <kbd>/p:CollectCoverage=true</kbd> and <kbd>/p:CoverletOutputFormat=cobertura</kbd>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1062 image-border" src="assets/0c075189-0c29-43ce-bc30-ab4574b3ba94.png" style="width:42.58em;height:28.83em;"/></p>
<ol start="3">
<li>Add the <span class="packt_screen">Publish code coverage</span> task:
<ol>
<li>Set the code coverage tool to <kbd>cobertura</kbd>.</li>
<li>Configure <kbd>$(System.DefaultWorkingDirectory)/**/coverage.cobertura.xml</kbd> as the summary file:</li>
</ol>
</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1063 image-border" src="assets/4242f2a9-0b46-4429-bee6-f3b6cd305884.png" style="width:41.00em;height:23.00em;"/></p>
<ol start="4">
<li>The build's run details will now contain code coverage reports.</li>
</ol>
<p>This is all of the configuration needed to generate detailed code coverage reports. The generated reports contain the number of covered and uncovered code blocks and the calculated coverage percentage. These reports are part of the build results page.</p>
<p>Next to unit tests, integration tests can also be run as part of the pipeline and they often come with the challenge of dealing with managing configuration settings.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running integration tests</h1>
                </header>
            
            <article>
                
<p>Integration tests are often written in the same framework as unit tests. Still, they come with a unique challenge of their own. Often, they require one or more settings that specify how to integrate with one or more other components that are part of the test. Looking back at the integration test of the <kbd>MessageSender</kbd> class discussed before, this is an example of this problem.</p>
<p>Remember that this test had a <kbd>.runsettings</kbd> file that should specify <kbd>connectionString</kbd> to the queue that it should use? This <kbd>connectionString</kbd> setting cannot be checked into source control. Instead, a placeholder can be checked into source control, which is then replaced with the actual secret during pipeline execution.</p>
<p>In this case, this would mean that the following <kbd>pipeline.runsettings</kbd> file would be checked into source control:</p>
<pre>&lt;?xml version="1.0" encoding="utf-8"?&gt;<br/>&lt;RunSettings&gt;<br/>    &lt;TestRunParameters&gt;<br/>        &lt;Parameter name="MessageSenderConnectionString" value="#{MessageSenderConnectionString}#" /&gt;<br/> &lt;/TestRunParameters&gt;<br/>&lt;/RunSettings&gt;</pre>
<p>Before starting the actual test execution, another task is run to replace the placeholders with the actual values. These values can be securely retrieved from a variable group, key vault, or pipeline variable as discussed in <a href="7dcfa6ee-1460-4c49-a156-58073b263c90.xhtml">Chapter 3</a>, <em>Moving to Continuous Integration</em>. There are multiple extensions for Azure DevOps available that can be used for replacing placeholders with actual values. The following YAML is an example of how to do this:</p>
<pre>  - task: qetza.replacetokens.replacetokens-task.replacetokens@3<br/>    displayName: 'Replace tokens in pipeline.runsettings'<br/>    inputs:<br/>      targetFiles: $(System.DefaultWorkingDirectory)/integrationtests-location/pipeline.runsettings</pre>
<p>After the execution of the replace tokens task, the test runner can be invoked just as with unit tests.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running external tests</h1>
                </header>
            
            <article>
                
<p>Besides unit and integration tests, you will probably want to execute tests using other systems. For example, Azure DevOps has no built-in support for executing load tests or automated UI tests. For these types of tests, it is necessary to invoke other systems from the pipeline. Many systems can be integrated in this way.</p>
<p>How to do this differs from system to system, but most of the time, the following steps will apply:</p>
<ol>
<li>Configure the tests in the external system.</li>
<li>Install an extension for Azure DevOps that makes new tasks available for calling into that external system from the pipeline.</li>
</ol>
<ol start="3">
<li>Create a service connection to the external system.</li>
<li>Add the task to the pipeline.</li>
</ol>
<p>For details on configuring integrations, a good starting point is often the website of the vendor of the third-party product.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Maintaining quality</h1>
                </header>
            
            <article>
                
<p>The previous sections detailed various types of tests and metrics that can be used for describing the quality of an application. With these in mind, it is time to start thinking about the tools that can be used for maintaining high quality or even increasing quality.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Code reviews</h1>
                </header>
            
            <article>
                
<p>One of the most powerful tools for guarding code quality is the code review. When working with Git, a pull request needs to be performed to merge the changes of a developer back into the mainline. A pull request allows one or more other developers to review all changes and comment on them. The developer that opened the pull request can review the comments and make changes accordingly, increasing the quality of the changes while they keep working.</p>
<p>For code reviews to work at their best, it is important <span>not</span><span> </span><span>to</span> <span>see them as a gate that you must get your changes through with as little effort as possible. It is much more fruitful to have an open attitude based on the assumption that everyone is trying to create high-quality code, and see the code review as a starter of a discussion on code quality. It is important to change perspectives, from seeing the code review as an annoying ritual in software development where others will complain about your code to an opportunity for welcoming others to give their input about your code and helping you to write code of higher quality.</span></p>
<p>Once such an attitude is in place, code reviews will become a source of learning. They will result in discussions between peers about the best way forward for tackling an issue: the best way not just for now, but for the future as well, taking no technical debt and having enough unit and integration tests along with the code that is to be merged. Code reviews are also a great tool for mentoring junior developers, allowing them to receive feedback on their own work. It can even be more valuable to have junior developers review the code of senior developers. This way, they can ask questions about things they do not yet know, and it will often lead to them pointing out overly complex solutions that might become technical debt over time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Automatically gathering quality metrics</h1>
                </header>
            
            <article>
                
<p>Next to manual reviews, there are also many tools available for automatically determining the quality of a code base. Some are built into Azure Pipelines, but more elaborate functionality comes from separate code-scanning tools. There are different mathematical approaches to measuring technical debt, and using a tool to do so provides great insights into not only the quality of an application but also the changes over time.</p>
<p>One possible tool for measuring the quality of an application is SonarCloud. SonarCloud is the SaaS offering based on SonarCube. This tool can automatically scan a code base for possible bugs, security risks, technical debt, and other metrics for quality. This is a paid, separate offering that integrates with the Azure DevOps pipelines. To work with SonarCloud, you have to create an account and retrieve a project key to invoke a SonarCloud scan from Azure DevOps.</p>
<p>For invoking SonarCloud, a set of three tasks is used that are part of an extension for Azure DevOps. After installing the extension and configuring a SonarCloud service connection, three tasks are added to the pipeline to set up the analysis, execute it, and (optionally) fail the build if the quality degrades. The first task is the only one that takes configuration, which is shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1064 image-border" src="assets/42a64a8a-f5fe-4ee5-96dc-fbf895c98852.png" style="width:58.92em;height:36.58em;"/></p>
<p>Every build that is now executed will automatically have its code scanned by SonarCloud, where extensive reports about the quality will be available. On top of these reports, a dashboard is generated that provides a quick overview of some key quality metrics:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1205 image-border" src="assets/563c7578-719b-492b-a5e9-62d26da05f7e.png" style="width:53.42em;height:44.75em;"/></p>
<p>Here is another glimpse of the dashboard showing quality metrics:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1206 image-border" src="assets/ad9442d8-72dd-470e-be51-4e065ecc3a03.png" style="width:56.83em;height:34.83em;"/></p>
<p>Code-scanning tools can be used for reporting the quality of the code, but can also act as a quality gate that will stop the merge of changes or deployment to a specific environment if insufficient quality is detected.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualizing quality</h1>
                </header>
            
            <article>
                
<p>Measuring the quality of an application continuously has no value unless it is acted upon. Dashboards can be a powerful tool for gaining continuous insight into the current level of quality and how quality has changed over time.</p>
<p>Most code quality tools have built-in reporting options, and they can be valuable for quality assurance engineers. They provide detailed insight into which parts of the application are of higher quality and which types of issues <span>recently </span><span>occurred more frequently.</span></p>
<p>The downside of this type of dashboard is that they can be hard to read and that they are not in the tool where developers perform most of their work. For this reason, it can be beneficial to also create dashboards in Azure DevOps to report on quality. An example of such a dashboard is shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1066 image-border" src="assets/2b3d9268-5c56-452d-8164-53df1f5637f7.png" style="width:59.00em;height:32.42em;"/></p>
<p>This dashboard shows an overview of the current quality and application code, as well as some recent history. Here, you can find the following information:</p>
<ul>
<li>The number of recent changes is shown on the top, along with the result of the most recent SonarCloud <span class="packt_screen">Quality Gate</span> outcome, which currently reads <span class="packt_screen">Passed</span>.</li>
<li>The results of the two different builds in this project are shown in row two.</li>
<li>Rows three and four show aggregations all of the builds and releases within the project. Symbols are used to denote the status of the builds and releases: successful, failed, or still running.</li>
<li>On the right, two widgets are used to show the percentage of failed tests and the corresponding number of failed tests over the last 10 builds.</li>
<li>The results of the latest release runs per environment are shown below this.</li>
</ul>
<p>Dashboards such as these can be created per team or per project using built-in widgets or extensions. There are many extensions available in the Azure DevOps marketplace. For example, in the preceding dashboard, the <span><span class="packt_screen">Team Project Health</span></span> e<span>xtension is used</span><span>.</span></p>
<p>Azure DevOps dashboards can be configured to automatically refresh every five minutes, making them usable as wallboards as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Quality gates</h1>
                </header>
            
            <article>
                
<p>Measuring, reporting, and even visualizing quality is important and valuable; however, if no one is acting upon all of these metrics, it has no value to the development team. To prevent this, automatic quality gates or checks can be introduced.</p>
<p>One way to implement quality gates is by failing the continuous integration build whenever a test fails, the test coverage falls too low, or the thresholds that were set for the code-scanning tool are no longer being met. These are all things that have been discussed before. Another option to enforce standards is by adding gates or checks to pipelines. This way, certain conditions have to be met before the pipeline can continue.</p>
<p>How to do this differs between classic releases and YAML multi-stage pipelines.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classic releases</h1>
                </header>
            
            <article>
                
<p>One other option is the use of <em>gates</em> on Azure release pipelines. Here, it is possible to specify one or more conditions that have to be met before a release is allowed to be deployed to a specific environment. Gates can also be part of an extension, such as the SonarCloud extension that has been discussed before.</p>
<p>Gates can be added by selecting any stage in a release pipeline and editing the pre-deployment conditions. After enabling gates, one or more gates can be added. The following screenshot of a release pipeline shows how to disallow the deployment of any build of insufficient quality to an environment:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1067 image-border" src="assets/465b860b-b319-4278-a6cf-ad246cccc912.png" style="width:79.25em;height:46.08em;"/></p>
<p>The use of deployment approves and gates are not mutually exclusive, so a mix of both can be used.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multi-stage pipelines</h1>
                </header>
            
            <article>
                
<p>Gates, as they are available for classic releases, are present in multi-stage YAML pipelines. In YAML pipelines, another mechanism is available: checks. Checks are configured to automatically validate if one or more conditions are met before allowing a pipeline to continue. Checks can be added to resources that are used in a stage. If one or more checks are found on one or more resources in a stage, all of the checks have to be passed before the pipeline continues to that stage. Checks can be added to environments and service connections.</p>
<p>To add a check to an environment, navigate to that environment:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1068 image-border" src="assets/2a067ac8-9272-423a-8f5d-214c1d78a5f7.png" style="width:33.33em;height:22.08em;"/></p>
<p><span> </span><span>Now perform the following steps:</span></p>
<ol start="1">
<li>At the top-right, expand the menu and choose <span class="packt_screen">Approvals and checks</span>:</li>
<li>In the new view that opens, choose <span class="packt_screen">See all</span> to see all of the different types of checks that are available. Choose <span class="packt_screen">Invoke Azure Function</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1069 image-border" src="assets/966a8a22-3133-48fb-926e-ac030750cf5e.png" style="width:116.25em;height:51.50em;"/></p>
<ol start="4">
<li>In the popup that opens, configure the Azure function to be called. At a minimum, the function URL and key have to be provided.</li>
<li>Choose <span class="packt_screen">Create</span>.</li>
</ol>
<p>Once the check is created, every deployment job (see <a href="8ab4597a-becd-4855-9b45-89045982c14a.xhtml"/><a href="8ab4597a-becd-4855-9b45-89045982c14a.xhtml">Chapter 4</a>, <em>Continuous Deployment</em>) that targets the environment has to pass this check. The check is passed when the function that is called returns a successful response code.</p>
<p>The following types of checks are supported:</p>
<ul>
<li><strong>Evaluate artifact</strong>: Validate that an artifact of the type container image passes a custom policy. These policies are defined in a language called <em>Rego</em>.</li>
<li><strong>Invoke REST API</strong>: Post details about the pipeline to an Azure function to execute custom logic. If the API returns a successful HTTP status code, the pipeline is allowed to continue.</li>
<li><strong>Invoke Azure Function</strong>: The same as the <span class="packt_screen">Invoke REST API</span> check, but with some defaults for Azure Functions.</li>
<li><strong>Query Azure Monitor alerts</strong>: Only continue if the specified alerts are not in an active state.</li>
<li><strong>Required template</strong>: Only allow the pipeline to continue if the current YAML pipeline extends one or more configured base YAML pipelines.</li>
</ul>
<p>Checks can be a powerful mechanism for guaranteeing that one or more conditions are met before allowing a pipeline to continue.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you learned about how to measure and assert the quality of software development processes. Releasing quickly and often requires the software that is written to be of high quality. Testing is needed to ensure that you write software of high quality with little technical debt in it. You learned about the different types of tests and the pros and cons of the different types of automated and manual tests. Finally, you learned how code reviews and tools can help to maintain high quality in your project by reporting on quality and by serving as a quality gate.</p>
<p>With this knowledge, you are now able to discuss the tests and test types to help you to decide which tests are needed for your applications, which risks you can address using which types of tests, and whether you need many of them or can omit them. You are now also capable of setting up and configuring code-scanning tools to ensure that changes of insufficient quality are not merged to the mainline.</p>
<p>In the next chapter, you will learn about security and compliance, two topics that remain equally important when practicing DevOps.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<p>As we conclude, here is a list of questions for you to test your knowledge regarding this chapter's material. You will find the answers in the <em>Assessments</em> section of the Appendix:</p>
<ol>
<li>True or false: A unit test verifies the working of a single unit in isolation.</li>
<li>True or false: An integration test verifies the working of a fully assembled system.</li>
<li>Which of the following statements is correct, regarding the principles of the testing pyramid?
<ol>
<li>Have many integration tests, fewer unit tests, and even fewer system tests.</li>
<li>Have many unit tests, fewer integration tests, and even fewer system tests.</li>
<li>Have many system tests, fewer integration tests, and many unit tests.</li>
</ol>
</li>
</ol>
<ol start="4">
<li>Which of the following is not a non-functional type of test?
<ol>
<li>Load testing</li>
<li>Usability testing</li>
<li>Applicability testing</li>
<li>Performance testing</li>
</ol>
</li>
<li>Testing is about gaining insights into the quality of work. Which techniques can be employed to prevent work of insufficient quality propagating through to production?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li>More information about the testing trophy model can be found at <a href="https://testingjavascript.com/">https://testingjavascript.com/</a> and <a href="https://kentcdodds.com/blog/write-tests/">https://kentcdodds.com/blog/write-tests/</a>.</li>
<li>More information about writing tests using C# can be found at <a href="https://docs.microsoft.com/en-us/visualstudio/test/walkthrough-creating-and-running-unit-tests-for-managed-code?view=vs-2019">https://docs.microsoft.com/en-us/visualstudio/test/walkthrough-creating-and-running-unit-tests-for-managed-code?view=vs-2019</a> and <a href="https://docs.microsoft.com/en-us/dotnet/core/testing/unit-testing-best-practices">https://docs.microsoft.com/en-us/dotnet/core/testing/unit-testing-best-practices</a>.</li>
</ul>
<ul>
<li>More information about the Test &amp; Feedback extension can be found at <a href="https://marketplace.visualstudio.com/items?itemName=ms.vss-exploratorytesting-web">https://marketplace.visualstudio.com/items?itemName=ms.vss-exploratorytesting-web</a>.</li>
<li>Practical labs to practice with load testing can be found at <a href="https://docs.microsoft.com/en-us/learn/modules/load-test-web-app-azure-devops/">https://docs.microsoft.com/en-us/learn/modules/load-test-web-app-azure-devops/</a> and <a href="https://docs.microsoft.com/en-us/learn/modules/run-non-functional-tests-azure-pipelines/index">https://docs.microsoft.com/en-us/learn/modules/run-non-functional-tests-azure-pipelines/index</a>.</li>
<li>Practical labs to practice with automating UI tests can be found at <a href="https://docs.microsoft.com/en-us/learn/modules/run-functional-tests-azure-pipelines/index">https://docs.microsoft.com/en-us/learn/modules/run-functional-tests-azure-pipelines/index</a>.</li>
<li>More information about SonarCloud can be found at <a href="https://sonarcloud.io">https://sonarcloud.io</a>.</li>
<li>The team project health extension can be found at <a href="https://marketplace.visualstudio.com/items?itemName=ms-devlabs.TeamProjectHealth">https://marketplace.visualstudio.com/items?itemName=ms-devlabs.TeamProjectHealth</a>.</li>
<li>More information about Rego can be found at <a href="https://www.openpolicyagent.org/docs/latest/policy-language/">https://www.openpolicyagent.org/docs/latest/policy-language/</a>.</li>
</ul>


            </article>

            
        </section>
    </body></html>