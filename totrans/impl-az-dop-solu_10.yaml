- en: Continuous Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, you learned about the different types of techniques
    that are used to help to increase the rate at which you deliver changes to your
    production environment. If you are already using these techniques in your daily
    work, you will quickly notice that this is only possible if your work is of sufficient
    quality. If the quality of your work is not high enough, you will face many outages
    or issues and your end users will not be happy. To be successful, increasing the
    rate of change and increasing the quality of your work must go hand in hand. To
    recognize the quality of your work and to increase it, you first need to know
    what is meant by quality. This is where testing comes in. Testing is the discipline
    of reporting about the quality of software.
  prefs: []
  type: TYPE_NORMAL
- en: To introduce the topic of testing, this chapter will start by looking at how
    the quality of software development can be measured. Next, the topic of functional
    testing will be explored. First, the testing cone and pyramid will be introduced.
    These are models that can be used to determine which types of tests are needed
    and how many of each should be used. After this, the different types of tests
    will be discussed one by one. You will learn about how they work, what they test,
    and the benefits and downsides of the different types of tests. Finally, the last
    section will focus on how all metrics and test results, once generated and collected
    by your pipelines, can continuously report on the quality of the work of your
    team and even prevent changes of insufficient quality propagating to your users.
    All of this will help you to maintain the high quality of your software and enable
    you to confidently deliver that software quickly and frequently.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding test types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing functional tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing nonfunctional tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintaining quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To experiment with the techniques described in this chapter, you might need
    one or more of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An Azure DevOps project with access to build and release pipelines and dashboards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visual Studio 2019
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Basic + Test Plans license for Azure DevOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A SonarCloud subscription
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these are available for free or can be obtained for free for a limited
    trial period.
  prefs: []
  type: TYPE_NORMAL
- en: Defining quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the primary goals of the DevOps mindset discussed in [Chapter 1](889f9224-f1b6-414d-bc80-16563f66e1e7.xhtml),
    *Introduction to DevOps*, is increasing the flow of value to end users. To do
    this, software must be deployed frequently, maybe even multiple times per day.
    To make frequent deployments possible, two things are important: automation and
    quality. Automation has been discussed extensively in the previous chapters, and
    so now it is time to move on to the topic of quality.'
  prefs: []
  type: TYPE_NORMAL
- en: Once an automated build and release pipeline is in place and changes are starting
    to flow to production at an increasing speed, it is time to start measuring the
    quality of these changes. Even more importantly, this allows us to abort changes
    that are not of sufficient quality. What actually makes quality *sufficient* can
    differ from project to project. When creating games, a few bugs might be annoying
    for the user but nothing more. When creating software for airplanes or medical
    use, a single bug may cost lives. In software, higher quality is more expensive
    and/or takes more time. So, there is a trade-off between the number of features
    we can deliver and the quality that can be guaranteed. For every project, there
    is a different optimal trade-off between these.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before quality can be measured, it is important that you first establish how
    to measure the quality of software. A common approach to monitoring the quality
    of software is to gather one or more metrics. For example, it could be decided
    to collect a set of five measurements every week. Graphing these metrics over
    time provides insight into how the quality of the software is evolving. An example
    of this might look something like the graph shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3bb1f3cd-63f4-4466-8bb4-64abdbf00639.png)'
  prefs: []
  type: TYPE_IMG
- en: The next sections discuss several examples of metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics for quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Metrics are a means of capturing something that is measured as a number. In
    software development, metrics are often used to represent a particular quality
    aspect that can be hard to quantify in itself. For example, the quality of a piece
    of software can be very hard to describe by itself. This holds even more for how
    quality changes. For this reason, we often capture numbers that, taken together,
    say something about the quality of software.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to realize that metrics are a great tool, but should always
    be used with caution. For one thing, there might be more factors influencing the
    (perceived) quality of software than the metrics that are being measured. Also,
    once people know that a specific metric is recorded, they can optimize their work
    to increase or decrease the metric. While this might show the desired numbers
    in reports, this might not necessarily mean software quality is really improving.
    To combat this, often, more than one metric is recorded.
  prefs: []
  type: TYPE_NORMAL
- en: A well-known example is that of story point velocity in agile work environments.
    Recording the sprint velocity for a team to see whether it is becoming more efficient
    over time sounds effective; however, if the team size varies from sprint to sprint,
    then the metric might be useless since attendance is influencing velocity as well.
    Also, the metric can be easily falsified by a team agreeing on multiplying all
    estimations by a random number every sprint. While this would increase the numbers
    every sprint, this would not relate to an increase in team throughput anymore.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on to metrics for measuring the quality of software, it can be difficult
    to objectively measure the quality of written code. Developers often have many
    opinions as to what constitutes *good code*, and the more the topic is discussed,
    the harder it can be to find consent in a team; however, when shifting attention
    to the results that come from using that code, it becomes easier to identify metrics
    that can help to provide insights into the quality of the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples of this are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The percentage of integration builds that fails**: If code does not compile
    or pass automated tests, then this is an indication that it is of insufficient
    quality. Since tests can be executed automatically by build pipelines whenever
    a new change is pushed, they are an excellent tool for determining the quality
    of code. Also, since they can be run and their results gathered before we deploy
    a change to production, the results can be used to cancel a change before deploying
    it to the next stage of a release pipeline. This way, only changes of sufficient
    quality propagate to the next stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The percentage of code covered by automated tests**: If a larger part of
    the code is being tested by unit tests, this increases the quality of the software.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The change failure rate**: This is the percentage of deployments of new versions
    of the code that lead to issues. An example of this is a situation where the web
    server runs out of memory after the deployment of a new version of the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The amount of unplanned work**: The amount of unplanned work that has to
    be performed in any period of time can be a great metric of quality. If the team
    is creating a SaaS offering that it is also operating, there will be time spent
    on operational duties. This is often referred to as unplanned work. The amount
    of unplanned work can be an indicator of the quality of the planned work. If the
    amount of unplanned work increases, then this may be because the quality has gone
    down. Examples of unplanned work can be live site incidents, following up on alerts,
    hotfixes, and patches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The number of defects that are being reported by users**: If the number of
    bugs reported by users increases, this can be a sign that quality has been declining.
    Often, this is a lagging indicator, so once this number starts increasing, quality
    might have been going down for a while already. Of course, there can be many other
    reasons for this number increasing: new operating systems, an increase in the
    number of users, or changing expectations from users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The number of known issues**: Even if there are very few new defects being
    found or reported, if defects are never fixed and the number of known issues keeps
    increasing slowly, then the quality of the software will slowly decline over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The amount of technical debt**: Technical debt is a term used to describe
    the consequences of sacrificing code quality for short-term gains, such as the
    quick delivery of code. Technical debt is discussed in detail in the next section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing is an activity that is performed to find and report on the quality of
    software. Test results (insights into quality) can be used to allow or cancel
    a change progressing to the next release stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, another dimension of quality is explored: the amount of
    technical debt in a code base.'
  prefs: []
  type: TYPE_NORMAL
- en: Technical debt
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Technical debt is a term that describes the future costs of sacrificing code
    quality for something else. For example, to expedite the delivery of a new feature,
    a developer may choose to quickly expand an existing class with a few new methods
    to realize this feature. If the resulting class does not adhere to the principles
    of object-oriented design or grows to be too large, this can make for a class
    that is difficult to understand and maintain or change later. The term "debt"
    implies that something (time, quality, attention, or work) is owed to the solution.
    So long as this debt is not paid off, you have to pay interest in the form of
    all other work being slowed down a little bit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Technical debt can take many forms, but some examples are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Code that is not covered by any unit test where changes to the implementation
    of said code cannot be verified using the original tests that were used to create
    it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code that is not written in a self-explanatory fashion using meaningful variable
    and method names
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code that does not adhere to coding principles, such as KISS, YAGNI, DRY, and/or
    SOLID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classes that are too complex because they have too many variables and methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methods that are too complex because they have too many statements (flow-control
    statements specifically)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classes or namespaces that have circular dependencies through different parts
    of the application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classes that do not adhere to the architectural design for the application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many forms of technical debt, and it can be daunting to oversee all
    of them. For this reason, there are many tools available that can measure the
    technical debt in a code base automatically and report on that. Tools for doing
    this will be discussed in the *Maintaining quality *section.
  prefs: []
  type: TYPE_NORMAL
- en: While technical debt is often considered a bad thing, there might be good reasons
    for creating technical debt on purpose. Just as with a regular debt, it is important
    to manage the height of the debt and to ensure that interest can be paid and the
    debt itself can be paid off.
  prefs: []
  type: TYPE_NORMAL
- en: Companies often take on technical debt during the start-up phase, where it is
    often a conscious decision to quickly create a working solution. While this first
    version is used to validate the business proposition and attract funds, developers
    can pay off this debt by reimplementing or refactoring (parts of) the application.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason might be a market opportunity or an important business event
    that has been planned months in advance. Taking on some technical debt to make
    deadlines and deliver on time can be worth the cost.
  prefs: []
  type: TYPE_NORMAL
- en: However, never paying the debt and only taking on more debt over time will also
    increase the metaphorical interest to be paid every time a developer needs to
    make a change. The result will be that any change will take longer than the previous
    one. If this starts happening, it is unavoidable that at some point no change
    will be worthwhile anymore, since the cost always outweighs the benefits. At this
    point, a project or product has failed.
  prefs: []
  type: TYPE_NORMAL
- en: When talking about tests, it is important to understand which types of tests
    exist. The next section will go into this subject.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding test types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In traditional software development, tests were often executed when *development
    was complete*, the *application was declared dev-done*, the *feature set was frozen*,
    or a similar statement. After declaring the development done, testing was performed,
    and often, a long period of going back and forth between testing and bug fixing
    started. The result was often that many bugs were still found after going live.
  prefs: []
  type: TYPE_NORMAL
- en: Shifting left is a testing principle that states that automated testing should
    be done earlier in the development process. If all activities involved with software
    development are drawn on a line from inception to release, then shifting left
    means moving automated testing activities closer to inception.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, a wide selection of different types of tests are recognized—for
    example, unit tests, integration tests, and system tests. Different sources can
    suggest different types of tests, but these are some of the more well-known types.
    No matter the specific name of a type of test, when looking at tests with a high
    level of abstraction, they are often divided into the following two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Functional tests**: Functional tests are in place to test whether the desired
    functionality is actually realized by the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-functional tests**: Non-functional tests are used to verify whether the
    other desired properties of an application are realized and whether undesirable
    properties are not present.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These types are further broken down into smaller subcategories, as shown in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bbf7a7ed-dc9f-4f07-94e8-5891cd6dc942.png)'
  prefs: []
  type: TYPE_IMG
- en: The following three sections contain brief recaps of the different types of
    functional and non-functional tests. This is to facilitate later discussions on
    which type of test to choose in which situation and how much of each type of test
    your project might need.
  prefs: []
  type: TYPE_NORMAL
- en: Types of automated functional tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When talking about automated functional tests, the three most-used types are
    unit tests, integration tests, and system tests. These types of test can be compared
    along several axes: the time it takes to create a test, the time it takes to execute
    a test, and the scope that they test:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unit tests**: Unit tests are the quickest to write, and they execute very
    quickly, often in less than a millisecond. They test the smallest possible scope
    in an application, often a single class or method. This means that, once written,
    it should virtually never be necessary to change a unit test. For many systems,
    it is more likely that a test will be deleted rather than changed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration tests**: Integration tests take more time to write since they
    concern themselves with multiple units that have to be set up to work together.
    The execution of these tests should still be fast, averaging from below a second
    up to tens of seconds. Integration tests have a larger test scope, which means
    that, in return for this, they will cover a larger part of the code and are more
    likely to detect defects that are introduced with a change.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**System tests**: System tests test a fully assembled and running application.
    Depending on the type of application, these are often API tests or automated UI
    tests. These tests take a lot of time to create since they rely on a deployed
    system to run and often require the setting up of an initial state in a database
    or another persistent store. The tests take a long time to execute, sometimes
    minutes per test. They are also less reliable and much more fragile than unit
    and integration tests. Even a minor change in an interface can cause a whole series
    of tests to fail. On the other hand, system tests can detect errors that both
    unit and integration tests cannot, since they actually test the running system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please note that having a large test scope in a test has both an upside and
    a downside. The upside is that it can detect many errors. The downside is that
    a failing test with a very large test scope provides only a limited insight into
    what has gone wrong. Such a test failure will often require more investigation
    than a failing test with a smaller test scope.
  prefs: []
  type: TYPE_NORMAL
- en: The following sections explore each type of test in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unit tests are used to test a single unit in isolation. When working in an object-oriented
    programming language, this will come down to having one test class for every class
    in an application. For full test coverage, the test class will then have one or
    more tests for every public method of the corresponding application class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unit tests should run extremely fast—on average, in a few milliseconds or less.
    To make this possible, each class is instantiated without its dependencies. This
    is enabled by the use of interfaces, where classes depend on interfaces instead
    of directly on other classes. For tests, the dependencies are then replaced with
    mock classes, as shown in the following diagram. On the left, the runtime configuration
    is shown; on the right, the configuration during tests is shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd411400-b29b-4582-887e-6cdeb93cb918.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A mock class implements the same interface but has no behavior associated by
    default. Specific behavior can be set up on a per-test basis. Mocks can also be
    used to verify that certain operations or functions on a dependency are called.
    As an example, take the following C# class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To instantiate this class in an automated test, an implementation of the `IMessageSender`
    interface is needed. To work around this dependency, a mocking framework such
    as Moq can be used to test `WorkDivider`, as follows. In these examples, `NUnit`
    is used as the testing framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This means that it is not possible to write unit tests for classes that interact
    with other systems, such as databases, caches, or service buses. To ensure that
    this does not make it impossible to cover large parts of the application with
    tests, it is common practice to isolate the integration with other systems in
    separate classes. These classes contain the interaction with a remote system,
    but no business logic and as little code as possible. It is then accepted that
    these classes are not covered by unit tests. The typical design patterns that
    are used to do this are the facade, adapter, and repository patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Links to a more detailed guide on writing unit tests and how to mock classes
    are included at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests should be ready to run at the computer of every developer that clones
    the code base of an application. They should not require any special configuration
    or setup on the local computer and should be ready to go. This way, everyone who
    works with the code base can run the unit tests on their local computer. It is,
    therefore, a good practice for developers to run all unit tests on their own computers
    before pushing changes to the central repository.
  prefs: []
  type: TYPE_NORMAL
- en: Next to this local verification step, unit tests should also be a part of the
    continuous integration build. You will learn how to do this in the *Executing
    tests in a pipeline* section later on. As long as there are failing unit tests
    in a pull request, it is better not to merge the changes to the master branch.
    This can even be made impossible using Git repo branch policies, which were discussed
    in [Chapter 2](2be30fb3-5e71-4180-9830-f119e5a6cd76.xhtml), *Everything Starts
    with Source Control*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, the discussion of automated functional tests continues
    with integration tests.
  prefs: []
  type: TYPE_NORMAL
- en: Integration tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Integration tests are used to test whether a group of components works together
    correctly. These tests are used for two purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the test coverage for those parts of an application that are not
    covered by unit tests—for example, classes that interact with other systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Addressing risks that are not addressed in unit tests and deal with classes
    interacting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be hard to understand what integration risks are since it might seem
    clear that the whole will work as expected, as soon as all parts are working as
    expected. To understand this risk better, imagine that two components working together
    are responsible for climate control. One is written measuring the temperature
    in degrees Celsius and the other is acting on that temperature, expecting its
    input in degrees Fahrenheit. It will quickly become clear that, while both components
    are working as intended, exchanging numbers and taking action based on those numbers,
    the combination will not produce the desired outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Integration tests, especially those that interact with other systems, will not
    only take longer to run than unit tests but often require more setup or configuration
    to run as well. This may even include secrets such as usernames, passwords, or
    certificates. To handle configuration such as this, a settings file can be created
    next to the tests from which settings are loaded before the tests are executed.
    Every developer can then create their own copy of that file and run the tests
    using their own configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing the example from the previous section, let''s assume that the `MessageSender`
    class that implements the `IMessageSender` interface needs a connection string
    to do its work. A test class for `MessageSender` might then look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`connectionString` needed for constructing the `MessageSender` class is received
    from the `Parameters` object on `TestContext.` This is the `NUnit` approach for
    making settings from a `.runsettings` file available. The exact approach can vary
    per test framework. An example `.runsettings` file would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Moving the settings out to a separate file ensures that secrets are not checked
    into source control. In the *Executing tests in a pipeline* section, you will
    learn how to build a `.runsettings` file for running tests in a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is because integration tests should also be part of the continuous integration
    build if possible. However, there is a risk that this will make a continuous integration
    build too slow. To counter this, one of the following solutions can be implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: Integration tests are executed in a separate build that is triggered in parallel
    to the continuous integration build. This way, the duration of the continuous
    integration build stays low while the integration tests are still continuously
    executed, and developers get fast feedback on their work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration tests are executed later in the pipeline, closer to the release
    of the software—for example, before or after the deployment to a test environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The downside of the first approach is that executing integration tests this
    way will mean that the tests will no longer work as a quality gate before code
    is merged to the master. They will, of course, continue working as a quality-reporting
    mechanism. This means that, while errors might be merged, they will be detected
    and reported by the build.
  prefs: []
  type: TYPE_NORMAL
- en: The second approach does not have this risk since executing the tests is still
    part of the pipeline from source control to production; however, in this approach,
    the execution of the tests might be deferred to a later moment in time if not
    every build enters at least part of the release pipeline. This means that defects
    might become visible later on, extending the time between detecting and fixing
    an issue.
  prefs: []
  type: TYPE_NORMAL
- en: In either approach, failing integration tests will no longer block merging changes
    and you hence have to find another way to ensure that developers will take responsibility
    for fixing the defect that caused the tests to fail.
  prefs: []
  type: TYPE_NORMAL
- en: These trade-offs become even more evident with system tests, which often take
    so long that it is not possible to make them part of the continuous integration
    build.
  prefs: []
  type: TYPE_NORMAL
- en: System tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The third and final type of automated functional tests is system tests. These
    tests are meant to run against a fully assembled and running application. System
    tests come in two flavors, depending on the type of application: an API test or
    a UI test. System tests can take a long time to execute, and it is not uncommon
    for long tests with an elaborate setup of test data to take well over a minute.'
  prefs: []
  type: TYPE_NORMAL
- en: You might come across something called coded UI tests. This is a now-deprecated
    Microsoft solution for writing UI tests. These tests could be executed from Azure
    Pipelines. Luckily, there are many alternatives, as referenced in Microsoft's
    deprecation message at [https://devblogs.microsoft.com/devops/changes-to-coded-ui-test-in-visual-studio-2019](https://devblogs.microsoft.com/devops/changes-to-coded-ui-test-in-visual-studio-2019).
  prefs: []
  type: TYPE_NORMAL
- en: System tests execute against a running application, which means that they will
    need configuration and setup before they can be run. The application needs to
    be running in a controlled environment and all of the integrations with data stores
    need to be fully operational. Integrations with other systems need to be either
    up and running or swapped out with a replacement mock to ensure that all operations
    that integrate with those systems will function properly.
  prefs: []
  type: TYPE_NORMAL
- en: These conditions make it less likely that developers will execute these tests
    on their local machines as they are making changes to the application. It is only
    when they are creating a new test or changing a test that they might do so. However,
    even then they may be executing these tests not against a locally run version
    of the application, but against a version that is already deployed to a test environment.
    This is not necessarily a good thing, but often just the reality in most teams.
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to creating API or UI tests is unfortunately beyond the scope
    of this book. There are many products available on the market and which one is
    the best to use will differ from project to project.
  prefs: []
  type: TYPE_NORMAL
- en: When executing system tests as part of the pipeline, they are often run after
    the code has been deployed to at least one environment. This will often be the
    test environment. This implies that the system tests are on the critical path
    from a source code change to the deployment to production. If this path becomes
    too long, they can also be taken out of the pipeline. They are then run on a schedule—for
    example, every night. Just as with integration tests, this speeds up the pipeline,
    but it removes the opportunity to use system tests as a quality gate.
  prefs: []
  type: TYPE_NORMAL
- en: System tests, and UI tests in particular, are often fragile and can stop working
    unexpectedly after minor changes. For this reason, it is advised that you keep
    their number as low as possible; however, keep in mind that these are the tests
    that can catch particular errors, such as misconfiguration or other runtime errors,
    database-application mismatches, or series of operations that create error states.
  prefs: []
  type: TYPE_NORMAL
- en: Besides automated function tests, there are also manual functional tests that
    have value in many DevOps projects. These are discussed next.
  prefs: []
  type: TYPE_NORMAL
- en: Types of manual functional tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While automated tests are a great tool for receiving feedback on development
    work quickly and often, there are still things that will be tested manually. While
    automating repetitive tests is the best way to continuously monitor quality, some
    things will require the human eye.
  prefs: []
  type: TYPE_NORMAL
- en: Manual testing is the tipping point for shifting left*.* Whenever any type of
    test or validation is shifted left, this means that it is executed before manual
    tests are performed. The benefit of this is that all of these automated activities
    add to the amount of confidence that we might have in the version of the application
    that is being tested, increasing the chances that the version will also pass manual
    testing. In other words, when manual testing starts, it should be very unlikely
    that any new issues will be uncovered.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of manual tests:'
  prefs: []
  type: TYPE_NORMAL
- en: Scripted tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploratory tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both types of tests will be discussed in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Scripted testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scripted testing is a technique that is used to minimize the amount of time
    spent on the test execution while still ensuring full coverage of all relevant
    test cases. This is done by splitting the testing into two distinct phases: test
    preparation and test execution. Test preparation is done in parallel to the development
    of the feature that is to be tested or even before development starts. During
    test preparation, the feature is analyzed and formal test cases are identified.'
  prefs: []
  type: TYPE_NORMAL
- en: Once the test cases that must be executed are identified, manual test scripts
    are written that describe every step that is to be taken during the test execution
    phase later. These scripts are engineered in such a way that they are easy to
    follow and leave no room for questions or doubts. They are also written in such
    a way that the number of steps to execute is as low as possible. While this may
    cost more time to prepare, all of it is done to ensure that as little time as
    possible is spent during the test execution.
  prefs: []
  type: TYPE_NORMAL
- en: A deeper discussion of test analysis and how to identify test cases is beyond
    the scope of this book. While you are responsible for test case creation, Azure
    DevOps supports you in this. Using the Test Plans service, you can create test
    plans and record the test cases within them for quick execution later on.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a new test plan, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the Azure Test Plans menu:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a6d9a160-c159-410c-95ed-bf3874089d27.png)'
  prefs: []
  type: TYPE_IMG
- en: In this menu, click on Test Plans. Here, you will be presented with an overview
    of all of the test plans you currently have.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click the New Test Plan button to start creating a new test plan. This will
    open a new dialog, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/80d5b118-758f-4880-bb0c-c9b6cbd8783a.png)'
  prefs: []
  type: TYPE_IMG
- en: Give a meaningful name to the test plan, for example, a name that illustrates
    what the test plan is for.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Link the test plan to the correct product area path.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the correct iteration, or sprint, that this test relates to.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Press Create to finalize creating the test plan. This will automatically open
    this test plan, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8c9a1059-2611-4782-99ee-43c9d3a88ca4.png)'
  prefs: []
  type: TYPE_IMG
- en: A test plan can be split into multiple test suites, which in turn can be split
    into test suites again. In essence, test suites are for tests what folders are
    for files. Suites can be managed by pressing the ellipsis button that appears
    when hovering over the test suite. This is shown in the preceding screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating a test plan, it is time to add one or more test cases to the
    plan. To do this, ensure that the Define tab is open for a test suite and click
    the New Test Case button. A new popup will open:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2669c64-0e59-4ded-8133-31bd11039dfb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the test steps and expected outcomes can be defined. To define a new
    test case, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Enter a title for the test case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the dialog, enter one or more actions and expected results that describe
    the test case in detail.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the test case is completely described, press the Save & Close button to
    save the test case and return to the previous screen where you can manage the
    test suites.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the preparation is done and a feature is ready to be tested, all tests
    are executed. Since all tests are scripted in detail, this can be done quickly
    and effectively. There might even be developers, business analysts, or people
    from other parts of the company helping with the test execution. This means that
    the test execution itself will be really quick.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start the execution of a test suite or plan, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the Execute tab:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c5b38e53-2506-453e-83f9-f32e6cfad57e.png)'
  prefs: []
  type: TYPE_IMG
- en: Select one or more test cases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select one of the run options at the top-right.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When choosing to run the tests against a web application, a new browser window
    with a *test runner* will open. This test runner can be used to go through all
    of the test cases and for every test case, through all of the steps, and keep
    track of all successes and errors as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a9829353-3312-453f-977e-1995ee5ce661.png)'
  prefs: []
  type: TYPE_IMG
- en: The tick or cross after every test step can be used to keep track of the outcomes
    for individual steps. If a step is marked as incorrect, a comment with the defect
    can be added. To mark a test case as passed or marked, the blue drop-down menu
    at the top-right can be used for marking the outcome. Once a test outcome is selected,
    the runner automatically progresses to the next test. Once all tests are performed,
    the results can be saved using the Save and close button on the top-left.
  prefs: []
  type: TYPE_NORMAL
- en: 'To view the outcome of a test run, navigate to Test Plans and then Runs to
    get the following dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19a4fdc6-e416-4905-bee5-b272bd762c41.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, you can select the run for which you want to see the outcomes to get a
    quick overview of the test outcomes. On the second tab, Test results, it is possible
    to view a list of all test cases and whether they passed or not.
  prefs: []
  type: TYPE_NORMAL
- en: A major benefit of having detailed scripts is that the same tests can be performed
    more than once, reducing the cost per execution. If a test plan is executed multiple
    times, all run history is maintained and can be accessed using the view shown
    in the preceding screenshot. This is useful if manual tests are used as part of
    a regression test; however, once this becomes the case, it is often even more
    beneficial to automate the tests using system tests, if possible.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to execute the same test multiple times, but for different *configurations*.
    When developing web applications, this is often done to test using different browsers.
    For desktop applications, this might be used to test for different operating systems.
    Working with configurations is detailed in the Microsoft documentation at [https://docs.microsoft.com/en-us/azure/devops/test/mtm/test-configurations-specifying-test-platforms?view=azure-devops](https://docs.microsoft.com/en-us/azure/devops/test/mtm/test-configurations-specifying-test-platforms?view=azure-devops)
  prefs: []
  type: TYPE_NORMAL
- en: The next section will discuss a final form of functional testing, namely, exploratory
    testing.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Writing and executing detailed test scripts can take a lot of time from both
    the test engineer and test executioner, so often these tests are automated. Once
    they are automated, they will fall into the category of system tests and automated
    UI tests in particular.
  prefs: []
  type: TYPE_NORMAL
- en: This does not necessarily mean that manual tests provide no value or no good
    return on investment at all. There are just some things that the human eye will
    catch that a computer will not, such as interfaces that are just not user friendly,
    misaligned interface elements, and text lines or images that are not fully displayed
    but get hidden behind other elements.
  prefs: []
  type: TYPE_NORMAL
- en: To catch these errors while not spending large amounts of time on detailed test
    scripting, exploratory testing might be a solution. In this approach, a tester
    opens the application and starts investigating those parts of the application
    that they feel contain the most risks with regard to the upcoming release. While
    exploring the application, the tester keeps track of which parts of the application
    they have visited and which test cases they have performed. Meanwhile, the tester
    also keeps track of new risks they identify or test cases they have not performed
    yet. In doing so, they are creating a list of covered and uncovered test cases
    while they are working. It also allows the tester to keep focusing on the most
    important risk and test cases all of the time. Once the exploratory test run is
    over, the tester can report on which application areas and test cases have been
    covered, which have not, and which risks are still not explored at all. This report
    can be valuable input for a product manager who must decide whether to move forward
    with a release or not.
  prefs: []
  type: TYPE_NORMAL
- en: A common misconception is that exploratory testing means that a tester is just
    clicking around to see whether the application is working okay. This is not the
    case, and the previous paragraphs have shown that exploratory testing is a highly
    structured activity that requires practice. If performed well, test preparation
    and test execution are interwoven during an exploratory testing session.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory testing is a great tool for when there is limited time or the amount
    of time available for testing is not known upfront. Exploratory testing may yield
    findings that need to be recorded as defects. How to do this is up next.
  prefs: []
  type: TYPE_NORMAL
- en: Reporting manual test results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the activities that is also part of testing is the reporting of any defects
    or other issues found. This is often tedious and time-consuming work. You must
    try and reproduce the issue one more time, trying to remember how the issue manifested
    itself again, and write down all of these steps. Then, both the desired and undesired
    outcomes must be described, screenshots must be taken, and everything has to be
    inserted into a bug tracker or work management tool, such as Azure DevOps.
  prefs: []
  type: TYPE_NORMAL
- en: To make this easier, there is a **Test & Feedback** extension for Azure DevOps
    available. The extension simply provides buttons for recording screenshots or
    videos and annotating them with text or drawings. Once an issue is found and documented
    by a recording or screenshot, it can be automatically submitted to Azure DevOps
    boards.
  prefs: []
  type: TYPE_NORMAL
- en: This extension is freely available from the Azure DevOps marketplace and runs
    in both Firefox and Chrome. Support for Edge is being worked on at the time of
    writing. A link to the extension is included at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The Test & Feedback extension can be used when both executing scripted tests
    and when performing exploratory tests.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the discussion of different types of functional tests. The next
    section will help you to decide which type of test to use in your projects.
  prefs: []
  type: TYPE_NORMAL
- en: Strategies for deciding which types of functional tests you need
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With so many different types of tests, which type of test is the best for your
    project? Given the wide range of types of tests and their different properties,
    the answer is as you might expect: a mix of all of them, as they all have different
    properties.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the relation between the time the different types
    of tests take to execute and the confidence in the quality of the software they
    provide. It shows that while manual tests that complete successfully have the
    highest likelihood of identifying any defects, they also take the longest to execute.
    For automated tests, the time taken for tens of thousands of unit tests can often
    be kept to a few minutes, while ten to a hundred system tests can take over 30
    minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55e1c0d2-a1ce-4c40-91d3-2fc8ae012b01.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking at this trade-off, it often makes sense to prefer unit tests over integration
    tests, integration tests over system tests, and any type of manual test over automated
    tests.
  prefs: []
  type: TYPE_NORMAL
- en: If the quality of unit and integration tests increases, then this line will
    climb even more to the top-left. High-quality software architecture will also
    help to reduce the need for system and integration tests and increase the guarantees
    that unit tests bring. Both of these can make the positive effects of automated
    tests that run fast and often even stronger.
  prefs: []
  type: TYPE_NORMAL
- en: 'The understanding of this trade-off also helps to understand two models that
    can be used on deciding on your testing strategy: the testing pyramid and the
    testing trophy, which are discussed in the following two sections.'
  prefs: []
  type: TYPE_NORMAL
- en: The testing pyramid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many older projects, there are not too many automated functional tests. Often,
    many of these tests are slow to run, have a large test scope, are hard to maintain,
    and fail regular without a clear cause. The value that these tests provide is
    often very limited. To counter the lack of good automated tests, there are then
    many manual tests that are used to do a full regression test of the application
    before a new version is deployed. These automated tests are very time consuming
    and rarely executed. There is no fast feedback to developers and defects are often
    detected late. It is hard to practice DevOps in such a situation since the focus
    in DevOps is on creating new versions often and at a high rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Such a group of tests for an application is often called an ice-cream cone
    of tests: many manual tests and few automated tests, of which only a few are unit
    tests. The ice-cream cone of tests is an anti-pattern, yet often found in older
    and/or long-running projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b02dd29-7a62-410c-9acf-2c26d5aaa097.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To battle this, another, opposing, model was introduced: the test pyramid.
    This model advocates having many unit tests that give feedback on the quality
    of the application within minutes, quickly pointing out most of the errors. On
    top of this, other types of slower tests are layered to catch only those errors
    that previous layers cannot catch. Using this approach, there is a good trade-off
    between test coverage and test duration.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note that the test pyramid does NOT advocate a layered approach. Do
    not first build a layer of unit tests and only proceed to integration tests when
    all unit tests are done. Instead, it advocates proportions: you should have a
    healthy ratio between unit tests, integration tests, and system tests.'
  prefs: []
  type: TYPE_NORMAL
- en: General advice on the best ratio between different types of tests is very hard
    to give. But in most projects, a ratio of 1:5-15 for each step in the pyramid
    can be reasonable.
  prefs: []
  type: TYPE_NORMAL
- en: The testing trophy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the testing pyramid is a well-known and often-used approach for classifying
    tests and deciding on which types of tests to create, this approach has been criticized
    as well. While moving away from manual and system tests is widely accepted to
    be needed in DevOps teams, the focus on unit tests is not universally accepted.
    Some object to the fact that the testing pyramid hints at creating many more unit
    tests than integration tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'These reasons for this objection are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unit tests tend to be closely tied to the implementation that they test.**
    Looking back at the test of `WorkDivider` in the section on unit tests*,* it can
    be seen that it relies on knowing how the `DivideWork` method is implemented.
    The test actually verifies the actual implementation: the call to `SendMessage()`.
    Many unit tests have this characteristic and, as a result, adding many unit tests
    increases the effort needed to change the implementation of the class-level design
    of a solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unit tests tend to have a higher rate of change than integration tests.**
    Unit test classes are closely tied to the class they test. That means that if
    the class they test is replaced, the unit tests for this class also lose all value.
    For this reason, it is argued that integration tests might have a higher return
    on investment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real value comes from integrating components, not from individual components.**
    Even when all units are working in isolation, there might not be any value delivered
    by a system. The real value of software only comes once it is integrated and ready
    to run. Since testing should confirm value delivery, it is argued that the focus
    should be on writing integration tests over unit tests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To deal with these objections, the testing trophy was introduced by *Kent C.
    Dodds*. This model adopts the testing pyramid in the sense that it advocates as
    few manual and system tests as possible but differs in the fact that it does not
    emphasize unit tests over integration tests, but the other way around. The name
    testing trophy comes from the fact that if this was drawn, this would result in
    a figure that would resemble a trophy.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there is no silver bullet and the best advice is to know about
    all three models and the reasoning behind them and apply the appropriate lines
    of reasoning to your current situation. When it comes to testing, there is no
    single best solution for all.
  prefs: []
  type: TYPE_NORMAL
- en: Types of non-functional tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Functional tests are mostly concerned with verifying whether the behavior displayed
    by an application is the behavior that is expected; however, there are more risks
    when it comes to application development: whether an application performs actions
    quickly enough, whether this performance degrades if more users use the system
    concurrently, and whether the system is easy for end users to use. Tests that
    verify these properties of a system under test are called non-functional tests.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many types of non-functional tests, but three of them that are important
    in DevOps scenarios are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Performance testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usability testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's go over them one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Performance testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance tests are executed to establish how quickly an application can perform
    an action, given a set of resources. Performance tests are often executed using
    specialized tools and run against a fully assembled system. If the tools used
    for automated API or UI tests record the duration of a test, the duration of these
    tests can be used as performance results as well.
  prefs: []
  type: TYPE_NORMAL
- en: To compare results over multiple test runs, it is important to ensure that all
    factors influencing performance are kept the same between tests. The setup of
    virtual machines for both test subjects and test runners should stay the same.
    The application configuration should remain constant and integration points should
    be in the same state as much as possible—for example, instead of reusing the same
    database, the same database should be restored from a backup before every performance
    test. This ensures that the results are comparable.
  prefs: []
  type: TYPE_NORMAL
- en: While performance and load tests are often mixed up, they are two different
    types of tests.
  prefs: []
  type: TYPE_NORMAL
- en: Load testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load tests are performed to measure how much load the system can take before
    it breaks. These types of tests are sometimes also called stress tests. Unlike
    in a performance test, there are many requests executed in parallel. What is measured
    is the average performance of all requests, while slowly increasing the number
    of requests to the system. In most cases, this will identify a breaking point,
    a specific number of requests per second at which the performance will suddenly
    decrease. This is the number of requests per second that the system can maximally
    serve. When executing a load test, gathering the average performance while increasing
    the maximum number of requests will often result in a graph like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8c4d209-1327-409d-951f-70ee08b4bd42.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This graph shows why it is important to know the breaking point of an application:
    too much load might crumble a system unexpectedly because of the sudden nature
    of the change in response times. Knowing where this point is allows operators
    to act before this point is reached in a production environment.'
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this chapter is a link to an online Microsoft lab for developers
    to practice load testing.
  prefs: []
  type: TYPE_NORMAL
- en: Usability testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another important type of testing is usability testing. While other types of
    tests focus on verifying whether the implementation has the behavior desired by
    the product team, usability tests focus on verifying whether the expectations
    of the user are actually met. This means that the test scope is even larger and
    these tests can identify user interfaces that are clumsy and help to find unclear
    text or user requests that were misinterpreted.
  prefs: []
  type: TYPE_NORMAL
- en: Usability tests are run by letting the user work with the final application
    on one or more tasks and observing or asking about how they interacted with the
    application. Results are often much more verbose than "passed" or "not passed,"
    and results are often given back to a product owner to write new user stories
    or change requirements.
  prefs: []
  type: TYPE_NORMAL
- en: A great technique for enabling usability testing is the use of feature flags.
    Feature flags enable us to gradually expose a new feature to more users. This
    capability can also be used to at first only expose a new feature to a select,
    limited set of users that are part of a usability study. This allows researchers
    or product owners to closely observe these users using the new feature, while
    other users cannot access it yet.
  prefs: []
  type: TYPE_NORMAL
- en: Feature flags were discussed in [Chapter 4](8ab4597a-becd-4855-9b45-89045982c14a.xhtml), *Continuous
    Deployment,* as a strategy for progressive exposure. Progressive exposure of new
    features is in itself a form of usability or user acceptance testing.
  prefs: []
  type: TYPE_NORMAL
- en: This approach can be extended to execute A/B tests. In these types of tests,
    half of the users are exposed to a new feature while the other half are not. Metrics
    are then gathered about all of the users to see whether the new feature brings
    users the benefits that were predicted for it—for example, if users use the application
    for more hours per day or not. This topic will be expanded upon in [Chapter 11](d32e245c-5a95-45a5-907f-b57ae17f60d3.xhtml),
    *Gathering User Feedback,* which looks at how to gather user feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Doing this shifts usability testing closer to the right in the release process.
    It can also be shifted to the left by performing usability tests not with the
    final application, but with mockups.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the discussion of the different types of tests. In the next section,
    metrics and tests will be used to automatically measure quality and implement
    quality gates.
  prefs: []
  type: TYPE_NORMAL
- en: Executing tests in a pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Developers should execute tests on their local machine before opening a merge
    request for their code. That way, they can be confident that any of the changes
    they made did not break any of the previous behaviors of their code. In theory,
    this provides the guarantee that all code merged to the master branch compiles
    and has all tests passing. In practice, there are many reasons why this is not
    the case. Some can be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Some tests might not be able to be run locally. They depend on confidential
    configuration values or are configured to run against a fully configured system.
    One or both of these are often the case for system tests. There are many situations
    where it is impossible to run system tests from the local system. Not all of these
    situations are necessarily desirable or insurmountable—but still, this is often
    the case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developers are only humans. They might forget to run the tests on their local
    machines after that one final tweak or are convinced that their changes did not
    break any existing behavior. Especially when delivering a bug fix under pressure,
    it can be tempting to skip running tests for the sake of speed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To prevent these situations from allowing code that is not fully tested to propagate
    through the pipeline, it is recommended to have all tests also execute from within
    the pipeline. The following sections will show how to do this for unit tests and
    integration tests and for tests that are being run using other systems. First
    up are unit tests.
  prefs: []
  type: TYPE_NORMAL
- en: Running unit tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For many languages, support for running unit tests from the pipeline is built
    into Azure DevOps. Unit tests can be executed for C#, TypeScript, Python, Maven,
    C++, Go, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: For some of these languages, a single ready-made task is available. One example
    of this are tests written in C#. During the execution of .NET tests—for example,
    in C#—test results are automatically stored in an XML format that is understood
    by the build agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'This allows the pipeline agent to interpret the test results and visualize
    them in the build results, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/073d95c2-9ccb-471c-a457-b2b84d0c98a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For some languages, more than one task has to be executed. For example, tests
    written in TypeScript are often executed via an NPM command. The following YAML
    can be used to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This will execute a custom NPM command as specified in `package.json`. Unfortunately,
    this will not store the test results in a format that the pipeline agent understands.
    To translate the outcomes into the correct format, another task is needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Whether test results are available directly or have to be translated varies
    from programming language to programming language. Besides publishing test results,
    it is also recommended to gather test coverage results.
  prefs: []
  type: TYPE_NORMAL
- en: Recording unit test code coverage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is a best practice to not only run all unit tests during the build but to
    also determine the percentage of the code base that was executed during any of
    these tests. This is called *unit test code coverage* and is an indication of
    how thorough the tests are. The build can also be configured to publish the code
    coverage achieved by unit tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'To configure the build to publish test coverage for .NET Core unit tests, the
    following steps must be performed:'
  prefs: []
  type: TYPE_NORMAL
- en: Install the NuGet package, `coverlet.msbuild`, into the unit test project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the .NET Core task to execute the test and add two parameters to also generate
    coverage reports, `/p:CollectCoverage=true` and `/p:CoverletOutputFormat=cobertura`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0c075189-0c29-43ce-bc30-ab4574b3ba94.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Add the Publish code coverage task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the code coverage tool to `cobertura`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Configure `$(System.DefaultWorkingDirectory)/**/coverage.cobertura.xml` as
    the summary file:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4242f2a9-0b46-4429-bee6-f3b6cd305884.png)'
  prefs: []
  type: TYPE_IMG
- en: The build's run details will now contain code coverage reports.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is all of the configuration needed to generate detailed code coverage reports.
    The generated reports contain the number of covered and uncovered code blocks
    and the calculated coverage percentage. These reports are part of the build results
    page.
  prefs: []
  type: TYPE_NORMAL
- en: Next to unit tests, integration tests can also be run as part of the pipeline
    and they often come with the challenge of dealing with managing configuration
    settings.
  prefs: []
  type: TYPE_NORMAL
- en: Running integration tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Integration tests are often written in the same framework as unit tests. Still,
    they come with a unique challenge of their own. Often, they require one or more
    settings that specify how to integrate with one or more other components that
    are part of the test. Looking back at the integration test of the `MessageSender`
    class discussed before, this is an example of this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that this test had a `.runsettings` file that should specify `connectionString`
    to the queue that it should use? This `connectionString` setting cannot be checked
    into source control. Instead, a placeholder can be checked into source control,
    which is then replaced with the actual secret during pipeline execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, this would mean that the following `pipeline.runsettings` file
    would be checked into source control:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Before starting the actual test execution, another task is run to replace the
    placeholders with the actual values. These values can be securely retrieved from
    a variable group, key vault, or pipeline variable as discussed in [Chapter 3](7dcfa6ee-1460-4c49-a156-58073b263c90.xhtml), *Moving
    to Continuous Integration*. There are multiple extensions for Azure DevOps available
    that can be used for replacing placeholders with actual values. The following
    YAML is an example of how to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: After the execution of the replace tokens task, the test runner can be invoked
    just as with unit tests.
  prefs: []
  type: TYPE_NORMAL
- en: Running external tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Besides unit and integration tests, you will probably want to execute tests
    using other systems. For example, Azure DevOps has no built-in support for executing
    load tests or automated UI tests. For these types of tests, it is necessary to
    invoke other systems from the pipeline. Many systems can be integrated in this
    way.
  prefs: []
  type: TYPE_NORMAL
- en: 'How to do this differs from system to system, but most of the time, the following
    steps will apply:'
  prefs: []
  type: TYPE_NORMAL
- en: Configure the tests in the external system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install an extension for Azure DevOps that makes new tasks available for calling
    into that external system from the pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a service connection to the external system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the task to the pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For details on configuring integrations, a good starting point is often the
    website of the vendor of the third-party product.
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous sections detailed various types of tests and metrics that can be
    used for describing the quality of an application. With these in mind, it is time
    to start thinking about the tools that can be used for maintaining high quality
    or even increasing quality.
  prefs: []
  type: TYPE_NORMAL
- en: Code reviews
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most powerful tools for guarding code quality is the code review.
    When working with Git, a pull request needs to be performed to merge the changes
    of a developer back into the mainline. A pull request allows one or more other
    developers to review all changes and comment on them. The developer that opened
    the pull request can review the comments and make changes accordingly, increasing
    the quality of the changes while they keep working.
  prefs: []
  type: TYPE_NORMAL
- en: For code reviews to work at their best, it is important not to see them as a
    gate that you must get your changes through with as little effort as possible.
    It is much more fruitful to have an open attitude based on the assumption that
    everyone is trying to create high-quality code, and see the code review as a starter
    of a discussion on code quality. It is important to change perspectives, from
    seeing the code review as an annoying ritual in software development where others
    will complain about your code to an opportunity for welcoming others to give their
    input about your code and helping you to write code of higher quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once such an attitude is in place, code reviews will become a source of learning.
    They will result in discussions between peers about the best way forward for tackling
    an issue: the best way not just for now, but for the future as well, taking no
    technical debt and having enough unit and integration tests along with the code
    that is to be merged. Code reviews are also a great tool for mentoring junior
    developers, allowing them to receive feedback on their own work. It can even be
    more valuable to have junior developers review the code of senior developers.
    This way, they can ask questions about things they do not yet know, and it will
    often lead to them pointing out overly complex solutions that might become technical
    debt over time.'
  prefs: []
  type: TYPE_NORMAL
- en: Automatically gathering quality metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next to manual reviews, there are also many tools available for automatically
    determining the quality of a code base. Some are built into Azure Pipelines, but
    more elaborate functionality comes from separate code-scanning tools. There are
    different mathematical approaches to measuring technical debt, and using a tool
    to do so provides great insights into not only the quality of an application but
    also the changes over time.
  prefs: []
  type: TYPE_NORMAL
- en: One possible tool for measuring the quality of an application is SonarCloud.
    SonarCloud is the SaaS offering based on SonarCube. This tool can automatically
    scan a code base for possible bugs, security risks, technical debt, and other
    metrics for quality. This is a paid, separate offering that integrates with the
    Azure DevOps pipelines. To work with SonarCloud, you have to create an account
    and retrieve a project key to invoke a SonarCloud scan from Azure DevOps.
  prefs: []
  type: TYPE_NORMAL
- en: 'For invoking SonarCloud, a set of three tasks is used that are part of an extension
    for Azure DevOps. After installing the extension and configuring a SonarCloud
    service connection, three tasks are added to the pipeline to set up the analysis,
    execute it, and (optionally) fail the build if the quality degrades. The first
    task is the only one that takes configuration, which is shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/42a64a8a-f5fe-4ee5-96dc-fbf895c98852.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Every build that is now executed will automatically have its code scanned by
    SonarCloud, where extensive reports about the quality will be available. On top
    of these reports, a dashboard is generated that provides a quick overview of some
    key quality metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/563c7578-719b-492b-a5e9-62d26da05f7e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is another glimpse of the dashboard showing quality metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad9442d8-72dd-470e-be51-4e065ecc3a03.png)'
  prefs: []
  type: TYPE_IMG
- en: Code-scanning tools can be used for reporting the quality of the code, but can
    also act as a quality gate that will stop the merge of changes or deployment to
    a specific environment if insufficient quality is detected.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Measuring the quality of an application continuously has no value unless it
    is acted upon. Dashboards can be a powerful tool for gaining continuous insight
    into the current level of quality and how quality has changed over time.
  prefs: []
  type: TYPE_NORMAL
- en: Most code quality tools have built-in reporting options, and they can be valuable
    for quality assurance engineers. They provide detailed insight into which parts
    of the application are of higher quality and which types of issues recently occurred
    more frequently.
  prefs: []
  type: TYPE_NORMAL
- en: 'The downside of this type of dashboard is that they can be hard to read and
    that they are not in the tool where developers perform most of their work. For
    this reason, it can be beneficial to also create dashboards in Azure DevOps to
    report on quality. An example of such a dashboard is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b3d9268-5c56-452d-8164-53df1f5637f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This dashboard shows an overview of the current quality and application code,
    as well as some recent history. Here, you can find the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of recent changes is shown on the top, along with the result of the
    most recent SonarCloud Quality Gate outcome, which currently reads Passed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results of the two different builds in this project are shown in row two.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rows three and four show aggregations all of the builds and releases within
    the project. Symbols are used to denote the status of the builds and releases:
    successful, failed, or still running.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the right, two widgets are used to show the percentage of failed tests and
    the corresponding number of failed tests over the last 10 builds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results of the latest release runs per environment are shown below this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dashboards such as these can be created per team or per project using built-in
    widgets or extensions. There are many extensions available in the Azure DevOps
    marketplace. For example, in the preceding dashboard, the Team Project Health
    extension is used.
  prefs: []
  type: TYPE_NORMAL
- en: Azure DevOps dashboards can be configured to automatically refresh every five
    minutes, making them usable as wallboards as well.
  prefs: []
  type: TYPE_NORMAL
- en: Quality gates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Measuring, reporting, and even visualizing quality is important and valuable;
    however, if no one is acting upon all of these metrics, it has no value to the
    development team. To prevent this, automatic quality gates or checks can be introduced.
  prefs: []
  type: TYPE_NORMAL
- en: One way to implement quality gates is by failing the continuous integration
    build whenever a test fails, the test coverage falls too low, or the thresholds
    that were set for the code-scanning tool are no longer being met. These are all
    things that have been discussed before. Another option to enforce standards is
    by adding gates or checks to pipelines. This way, certain conditions have to be
    met before the pipeline can continue.
  prefs: []
  type: TYPE_NORMAL
- en: How to do this differs between classic releases and YAML multi-stage pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Classic releases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One other option is the use of *gates* on Azure release pipelines. Here, it
    is possible to specify one or more conditions that have to be met before a release
    is allowed to be deployed to a specific environment. Gates can also be part of
    an extension, such as the SonarCloud extension that has been discussed before.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gates can be added by selecting any stage in a release pipeline and editing
    the pre-deployment conditions. After enabling gates, one or more gates can be
    added. The following screenshot of a release pipeline shows how to disallow the
    deployment of any build of insufficient quality to an environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/465b860b-b319-4278-a6cf-ad246cccc912.png)'
  prefs: []
  type: TYPE_IMG
- en: The use of deployment approves and gates are not mutually exclusive, so a mix
    of both can be used.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-stage pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gates, as they are available for classic releases, are present in multi-stage
    YAML pipelines. In YAML pipelines, another mechanism is available: checks. Checks
    are configured to automatically validate if one or more conditions are met before
    allowing a pipeline to continue. Checks can be added to resources that are used
    in a stage. If one or more checks are found on one or more resources in a stage,
    all of the checks have to be passed before the pipeline continues to that stage.
    Checks can be added to environments and service connections.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To add a check to an environment, navigate to that environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a067ac8-9272-423a-8f5d-214c1d78a5f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the top-right, expand the menu and choose Approvals and checks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the new view that opens, choose See all to see all of the different types
    of checks that are available. Choose Invoke Azure Function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/966a8a22-3133-48fb-926e-ac030750cf5e.png)'
  prefs: []
  type: TYPE_IMG
- en: In the popup that opens, configure the Azure function to be called. At a minimum,
    the function URL and key have to be provided.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose Create.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the check is created, every deployment job (see [Chapter 4](8ab4597a-becd-4855-9b45-89045982c14a.xhtml),
    *Continuous Deployment*) that targets the environment has to pass this check.
    The check is passed when the function that is called returns a successful response
    code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following types of checks are supported:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluate artifact**: Validate that an artifact of the type container image
    passes a custom policy. These policies are defined in a language called *Rego*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Invoke REST API**: Post details about the pipeline to an Azure function to
    execute custom logic. If the API returns a successful HTTP status code, the pipeline
    is allowed to continue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Invoke Azure Function**: The same as the Invoke REST API check, but with
    some defaults for Azure Functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Query Azure Monitor alerts**: Only continue if the specified alerts are not
    in an active state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Required template**: Only allow the pipeline to continue if the current YAML
    pipeline extends one or more configured base YAML pipelines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checks can be a powerful mechanism for guaranteeing that one or more conditions
    are met before allowing a pipeline to continue.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about how to measure and assert the quality of
    software development processes. Releasing quickly and often requires the software
    that is written to be of high quality. Testing is needed to ensure that you write
    software of high quality with little technical debt in it. You learned about the
    different types of tests and the pros and cons of the different types of automated
    and manual tests. Finally, you learned how code reviews and tools can help to
    maintain high quality in your project by reporting on quality and by serving as
    a quality gate.
  prefs: []
  type: TYPE_NORMAL
- en: With this knowledge, you are now able to discuss the tests and test types to
    help you to decide which tests are needed for your applications, which risks you
    can address using which types of tests, and whether you need many of them or can
    omit them. You are now also capable of setting up and configuring code-scanning
    tools to ensure that changes of insufficient quality are not merged to the mainline.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about security and compliance, two topics
    that remain equally important when practicing DevOps.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we conclude, here is a list of questions for you to test your knowledge
    regarding this chapter''s material. You will find the answers in the *Assessments*
    section of the Appendix:'
  prefs: []
  type: TYPE_NORMAL
- en: 'True or false: A unit test verifies the working of a single unit in isolation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True or false: An integration test verifies the working of a fully assembled
    system.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which of the following statements is correct, regarding the principles of the
    testing pyramid?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Have many integration tests, fewer unit tests, and even fewer system tests.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Have many unit tests, fewer integration tests, and even fewer system tests.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Have many system tests, fewer integration tests, and many unit tests.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which of the following is not a non-functional type of test?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load testing
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Usability testing
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Applicability testing
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Performance testing
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Testing is about gaining insights into the quality of work. Which techniques
    can be employed to prevent work of insufficient quality propagating through to
    production?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More information about the testing trophy model can be found at [https://testingjavascript.com/](https://testingjavascript.com/)
    and [https://kentcdodds.com/blog/write-tests/](https://kentcdodds.com/blog/write-tests/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More information about writing tests using C# can be found at [https://docs.microsoft.com/en-us/visualstudio/test/walkthrough-creating-and-running-unit-tests-for-managed-code?view=vs-2019](https://docs.microsoft.com/en-us/visualstudio/test/walkthrough-creating-and-running-unit-tests-for-managed-code?view=vs-2019)
    and [https://docs.microsoft.com/en-us/dotnet/core/testing/unit-testing-best-practices](https://docs.microsoft.com/en-us/dotnet/core/testing/unit-testing-best-practices).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More information about the Test & Feedback extension can be found at [https://marketplace.visualstudio.com/items?itemName=ms.vss-exploratorytesting-web](https://marketplace.visualstudio.com/items?itemName=ms.vss-exploratorytesting-web).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practical labs to practice with load testing can be found at [https://docs.microsoft.com/en-us/learn/modules/load-test-web-app-azure-devops/](https://docs.microsoft.com/en-us/learn/modules/load-test-web-app-azure-devops/)
    and [https://docs.microsoft.com/en-us/learn/modules/run-non-functional-tests-azure-pipelines/index](https://docs.microsoft.com/en-us/learn/modules/run-non-functional-tests-azure-pipelines/index).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practical labs to practice with automating UI tests can be found at [https://docs.microsoft.com/en-us/learn/modules/run-functional-tests-azure-pipelines/index](https://docs.microsoft.com/en-us/learn/modules/run-functional-tests-azure-pipelines/index).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More information about SonarCloud can be found at [https://sonarcloud.io](https://sonarcloud.io).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The team project health extension can be found at [https://marketplace.visualstudio.com/items?itemName=ms-devlabs.TeamProjectHealth](https://marketplace.visualstudio.com/items?itemName=ms-devlabs.TeamProjectHealth).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More information about Rego can be found at [https://www.openpolicyagent.org/docs/latest/policy-language/](https://www.openpolicyagent.org/docs/latest/policy-language/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
