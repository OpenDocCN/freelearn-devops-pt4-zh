<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Gathering User Feedback</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, you learned how to measure how your applications are performing in production. You learned how to gather crash reports and logs and how to instrument an application. However, the purpose of software is not just to deliver perfectly running applications, but to create business value. Gathering user feedback is necessary to determine whether your application is also achieving this higher goal. In this chapter, you will learn techniques to measure whether your users are satisfied, which features they are using and which they are not, and how you can use this information to steer future developments.</p>
<p>To do this, this chapter starts by introducing the concept of continuous feedback. Next, it moves on to introduce different approaches to asking users for feedback and recording their responses. This can be both in-application or via other channels. Besides gathering feedback directly, you can also tap into other, indirect channels. Examples are reactions to your software on Twitter and the usage of features in your application. Finally, this chapter will introduce hypothesis-driven development, an approach to software development practiced by Microsoft.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Understanding continuous feedback</li>
<li>Asking for feedback</li>
<li>Gathering indirect feedback</li>
<li>Implementing hypothesis-driven development</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>There are no technical requirements for this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding continuous feedback</h1>
                </header>
            
            <article>
                
<p>As explained in <a href="889f9224-f1b6-414d-bc80-16563f66e1e7.xhtml">Chapter 1</a>, <em>Introduction to DevOps</em>, DevOps is a cultural movement that tries to bring developers and operators closer together, to help them to deliver business value faster and more reliable. Feedback loops are an important element in doing this. In the previous chapter, we saw numerous feedback loops:</p>
<ul>
<li>Developers can run unit tests on their local machine to verify that their changes did not break existing behaviors.</li>
<li>After source code check in, all unit tests are run again and a pipeline with more tests starts running.</li>
<li>Besides functional tests, security tests and dependency scans can be run.</li>
<li>After releasing, logs and metrics are gathered to determine whether the application is running smoothly.</li>
</ul>
<p>All of this provides feedback on the technical quality of the work and now it is time to add one more feedback loop—a loop intended to verify whether the application actually fulfills the needs of its users.</p>
<p>As obvious as this may sound, it is more often forgotten than most developers would care to admit. In many companies, there is faith in product owners or business analysts and they are trusted to be able to predict which features users need and to sort them in order of priority.</p>
<p>This is while we know that developing software is a complex activity where the results of a change <span>often</span><span> </span><span>can</span><span>not be predicted in advance. In such situations, it is important to continuously look for feedback from the user to identify whether features are delivering the value they should.</span></p>
<p>Continuously looking for feedback will help to make decisions such as the following:</p>
<ul>
<li>Removing features that are not being used by most of the users; this removes the need for maintenance on them, therefore reducing cost and freeing up development time</li>
<li>Expanding features that are most used by users, making them more prominent in the interface</li>
<li>Increasing or decreasing testing efforts, based on the perceived quality of the application by users</li>
</ul>
<p>Going further along this line of reasoning, we might conclude that it is impossible to predict whether a feature will actually deliver enough business value to justify its existence or not. Companies that do this often adopt the practice of hypothesis-driven development, which will be discussed later.</p>
<p>First, the next section will introduce different approaches for asking application users for feedback.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Asking for direct feedback</h1>
                </header>
            
            <article>
                
<p>One very straightforward way to collect user feedback is by just asking for it. Over the last few years, more and more applications have been enriched with feedback mechanisms built into the application. Other approaches that are commonly used are publishing a public roadmap and engaging with customers directly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Advantages of in-product feedback</h1>
                </header>
            
            <article>
                
<p>Collecting feedback in-product is a good way to get started with direct user feedback. Examples of in-product feedback are grading a specific view or action, giving a thumbs up or down, or sending a happy or sad smiley face.</p>
<p>Collecting in-product feedback has the following advantages:</p>
<ul>
<li>It is one of the easiest ways for customers to give feedback, taking virtually none of their time.</li>
<li>Due to the non-intrusiveness of this approach, a larger group of users might choose to respond.</li>
<li>Recorded feedback can be context-aware.</li>
<li>When recording a grade, smiley face, or thumbs up or down for feedback, an application can also record the current state and most recent user activities and send all of that along with the user feedback. This makes a single click by the user much more valuable than it seems at first sight. It allows quick insights into the most loved and most hated parts of an application.</li>
<li>Finally, allowing in-product feedback makes the user feel heard and listened to.</li>
</ul>
<div class="packt_tip">Of course, recording data about users and how they use an application requires their consent. It needs to be fully transparent what you intend do with the information gathered about users. Also, an opt-in for explicit content is often required as well as an option to revoke a previously given consent. The precise requirements vary from country to country and are a legal consideration.</div>
<p>The disadvantage of this type of feedback is that it can be too much to analyze in detail. Also, since the results are often anonymized, it is not possible to follow up on feedback. This makes it hard to understand <em>why</em> a user was satisfied or dissatisfied with a screen. Sometimes this is countered by adding a checkbox under the feedback box stating something like: "I give one-time permission to be contacted about this subject".</p>
<p>For understanding the reasons for a user's feedback, other feedback mechanisms such as interviews or focus groups might be more appropriate.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Having a public roadmap</h1>
                </header>
            
            <article>
                
<p>Another approach for gathering user feedback is by publicly sharing what is currently in the backlog and what isn't. One team that publicly shares which features they are working on is the Azure DevOps team. Naturally, this list does not contain all features the product group is planning. The reasons for this might be to keep a competitive edge or to keep some new feature secret until a big announcement. However, their backlog provides a good insight into what is currently brewing.</p>
<p>Adopting this practice allows the users of a product to reach out and comment on this public list. It allows them to request features to be moved up or down the list of priorities and they can share which features they are missing.</p>
<p>This can bring the following advantage to a company: When users engage with feedback on the list of features, they are encouraged to specify why they make a certain request. This might provide new insights into customer demand and may lead to a shift in priorities.</p>
<p>There are also downsides to this approach:</p>
<ul>
<li>Not all users will engage in and provide feedback on a public backlog. This might result in a bias toward more vocal or more demanding customers in the group that provides feedback. While not necessarily an issue, it is good to keep this in mind.</li>
<li>Engaging with users over feature requests or features that they want to be moved up or down the list can be very time-consuming. Especially when comparing with in-product feedback, this approach takes more time.</li>
</ul>
<p>As well as having a public feature roadmap, there are also other ways to give users an insight into what a company is currently working on and what they are planning. Some examples include the following:</p>
<ul>
<li><strong>UserVoice</strong>: UserVoice is a platform that allows users to propose new features and vote on features proposed by others. It allows gathering user ideas, without opening the actual backlog to users.</li>
<li><strong>Bugtrackers</strong>: If customers are very vocal about reporting bugs and errors in an application, it can help to open up a bugtracker. This allows users to see which issues are already known and if and when they might be fixed.</li>
</ul>
<p>Public backlogs and UserVoice-like platforms are more common than open backlogs. Open lists of bugs or issues are more often seen in open source development.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using interviews or focus groups</h1>
                </header>
            
            <article>
                
<p>Other forms of requesting user feedback are one-on-one interviews and focus groups. While these are even more time-intensive then open backlogs and public discussions, they also have the benefit of allowing more balanced user selection.</p>
<p>For example, if an application is clearly targeting four different market segments, it can be beneficial to have five focus groups—one for each market segment and an additional one with a mix of those. The first four will allow focusing on the specific needs of each group, while the fifth will incite a lot of discussion and allows getting insight into how different wishes from different groups compare.</p>
<p>Interviews and focus groups are also more suitable for not only getting the feedback but for understanding the reasoning of users. Sitting face to face with users allows exploring their way of reasoning and how they perceive an application.</p>
<p>This concludes the discussion of direct user feedback. In the next section, indirect user feedback is discussed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gathering indirect feedback</h1>
                </header>
            
            <article>
                
<p>A well known saying in software development is that <em>users do not know what they want</em>. While this may sound harsh, there are a few reasons why direct user feedback from discussions, interviews, and focus groups does not necessarily lead to good product feedback:</p>
<ul>
<li>One reason for this is that everyone wants to be liked. When conducting an interview, or talking to a group of users, there is a chance that they will only say what they believe the interviewer wants to hear.</li>
<li>It has a high turn-around time. Scheduling interviews and focus groups takes time and finding a time that everyone can attend can easily take days or even weeks.</li>
<li>It is hard to keep asking the same group of users for feedback every few weeks. This is especially important when trying to determine whether the quality of a feature is improving with the newest updates or not.</li>
</ul>
<p>For these reasons, it can be worthwhile to cut back on asking for feedback, but instead, measure how users are interacting with an application on a functional level and whether they are satisfied with the value they receive from an application.</p>
<p>One way to do this is by measuring user behavior in an application and emitting metrics based on that. In <a href="bbccbc83-55fc-4fcf-b6a4-1721cdfea791.xhtml">Chapter 10</a>, <em>Application Monitoring</em>, Application Insights was introduced for gathering application-level metrics. While metrics are traditionally used for emitting metrics regarding application performance, metrics can also be used to emit metrics regarding application usage. Some examples are the following:</p>
<ul>
<li>How frequently is every page visited?</li>
<li>How many times are specific operations performed?</li>
<li>How long it takes to complete a certain view?</li>
<li>How many users open a specific form, only to never complete it?</li>
</ul>
<p>Gathering these metrics can deliver important insights into how users are interacting with an application and which parts they use or do not use.</p>
<p>Besides usage, another indicator of user satisfaction can be Twitter sentiment or the number of support requests.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sentiment analysis</h1>
                </header>
            
            <article>
                
<p>Besides gathering metrics in-product, there are also metrics that can be gathered outside of the product. One example source of information is Twitter. Using the Azure cloud and machine learning algorithms, it is now possible to continuously analyze all of the tweets that are directed to a Twitter handle or a hashtag and automatically detect sudden changes.</p>
<p>This even goes so far as that there is an Azure Pipelines extension that allows continuously measuring Twitter sentiment and canceling the progress of a release to the next stage if sentiment turns too negative. This extension is implemented as a pipeline gate and is available in the Azure DevOps Marketplace.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Support requests</h1>
                </header>
            
            <article>
                
<p>Just like the Twitter sentiment, there might be other indicators of user satisfaction that can be gathered automatically. Continuously collecting the number of support calls or emails per minute and detecting a certain spike can be a clear indicator of a user issue. Using machine learning and system integrations this can be harnessed for automated responses or signaling a user to the results.</p>
<p>Adopting practices like this can save minutes or hours detecting production issues. Taking user feedback and making decisions based on that sentiment can go even further. This is called hypothesis-driven development, which is discussed next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing hypothesis-driven development</h1>
                </header>
            
            <article>
                
<p>A risk in software development is that teams are so busy creating more and more features that they forget to reflect upon their business value while everyone knows that not every feature is a success. Some features may not be used at all or may even be disliked by users. As an industry, we have come to learn that product owners have a hard time predicting which features will be really liked by users and which will not. Even when using all of the feedback mechanisms discussed previously, predicting what users want is difficult.</p>
<p>Another important thing to recognize is that every feature in the product also brings a future cost. Every feature requires documentation, support, and maintenance. This means that unnecessary features are driving costs up as well. From this stance, it makes sense to not only leave non-value features but to even remove them from the product as soon as possible.</p>
<p class="mce-root"/>
<p>Hypothesis-driven development is a practice that starts with acknowledging that it is impossible to predict whether a feature will add value, add no value, or, even worse, decrease business value. Next, it recommends transforming features in the backlog into quick, lightweight experiments that are run in the product to determine whether a new feature adds value or not.</p>
<p>Such an experiment can be written in a similar shape as a user story, for example, like this: <em>We believe that users want a new one-field popup to quickly create an appointment, instead of the full dialog. We are convinced that this is the case when we see that over 25% of appointments are created using this new dialog and that the average approval rate of appointments goes up by 2 points or more.</em> The first part is called the hypothesis, and the second is the threshold for confirmation of that hypothesis.</p>
<p>Once this is written down, a minimal implementation of such a one-field popup is created and its usage and the usage of the original form are monitored using metrics. Depending on the measurements, one of the following can occur:</p>
<ul>
<li>The belief stated in the hypothesis is confirmed to be true and the new feature adds value. More stories surrounding this feature can be added to the backlog to increase the business value the product brings.</li>
<li>The belief stated in the hypothesis is not confirmed and further experimentation is not expected to yield different results. The feature is dropped from the backlog and the current, minimal implementation might even be removed from the product.</li>
<li>The belief stated in the hypothesis is not confirmed but experimentation continues. This can happen when there are numerous user complaints about a certain feature that the team is set on fixing. If one approach does not work, they might try another.</li>
</ul>
<p>Using the approach outlined before, teams can increase the impact they make on business value by minimizing the time they spend on features that, after experimentation, do not add value and even remove them from the product again.</p>
<p>Often, hypothesis-driven development is combined with phased roll-out mechanisms such as feature flags or deployment rings. The experiment is then run on only a small percentage of the users, which makes it easier to pull the feature if it does not add enough value.</p>
<p>This completes the discussion of the means for gathering and using user feedback on applications and how user feedback ties into the DevOps goal of delivering business value to end users. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you learned how to measure the business outcomes of software development activities. First, you learned about the importance of feedback and how this helps to understand customer needs and whether those needs are actually being met. Then, numerous approaches to asking for feedback were introduced, both direct and indirect. Finally, you learned about hypothesis-driven development and how a mindset of experimentation can help to cut down waste.</p>
<p>With this knowledge, you can now choose and implement feedback mechanisms that allow you to learn what the user sentiment regarding your application is. You are now able to implement an experiment-based approach to creating software, focusing on value-adding features and ignoring or even removing features that do not add value.</p>
<p>In the next chapter, you will learn all about containers. Containers are rapidly changing the way software is delivered and are often used for applying DevOps principles to both existing and new applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<p>As we conclude, here is a list of questions for you to test your knowledge regarding this chapter's material. You will find the answers in the <em>Assessments</em> section of the Appendix:</p>
<ol>
<li>True or false: There are no downsides to publicly sharing a roadmap.</li>
<li>What is an important concern to keep in mind when evaluating user feedback on a public roadmap?</li>
<li>What are two indirect indicators of user satisfaction that are relatively easy to capture?</li>
<li>Which of the following is not part of a hypothesis, as used in hypothesis-driven development?
<ol>
<li>A hypothesis</li>
<li>A confirmation threshold</li>
<li>A conclusion</li>
</ol>
</li>
<li>What are two benefits of interviews or focus groups over other means of gathering feedback?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li>The list of features planned for Azure DevOps can be found at <a href="https://docs.microsoft.com/en-us/azure/devops/release-notes/features-timeline">https://docs.microsoft.com/en-us/azure/devops/release-notes/features-timeline</a>.</li>
<li>The Twitter Sentiment extension can be found on the Azure DevOps Marketplace at <a href="https://marketplace.visualstudio.com/items?itemName=ms-devlabs.vss-services-twittersentimentanalysis">https://marketplace.visualstudio.com/items?itemName=ms-devlabs.vss-services-twittersentimentanalysis</a>.</li>
</ul>


            </article>

            
        </section>
    </body></html>