["```\nvagrant plugin install vagrant-hostmanager      \n```", "```\nvagrant box add bento/ubuntu-16.04\n```", "```\nnodes = [ \n  { :hostname => 'ansible', :ip => '192.168.0.40', :box => 'xenial64' }, \n  { :hostname => 'mon1', :ip => '192.168.0.41', :box => 'xenial64' }, \n  { :hostname => 'mon2', :ip => '192.168.0.42', :box => 'xenial64' }, \n  { :hostname => 'mon3', :ip => '192.168.0.43', :box => 'xenial64' }, \n  { :hostname => 'osd1',  :ip => '192.168.0.51', :box => 'xenial64', :ram => 1024, :osd => 'yes' }, \n  { :hostname => 'osd2',  :ip => '192.168.0.52', :box => 'xenial64', :ram => 1024, :osd => 'yes' }, \n  { :hostname => 'osd3',  :ip => '192.168.0.53', :box => 'xenial64', :ram => 1024, :osd => 'yes' } \n] \n\nVagrant.configure(\"2\") do |config| \n  nodes.each do |node| \n    config.vm.define node[:hostname] do |nodeconfig| \n      nodeconfig.vm.box = \"bento/ubuntu-16.04\" \n      nodeconfig.vm.hostname = node[:hostname] \n      nodeconfig.vm.network :private_network, ip: node[:ip] \n\n      memory = node[:ram] ? node[:ram] : 512; \n      nodeconfig.vm.provider :virtualbox do |vb| \n        vb.customize [ \n          \"modifyvm\", :id, \n          \"--memory\", memory.to_s, \n        ] \n        if node[:osd] == \"yes\"         \n          vb.customize [ \"createhd\", \"--filename\", \"disk_osd-#{node[:hostname]}\", \"--size\", \"10000\" ] \n          vb.customize [ \"storageattach\", :id, \"--storagectl\", \"SATA Controller\", \"--port\", 3, \"--device\", 0, \"--type\", \"hdd\", \"--medium\", \"disk_osd-#{node[:hostname]}.vdi\" ] \n        end \n      end \n    end \n    config.hostmanager.enabled = true \n    config.hostmanager.manage_guest = true \n  end \nend \n```", "```\nvagrant ssh ansible\n```", "```\nvagrant destroy --force\n```", "```\nVagrant up ansible mon1 osd1  \n```", "```\n$ sudo apt-add-repository ppa:ansible/ansible-2.6     \n```", "```\n$ sudo apt-get update && sudo apt-get install ansible -y    \n```", "```\n$ ssh-keygen\n```", "```\n$ ssh-copy-id mon1\n```", "```\n$ sudo nano /etc/ansible/hosts\n```", "```\n[mons] \nmon1 \nmon2 \nmon3\n\n[mgrs]\nmon1 \n\n[osds] \nosd1 \nosd2 \nosd3 \n\n[ceph:children] \nmons \nosds\nmgrs \n```", "```\n$ ansible mon1 -m ping  \n```", "```\n$ ansible mon1 -a 'uname -r'  \n```", "```\n- hosts: mon1 osd1\n tasks:\n - name: Echo Variables\n debug: msg=\"I am a {{ a_variable }}\"\n```", "```\n$ ansible-playbook /etc/ansible/playbook.yml\n```", "```\nVagrant destroy --force\n```", "```\ngit clone https://github.com/ceph/ceph-ansible.git\ngit checkout stable-3.2 sudo cp -a ceph-ansible/* /etc/ansible/\n```", "```\nsudo apt-get install python-pip\n```", "```\nsudo pip install notario netaddr\n```", "```\n #mon_group_name: mons\n    #osd_group_name: osds\n    #rgw_group_name: rgws\n    #mds_group_name: mdss\n    #nfs_group_name: nfss\n    ...\n    #iscsi_group_name: iscsigws \n```", "```\n#ceph_origin: 'upstream' # or 'distro' or 'local'  \n```", "```\n#fsid: \"{{ cluster_uuid.stdout }}\"\n#generate_fsid: true  \n```", "```\n#monitor_interface: interface\n#monitor_address: 0.0.0.0  \n```", "```\n#ceph_conf_overrides: {}  \n```", "```\n ceph_conf_overrides:\n      global:\n        variable1: value\n      mon:\n        variable2: value\n      osd:\n        variable3: value \n```", "```\n #copy_admin_key: false \n```", "```\n #devices: []\n #osd_auto_discovery: false\n #journal_collocation: false\n #raw_multi_journal: false\n #raw_journal_devices: [] \n```", "```\nceph_origin: 'repository'\nceph_repository: 'community'\nceph_mirror: http://download.ceph.com\nceph_stable: true # use ceph stable branch\nceph_stable_key: https://download.ceph.com/keys/release.asc\nceph_stable_release: mimic # ceph stable release\nceph_stable_repo: \"{{ ceph_mirror }}/debian-{{ ceph_stable_release }}\"\nmonitor_interface: enp0s8 #Check ifconfig\npublic_network: 192.168.0.0/24\n```", "```\nosd_scenario: lvm\nlvm_volumes:\n- data: /dev/sdb\n```", "```\nsudo mkdir /etc/ansible/fetch\nsudo chown vagrant /etc/ansible/fetch\n```", "```\ncd /etc/ansible\nsudo mv site.yml.sample site.yml\nansible-playbook -K site.yml\n```", "```\nvagrant@mon1:~$ sudo ceph -s\n```", "```\nvagrant suspend\n```", "```\nvagrant resume\n```", "```\nnodes = [\n  { :hostname => 'kube1',  :ip => '192.168.0.51', :box => 'xenial64', :ram => 2048, :osd => 'yes' },\n  { :hostname => 'kube2',  :ip => '192.168.0.52', :box => 'xenial64', :ram => 2048, :osd => 'yes' },\n  { :hostname => 'kube3',  :ip => '192.168.0.53', :box => 'xenial64', :ram => 2048, :osd => 'yes' }\n]\n\nVagrant.configure(\"2\") do |config|\n  nodes.each do |node|\n    config.vm.define node[:hostname] do |nodeconfig|\n      nodeconfig.vm.box = \"bento/ubuntu-16.04\"\n      nodeconfig.vm.hostname = node[:hostname]\n      nodeconfig.vm.network :private_network, ip: node[:ip]\n\n      memory = node[:ram] ? node[:ram] : 4096;\n      nodeconfig.vm.provider :virtualbox do |vb|\n        vb.customize [\n          \"modifyvm\", :id,\n          \"--memory\", memory.to_s,\n        ]\n        if node[:osd] == \"yes\"        \n          vb.customize [ \"createhd\", \"--filename\", \"disk_osd-#{node[:hostname]}\", \"--size\", \"10000\" ]\n          vb.customize [ \"storageattach\", :id, \"--storagectl\", \"SATA Controller\", \"--port\", 3, \"--device\", 0, \"--type\", \"hdd\", \"--medium\", \"disk_osd-#{node[:hostname]}.vdi\" ]\n        end\n      end\n    end\n    config.hostmanager.enabled = true\n    config.hostmanager.manage_guest = true\n  end\nend\n```", "```\nsudo apt-get install docker.io\n```", "```\nsudo systemctl start docker\nsudo systemctl enable docker\n```", "```\nsudo swapoff -a\n```", "```\nsudo add-apt-repository “deb http://apt.kubernetes.io/ kubernetes-xenial main”\n```", "```\nsudo curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add\n```", "```\nsudo apt-get update && sudo apt-get install -y kubeadm kubelet kubectl\n```", "```\nsudo kubeadm init --apiserver-advertise-address=192.168.0.51 --pod-network-cidr=10.1.0.0/16 --ignore-preflight-errors=NumCPU\n```", "```\nsudo kubeadm join 192.168.0.51:6443 --token c68o8u.92pvgestk26za6md --discovery-token-ca-cert-hash sha256:3954fad0089dcf72d0d828b440888b6e97465f783bde403868f098af67e8f073\n```", "```\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n```", "```\nwget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n```", "```\nnano kube-flannel.yml\n```", "```\nkubectl apply -f kube-flannel.yml\n```", "```\n$ kubectl get nodes\n```", "```\n$ kubectl get pods --all-namespaces –o wide\n```", "```\n$ git clone https://github.com/rook/rook.git\n```", "```\n$ cd rook/cluster/examples/kubernetes/ceph/\n```", "```\n$ kubectl create -f operator.yaml\n```", "```\n$ kubectl create -f cluster.yaml\n```", "```\n$ kubectl get pods --all-namespaces -o wide\n```", "```\n$ kubectl create -f toolbox.yaml\n```", "```\nkubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath='{.items[0].metadata.name}') bash\n```", "```\nkubectl taint nodes $(hostname) node-role.kubernetes.io/master:NoSchedule-\n```", "```\nkubectl -n rook-ceph-system delete pods rook-ceph-operator-7dd46f4549-68tnk\n```", "```\n$ kubectl create -f filesystem.yaml\n```", "```\napiVersion: v1\nkind: Pod\nmetadata:\n name: nginx\nspec:\n containers:\n - name: nginx\n image: nginx:1.7.9\n ports:\n - containerPort: 80\n volumeMounts:\n - name: www\n mountPath: /usr/share/nginx/html\n volumes:\n - name: www\n flexVolume:\n driver: ceph.rook.io/rook\n fsType: ceph\n options:\n fsName: myfs\n clusterNamespace: rook-ceph\n```", "```\n$ kubectl exec -it nginx bash\n```"]