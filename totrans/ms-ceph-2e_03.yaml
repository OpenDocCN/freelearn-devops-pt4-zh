- en: Deploying Ceph with Containers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用容器部署 Ceph
- en: Once you have planned your Ceph project and are ready to deploy either a test
    or production cluster, you will need to consider the method you wish to use to
    both deploy and maintain it. This chapter will demonstrate how to quickly deploy
    test environments for testing and development by the use of Vagrant. It will also
    explain why you might want to consider using an orchestration tool to deploy Ceph
    rather than using the supplied Ceph tools. As a popular orchestration tool, Ansible
    will be used to show how quickly and reliably a Ceph cluster can be deployed and
    the advantages that using it can bring.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你规划好 Ceph 项目，并准备好部署测试或生产集群，你需要考虑部署和维护的方式。本章将演示如何通过使用 Vagrant 快速部署用于测试和开发的测试环境。同时，也会解释为何你可能希望使用编排工具来部署
    Ceph，而不是使用 Ceph 自带的工具。作为一个流行的编排工具，本书将使用 Ansible 展示如何快速可靠地部署 Ceph 集群，并说明使用它的优势。
- en: 'In this chapter, we will learn the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下内容：
- en: How to prepare a testing environment with Vagrant and VirtualBox
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 Vagrant 和 VirtualBox 准备测试环境
- en: The differences between Ceph's deploy and orchestration tools
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ceph 的部署和编排工具之间的区别
- en: The advantages over using orchestration tools
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相较于使用编排工具的优势
- en: How to install and use Ansible
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何安装和使用 Ansible
- en: How to configure Ceph Ansible modules
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何配置 Ceph Ansible 模块
- en: How to deploy a test cluster with Vagrant and Ansible
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 Vagrant 和 Ansible 部署测试集群
- en: Ideas concerning how to manage your Ceph configuration
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何管理 Ceph 配置的想法
- en: What the Rook project is and what it enables a Ceph operator to do
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rook 项目是什么，它使 Ceph 操作员能做什么
- en: How to deploy a basic Kubernetes cluster
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何部署基本的 Kubernetes 集群
- en: How to use Rook to deploy Ceph on Kubernetes
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 Rook 在 Kubernetes 上部署 Ceph
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In order to be able to run the Ceph environment described later in this chapter,
    it''s important that your computer meets a number of requirements to ensure that
    the VM can be provided with sufficient resources. These requirements are as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够运行本章后续描述的 Ceph 环境，确保计算机满足一定的要求是很重要的，以便为虚拟机提供足够的资源。具体要求如下：
- en: Operating system compatible with Vagrant and VirtualBox, including Linux, macOS,
    and Windows
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 兼容 Vagrant 和 VirtualBox 的操作系统，包括 Linux、macOS 和 Windows
- en: 2-core CPU
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 核 CPU
- en: 8 GB ram
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 8 GB 内存
- en: Virtualization instructions enabled in the BIOS
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 BIOS 中启用虚拟化指令
- en: Preparing your environment with Vagrant and VirtualBox
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Vagrant 和 VirtualBox 准备你的环境
- en: While a test cluster can be deployed on any hardware or virtual machine, for
    the purposes of this book a combination of Vagrant and VirtualBox will be used.
    This will allow rapid provision of the virtual machines and ensure a consistent
    environment.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可以在任何硬件或虚拟机上部署测试集群，但本书将使用 Vagrant 和 VirtualBox 的组合。这将快速提供虚拟机并确保环境的一致性。
- en: VirtualBox is a free and open source hypervisor currently being developed by
    Oracle; while its performance and features may be lacking compared to high-end
    hypervisors, its lightweight approach and multi-OS support lend itself to its
    being a prime candidate for testing.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: VirtualBox 是一个免费的开源虚拟机管理程序，目前由 Oracle 开发；虽然其性能和功能可能不如高端虚拟机管理程序，但其轻量级的方式和多操作系统支持使其成为测试的理想选择。
- en: Vagrant assists in allowing an environment that may comprise many machines to
    be created quickly and efficiently. It works with the concepts of boxes, which
    are predefined templates for use with hypervisors and its Vagrantfile, which defines
    the environment to be built. It supports multiple hypervisors and allows a Vagrantfile
    to be portable across them.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Vagrant 可以帮助快速高效地创建可能包含多台机器的环境。它基于 box 的概念，box 是为虚拟机管理程序预定义的模板，而 Vagrantfile
    则定义了要构建的环境。它支持多个虚拟机管理程序，并允许 Vagrantfile 在它们之间进行移植。
- en: How to install VirtualBox
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何安装 VirtualBox
- en: 'Consult the VirtualBox website for the appropriate method to install VirtualBox
    on your operating system: [https://www.virtualbox.org/wiki/Downloads](https://www.virtualbox.org/wiki/Downloads):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考 VirtualBox 网站，以获取适用于你操作系统的 VirtualBox 安装方法：[https://www.virtualbox.org/wiki/Downloads](https://www.virtualbox.org/wiki/Downloads)
- en: '![](img/6564aeef-931e-4d05-b30f-acb66cc210a0.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6564aeef-931e-4d05-b30f-acb66cc210a0.png)'
- en: How to set up Vagrant
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何设置 Vagrant
- en: 'Follow the installation instructions on Vagrant''s website to get Vagrant installed
    on your chosen OS: [https://www.vagrantup.com/downloads.html](https://www.vagrantup.com/downloads.html):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照 Vagrant 网站上的安装说明，在您选择的操作系统上安装 Vagrant：[https://www.vagrantup.com/downloads.html](https://www.vagrantup.com/downloads.html)：
- en: '![](img/f6af1621-45ce-48d8-8190-bb636a65824e.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6af1621-45ce-48d8-8190-bb636a65824e.png)'
- en: Create a new directory for your Vagrant project, for example `ceph-ansible`
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为您的 Vagrant 项目创建一个新目录，例如 `ceph-ansible`
- en: 'Change to this directory and run the following commands:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换到此目录并运行以下命令：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](img/edc06af7-db73-43f5-9928-d5eb0fa179b5.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/edc06af7-db73-43f5-9928-d5eb0fa179b5.png)'
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](img/2d24df85-01f5-49bb-ab4c-19cd0a78f815.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d24df85-01f5-49bb-ab4c-19cd0a78f815.png)'
- en: 'Now create an empty file called `Vagrantfile` and place the following into
    it:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在创建一个名为 `Vagrantfile` 的空文件，并将以下内容放入其中：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Run `vagrant up` to bring up the virtual machines defined in the `Vagrantfile`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 `vagrant up` 来启动 `Vagrantfile` 中定义的虚拟机：
- en: '![](img/dc46eff2-b018-4dbb-95ac-354e3a6c8d3e.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc46eff2-b018-4dbb-95ac-354e3a6c8d3e.png)'
- en: 'Now, let''s `ssh` into one of them:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们 `ssh` 连接到其中一台虚拟机：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](img/3b09d065-1709-415a-9ab9-36540009fe10.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3b09d065-1709-415a-9ab9-36540009fe10.png)'
- en: If you are running `vagrant` on Windows, the `ssh` command will inform you that
    you need to use a SSH client of your choosing and provide the details to use with
    it. Putty would be a good suggestion for a SSH client. On Linux, the command will
    connect you straight onto the VM.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在 Windows 上运行 `vagrant`，`ssh` 命令会提醒您需要使用您选择的 SSH 客户端，并提供相关的使用信息。Putty 是一个不错的
    SSH 客户端建议。在 Linux 上，该命令会直接连接到虚拟机。
- en: 'The username and password are both `vagrant`. After logging in, you should
    find yourself at the bash prompt for the `ansible vm`:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 用户名和密码都是 `vagrant`。登录后，您应该看到 `ansible vm` 的 bash 提示符：
- en: '![](img/1726344c-a906-4a00-a609-2ddcb9fffd4a.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1726344c-a906-4a00-a609-2ddcb9fffd4a.png)'
- en: Simply type exit to return to your host machine.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 只需输入 exit 即可返回到您的主机。
- en: 'Congratulations, you have just deployed three servers for use as Ceph monitors,
    three servers for use as Ceph OSDs, and an Ansible server. The `Vagrantfile` could
    have also contained extra steps to execute commands on the servers to configure
    them but for now let''s shut down the servers; we can bring them back up when
    required by the examples later in this chapter:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜您，您已经成功部署了三个 Ceph 监视器服务器，三个 Ceph OSD 服务器，以及一台 Ansible 服务器。`Vagrantfile` 还可以包含一些额外步骤，用于在服务器上执行命令进行配置，但现在我们先关闭这些服务器；当本章后续的示例需要时，我们可以重新启动它们：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Ceph-deploy
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ceph-deploy
- en: Ceph-deploy is the official tool for deploying Ceph clusters. It works on the
    principle of having an admin node with password-less SSH access to all machines
    in your Ceph cluster; it also holds a copy of the Ceph configuration file. Every
    time you carry out a deployment action, it uses SSH to connect to your Ceph nodes
    to carry out the necessary steps. While the Ceph-deploy tool is an entirely supported
    method that will leave you with a perfectly functioning Ceph cluster, ongoing
    management of Ceph will not be as easy as desired.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Ceph-deploy 是官方的 Ceph 集群部署工具。它的工作原理是通过一个管理员节点，通过免密码 SSH 访问所有 Ceph 集群中的机器；并且该管理员节点还保存一份
    Ceph 配置文件。每次执行部署操作时，Ceph-deploy 工具会通过 SSH 连接到 Ceph 节点，执行必要的步骤。虽然 Ceph-deploy 工具是一个完全支持的方法，可以确保
    Ceph 集群功能正常，但 Ceph 的后续管理并不会像预期的那样简便。
- en: Larger-scale Ceph clusters will also cause a lot of management overhead if Ceph-deploy
    is used. For that reason, it is recommended that Ceph-deploy be limited to test
    or small-scale production clusters, although as you will see an orchestration
    tool allows for the rapid deployment of Ceph and is probably better suited for
    test environments where you might need to continually be building new Ceph clusters.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用 Ceph-deploy 来管理大规模的 Ceph 集群，会增加很多管理开销。因此，建议将 Ceph-deploy 限制在测试或小规模的生产集群中，尽管正如您所看到的，编排工具可以快速部署
    Ceph，并且可能更适合用于测试环境，尤其是当您需要不断构建新的 Ceph 集群时。
- en: Orchestration
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编排
- en: 'One solution to make installing and managing Ceph easier is to use an orchestration
    tool. There are several tools available, such as Puppet, Chef, Salt, and Ansible,
    all of which have Ceph modules available. If you are already using an orchestration
    tool in your environment, then it is recommended that you stick to using that
    tool. For the purposes of this book, Ansible will be used. This is due a number
    of reasons, as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让安装和管理Ceph更容易的一个解决方案是使用编排工具。现在有多种可用的工具，如Puppet、Chef、Salt和Ansible，这些工具都有Ceph模块可用。如果你在环境中已经使用了编排工具，建议继续使用该工具。本书将使用Ansible，原因如下：
- en: It's the deployment method that is favored by Red Hat, who are the owners of
    both the Ceph and Ansible projects
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是Red Hat偏爱的部署方法，Red Hat是Ceph和Ansible项目的拥有者。
- en: It has a well-developed, mature set of Ceph roles and playbooks
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它拥有一套成熟且完备的Ceph角色和剧本。
- en: Ansible tends to be easier to learn if you have never used an orchestration
    tool before
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你以前从未使用过编排工具，Ansible往往更容易学习。
- en: It doesn't require a central server to be set up, which means demonstrations
    are more focused on using the tool than installing it
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不需要设置中央服务器，这意味着演示更专注于使用工具，而不是安装工具。
- en: All tools follow the same principle, where you provide them with an inventory
    of hosts and a set of tasks to be carried out on the hosts. These tasks often
    reference variables that allow customization of the task at runtime. Orchestration
    tools are designed to be run on a schedule so that, if for any reason the state
    or configuration of a host changes, it will be correctly changed back to the intended
    state during the next run.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 所有工具遵循相同的原则，即提供主机清单和要在主机上执行的任务集。这些任务通常引用变量，允许在运行时定制任务。编排工具设计为按计划运行，这样，如果由于某种原因主机的状态或配置发生变化，在下一次运行时将会正确地恢复到预定状态。
- en: Another advantage of using orchestration tools is documentation. While they
    are not a replacement for good documentation, the fact that they clearly describe
    your environment, including roles and configuration options, means that your environment
    starts to become self-documenting. If you ensure that any installations or changes
    are carried out via your orchestration tool, then the configuration file of the
    orchestration tool will clearly describe the current state of your environment.
    If this is combined with something such as a Git repository to store the orchestration
    configuration, you have the makings of a change control system. This is covered
    in more detail later in this chapter. The only disadvantages center around the
    extra time it takes to carry out the initial setup and configuration of the tool.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用编排工具的另一个优点是文档管理。虽然它们不能替代良好的文档，但它们清晰地描述了你的环境，包括角色和配置选项，这意味着你的环境开始自带文档功能。如果确保通过编排工具执行任何安装或更改，编排工具的配置文件将清晰地描述你环境的当前状态。如果将其与Git仓库等存储编排配置的工具结合使用，你就能拥有一个变更控制系统。本章稍后将更详细地介绍这一点。唯一的缺点是需要花费额外的时间来完成工具的初始设置和配置。
- en: So, by using an orchestration tool, not only do you get a faster and less error-prone
    deployment, you also get documentation and change management for free. If you
    haven't got the hint by now, this is something you should really be looking at.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过使用编排工具，你不仅能够实现更快速、出错率更低的部署，还能免费获得文档和变更管理。如果你现在还没有意识到这一点，这正是你应该关注的内容。
- en: Ansible
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ansible
- en: As mentioned earlier, Ansible will be the orchestration tool of choice for this
    book, so let's look at it in a bit more detail.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Ansible将是本书首选的编排工具，让我们更详细地了解它。
- en: Ansible is an agent-less orchestration tool written in Python that uses SSH
    to carry out configuration tasks on remote nodes. It was first released in 2012,
    has gained widespread adoption, and is known for its ease of adoption and low
    learning curve. Red Hat purchased the commercial company Ansible Inc. in 2015
    and so has a very well developed and close-knit integration for deploying Ceph.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Ansible是一个无代理的编排工具，用Python编写，通过SSH在远程节点上执行配置任务。它首次发布于2012年，已广泛采用，且因其易于采用和学习曲线低而著名。Red
    Hat在2015年收购了Ansible公司，因此为部署Ceph提供了一个高度开发且紧密集成的方案。
- en: Files called playbooks are used in Ansible to describe a list of commands, actions,
    and configurations to be carried out on specified hosts or groups of hosts and
    are stored in the `yaml` file format. Instead of having large unmanageable playbooks,
    Anisble roles can be created to allow a playbook to contain a single task, which
    may then carry out a number of tasks associated with the role.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: The use of SSH to connect to remote nodes and execute playbooks means that it
    is very lightweight and does not require either an agent or a centralized server.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: For testing Ansible also integrates well with Vagrant; an Ansible playbook can
    be specified as part of the Vagrant provisioning configuration and will automatically
    generate an inventory file from the VMs Vagrant that has been created and will
    run the playbook once the servers have booted. This allows a Ceph cluster, including
    its OS, to be deployed via just a single command.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Installing Ansible
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You''ll bring your Vagrant environment you created earlier back up that and
    ssh onto the Ansible server. For this example only `ansible`, `mon1`, and `osd1`
    will be needed:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Add the Ansible PPA:'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**![](img/feb347d6-a7f2-4501-9912-779f3cec60de.png)**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'Update `apt-get` sources and install Ansible:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**![](img/f52965dc-038e-4003-a0a7-608c0566d546.png)**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Creating your inventory file
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Ansible inventory file is used by Ansible to reference all known hosts and
    specify which group they belong to. A group is defined by placing its name in
    square brackets; groups can be nested inside other groups by the use of the children
    definition.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we add hosts to the inventory file, we first need to configure the remote
    nodes for password-less SSH, otherwise we will have to enter a password every
    time Ansible tries to connect to a remote machine as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate a SSH key:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](img/1c87f1a8-3433-4066-be27-7ca9cf6b493d.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: 'Copy the key to the remote hosts:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/c79eb509-7219-4122-9c33-a11fff81774b.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: This will need to be repeated for each host. Normally, you would include this
    step in your Vagrant provisioning stage, but it is useful to carry out these tasks
    manually the first couple of times, so that you understand the process.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'Now try logging in to the machine with: `ssh mon1`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3f98e3b-11a6-4111-aaa1-acdce0d9343a.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: 'Type `exit` to return to the Ansible VM. Now let''s create the `Ansible` inventory
    file. Edit the file called `hosts` in `/etc/ansible`:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Create three groups called `osds, mgrs`, and `mons` and finally a fourth group
    called `ceph`. This fourth group will contain the `osds` and `mons` groups as
    children.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter a list of your hosts under the correct group:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Variables
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most playbooks and roles will make use of variables, which can be overridden
    in several ways. The simplest way is to create files in the `host_vars` and `groups_vars`
    folders; these allow you to override variables either based on the host or group
    membership, respectively.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `/etc/ansible/group_vars` directory. Create a file in `group_vars`
    called `mons` and place the following inside it:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '`a_variable: "foo"`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a file in `group_vars` called `osds` and place the following inside
    it:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '`a_variable: "bar"`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Variables follow a precedence order; you can also create an `all` file which
    will apply to all groups. However, a variable of the same name that is in a more
    specific matching group will override it. Ceph Ansible modules make use of this
    to allow you to have a set of default variables and then specify different values
    for the specific roles.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Testing
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To verify that Ansible is working correctly and that we can successfully connect
    and run commands remotely, let''s use ping with Ansible to check one of our hosts.
    Note: this is not like a network ping; Ansible''s ping confirms that it can communicate
    via SSH and execute commands remotely:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](img/fe8b4d00-26a0-419f-ae9a-de2165321a9f.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: 'Excellent, that worked. Now let''s run a simple command remotely to demonstrate
    Ansible''s capabilities. The following command will retrieve the currently running
    kernel version on the specified remote node:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](img/b8815f88-f9ce-4d8b-b952-85b81e107536.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: A very simple playbook
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To demonstrate how playbooks works, the following example will showcase a small
    playbook that also makes use of the variables we configured earlier:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'And now run the playbook. Notice that the command to run a playbook differs
    from running ad hoc Ansible commands:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**![](img/ee0095ec-b237-4767-b8c8-3925cf90aa4e.png)**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: The output shows the playbook being executed on both `mon1` and `osd1` as they
    are in groups, which are children of the parent group, Ceph. Also note how the
    output of the two servers is different as they pick up the variables that you
    set earlier in the `group_vars` directory.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the last couple of lines show the overall run status of the playbook
    run. You can now destroy your `Vagrant` environment again, ready for the next
    section:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This concludes the introduction to Ansible, but is no means meant to be a complete
    guide. It's recommended that you explore other resources to gain a more in-depth
    knowledge of Ansible before using it in a production environment.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Adding the Ceph Ansible modules
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can use Git to clone the Ceph Ansible repository, as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/b937eb3f-be81-431e-8677-68134760b576.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: 'We also need to install a few extra packages that `ceph-ansible` requires:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](img/5c0c8b9d-406f-4358-bf86-cc6d87520c83.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](img/faceecf7-55ad-4338-b0db-b7f1e5af013e.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: 'Let''s also explore some key folders in the Git repository:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '`group_vars`: We''ve already covered what lives here and will explore possible
    configuration options in more detail later'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`infrastructure-playbooks`: This directory contains pre-written playbooks to
    carry out some standard tasks, such as deploying clusters or adding OSDs to an
    existing one. The comments at the top of the playbooks give a good idea of what
    they do.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`roles`: This directory contains all the roles that make up the Ceph Ansible
    modules. You will see that there is a role for each Ceph component; these are
    called via playbooks to install, configure, and maintain Ceph.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`roles`：此目录包含构成 Ceph Ansible 模块的所有角色。你会看到每个 Ceph 组件都有一个角色；这些角色通过 playbook 被调用，以便安装、配置和维护
    Ceph。'
- en: 'In order to be able to deploy a Ceph cluster with Ansible, a number of key
    variables need to be set in the `group_vars` directory. The following variables are required
    to be set; alternatively, it''s recommended you change them from their defaults.
    For the remaining variables, it suggested that you read the comments in the variable
    files. Key global variables include the following:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够使用 Ansible 部署 Ceph 集群，需要在 `group_vars` 目录中设置若干关键变量。以下变量是必需设置的；另外，建议你修改它们的默认值。对于其余变量，建议你阅读变量文件中的注释。关键的全局变量包括以下内容：
- en: '[PRE20]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'These control what group name modules use to identify the Ceph host types.
    If you will be using Ansible in a wider setting, it might be advisable to prepend
    `ceph-` to the start to make it clear that these groups are related to Ceph:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这些控制模块使用什么组名称来识别 Ceph 主机类型。如果你将在更广泛的环境中使用 Ansible，建议在组名前加上 `ceph-`，以明确这些组与 Ceph
    相关：
- en: '[PRE21]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Employ the `''upstream''` setting to use packages generated by the Ceph team,
    or `distro` for packages generated by your distribution maintainer. The former
    is recommended if you want to be able to upgrade Ceph independently of your distribution:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `'upstream'` 设置来使用 Ceph 团队生成的包，或使用 `distro` 设置来使用分发版维护者生成的包。如果你希望能够独立于你的分发版升级
    Ceph，建议使用前者：
- en: '[PRE22]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'By default, a `fsid` will be generated for your cluster and stored in a file
    where it can be referenced again. You shouldn''t need to touch this unless you
    want control over the `fsid` or you wish to hardcode the `fsid` in the group variable
    file:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`fsid` 会为你的集群生成并存储在一个文件中，以便以后可以再次引用。除非你希望控制 `fsid` 或将 `fsid` 硬编码到组变量文件中，否则你不需要修改它：
- en: '[PRE23]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'One of the preceding commands should be specified. If you are using a variable
    in `group_vars` then you probably want to use `monitor_interface`, which is the
    interface name in Linux, as they will probably be the same across all `mons`.
    Otherwise if you specify `monitor_address` in `host_vars`, you can specify the
    IP of the interface, which obviously will be different across your three or more
    `mons`:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 应指定上述命令之一。如果你在 `group_vars` 中使用了变量，那么你可能希望使用 `monitor_interface`，这是 Linux 中的接口名称，通常在所有
    `mons` 中是相同的。否则，如果你在 `host_vars` 中指定了 `monitor_address`，你可以指定接口的 IP，显然，在三个或更多
    `mons` 中它们会有所不同：
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Not every Ceph variable is directly managed by Ansible, but the preceding variable
    is provided to allow you to pass any extra variables through to the `ceph.conf`
    file and its corresponding sections. An example of how this would look follows
    (notice the indentation):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 并非每个 Ceph 变量都由 Ansible 直接管理，但提供了上述变量，以便你可以将任何额外的变量传递给 `ceph.conf` 文件及其对应的部分。以下是如何实现的示例（请注意缩进）：
- en: '[PRE25]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Key variables from the OSD variable file are as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: OSD 变量文件中的关键变量如下：
- en: '[PRE26]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'If you want to be able to manage your cluster from your OSD nodes instead of
    just your monitors, set this to `true`, which will copy the admin key to your
    OSD nodes:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望能够从 OSD 节点而不仅仅是从监视节点管理你的集群，将此设置为 `true`，这将把管理员密钥复制到你的 OSD 节点：
- en: '[PRE27]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: These are probably the most crucial set of variables in the entire configuration
    of Ansible. They control what disks get used as OSDs and how journals are placed.
    You can either manually specify the devices that you wish to use as OSDs or you
    can use auto discovery. The examples in this book use static device configuration.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这些可能是整个 Ansible 配置中最关键的一组变量。它们控制哪些磁盘被用作 OSD 以及日志的位置。你可以手动指定你希望用作 OSD 的设备，或者使用自动发现。本书中的示例使用的是静态设备配置。
- en: The `journal_collocation` variable sets whether you want to store the journal
    on the same disk as the OSD data; a separate partition will be created for it.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`journal_collocation` 变量设置是否希望将日志存储在与 OSD 数据相同的磁盘上；将为其创建一个单独的分区。'
- en: '`raw_journal_devices` allows you to specify the devices you wish to use for
    journals. Quite often, a single SSD will be a journal for several OSDs; in this
    case, enable `raw_multi_journal` and simply specify the journal device multiple
    times; no partition numbers are needed if you want Ansible to instruct ceph-disk
    to create them for you.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`raw_journal_devices`允许你指定希望用作日志的设备。通常，一个单独的SSD会作为多个OSD的日志；在这种情况下，启用`raw_multi_journal`，并简单地多次指定日志设备；如果你希望Ansible指示ceph-disk为你创建它们，则不需要指定分区号。'
- en: These are the main variables that you should need to consider; it is recommended
    you read the comments in the variable files to see if there are any others you
    may need to modify for your environment.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是你应该考虑的主要变量；建议你阅读变量文件中的注释，查看是否有其他变量需要根据你的环境进行修改。
- en: Deploying a test cluster with Ansible
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Ansible部署测试集群
- en: There are several examples on the internet that contain a fully configured `Vagrantfile`
    and associated Ansible playbooks; this allows you to bring up a fully functional
    Ceph environment with just one command. As handy as this may be, it doesn't help
    you learn how to correctly configure and use the Ceph Ansible modules as you would
    if you were deploying a Ceph cluster on real hardware in a production environment.
    As such, this book will guide you through configuring Ansible from the start,
    even though it's running on Vagrant provisioned servers. Its important to note
    that, like Ceph itself, Ansible playbooks are constantly changing and therefore
    it is recommended that you review the `ceph-ansible` documentation for any breaking
    changes.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 网上有多个示例，包含完全配置的`Vagrantfile`和相关的Ansible剧本；这允许你只用一个命令启动一个完全功能的Ceph环境。虽然这非常方便，但它并没有帮助你学习如何正确配置和使用Ceph的Ansible模块，就像你在生产环境中部署Ceph集群时那样。因此，本书将从头开始指导你配置Ansible，即使它运行在Vagrant配置的服务器上。需要特别注意的是，像Ceph本身一样，Ansible剧本也在不断变化，因此建议你查看`ceph-ansible`文档以了解是否有任何重大变更。
- en: At this point, your Vagrant environment should be up-and-running and Ansible
    should be able to connect to all six of your Ceph servers. You should also have
    a cloned copy of the Ceph Ansible module.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，你的Vagrant环境应该已经启动，并且Ansible应该能够连接到所有六个Ceph服务器。你还应该有一个Ceph Ansible模块的克隆副本。
- en: 'Create a file called `/etc/ansible/group_vars/ceph`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为`/etc/ansible/group_vars/ceph`的文件：
- en: '[PRE28]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Create a file called `/etc/ansible/group_vars/osds`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为`/etc/ansible/group_vars/osds`的文件：
- en: '[PRE29]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Create a `fetch` folder and change the owner to the `vagrant` user:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个`fetch`文件夹，并将所有者更改为`vagrant`用户：
- en: '[PRE30]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Run the Ceph cluster deployment playbook:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 运行Ceph集群部署剧本：
- en: '[PRE31]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `K` parameter tells Ansible that it should ask you for the `sudo` password.
    Now sit back and watch Ansible deploy your cluster:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`K`参数告诉Ansible应该询问你输入`sudo`密码。现在请放松并观看Ansible部署你的集群：'
- en: '![](img/52d49e38-7c73-4c1a-8279-65c86ad8e5a7.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52d49e38-7c73-4c1a-8279-65c86ad8e5a7.png)'
- en: 'Once this is completed, and assuming Ansible completed without errors, `ssh`
    into `mon1` and run:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，并且假设Ansible没有错误地完成，`ssh`进入`mon1`并运行：
- en: '[PRE32]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![](img/8615022f-765a-40ee-a30e-d0723f545988.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8615022f-765a-40ee-a30e-d0723f545988.png)'
- en: And that concludes the deployment of a fully functional Ceph cluster via Ansible.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是通过Ansible部署一个完全功能的Ceph集群的全部过程。
- en: 'If you want to be able to stop the Vagrant Ceph cluster without losing your
    work so far, you can run the following command:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想能够停止Vagrant Ceph集群而不丢失到目前为止的工作，可以运行以下命令：
- en: '[PRE33]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'To pause all the VM''s in their current state, run the following:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 要暂停所有虚拟机的当前状态，请运行以下命令：
- en: '[PRE34]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This will power on the VMs; they'll resume running at the state you left them
    in.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动虚拟机；它们将恢复到你离开时的状态。
- en: Change and configuration management
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变更与配置管理
- en: If you deploy your infrastructure with an orchestration tool such as Ansible,
    managing Ansible playbooks becomes important. As we have seen, Ansible allows
    you to rapidly deploy both the initial Ceph cluster and also configuration updates
    further down the line. It must be appreciated that this power can also have devastating
    effects if incorrect configurations or operations are deployed. By implementing
    some form of configuration management, Ceph administrators will be able to see clearly what
    changes have been made to the Ansible playbooks before running them.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你通过 Ansible 等编排工具部署基础设施，管理 Ansible playbook 就变得很重要。正如我们所看到的，Ansible 允许你快速部署初始的
    Ceph 集群，也能在后续进行配置更新。必须认识到，如果部署了错误的配置或操作，这种强大的能力也可能产生毁灭性的影响。通过实施某种形式的配置管理，Ceph
    管理员将能够在运行 Ansible playbook 之前清晰地看到已做出的更改。
- en: A recommended approach would be to store your Ceph Ansible configuration in
    a Git repository; this will allow you to track changes and implement some form
    of change control either by monitoring Git commits or by forcing people to submit
    merge requests into the master branch.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一种推荐的方法是将 Ceph 的 Ansible 配置存储在 Git 仓库中；这样可以跟踪更改，并通过监控 Git 提交或强制用户向主分支提交合并请求来实现某种形式的变更控制。
- en: Ceph in containers
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ceph 容器化
- en: We have seen previously that by using orchestration tools such as Ansible we
    can reduce the work required to deploy, manage, and maintain a Ceph cluster. We
    have also seen how these tools can help you discover available hardware resources
    and deploy Ceph to them.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经看到，通过使用 Ansible 等编排工具，我们可以减少部署、管理和维护 Ceph 集群所需的工作量。我们还看到这些工具如何帮助你发现可用的硬件资源，并将
    Ceph 部署到这些资源上。
- en: However, using Ansible to configure bare-metal servers still results in a very
    static deployment, possibly not best suited for today's more dynamic workloads.
    Designing Ansible playbooks also needs to take into account several different
    Linux distributions and also any changes that may occur between different releases;
    systemd is a great example of this. Furthermore, a lot of development in orchestration
    tools needs to be customized to handle discovering, deploying, and managing Ceph.
    This is a common theme that the Ceph developers have thought about; with the use
    of Linux containers and their associated orchestration platforms, they hope to
    improve Ceph's deployment experience.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用 Ansible 配置裸金属服务器仍然会导致非常静态的部署，可能不太适合今天更动态的工作负载。设计 Ansible playbook 时还需要考虑多个不同的
    Linux 发行版，以及可能在不同版本之间发生的任何变化；systemd 就是一个很好的例子。此外，许多编排工具的开发需要定制化，以便处理发现、部署和管理
    Ceph。这是 Ceph 开发人员思考过的一个常见主题；通过使用 Linux 容器及其相关的编排平台，他们希望改善 Ceph 的部署体验。
- en: One such approach, which has been selected as the preferred option, is to join
    forces with a project called Rook. Rook works with the container management platform
    Kubernetes to automate the deployment, configuration, and consumption of Ceph
    storage. If you were to draw up a list of requirements and features which a custom
    Ceph orchestration and management framework would need to implement, you would
    likely design something which functions in a similar fashion to Kubernetes. So
    it makes sense to build functionality on top of the well-established Kubernetes
    project, and Rook does exactly that.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一种被选为首选方案的方法是与一个名为 Rook 的项目合作。Rook 与容器管理平台 Kubernetes 配合使用，自动化 Ceph 存储的部署、配置和使用。如果你列出一个自定义
    Ceph 编排和管理框架所需实现的需求和功能，你很可能会设计出一个与 Kubernetes 类似的框架。所以，在已经成熟的 Kubernetes 项目上构建功能是合乎逻辑的，而
    Rook 正是这样做的。
- en: One major benefit of running Ceph in containers is that is allows collocation
    of services on the same hardware. Traditionally in Ceph clusters it was expected
    that Ceph monitors would run on dedicated hardware; when utilizing containers
    this requirement is removed. For smaller clusters, this can amount to a large
    saving in the cost of running and purchasing servers. If resources permit, other
    container-based workloads could also be allowed to run across the Ceph hardware,
    further increasing the Return on Investment for the hardware purchase. The use
    of Docker containers reserves the required hardware resources so that workloads
    cannot impact each other.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在容器中运行 Ceph 的一个主要好处是它允许将服务部署在相同的硬件上。传统上，在 Ceph 集群中，Ceph 监视器需要运行在专用硬件上；而使用容器时，这一要求被去除了。对于较小的集群，这可以在运行和购买服务器的成本上节省大量开支。如果资源允许，其他基于容器的工作负载也可以在
    Ceph 硬件上运行，从而进一步提高硬件采购的投资回报率。使用 Docker 容器可以预留所需的硬件资源，以避免不同工作负载之间互相影响。
- en: To better understand how these two technologies work with Ceph, we first need
    to cover Kubernetes in more detail and actual containers themselves.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这两种技术如何与 Ceph 配合使用，我们首先需要更详细地了解 Kubernetes 和容器本身。
- en: Containers
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器
- en: Although containers in their current form are a relatively new technology, the
    principle of isolating sets of processes from each other has been around for a
    long time. What the current set of technologies enhances is the completeness of
    the isolation. Previous technologies maybe only isolated parts of the filesystem,
    whereas the latest container technologies also isolate several areas of the operating
    system and can also provide quotas for hardware resources. One technology in particular,
    Docker, has risen to become the most popular technology when talking about containers,
    so much so that the two words are often used interchangeably. The word **container** describes
    a technology that performs operating system-level virtualization. Docker is a
    software product that controls primarily Linux features such as groups and namespaces
    to isolate sets of Linux processes.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管容器技术作为一种新兴技术已经出现不久，但将一组进程相互隔离的原则已经存在很长时间。当前的技术增强了隔离的完整性。以往的技术可能仅仅隔离了文件系统的某些部分，而最新的容器技术则还隔离了操作系统的多个区域，并且还可以为硬件资源提供配额。尤其是
    Docker 技术，它已成为讨论容器时最受欢迎的技术，以至于这两个词经常被交替使用。**容器**一词描述了一种执行操作系统级虚拟化的技术。Docker 是一个控制主要是
    Linux 特性的软体产品，例如控制组（cgroups）和命名空间（namespaces），用于隔离一组 Linux 进程。
- en: It's important to note that, unlike full-blown virtualization solutions such
    as VMWare, Hyper-V, and KVM, which provides virtualized hardware and require a
    separate OS instance, containers utilize the operating system of the host. The
    full OS requirements of virtual machines may lead to several 10s of GB of storage
    being wasted on the operating system installation and potentially several GB of
    RAM as well. Containers typically consume overheads of storage and RAM measured
    in MB, meaning that a lot more containers can be squeezed onto the same hardware
    when compared to full virtualization technologies.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，与 VMware、Hyper-V 和 KVM 等完整的虚拟化解决方案不同，后者提供虚拟化的硬件并需要一个单独的操作系统实例，容器利用宿主机的操作系统。虚拟机的完整操作系统需求可能导致存储空间浪费数十
    GB 用于操作系统安装，并可能浪费数 GB 的 RAM。而容器通常只消耗以 MB 为单位的存储和 RAM 开销，这意味着与完全虚拟化技术相比，更多的容器可以被部署在相同的硬件上。
- en: Containers are also much easier to orchestrate as they are completely configurable
    from the host system; this, when combined with their ability to be started in
    milliseconds, means that they are very well suited to dynamically changing environments.
    Particularly in DevOps environments, they are becoming extremely popular where
    the line between infrastructure and application is starting to become blurred.
    The management of infrastructure, which tends to operate at a slower pace than
    application development, means that in an Agile development environment the infrastructure
    team is often always playing catch-up. With DevOps and containers, the infrastructure
    team can concentrate on providing a solid base and the developers can ship their
    application combined with the OS and middleware required to run.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ability to quickly and efficiently spin up 10's of containers in seconds
    soon makes you realize that. if VM sprawl was bad enough, with containers the
    problem can easily get a whole lot worse. With the arrival of Docker in the modern
    IT infrastructure, a need to manage all these containers arose. Enter Kubernetes.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Although several container orchestration technologies are available, Kubernetes
    has enjoyed wide-ranging success and, as it is the product on which Rook is built,
    this book will focus on it.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is an open source container-orchestration system for automating the
    deployment, scaling, and management of containerized applications. It was originally
    developed at Google to run their internal systems but has since been open sourced
    and seen its popularity flourish.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Although this chapter will cover deploying an extremely simple Kubernetes cluster
    to deploy a Ceph cluster with Rook, it is not meant to be a full tutorial and
    readers are encouraged to seek other resources in order to learn more about Kubernetes.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a Ceph cluster with Rook
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To deploy a Ceph cluster with Rook and Kubernetes, Vagrant will be used to create
    three VMs that will run the Kubernetes cluster.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: The first task you'll complete is the deployment of three VMs via Vagrant. If
    you have followed the steps at that start of this chapter and used Vagrant to
    build an environment for Ansible, then you should have everything you require
    to deploy VMs for the Kubernetes cluster.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the `Vagrantfile` to bring up three VMs; as before, place
    the contents into a file called `Vagrantfile` in a new directory and then run
    `vagrant up`:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](img/59b45113-acd5-4b83-a72c-88c34f0ef2d7.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: 'SSH in to the first VM, `Kube1`:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b60c47d-783d-4180-bcc7-4090fc1c9afb.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: 'Update the kernel to a newer version; this is required for certain Ceph features
    in Rook to function correctly:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5edf43c2-a86e-4ca5-8aa7-4bc2290cbbd0.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: 'Install Docker, as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![](img/f166db31-a380-4785-aaf3-72f5888d7391.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: 'Enable and start the Docker service, as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](img/5db6f822-1bfa-499a-9f04-c629487902a0.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: 'Disable swap for future boots by editing `/etc/fstab` and commenting out the
    swap line:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f56c9af-52e2-44f9-9b1f-976c8362ffd0.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: 'And also disable swap now, as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![](img/b14483a2-93dd-4c74-8fe5-7b2058b92481.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
- en: 'Add the Kubernetes repository, as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![](img/b7017f9c-bfd1-4a91-a072-20c8f598dca9.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: 'Add the Kubernetes GPG key, as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![](img/9f1e1e1e-d49d-4a24-88c9-4f2bae8a111d.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
- en: 'Install Kubernetes, as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![](img/59bbee82-521a-4acb-a932-209b54c4012c.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: Repeat the installation steps for Docker and Kubernetes on both the `kube2`
    and `kube3` VMs.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'Once all the VMs have a working copy of Docker and Kubernetes, we can now initialize
    the Kubernetes cluster:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![](img/4011cf71-ac86-43c5-91d9-49698abcd82a.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
- en: 'At the end of the process, a command string is output; make a note of this
    as it is needed to join our additional nodes to the cluster. An example of this
    is as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d462c64b-b453-43e4-aada-b5f72764ceef.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have installed Docker and Kubernetes on all our nodes and have
    initialized the master, let''s add the remaining two nodes into the cluster. Remember
    that string of text you were asked to note down? Now we can run it on the two
    remaining nodes:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '![](img/45f7274d-3629-4955-8fd3-a76da38d88d9.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: '[PRE44]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![](img/ba167f19-0e66-41ba-bd9f-bbcbb2f67e22.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
- en: 'We can now install some additional container networking support. Flannel, a
    simple networking add-on for Kubernetes, uses VXLAN as an overlay to enable container-to-container
    networking. First download the `yaml` file from GitHub:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '![](img/a5fb2c92-1a01-4390-afa7-e7e6eef19c79.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
- en: 'Before we install the Flannel networking component, we need to make a few changes
    to the `YAML` spec file:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Don't indent with tabs, use spaces.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to find the following lines and make the required changes, as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'Line 76: `"Network": "10.1.0.0/16"`:'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/5cd9200d-2e46-4ac2-adc7-b18489214402.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
- en: 'Line 126: `- --iface=eth1`:'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/eeae51a7-63e3-4e26-9a6f-106802fd377b.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
- en: 'Now we can issue the relevant Kubernetes command to apply the specification
    file and install Flannel networking:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![](img/e0e3136a-ddf5-4621-843a-3ff3474cd2ec.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: 'After networking has been installed, we can confirm everything is working and
    that our Kubernetes worker nodes are ready to run workloads:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![](img/988ac417-c9ca-47ee-b5b8-390b807b4f78.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s also check that all containers that support internal Kubernetes
    services are running:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![](img/dfe36616-5a15-45ba-ab82-1d4aec8ecbd0.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: Note that the container networking service (Flannel) that we installed in the
    previous step has automatically been deployed  across all three nodes. At this
    point, we have a fully functioning Kubernetes cluster that is ready to run whatever
    containers we wish to run on it.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now deploy Rook into the Kubernetes cluster. First, let''s clone the
    Rook project from GitHub:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '![](img/273847d6-4b95-4851-bbb4-15034b9dc481.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: 'Change to the `examples` directory, as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '![](img/2c75952e-012a-47b1-83ba-df86c60bfd12.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
- en: 'And now finally create the Rook-powered Ceph cluster by running the following
    two commands:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '![](img/7d0417fd-0ef8-41cd-8b51-634521f47ea1.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
- en: '[PRE53]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '![](img/4a40c789-6611-4a70-ba20-d01085686be0.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
- en: 'To confirm our Rook cluster is now working, let''s check the running containers
    under the Rook namespace:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '![](img/33fa2330-3efa-4b78-b420-fa0125fcc989.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
- en: 'You will see that Rook has deployed a couple of `mons` and has also started
    some discover containers. These discover containers run a discovery script to
    locate storage devices attached to the Kubernetes physical host. Once the discovery
    process has completed for the first time, Kubernetes will then run a one-shot
    container to prepare the OSD by formatting the disk and adding the OSD into the
    cluster. If you wait a few minutes and re-run the `get pods` command, you should
    hopefully see that Rook has detected the two disks connected to `kube2` and `kube3`
    and created `osd` containers for them:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13c8d2e1-72f3-4dd0-a6d8-a8fc8be85ad2.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
- en: 'To interact with the cluster, let''s deploy the toolbox container; this is
    a simple container containing the Ceph installation and the necessary cluster
    keys:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![](img/04652c84-6c80-4481-a54a-ba8745af9028.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
- en: 'Now execute `bash` in the toolbox container:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'This will present you with a root shell running inside the Ceph toolbox container,
    where we can check the status of the Ceph cluster by running `ceph –s` and see
    the current OSDs with `ceph osd tree`:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79cffe21-64bb-4642-a441-7a739d0c8e08.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
- en: You will notice that, although we built three VMs, Rook has only deployed OSDs
    on `kube2` and `kube3`. This is because by default Kubernetes will not schedule
    containers to run on the master node; in a production cluster this is the desired
    behavior, but for testing we can remove this limitation.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'Exit back to the master Kubernetes node and run the following:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '![](img/0ed75efe-b23f-4e1e-9499-02ac2a7ce963.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
- en: You will notice that Kubernetes will deploy a couple of new containers onto
    `kube1`, but it won't deploy any new OSDs; this is due to a current limitation
    to the effect that the `rook-ceph-operator` component only deploys new OSDs on
    first startup. In order to detect newly available disks and prepare them as OSDs,
    the `rook-ceph-operator` container needs to be deleted.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command, but replace the container name with the one that
    is listed from the `get pods` command:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '![](img/26d6e5b1-6938-47d4-a7e3-50e80e3cfd9e.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
- en: 'Kubernetes will now automatically spin up a new `rook-ceph-operator` container
    and in doing so will kick-start the deployment of the new `osd`; this can be confirmed
    by looking at the list of running containers again:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f7eeb3cb-07bc-4736-a8e7-430888406359.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
- en: You can see `kube1` has run a `rook-discover` container, a `rook-ceph-osd-prepare`,
    and finally a `rook-ceph-osd` container, which in this case is `osd` number `2`.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also check, by using our toolbox container as well, that the new `osd`
    has joined the cluster successfully:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bedb6265-2abc-4c9b-9d25-892d01eee246.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
- en: Now that Rook has deployed our full test Ceph cluster, we need to make use of
    it and create some RADOS pools and also consume some storage with a client container.
    To demonstrate this process, we will deploy a CephFS filesystem.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we jump straight into deploying the filesystem, let''s first have a
    look at the example `yaml` file we will be deploying. Make sure you are still
    in the `~/rook/cluster/examples/kubernetes/ceph` directory and use a text editor
    to view the `filesystem.yaml` file:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4371fe2-f5db-4e22-aa93-c9256587b5df.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
- en: You can see that the file contents describe the RADOS pools that will be created
    and the MDS instances that are required for the filesystem. In this example, three
    pools will be deployed, two replicated and one erasure-coded for the actual data.
    Two MDS servers will be deployed, one running as active and the other running
    as a standby-replay.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'Exit the text editor and now deploy the CephFS configuration in the `yaml`
    file:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '![](img/fbb6d097-f8a4-498b-941a-4ba3b797b496.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s jump back into our toolbox container, check the status, and see
    what''s been created:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7078fbe6-a088-41a8-8608-b5550f73d735.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
- en: '![](img/4a9d97df-9ee0-44a8-8e1f-7343260e19f5.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
- en: We can see that two pools have been created, one for the CephFS metadata and
    one for the actual data stored on the CephFS filesystem.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: To give an example of how Rook can then be consumed by application containers,
    we will now deploy a small NGINX web server container that stores its HTML content
    on the CephFS filesystem.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'Place the following inside a file called `nginx.yaml`:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'And now use the `kubectl` command to create the `pod/nginx`:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77c41c5a-2fe6-4930-8bf0-736b9b01b8e8.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
- en: 'After a while, the container will be started and will enter a running state;
    use the `get pods` command to verify this:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79b79484-adba-4a97-9793-58e32fd4c3b0.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
- en: 'We can now start a quick Bash shell on this container to confirm the CephFS
    mount has worked:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '![](img/ef51b95e-65d1-41ab-815b-4aa31ba2c286.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
- en: We can see that the CephFS filesystem has been mounted into `/usr/share/nginx/html`.
    This has been done without having to install any Ceph components in the container
    and without any configuration or copying of key rings. Rook has taken care of
    all of this behind the scenes; once this is understood and appreciated, the real
    power of Rook can be seen. If the simple NGINX pod example is expanded to become
    an auto-scaling service that spins up multiple containers based on load, the flexibility
    given by Rook and Ceph to automatically present the same shared storage across
    the web farm with no additional configuration is very useful.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about Ceph's various deployment methods and the
    differences between them. You will now also have a basic understanding of how
    Ansible works and how to deploy a Ceph cluster with it. It would be advisable
    at this point to continue investigating and practicing the deployment and configuration
    of Ceph with Ansible, so that you are confident enough to use it in production
    environments. The remainder of this book will also assume that you have fully
    understood the contents of this chapter in order to manipulate the configuration
    of Ceph.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: You have also learned about the exciting new developments in deploying Ceph
    in containers running on the Kubernetes platform. Although the Rook project is
    still in the early stages of development, it is clear it is already a very powerful
    tool that will enable Ceph to function to the best of its ability while at the
    same time simplifying the deployment and administration required. With the continued
    success enjoyed by Kubernetes in becoming the recommended container management
    platform, integrating Ceph with the use of Rook will result in a perfect match
    of technologies.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: It is highly recommended that the reader should continue to learn further about
    Kubernetes as this chapter has only scratched the surface of the functionality
    it offers. There are strong signs across the industry that containerization is
    going to be the technology for deploying and managing applications and having
    an understanding of both Kubernetes and how Ceph integrates with Rook is highly
    recommended.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What piece of software can be used to rapidly deploy test environments?
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Should vagrant be used to deploy production environments?
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What project enables the deployment of Ceph on top of Kubernetes?
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Docker?
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the Ansible file called which is used to run a series of commands?
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
