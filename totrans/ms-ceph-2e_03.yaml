- en: Deploying Ceph with Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you have planned your Ceph project and are ready to deploy either a test
    or production cluster, you will need to consider the method you wish to use to
    both deploy and maintain it. This chapter will demonstrate how to quickly deploy
    test environments for testing and development by the use of Vagrant. It will also
    explain why you might want to consider using an orchestration tool to deploy Ceph
    rather than using the supplied Ceph tools. As a popular orchestration tool, Ansible
    will be used to show how quickly and reliably a Ceph cluster can be deployed and
    the advantages that using it can bring.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How to prepare a testing environment with Vagrant and VirtualBox
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The differences between Ceph's deploy and orchestration tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The advantages over using orchestration tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to install and use Ansible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to configure Ceph Ansible modules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to deploy a test cluster with Vagrant and Ansible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideas concerning how to manage your Ceph configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What the Rook project is and what it enables a Ceph operator to do
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to deploy a basic Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use Rook to deploy Ceph on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to be able to run the Ceph environment described later in this chapter,
    it''s important that your computer meets a number of requirements to ensure that
    the VM can be provided with sufficient resources. These requirements are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Operating system compatible with Vagrant and VirtualBox, including Linux, macOS,
    and Windows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2-core CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 8 GB ram
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Virtualization instructions enabled in the BIOS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing your environment with Vagrant and VirtualBox
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While a test cluster can be deployed on any hardware or virtual machine, for
    the purposes of this book a combination of Vagrant and VirtualBox will be used.
    This will allow rapid provision of the virtual machines and ensure a consistent
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: VirtualBox is a free and open source hypervisor currently being developed by
    Oracle; while its performance and features may be lacking compared to high-end
    hypervisors, its lightweight approach and multi-OS support lend itself to its
    being a prime candidate for testing.
  prefs: []
  type: TYPE_NORMAL
- en: Vagrant assists in allowing an environment that may comprise many machines to
    be created quickly and efficiently. It works with the concepts of boxes, which
    are predefined templates for use with hypervisors and its Vagrantfile, which defines
    the environment to be built. It supports multiple hypervisors and allows a Vagrantfile
    to be portable across them.
  prefs: []
  type: TYPE_NORMAL
- en: How to install VirtualBox
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consult the VirtualBox website for the appropriate method to install VirtualBox
    on your operating system: [https://www.virtualbox.org/wiki/Downloads](https://www.virtualbox.org/wiki/Downloads):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6564aeef-931e-4d05-b30f-acb66cc210a0.png)'
  prefs: []
  type: TYPE_IMG
- en: How to set up Vagrant
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Follow the installation instructions on Vagrant''s website to get Vagrant installed
    on your chosen OS: [https://www.vagrantup.com/downloads.html](https://www.vagrantup.com/downloads.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6af1621-45ce-48d8-8190-bb636a65824e.png)'
  prefs: []
  type: TYPE_IMG
- en: Create a new directory for your Vagrant project, for example `ceph-ansible`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Change to this directory and run the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/edc06af7-db73-43f5-9928-d5eb0fa179b5.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/2d24df85-01f5-49bb-ab4c-19cd0a78f815.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now create an empty file called `Vagrantfile` and place the following into
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Run `vagrant up` to bring up the virtual machines defined in the `Vagrantfile`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc46eff2-b018-4dbb-95ac-354e3a6c8d3e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s `ssh` into one of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/3b09d065-1709-415a-9ab9-36540009fe10.png)'
  prefs: []
  type: TYPE_IMG
- en: If you are running `vagrant` on Windows, the `ssh` command will inform you that
    you need to use a SSH client of your choosing and provide the details to use with
    it. Putty would be a good suggestion for a SSH client. On Linux, the command will
    connect you straight onto the VM.
  prefs: []
  type: TYPE_NORMAL
- en: 'The username and password are both `vagrant`. After logging in, you should
    find yourself at the bash prompt for the `ansible vm`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1726344c-a906-4a00-a609-2ddcb9fffd4a.png)'
  prefs: []
  type: TYPE_IMG
- en: Simply type exit to return to your host machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Congratulations, you have just deployed three servers for use as Ceph monitors,
    three servers for use as Ceph OSDs, and an Ansible server. The `Vagrantfile` could
    have also contained extra steps to execute commands on the servers to configure
    them but for now let''s shut down the servers; we can bring them back up when
    required by the examples later in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Ceph-deploy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ceph-deploy is the official tool for deploying Ceph clusters. It works on the
    principle of having an admin node with password-less SSH access to all machines
    in your Ceph cluster; it also holds a copy of the Ceph configuration file. Every
    time you carry out a deployment action, it uses SSH to connect to your Ceph nodes
    to carry out the necessary steps. While the Ceph-deploy tool is an entirely supported
    method that will leave you with a perfectly functioning Ceph cluster, ongoing
    management of Ceph will not be as easy as desired.
  prefs: []
  type: TYPE_NORMAL
- en: Larger-scale Ceph clusters will also cause a lot of management overhead if Ceph-deploy
    is used. For that reason, it is recommended that Ceph-deploy be limited to test
    or small-scale production clusters, although as you will see an orchestration
    tool allows for the rapid deployment of Ceph and is probably better suited for
    test environments where you might need to continually be building new Ceph clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One solution to make installing and managing Ceph easier is to use an orchestration
    tool. There are several tools available, such as Puppet, Chef, Salt, and Ansible,
    all of which have Ceph modules available. If you are already using an orchestration
    tool in your environment, then it is recommended that you stick to using that
    tool. For the purposes of this book, Ansible will be used. This is due a number
    of reasons, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It's the deployment method that is favored by Red Hat, who are the owners of
    both the Ceph and Ansible projects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has a well-developed, mature set of Ceph roles and playbooks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ansible tends to be easier to learn if you have never used an orchestration
    tool before
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It doesn't require a central server to be set up, which means demonstrations
    are more focused on using the tool than installing it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All tools follow the same principle, where you provide them with an inventory
    of hosts and a set of tasks to be carried out on the hosts. These tasks often
    reference variables that allow customization of the task at runtime. Orchestration
    tools are designed to be run on a schedule so that, if for any reason the state
    or configuration of a host changes, it will be correctly changed back to the intended
    state during the next run.
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage of using orchestration tools is documentation. While they
    are not a replacement for good documentation, the fact that they clearly describe
    your environment, including roles and configuration options, means that your environment
    starts to become self-documenting. If you ensure that any installations or changes
    are carried out via your orchestration tool, then the configuration file of the
    orchestration tool will clearly describe the current state of your environment.
    If this is combined with something such as a Git repository to store the orchestration
    configuration, you have the makings of a change control system. This is covered
    in more detail later in this chapter. The only disadvantages center around the
    extra time it takes to carry out the initial setup and configuration of the tool.
  prefs: []
  type: TYPE_NORMAL
- en: So, by using an orchestration tool, not only do you get a faster and less error-prone
    deployment, you also get documentation and change management for free. If you
    haven't got the hint by now, this is something you should really be looking at.
  prefs: []
  type: TYPE_NORMAL
- en: Ansible
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, Ansible will be the orchestration tool of choice for this
    book, so let's look at it in a bit more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Ansible is an agent-less orchestration tool written in Python that uses SSH
    to carry out configuration tasks on remote nodes. It was first released in 2012,
    has gained widespread adoption, and is known for its ease of adoption and low
    learning curve. Red Hat purchased the commercial company Ansible Inc. in 2015
    and so has a very well developed and close-knit integration for deploying Ceph.
  prefs: []
  type: TYPE_NORMAL
- en: Files called playbooks are used in Ansible to describe a list of commands, actions,
    and configurations to be carried out on specified hosts or groups of hosts and
    are stored in the `yaml` file format. Instead of having large unmanageable playbooks,
    Anisble roles can be created to allow a playbook to contain a single task, which
    may then carry out a number of tasks associated with the role.
  prefs: []
  type: TYPE_NORMAL
- en: The use of SSH to connect to remote nodes and execute playbooks means that it
    is very lightweight and does not require either an agent or a centralized server.
  prefs: []
  type: TYPE_NORMAL
- en: For testing Ansible also integrates well with Vagrant; an Ansible playbook can
    be specified as part of the Vagrant provisioning configuration and will automatically
    generate an inventory file from the VMs Vagrant that has been created and will
    run the playbook once the servers have booted. This allows a Ceph cluster, including
    its OS, to be deployed via just a single command.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Ansible
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You''ll bring your Vagrant environment you created earlier back up that and
    ssh onto the Ansible server. For this example only `ansible`, `mon1`, and `osd1`
    will be needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the Ansible PPA:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**![](img/feb347d6-a7f2-4501-9912-779f3cec60de.png)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Update `apt-get` sources and install Ansible:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**![](img/f52965dc-038e-4003-a0a7-608c0566d546.png)**'
  prefs: []
  type: TYPE_NORMAL
- en: Creating your inventory file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Ansible inventory file is used by Ansible to reference all known hosts and
    specify which group they belong to. A group is defined by placing its name in
    square brackets; groups can be nested inside other groups by the use of the children
    definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we add hosts to the inventory file, we first need to configure the remote
    nodes for password-less SSH, otherwise we will have to enter a password every
    time Ansible tries to connect to a remote machine as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate a SSH key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/1c87f1a8-3433-4066-be27-7ca9cf6b493d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Copy the key to the remote hosts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c79eb509-7219-4122-9c33-a11fff81774b.png)'
  prefs: []
  type: TYPE_IMG
- en: This will need to be repeated for each host. Normally, you would include this
    step in your Vagrant provisioning stage, but it is useful to carry out these tasks
    manually the first couple of times, so that you understand the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now try logging in to the machine with: `ssh mon1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3f98e3b-11a6-4111-aaa1-acdce0d9343a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Type `exit` to return to the Ansible VM. Now let''s create the `Ansible` inventory
    file. Edit the file called `hosts` in `/etc/ansible`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Create three groups called `osds, mgrs`, and `mons` and finally a fourth group
    called `ceph`. This fourth group will contain the `osds` and `mons` groups as
    children.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter a list of your hosts under the correct group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most playbooks and roles will make use of variables, which can be overridden
    in several ways. The simplest way is to create files in the `host_vars` and `groups_vars`
    folders; these allow you to override variables either based on the host or group
    membership, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `/etc/ansible/group_vars` directory. Create a file in `group_vars`
    called `mons` and place the following inside it:'
  prefs: []
  type: TYPE_NORMAL
- en: '`a_variable: "foo"`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a file in `group_vars` called `osds` and place the following inside
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '`a_variable: "bar"`'
  prefs: []
  type: TYPE_NORMAL
- en: Variables follow a precedence order; you can also create an `all` file which
    will apply to all groups. However, a variable of the same name that is in a more
    specific matching group will override it. Ceph Ansible modules make use of this
    to allow you to have a set of default variables and then specify different values
    for the specific roles.
  prefs: []
  type: TYPE_NORMAL
- en: Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To verify that Ansible is working correctly and that we can successfully connect
    and run commands remotely, let''s use ping with Ansible to check one of our hosts.
    Note: this is not like a network ping; Ansible''s ping confirms that it can communicate
    via SSH and execute commands remotely:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/fe8b4d00-26a0-419f-ae9a-de2165321a9f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Excellent, that worked. Now let''s run a simple command remotely to demonstrate
    Ansible''s capabilities. The following command will retrieve the currently running
    kernel version on the specified remote node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/b8815f88-f9ce-4d8b-b952-85b81e107536.png)'
  prefs: []
  type: TYPE_IMG
- en: A very simple playbook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To demonstrate how playbooks works, the following example will showcase a small
    playbook that also makes use of the variables we configured earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'And now run the playbook. Notice that the command to run a playbook differs
    from running ad hoc Ansible commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**![](img/ee0095ec-b237-4767-b8c8-3925cf90aa4e.png)**'
  prefs: []
  type: TYPE_NORMAL
- en: The output shows the playbook being executed on both `mon1` and `osd1` as they
    are in groups, which are children of the parent group, Ceph. Also note how the
    output of the two servers is different as they pick up the variables that you
    set earlier in the `group_vars` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the last couple of lines show the overall run status of the playbook
    run. You can now destroy your `Vagrant` environment again, ready for the next
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This concludes the introduction to Ansible, but is no means meant to be a complete
    guide. It's recommended that you explore other resources to gain a more in-depth
    knowledge of Ansible before using it in a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: Adding the Ceph Ansible modules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can use Git to clone the Ceph Ansible repository, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/b937eb3f-be81-431e-8677-68134760b576.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We also need to install a few extra packages that `ceph-ansible` requires:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/5c0c8b9d-406f-4358-bf86-cc6d87520c83.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/faceecf7-55ad-4338-b0db-b7f1e5af013e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s also explore some key folders in the Git repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '`group_vars`: We''ve already covered what lives here and will explore possible
    configuration options in more detail later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`infrastructure-playbooks`: This directory contains pre-written playbooks to
    carry out some standard tasks, such as deploying clusters or adding OSDs to an
    existing one. The comments at the top of the playbooks give a good idea of what
    they do.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`roles`: This directory contains all the roles that make up the Ceph Ansible
    modules. You will see that there is a role for each Ceph component; these are
    called via playbooks to install, configure, and maintain Ceph.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to be able to deploy a Ceph cluster with Ansible, a number of key
    variables need to be set in the `group_vars` directory. The following variables are required
    to be set; alternatively, it''s recommended you change them from their defaults.
    For the remaining variables, it suggested that you read the comments in the variable
    files. Key global variables include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'These control what group name modules use to identify the Ceph host types.
    If you will be using Ansible in a wider setting, it might be advisable to prepend
    `ceph-` to the start to make it clear that these groups are related to Ceph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Employ the `''upstream''` setting to use packages generated by the Ceph team,
    or `distro` for packages generated by your distribution maintainer. The former
    is recommended if you want to be able to upgrade Ceph independently of your distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, a `fsid` will be generated for your cluster and stored in a file
    where it can be referenced again. You shouldn''t need to touch this unless you
    want control over the `fsid` or you wish to hardcode the `fsid` in the group variable
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the preceding commands should be specified. If you are using a variable
    in `group_vars` then you probably want to use `monitor_interface`, which is the
    interface name in Linux, as they will probably be the same across all `mons`.
    Otherwise if you specify `monitor_address` in `host_vars`, you can specify the
    IP of the interface, which obviously will be different across your three or more
    `mons`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Not every Ceph variable is directly managed by Ansible, but the preceding variable
    is provided to allow you to pass any extra variables through to the `ceph.conf`
    file and its corresponding sections. An example of how this would look follows
    (notice the indentation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Key variables from the OSD variable file are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to be able to manage your cluster from your OSD nodes instead of
    just your monitors, set this to `true`, which will copy the admin key to your
    OSD nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: These are probably the most crucial set of variables in the entire configuration
    of Ansible. They control what disks get used as OSDs and how journals are placed.
    You can either manually specify the devices that you wish to use as OSDs or you
    can use auto discovery. The examples in this book use static device configuration.
  prefs: []
  type: TYPE_NORMAL
- en: The `journal_collocation` variable sets whether you want to store the journal
    on the same disk as the OSD data; a separate partition will be created for it.
  prefs: []
  type: TYPE_NORMAL
- en: '`raw_journal_devices` allows you to specify the devices you wish to use for
    journals. Quite often, a single SSD will be a journal for several OSDs; in this
    case, enable `raw_multi_journal` and simply specify the journal device multiple
    times; no partition numbers are needed if you want Ansible to instruct ceph-disk
    to create them for you.'
  prefs: []
  type: TYPE_NORMAL
- en: These are the main variables that you should need to consider; it is recommended
    you read the comments in the variable files to see if there are any others you
    may need to modify for your environment.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a test cluster with Ansible
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several examples on the internet that contain a fully configured `Vagrantfile`
    and associated Ansible playbooks; this allows you to bring up a fully functional
    Ceph environment with just one command. As handy as this may be, it doesn't help
    you learn how to correctly configure and use the Ceph Ansible modules as you would
    if you were deploying a Ceph cluster on real hardware in a production environment.
    As such, this book will guide you through configuring Ansible from the start,
    even though it's running on Vagrant provisioned servers. Its important to note
    that, like Ceph itself, Ansible playbooks are constantly changing and therefore
    it is recommended that you review the `ceph-ansible` documentation for any breaking
    changes.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, your Vagrant environment should be up-and-running and Ansible
    should be able to connect to all six of your Ceph servers. You should also have
    a cloned copy of the Ceph Ansible module.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a file called `/etc/ansible/group_vars/ceph`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a file called `/etc/ansible/group_vars/osds`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `fetch` folder and change the owner to the `vagrant` user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the Ceph cluster deployment playbook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The `K` parameter tells Ansible that it should ask you for the `sudo` password.
    Now sit back and watch Ansible deploy your cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52d49e38-7c73-4c1a-8279-65c86ad8e5a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once this is completed, and assuming Ansible completed without errors, `ssh`
    into `mon1` and run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/8615022f-765a-40ee-a30e-d0723f545988.png)'
  prefs: []
  type: TYPE_IMG
- en: And that concludes the deployment of a fully functional Ceph cluster via Ansible.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to be able to stop the Vagrant Ceph cluster without losing your
    work so far, you can run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'To pause all the VM''s in their current state, run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This will power on the VMs; they'll resume running at the state you left them
    in.
  prefs: []
  type: TYPE_NORMAL
- en: Change and configuration management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you deploy your infrastructure with an orchestration tool such as Ansible,
    managing Ansible playbooks becomes important. As we have seen, Ansible allows
    you to rapidly deploy both the initial Ceph cluster and also configuration updates
    further down the line. It must be appreciated that this power can also have devastating
    effects if incorrect configurations or operations are deployed. By implementing
    some form of configuration management, Ceph administrators will be able to see clearly what
    changes have been made to the Ansible playbooks before running them.
  prefs: []
  type: TYPE_NORMAL
- en: A recommended approach would be to store your Ceph Ansible configuration in
    a Git repository; this will allow you to track changes and implement some form
    of change control either by monitoring Git commits or by forcing people to submit
    merge requests into the master branch.
  prefs: []
  type: TYPE_NORMAL
- en: Ceph in containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen previously that by using orchestration tools such as Ansible we
    can reduce the work required to deploy, manage, and maintain a Ceph cluster. We
    have also seen how these tools can help you discover available hardware resources
    and deploy Ceph to them.
  prefs: []
  type: TYPE_NORMAL
- en: However, using Ansible to configure bare-metal servers still results in a very
    static deployment, possibly not best suited for today's more dynamic workloads.
    Designing Ansible playbooks also needs to take into account several different
    Linux distributions and also any changes that may occur between different releases;
    systemd is a great example of this. Furthermore, a lot of development in orchestration
    tools needs to be customized to handle discovering, deploying, and managing Ceph.
    This is a common theme that the Ceph developers have thought about; with the use
    of Linux containers and their associated orchestration platforms, they hope to
    improve Ceph's deployment experience.
  prefs: []
  type: TYPE_NORMAL
- en: One such approach, which has been selected as the preferred option, is to join
    forces with a project called Rook. Rook works with the container management platform
    Kubernetes to automate the deployment, configuration, and consumption of Ceph
    storage. If you were to draw up a list of requirements and features which a custom
    Ceph orchestration and management framework would need to implement, you would
    likely design something which functions in a similar fashion to Kubernetes. So
    it makes sense to build functionality on top of the well-established Kubernetes
    project, and Rook does exactly that.
  prefs: []
  type: TYPE_NORMAL
- en: One major benefit of running Ceph in containers is that is allows collocation
    of services on the same hardware. Traditionally in Ceph clusters it was expected
    that Ceph monitors would run on dedicated hardware; when utilizing containers
    this requirement is removed. For smaller clusters, this can amount to a large
    saving in the cost of running and purchasing servers. If resources permit, other
    container-based workloads could also be allowed to run across the Ceph hardware,
    further increasing the Return on Investment for the hardware purchase. The use
    of Docker containers reserves the required hardware resources so that workloads
    cannot impact each other.
  prefs: []
  type: TYPE_NORMAL
- en: To better understand how these two technologies work with Ceph, we first need
    to cover Kubernetes in more detail and actual containers themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although containers in their current form are a relatively new technology, the
    principle of isolating sets of processes from each other has been around for a
    long time. What the current set of technologies enhances is the completeness of
    the isolation. Previous technologies maybe only isolated parts of the filesystem,
    whereas the latest container technologies also isolate several areas of the operating
    system and can also provide quotas for hardware resources. One technology in particular,
    Docker, has risen to become the most popular technology when talking about containers,
    so much so that the two words are often used interchangeably. The word **container** describes
    a technology that performs operating system-level virtualization. Docker is a
    software product that controls primarily Linux features such as groups and namespaces
    to isolate sets of Linux processes.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to note that, unlike full-blown virtualization solutions such
    as VMWare, Hyper-V, and KVM, which provides virtualized hardware and require a
    separate OS instance, containers utilize the operating system of the host. The
    full OS requirements of virtual machines may lead to several 10s of GB of storage
    being wasted on the operating system installation and potentially several GB of
    RAM as well. Containers typically consume overheads of storage and RAM measured
    in MB, meaning that a lot more containers can be squeezed onto the same hardware
    when compared to full virtualization technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Containers are also much easier to orchestrate as they are completely configurable
    from the host system; this, when combined with their ability to be started in
    milliseconds, means that they are very well suited to dynamically changing environments.
    Particularly in DevOps environments, they are becoming extremely popular where
    the line between infrastructure and application is starting to become blurred.
    The management of infrastructure, which tends to operate at a slower pace than
    application development, means that in an Agile development environment the infrastructure
    team is often always playing catch-up. With DevOps and containers, the infrastructure
    team can concentrate on providing a solid base and the developers can ship their
    application combined with the OS and middleware required to run.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ability to quickly and efficiently spin up 10's of containers in seconds
    soon makes you realize that. if VM sprawl was bad enough, with containers the
    problem can easily get a whole lot worse. With the arrival of Docker in the modern
    IT infrastructure, a need to manage all these containers arose. Enter Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Although several container orchestration technologies are available, Kubernetes
    has enjoyed wide-ranging success and, as it is the product on which Rook is built,
    this book will focus on it.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is an open source container-orchestration system for automating the
    deployment, scaling, and management of containerized applications. It was originally
    developed at Google to run their internal systems but has since been open sourced
    and seen its popularity flourish.
  prefs: []
  type: TYPE_NORMAL
- en: Although this chapter will cover deploying an extremely simple Kubernetes cluster
    to deploy a Ceph cluster with Rook, it is not meant to be a full tutorial and
    readers are encouraged to seek other resources in order to learn more about Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a Ceph cluster with Rook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To deploy a Ceph cluster with Rook and Kubernetes, Vagrant will be used to create
    three VMs that will run the Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The first task you'll complete is the deployment of three VMs via Vagrant. If
    you have followed the steps at that start of this chapter and used Vagrant to
    build an environment for Ansible, then you should have everything you require
    to deploy VMs for the Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the `Vagrantfile` to bring up three VMs; as before, place
    the contents into a file called `Vagrantfile` in a new directory and then run
    `vagrant up`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/59b45113-acd5-4b83-a72c-88c34f0ef2d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'SSH in to the first VM, `Kube1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b60c47d-783d-4180-bcc7-4090fc1c9afb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Update the kernel to a newer version; this is required for certain Ceph features
    in Rook to function correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5edf43c2-a86e-4ca5-8aa7-4bc2290cbbd0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Install Docker, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/f166db31-a380-4785-aaf3-72f5888d7391.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Enable and start the Docker service, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/5db6f822-1bfa-499a-9f04-c629487902a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Disable swap for future boots by editing `/etc/fstab` and commenting out the
    swap line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f56c9af-52e2-44f9-9b1f-976c8362ffd0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And also disable swap now, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/b14483a2-93dd-4c74-8fe5-7b2058b92481.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Add the Kubernetes repository, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/b7017f9c-bfd1-4a91-a072-20c8f598dca9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Add the Kubernetes GPG key, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/9f1e1e1e-d49d-4a24-88c9-4f2bae8a111d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Install Kubernetes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/59bbee82-521a-4acb-a932-209b54c4012c.png)'
  prefs: []
  type: TYPE_IMG
- en: Repeat the installation steps for Docker and Kubernetes on both the `kube2`
    and `kube3` VMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once all the VMs have a working copy of Docker and Kubernetes, we can now initialize
    the Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/4011cf71-ac86-43c5-91d9-49698abcd82a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At the end of the process, a command string is output; make a note of this
    as it is needed to join our additional nodes to the cluster. An example of this
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d462c64b-b453-43e4-aada-b5f72764ceef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have installed Docker and Kubernetes on all our nodes and have
    initialized the master, let''s add the remaining two nodes into the cluster. Remember
    that string of text you were asked to note down? Now we can run it on the two
    remaining nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/45f7274d-3629-4955-8fd3-a76da38d88d9.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/ba167f19-0e66-41ba-bd9f-bbcbb2f67e22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now install some additional container networking support. Flannel, a
    simple networking add-on for Kubernetes, uses VXLAN as an overlay to enable container-to-container
    networking. First download the `yaml` file from GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/a5fb2c92-1a01-4390-afa7-e7e6eef19c79.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Before we install the Flannel networking component, we need to make a few changes
    to the `YAML` spec file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Don't indent with tabs, use spaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to find the following lines and make the required changes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Line 76: `"Network": "10.1.0.0/16"`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/5cd9200d-2e46-4ac2-adc7-b18489214402.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Line 126: `- --iface=eth1`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/eeae51a7-63e3-4e26-9a6f-106802fd377b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can issue the relevant Kubernetes command to apply the specification
    file and install Flannel networking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/e0e3136a-ddf5-4621-843a-3ff3474cd2ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After networking has been installed, we can confirm everything is working and
    that our Kubernetes worker nodes are ready to run workloads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/988ac417-c9ca-47ee-b5b8-390b807b4f78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s also check that all containers that support internal Kubernetes
    services are running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/dfe36616-5a15-45ba-ab82-1d4aec8ecbd0.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the container networking service (Flannel) that we installed in the
    previous step has automatically been deployed  across all three nodes. At this
    point, we have a fully functioning Kubernetes cluster that is ready to run whatever
    containers we wish to run on it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now deploy Rook into the Kubernetes cluster. First, let''s clone the
    Rook project from GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/273847d6-4b95-4851-bbb4-15034b9dc481.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Change to the `examples` directory, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/2c75952e-012a-47b1-83ba-df86c60bfd12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And now finally create the Rook-powered Ceph cluster by running the following
    two commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/7d0417fd-0ef8-41cd-8b51-634521f47ea1.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/4a40c789-6611-4a70-ba20-d01085686be0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To confirm our Rook cluster is now working, let''s check the running containers
    under the Rook namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/33fa2330-3efa-4b78-b420-fa0125fcc989.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You will see that Rook has deployed a couple of `mons` and has also started
    some discover containers. These discover containers run a discovery script to
    locate storage devices attached to the Kubernetes physical host. Once the discovery
    process has completed for the first time, Kubernetes will then run a one-shot
    container to prepare the OSD by formatting the disk and adding the OSD into the
    cluster. If you wait a few minutes and re-run the `get pods` command, you should
    hopefully see that Rook has detected the two disks connected to `kube2` and `kube3`
    and created `osd` containers for them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13c8d2e1-72f3-4dd0-a6d8-a8fc8be85ad2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To interact with the cluster, let''s deploy the toolbox container; this is
    a simple container containing the Ceph installation and the necessary cluster
    keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/04652c84-6c80-4481-a54a-ba8745af9028.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now execute `bash` in the toolbox container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'This will present you with a root shell running inside the Ceph toolbox container,
    where we can check the status of the Ceph cluster by running `ceph –s` and see
    the current OSDs with `ceph osd tree`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79cffe21-64bb-4642-a441-7a739d0c8e08.png)'
  prefs: []
  type: TYPE_IMG
- en: You will notice that, although we built three VMs, Rook has only deployed OSDs
    on `kube2` and `kube3`. This is because by default Kubernetes will not schedule
    containers to run on the master node; in a production cluster this is the desired
    behavior, but for testing we can remove this limitation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exit back to the master Kubernetes node and run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0ed75efe-b23f-4e1e-9499-02ac2a7ce963.png)'
  prefs: []
  type: TYPE_IMG
- en: You will notice that Kubernetes will deploy a couple of new containers onto
    `kube1`, but it won't deploy any new OSDs; this is due to a current limitation
    to the effect that the `rook-ceph-operator` component only deploys new OSDs on
    first startup. In order to detect newly available disks and prepare them as OSDs,
    the `rook-ceph-operator` container needs to be deleted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command, but replace the container name with the one that
    is listed from the `get pods` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/26d6e5b1-6938-47d4-a7e3-50e80e3cfd9e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Kubernetes will now automatically spin up a new `rook-ceph-operator` container
    and in doing so will kick-start the deployment of the new `osd`; this can be confirmed
    by looking at the list of running containers again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f7eeb3cb-07bc-4736-a8e7-430888406359.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see `kube1` has run a `rook-discover` container, a `rook-ceph-osd-prepare`,
    and finally a `rook-ceph-osd` container, which in this case is `osd` number `2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also check, by using our toolbox container as well, that the new `osd`
    has joined the cluster successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bedb6265-2abc-4c9b-9d25-892d01eee246.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that Rook has deployed our full test Ceph cluster, we need to make use of
    it and create some RADOS pools and also consume some storage with a client container.
    To demonstrate this process, we will deploy a CephFS filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we jump straight into deploying the filesystem, let''s first have a
    look at the example `yaml` file we will be deploying. Make sure you are still
    in the `~/rook/cluster/examples/kubernetes/ceph` directory and use a text editor
    to view the `filesystem.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4371fe2-f5db-4e22-aa93-c9256587b5df.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that the file contents describe the RADOS pools that will be created
    and the MDS instances that are required for the filesystem. In this example, three
    pools will be deployed, two replicated and one erasure-coded for the actual data.
    Two MDS servers will be deployed, one running as active and the other running
    as a standby-replay.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exit the text editor and now deploy the CephFS configuration in the `yaml`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/fbb6d097-f8a4-498b-941a-4ba3b797b496.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s jump back into our toolbox container, check the status, and see
    what''s been created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7078fbe6-a088-41a8-8608-b5550f73d735.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/4a9d97df-9ee0-44a8-8e1f-7343260e19f5.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that two pools have been created, one for the CephFS metadata and
    one for the actual data stored on the CephFS filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: To give an example of how Rook can then be consumed by application containers,
    we will now deploy a small NGINX web server container that stores its HTML content
    on the CephFS filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Place the following inside a file called `nginx.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'And now use the `kubectl` command to create the `pod/nginx`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77c41c5a-2fe6-4930-8bf0-736b9b01b8e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After a while, the container will be started and will enter a running state;
    use the `get pods` command to verify this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79b79484-adba-4a97-9793-58e32fd4c3b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now start a quick Bash shell on this container to confirm the CephFS
    mount has worked:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/ef51b95e-65d1-41ab-815b-4aa31ba2c286.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the CephFS filesystem has been mounted into `/usr/share/nginx/html`.
    This has been done without having to install any Ceph components in the container
    and without any configuration or copying of key rings. Rook has taken care of
    all of this behind the scenes; once this is understood and appreciated, the real
    power of Rook can be seen. If the simple NGINX pod example is expanded to become
    an auto-scaling service that spins up multiple containers based on load, the flexibility
    given by Rook and Ceph to automatically present the same shared storage across
    the web farm with no additional configuration is very useful.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about Ceph's various deployment methods and the
    differences between them. You will now also have a basic understanding of how
    Ansible works and how to deploy a Ceph cluster with it. It would be advisable
    at this point to continue investigating and practicing the deployment and configuration
    of Ceph with Ansible, so that you are confident enough to use it in production
    environments. The remainder of this book will also assume that you have fully
    understood the contents of this chapter in order to manipulate the configuration
    of Ceph.
  prefs: []
  type: TYPE_NORMAL
- en: You have also learned about the exciting new developments in deploying Ceph
    in containers running on the Kubernetes platform. Although the Rook project is
    still in the early stages of development, it is clear it is already a very powerful
    tool that will enable Ceph to function to the best of its ability while at the
    same time simplifying the deployment and administration required. With the continued
    success enjoyed by Kubernetes in becoming the recommended container management
    platform, integrating Ceph with the use of Rook will result in a perfect match
    of technologies.
  prefs: []
  type: TYPE_NORMAL
- en: It is highly recommended that the reader should continue to learn further about
    Kubernetes as this chapter has only scratched the surface of the functionality
    it offers. There are strong signs across the industry that containerization is
    going to be the technology for deploying and managing applications and having
    an understanding of both Kubernetes and how Ceph integrates with Rook is highly
    recommended.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What piece of software can be used to rapidly deploy test environments?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Should vagrant be used to deploy production environments?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What project enables the deployment of Ceph on top of Kubernetes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Docker?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the Ansible file called which is used to run a series of commands?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
