<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">BlueStore</h1>
                </header>
            
            <article>
                
<p>In this chapter, you'll learn about BlueStore, the new object store in Ceph designed to replace the existing filestore. Its increased performance and enhanced feature set are designed to allow Ceph to continue to grow and provide a resilient high-performance distributed storage system for the future. Since the Luminous release, BlueStore is now the recommended and default object store that's used when creating new OSDs. This chapter will cover how BlueStore works and why it is much better suited than Filestore for Ceph's requirements. Then by following a step by step tutorial you will be guided through how to upgrade a Ceph cluster to BlueStore.</p>
<p>In this chapter, you'll learn the following topics:</p>
<ul>
<li>What is BlueStore?</li>
<li>The limitations of filestore</li>
<li>What problems BlueStore overcome</li>
<li>The components of BlueStore and how it works</li>
<li>Introduction to <kbd>ceph-volume</kbd></li>
<li>How to deploy BlueStore OSDs</li>
<li>Approaches to upgrading live clusters from filestore to BlueStore</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is BlueStore?</h1>
                </header>
            
            <article>
                
<p>BlueStore is a Ceph object store that's primarily designed to address the limitations of filestore, which, prior to the Luminous release, was the default object store. Initially, a new object store <span>named NewStore </span>was being developed to replace filestore. NewStore was a combination of RocksDB, a key–value store that stored metadata and a standard <strong>Portable Operating System Interface</strong> (<strong>POSIX</strong>) filesystem for the actual objects. However, it quickly became apparent that using a POSIX filesystem introduced high overheads and restrictions, which was one of the key reasons for trying to move away from using filestore in the first place.</p>
<p>Hence, BlueStore was born. Using raw block devices in combination with RocksDB, a number of problems <span>that had stunted NewStore </span><span>were solved. The name</span> BlueStore was a reflection of the combination of the words Block and NewStore:</p>
<div class="CDPAlignCenter CDPAlign packt_quote"><em>Block + NewStore = BlewStore = BlueStore</em></div>
<p>BlueStore is designed to remove the double write penalty associated with filestore and improve performance that can be obtained from the same hardware. Also, with the new ability to have more control over the way objects are stored on disk, additional features, such as checksums and compression, can be implemented.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why was it needed?</h1>
                </header>
            
            <article>
                
<p>The previous object store in Ceph, filestore, has a number of limitations that have started to limit the scale at which Ceph can operate, as well as the features that it can offer. The following are some of the main reasons why BlueStore was needed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ceph's requirements</h1>
                </header>
            
            <article>
                
<p>An object in Ceph along with its data also has certain metadata associated with it, and it's crucial that both the data and metadata are updated atomically. If either of this metadata or data is updated without the other, the whole consistency model of Ceph is at risk. To ensure that these updates occur atomically, they need to be carried out in a single transaction.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Filestore limitations</h1>
                </header>
            
            <article>
                
<p>Filestore was originally designed as an object store to enable developers to test Ceph on their local machines. Because of its stability, it quickly became the standard object store and found itself in use in production clusters throughout the world.</p>
<p>Initially, the thought behind filestore was that the upcoming <strong>B-tree file system</strong> (<strong>btrfs</strong>), which offered transaction support, would allow Ceph to offload the atomic requirements to btrfs. Transactions would allow an application to send a series of requests to <span>btrfs</span> and only receive acknowledgement once all were committed to stable storage. Without a transaction support, if there was an interruption halfway through a Ceph write operation, either the data or metadata could have been missing or one could be out of sync with the other.</p>
<p>Unfortunately, the reliance on <span>btrfs</span> to solve these problems turned out to be a false hope, and several limitations were discovered. B<span>trfs</span> can still be used with filestore, but there are numerous known issues that can affect the stability of Ceph.</p>
<p>In the end, it turned out that XFS was the best choice to use with filestore, but XFS had the major limitation that it didn't support transactions, meaning that there was no way for Ceph to guarantee atomicity of its writes. The solution to this was the write-ahead journal. All writes, including data and metadata, would first be written into a journal, located on a raw block device. Once the filesystem containing the data and metadata confirmed that all data had been safely flushed to disk, the journal entries could be flushed. A beneficial side effect of this is that, when using an SSD to hold the journal for a spinning disk, it acts like a write back cache, lowering the latency of writes to the speed of the SSD; however, if the filestore journal resides on the same storage device as the data partition, then throughput will be at least halved.</p>
<p>In the case of spinning-disk OSDs, this can lead to very poor performance, as the disk heads will constantly be moving between two areas of the disks, even for sequential operations. Although filestore on SSD-based OSDs doesn't suffer nearly the same performance penalty, their throughput is still effectively halved because double the amount of data needs to be written due to the filestore journal. In either case, this loss of performance is very undesirable, and in the case of flash drives, this wears the device faster, requiring the more expensive version of flash, called write endurance flash. The following diagram shows how filestore and its journal interacts with a block device. You can see that all data operations have to go through the filestore journal and the filesystems journal:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ca1b8d3f-46d9-49a9-8838-b31ab0185876.png" style="width:24.17em;height:10.67em;"/></p>
<p>Additional challenges with filestore arose from developers trying to control the actions of the underlying POSIX filesystem to perform and behave in a way that Ceph required. A large amount of work has been done over the years by filesystem developers to try and make filesystems intelligent and to predict how an application might submit I/O. In the case of Ceph, a lot of these optimizations interfere with what it's trying to instruct the filesystem to do, requiring more workarounds and complexity.</p>
<p>Object metadata is stored in combinations of filesystem attributes, called <span><strong>extended attributes</strong> (<strong>XATTRs</strong>),</span> and in a <strong>LevelDB</strong> key–value store, which also resides on the OSD disk. LevelDB was chosen at the time of filestore's creation rather than RocksDB, as RocksDB wasn't available and LevelDB suited a lot of Ceph's requirements.</p>
<p>Ceph is designed to scale to petabytes of data and store billions of objects. However, because of limitations around the number of files you can reasonably store in a directory, further workarounds to help limit this were introduced. Objects are stored in a hierarchy of hashed directory names; when the number of files in one of these folders reaches the set limit, the directory is split into another level and the objects are moved.</p>
<p>However, there's a trade-off to improving the speed of object enumeration: when these directory splits occur, they impact performance as the objects are moved into the correct directories. On larger disks, the increased number of directories puts additional pressure on the VFS cache and can lead to additional performance penalties for infrequently accessed objects.</p>
<p>As this book will cover in the <span>chapter on </span><span>performance tuning, a major performance bottleneck in filestore is when XFS has to start looking up inodes and</span> directory <span>entries that aren't currently cached in RAM. For scenarios where there are a large number of objects stored per OSD, there is currently no real solution to this problem, and it's quite common to for a Ceph cluster to gradually slow down as it fills up.</span></p>
<p>Moving away from storing objects on a POSIX filesystem is really the only way to solve most of these problems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why is BlueStore the solution?</h1>
                </header>
            
            <article>
                
<p>BlueStore was designed to address these limitations. Following the development of NewStore, it was obvious that trying to use a POSIX filesystem as the underlying storage layer in any approach would introduce a number of issues that were also present in filestore. In order for Ceph to be able to achieve a guaranteed level of performance, that was expected from the underlying storage, it also <span>needed to have direct block-level access to the storage devices </span><span>without the additional overheads of a separate Linux filesystem.</span> <span>By storing metadata in RocksDB and the actual object data directly on block devices, Ceph can leverage much better control over the underlying storage and at the same time provide better performance.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How BlueStore works</h1>
                </header>
            
            <article>
                
<p>The following diagram shows how BlueStore interacts with a block device. Unlike filestore, data is directly written to the block device and metadata operations are handled by RocksDB:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/946e9d75-d099-45a9-8452-85ca3c25db3a.png" style="width:21.83em;height:13.00em;"/></p>
<p>The block device is divided between RocksDB data storage and the actual user data stored in Ceph. Each object is stored as a number of blobs allocated from the block device. RocksDB contains metadata for each object and tracks the utilization and allocation information for the data blobs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RocksDB</h1>
                </header>
            
            <article>
                
<p>RocksDB is a high-performance key–value store that was originally forked from LevelDB, but, after development, Facebook went on to offer significant performance improvements suited for multiprocessor servers with low-latency storage devices. It has also had numerous feature enhancements, some of which are used in BlueStore.</p>
<p>RocksDB is used to store metadata about the stored objects, which was previously handled using a combination of LevelDB and XATTRs in filestore.</p>
<p>A key characteristic of RocksDB <span>is the way in which data is written down in the levels of the database. It owes this characteristic to </span><span>its origins in LevelDB. New data is written into a memory-based table with an optional transaction log on persistent storage, the WAL; as this memory-based table fills up, data is moved down to the next level of the database by a process called compaction. When that level fills up, data is migrated down </span><span>again,</span><span> and so on. All of these levels are stored in what RocksDB calls SST files. In Ceph, each of these levels are configured to be 10 times the size of the previous level, which brings some interesting factors into play if you're trying to store the whole of the RocksDB on SSD in a hybrid HDD–SSD layout.</span></p>
<p>All new data is written into the memory-based table and WAL, the memory based table is known as level 0. <span>BlueStore </span>configures level 0 as 256 MB. The default size multiplier between levels is a factor of ten, this means that level 1 is also 256 MB, level 2 is 2.56 GB, level 3 is 25.6 GB, and level 4 would be 256 GB. For most Ceph use cases the average total metadata size per OSD should be around 20-30GB, with the hot data set typically being less than this. It would be hoped that levels 0, 1, and 2 would contain most of the hot data for writes, and so sizing an SSD partition to at least 3 GB should mean that these levels are stored on SSD. Write performance should be good as the metadata for writes will be hitting the SSDs; however, when reading metadata—say, during client read requests—there is a chance that the metadata may live in level 3 or 4, and so will have to be read off of a spinning disk, which would have a negative impact on latency and increase disk load.</p>
<p>Therefore, the obvious solution would be to somehow calculate how big you believe the <span>BlueStore</span> metadata may grow for your dataset and size the RocksDB storage to ensure that it can all be stored on SSD. There are two difficulties in accomplishing this.</p>
<p>Firstly, it's very difficult to precalculate the size of the metadata based on the size of the actual data. Depending on the client model—RBD, CephFS, or RGW—differing amounts of metadata will be stored. Additionally, things such as snapshots and whether you are using replicated- or erasure-coded pools will also lead to differing sizes of metadata.</p>
<p>The next challenge is sizing your flash device correctly to ensure that all of the metadata fits. As mentioned previously, RocksDB compacts data down through the various levels of the database. When <span>BlueStore</span> creates the files for the RocksDB, it will only place a certain level on your flash device if the whole of that level would fit in it. Therefore, there are minimum sizes required for each level to ensure that the level is actually located on flash. For example, to ensure that the 2.56 GB level 2 part of the DB fits on flash, you need to have at least a 4-5 GB SSD partition. This is because level 0 and level 1 and level 2 all need to fit, as well as a small amount of overhead. For level 3 to fit in its entirety, you would need just over 30 G; any smaller and the extra space over level 2 would not be used. To ensure that level 4 would fit, you would likely need over 300 GB of flash space.</p>
<p>Storing the WAL on a faster storage device—which can help to lower the latency of RocksDB operations—is recommended if you are using flash storage for the actual data and you need further increases in performance. If you are using spinning disks, moving the WAL to a dedicated device will likely show minimal improvement. There are a number of possible storage layout configurations, where the WAL, DB, and data can be placed on different storage devices. The following list shows three examples of such configurations:</p>
<ul>
<li>WAL, DB, and data all on spinning disk or flash</li>
<li>WAL and DB on SSD, data on spinning disk</li>
<li>WAL on NVMe, DB on SSD, and data on spinning disk</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compression</h1>
                </header>
            
            <article>
                
<p>Another handy feature introduced with <span>BlueStore </span>is that it enables compression of data at the sub-object level, blobs inside <span>BlueStore.</span> This means that any data written into Ceph, no matter the client access model, can benefit from this feature. Compression is enabled on a per-pool basis but is disabled by default.</p>
<p>As well as the ability to enable compression per-pool, there are also a number of extra options to control the behavior of the compression, as shown in the following list:</p>
<ul>
<li><kbd>compression_algorithm</kbd><strong>:</strong> This controls which compression library is used to compress data. The default is snappy, a compression library written by Google. Although its compression ratio isn't the best, it has very high performance, and unless you have specific capacity requirements, you should probably stick with snappy. Other options are <kbd>zlib</kbd> and <kbd>zstd</kbd>.</li>
<li><kbd>compression_mode</kbd><strong>:</strong> This controls the operating status of compression on a per-pool basis. It can be set to either <kbd>none</kbd>, <kbd>passive</kbd>, <kbd>aggressive</kbd>, or <kbd>force</kbd>. The <kbd>passive</kbd> setting enables the use of compression, but will only compress objects that are marked to be compressed from higher levels. The <kbd>aggressive</kbd> setting will try and compress all objects unless explicitly told not to. The <kbd>force</kbd> setting will always try and compress data.</li>
<li><kbd>compress_required_ratio</kbd><strong>:</strong> By default, this is set at 87.5%. An object that has been compressed must have been compressed to at least below this value to be considered worth compressing; otherwise, the object will be stored in an uncompressed format.</li>
</ul>
<p>Although compression does require additional CPU, snappy is very efficient, and the distributed nature of Ceph lends itself well to this task as the compression duties are spread over a large number of CPUs across the cluster. In comparison, a legacy storage array would have to use more of its precious, finite dual controller CPU's resource.</p>
<p>An additional advantage of using compression over the reduction in space consumed is also I/O performance when reading or writing large blocks of data. Because of the data being compressed, the disks or flash devices will have less data to read or write, meaning faster response times. Additionally, flash devices will possibly see less write wear because of the reduced amount of total data written.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Checksums</h1>
                </header>
            
            <article>
                
<p>For increased protection of stored data, <span>BlueStore</span> calculates and stores the checksums of any data written. On each read request, <span>BlueStore </span>reads the checksum and compares with the data read from the device. If a mismatch is discovered, <span>BlueStore </span>will report a read error and repair the damage. Ceph will then retry the read from another OSD holding that object. Although modern hardware has sophisticated checksums and error detection of its own, introducing another level in <span>BlueStore </span>goes a long way to eliminating the risk of silent data corruption. By default, <span>BlueStore </span>creates checksums using crc32, which is highly likely to catch any silent data corruption; however, alternative algorithms are available, if required.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">BlueStore cache tuning</h1>
                </header>
            
            <article>
                
<p>Unlike in filestore, where any free RAM in the OSD node is used by the page cache, in BlueStore, RAM has to be statically assigned to the OSD on startup. For spinning disk OSDs, this amount is 1 GB; flash-based SSDs have 3 GB assigned to them. This RAM is used for a number of different caches internally: the RocksDB cache, the BlueStore metadata cache, and the BlueStore data cache. The first two are responsible for ensuring the smooth operating of the BlueStore internals when looking up essential metadata; the defaults have been set to offer good performance and increasing them further will show diminishing returns. The final BlueStore data cache will actually cache user data stored in the Ceph cluster. It's set relatively low by default compared to what some filestore OSDs may have stored in the page cache; this is to prevent BlueStore having high memory consumption by default.</p>
<p>If your OSD nodes have plenty of <span>free</span><span> </span><span>memory after all your OSDs are running and storing data, then it's possible to increase the amount of memory assigned to each OSD and decide how it's split between the different caches.</span></p>
<p>Recent versions of Ceph contain a feature in BlueStore that auto-tunes the assignment of memory between the different caches in BlueStore. By default, the OSD will aim to consume around 4 GB of memory, and by continually analyzing the memory usage will adjust the allocation to each cache. The major improvement that auto-tuning brings is that different workloads utilize the different caches in BlueStore differently, and trying to pre-allocate memory with static variables is an extremely difficult task. Aside from potentially tweaking the target memory threshold, the rest of the auto-tuning is largely automatic and hidden from the Ceph administrator.</p>
<p>If auto-tuning is disabled, then BlueStore will fall back to its manual cache assignment behavior. The following section describes the various BlueStore caches in detail that can be controlled via the manual mode. In this mode, there are two OSD-based settings that control the amount of memory assigned to each OSD, <span><kbd>bluestore_cache_size_hdd</kbd> and <kbd>bluestore_cache_size_ssd</kbd>. As per the name, you can adjust either one to control the assigned memory for either HDDs or SSDs. However, we can do more than just change the overall amount of memory assigned to an OSD; there are a number of further settings to control the split between the three caches, as shown in the following list:</span></p>
<ul>
<li>The <kbd>bluestore_cache_kv_ratio</kbd> setting, set by default to 0.5, will allocate 50% of the allocated memory to the RocksDB cache. This cache is used internally by RocksDB and is not directly managed by Ceph. It's currently believed to offer the best return in performance when deciding where to allocate memory.</li>
<li>The <span><kbd>bluestore_cache_meta_ratio</kbd> setting, set by default to 0.5, will allocate 50% of the available allocated memory to caching BlueStore metadata. Note that, depending on the available memory and the value of</span> <kbd>bluestore_cache_kv_min</kbd>, <span>less than 50% may end up being allocated to caching metadata. The BlueStore metadata cache contains the raw metadata before it's stored in RocksDB.</span></li>
<li>The <kbd>bluestore_cache_kv_min</kbd> setting, set by default to 512 MB, ensures that at least 512 MB of memory is used for the RocksDB cache. Anything over this value will be shared 50:50 with the BlueStore metadata cache.</li>
</ul>
<p>Finally, any memory left over from the preceding two ratios will be used for caching actual data. By default, because of <kbd>kv</kbd> and <kbd>meta_ratios</kbd>, this will be 0%. Most Ceph clients will have their own local read cache, which will hopefully keep extremely hot data cached; however, in the case where clients are used that don't have their own local cache, it might be worth investigating whether adjusting the caching ratios to reserve a small amount of cache for data use brings improvements.</p>
<p>By default, the auto-tuning of BlueStore should provide the best balance of memory usage and provide the best performance, and it isn't recommended that you change to the manual method.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deferred writes</h1>
                </header>
            
            <article>
                
<p>Unlike in filestore, where every write is written in its entirety to both the journal and then finally to disk, in BlueStore, the data part of the write in most cases is written directly to the block device. This removes the double-write penalty and, on pure spinning-disk OSDs, dramatically improves performance and lowers SSD wear. However, as mentioned previously, this double write has a positive side effect of decreasing write latency when the spinning disks are combined with SSD journals. BlueStore can also use flash-based storage devices to lower write latency by deferring writes, first writing data into the RocksDB WAL and then later flushing these entries to disk. Unlike filestore, not every write is written into the WAL; configuration parameters determine the I/O size cut-off as to what writes are deferred. The configuration parameter is shown in the following code:</p>
<pre>bluestore_prefer_deferred_size</pre>
<p>This controls the size of I/Os that will be written to the WAL first. For spinning disks, this defaults to 32 KB, and SSDs by default don't defer writes. If write latency is important and your SSD is sufficiently fast, then by increasing this value, you can increase the size of I/Os that you wish to defer to WAL.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">BlueFS</h1>
                </header>
            
            <article>
                
<p>Although the main motivation for BlueStore's development was to not use an underlying filesystem, BlueStore still needs a method to store RocksDB and the data on the OSD disk. BlueFS was developed to meet this requirement, which is an extremely <span><span>minimal</span></span> filesystem that provides just the minimal set of features that BlueStore requires. It also means that it has been designed to operate in a dependable manner for the slim set of operations that Ceph submits. It also removes the overhead of the double filesystem journal write that would be present when using a standard POSIX filesystem.</p>
<p>Unlike with filestore, you can't simply browse the folder structure and manually look at the objects as BlueFS is not a native Linux filesystem; however, it's possible to mount a BlueFS filesystem with the ceph-objectstore-tool to enable exploration or to be able to manually correct errors. This will be covered further in the <span>section on</span><span> disaster recovery.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ceph-volume</h1>
                </header>
            
            <article>
                
<p>Although not strictly part of BlueStore, the <kbd>ceph-volume</kbd> tool was released around the same time as BlueStore and is the recommended tool for provisioning Bluestore OSDs. It's a direct replacement for the <kbd>ceph-disk</kbd> tool, which had a number of issues surrounding race conditions and the predictability of OSDs being correctly enumerated and starting up. The <kbd>ceph-disk</kbd> tool used <kbd>udev</kbd> to identify OSDs that were then mounted and activated. The <kbd>ceph-disk</kbd> tool has now been deprecated, and all new OSDs should be created using <kbd>ceph-volume</kbd>.</p>
<p>Although <kbd>ceph-volume</kbd> can function in a simple mode, the recommended approach is to use the <kbd>lvm</kbd> mode. As the name suggests, this utilizes the Linux logical volume manager to store information regarding the OSDs and to manage the block devices. Additionally, the dm-cache, which is a part of <kbd>lvm</kbd>, can be used to provide block-level caching underneath the OSDs.</p>
<p>The <kbd>ceph-volume</kbd> tool also has a batch mode, which aims to intelligently provision OSDs given a list of block devices. Care should be taken to use the <kbd>--report</kbd> mode to ensure that its intended action matches your expectations. Otherwise, it's recommended that you manually partition and create OSDs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to use BlueStore</h1>
                </header>
            
            <article>
                
<p>To create a BlueStore OSD using <kbd>ceph-volume</kbd>, you run the following command, specifying the devices for the data and RocksDB storage. As previously mentioned, you can separate the DB and WAL parts of RocksDB if you so wish:</p>
<pre><strong>ceph-volume create --bluestore /dev/sda --block.wal /dev/sdb --block.db /dev/sdc (--dmcrypt)</strong></pre>
<div class="packt_tip">Shown in brackets is the encryption option. It's recommended that you encrypt all new OSDs unless you have a specific reason not to. Encryption with modern CPUs generates very little overhead, and makes the often-forgotten security measures around disk replacements much simpler. With the recent introduction of various new data-protection laws, such as GDPR in Europe, having data encrypted at rest is highly recommended.</div>
<p>The preceding code assumes that your data disk is <kbd>/dev/sda</kbd>. For this example, assume that you are using a spinning disk, and that you have a faster device, such as an SSD (<kbd>/dev/sdb</kbd>) and a very fast NVMe device (<kbd>/dev/sdc</kbd>). The <kbd>ceph-volume</kbd> tool would create two partitions on the data disk: one for storing the actual Ceph objects and another small XFS partition for storing details about the OSD. It would then place a link to the SSD to store the RocksDB on it and a link to the NVMe device to store the WAL. You can create multiple OSDs sharing the same SSD for DB and WAL by partitioning the devices, or by using <kbd>lvm</kbd> to carve logical volumes out of them.</p>
<p>However, as we discovered in <a href="dd1d6803-6e40-4bfb-8150-b605bcc08d59.xhtml">Chapter 2</a>, <em>Deploying Ceph with Containers</em>, using a proper deployment tool for your Ceph cluster helps to reduce deployment time and ensures consistent configuration across the cluster. Although the Ceph Ansible modules also support deploying BlueStore OSDs, at the time of writing, it doesn't currently support automatically creating multiple DB and WAL partitions on a single device.</p>
<p>Now that you understand how to create BlueStore OSD's the next topic that is required to be discussed is the upgrading of an existing cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Strategies for upgrading an existing cluster to BlueStore</h1>
                </header>
            
            <article>
                
<p>It's likely that some readers of this book are running existing Ceph clusters that are utilizing filestore. These readers might be wondering if they should upgrade to BlueStore, and if so, what the best method is for doing this.</p>
<p>It should be understood that while filestore is still supported, it's very much at the end of its life, with no further development work planned aside from any critical bug fixes that may be required. Therefore, it's highly recommended that you make plans to upgrade your cluster to BlueStore to take advantage of any current and future enhancements <span><span>and continue to run a supported Ceph release</span></span>. The support path for filestore in future releases hasn't been announced, but it would be wise to aim to be running BlueStore OSDs by the Ceph release after Nautilus. </p>
<p>There is no special migration path for upgrading an OSD to BlueStore; the process is to simply destroy the OSD, rebuild it as BlueStore, and then let Ceph recover the data on the newly created OSD. It's more than likely that, because of the differing size requirements between filestore journals and BlueStore's RocksDB, altering partition sizes will require multiple OSDs to be destroyed at once. Therefore, it may be worth considering whether operating system rebuilds should be carried out at this point.</p>
<p>There are two main approaches to the upgrade process that are largely determined by the Ceph operator's appetite for risk and the availability of spare capacity, listed as follows:</p>
<ul>
<li><strong>Degraded upgrade</strong>:<strong> </strong>A degraded upgrade destroys the current OSDs without redistributing their contents across the remaining OSDs. Once the OSDs come back online as BlueStore OSDs, then the missing copies of data are rebuilt. Until the cluster returns to full health, a portion of the data on the Ceph cluster will be in a degraded state, and although multiple copies will still exist, they'll be at a higher risk should the cluster experience a failure of some sort. Recovery times will depend on the number of OSDs that need to be recovered and the size of data stored on each OSD. As it's highly likely that several OSDs will be upgraded at once, expect the recovery time to be higher than it would be for a single OSD. Please also note that, with the default pool settings of <kbd>size=3</kbd> <span>and</span> <kbd>min_size=2</kbd><span>, should an additional disk fail, some PGs will only have one copy, and now that they're less than</span> <kbd>min_size</kbd><span>, the I/O will be suspended to these PGs until the recovery recreates a second copy. The benefits of performing a degraded upgrade is that you only have to wait for the cluster to re-balance once during recovery and you don't require any additional space, which may mean that this is the only option for clusters that are more or less full.</span></li>
<li><strong>Out-and-in upgrade</strong>: If you want to guard against any possibility of data loss or unavailability and have sufficient space to redistribute the contents of the OSDs that are to be upgraded across the cluster, then an out-and-in upgrade is the recommended approach. By marking the OSDs to be upgraded as out, Ceph will re-balance the PGs across other OSDs. Once this process has finished, the OSDs can be stopped and destroyed without impacting data durability or availability. When the BlueStore OSDs are reintroduced, the PGs will flow back and, throughout this period, there will be no reduction in the number of copies of data. Either method will result in exactly the same configuration and so will ultimately come down to personal preference. If your cluster has a large number of OSDs, then some form of automation may be required to lessen the burden on the operator; however, if you want to automate the process, then care should be taken around the destruction of the filestore OSD step, as one mistake could easily wipe more than the intended OSDs. A halfway measure may be to create a small script that automates the zapping, partitioning, and creation steps. This can then be run manually on each OSD node. </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Upgrading an OSD in your test cluster</h1>
                </header>
            
            <article>
                
<p><span>For the basis of demonstrating BlueStore, we will use </span><kbd>ceph-volume</kbd><span> to non-disruptively manually upgrade a live Ceph cluster's OSDs from filestore to BlueStore. If you wish to carry out this procedure for real, you could work through the <em>Ansible</em> section in <a href="dd1d6803-6e40-4bfb-8150-b605bcc08d59.xhtml">Chapter 2</a>, <em>Deploying Ceph with Containers</em>, to deploy a cluster with filestore OSDs and then go through the following instructions to upgrade them. The OSDs will be upgraded following the degraded method where OSDs are removed while still containing data.</span></p>
<p>Make sure that your Ceph cluster is in full health by checking with the <kbd>ceph -s</kbd> command, as shown in the following code. We'll be upgrading OSD by first removing it from the cluster and then letting Ceph recover the data onto the new BlueStore OSD, so we need to be sure that Ceph has enough valid copies of your data before we start. By taking advantage of the hot maintenance capability in Ceph, you can repeat this procedure across all of the OSDs in your cluster without downtime:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/209649af-637d-4d67-b4bc-167266f6bea1.png" style="width:37.00em;height:17.25em;"/></p>
<p>Now we need to stop all of the OSDs from running, unmount the disks, and then wipe them by going through the following steps:</p>
<ol>
<li>Use the following command to stop the OSD services:</li>
</ol>
<pre style="padding-left: 60px"><strong>   sudo systemctl stop ceph-osd@*</strong></pre>
<p style="padding-left: 60px">The preceding command gives the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9f1f231a-559f-4c9c-9d94-c7b87f8f7d35.png" style="width:27.83em;height:2.92em;"/></p>
<p class="mce-root CDPAlignLeft CDPAlign" style="padding-left: 60px">We can confirm that the OSDs have been stopped and that Ceph is still functioning using the <kbd>ceph -s</kbd> command again, as shown in the following screenshot:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/aaf5b105-8fd3-4ae0-adbe-1a80516d5358.png" style="width:65.67em;height:28.00em;"/></p>
<ol start="2">
<li> Now, unmount the XFS partitions; the errors can be ignored:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo umount /dev/sd*<br/></strong></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3955567f-f91f-4ed6-8226-2b8d16df40d3.png" style="color: #000000;width:24.33em;height:17.92em;"/></p>
<ol start="3">
<li>Unmounting the filesystems will mean that the disks are no longer locked and we can wipe the disks using the following code:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo ceph-volume lvm zap /dev/sd&lt;x&gt;</strong></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3c544ecc-0444-4bd8-9e95-cbcd8febd991.png" style="width:40.67em;height:13.33em;"/></p>
<ol start="4">
<li>Now we can also edit the partition table on the flash device to remove the filestore journals and recreate them as a suitable size for BlueStore's RocksDB using the following code. In this example, the flash device is an NVMe:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo fdisk /dev/sd&lt;x&gt;</strong></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/790a2dfe-e3b2-45a0-a4e5-1dc99cb2e1eb.png" style="width:31.08em;height:26.75em;"/></p>
<p style="padding-left: 60px">Delete each Ceph journal partition using the <kbd>d</kbd> command, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0f3fea3b-54c9-4487-ae0a-cc152cef9f8f.png" style="width:32.25em;height:15.50em;"/></p>
<p style="padding-left: 60px">Now create all of the new partitions for BlueStore, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/95fd7550-0ce1-4757-a6f2-3957f2d8cbb5.png" style="width:48.50em;height:7.33em;"/></p>
<p class="mce-root" style="padding-left: 60px">Add one partition for each OSD you intend to create. When finished, your partition table should look something like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/09c9f9a7-a858-4149-b502-42847393ede2.png" style="width:31.50em;height:20.58em;"/></p>
<p style="padding-left: 60px">Use the <kbd>w</kbd> command to write the new partition table to disk, as shown in the following screenshot. Upon doing so, you'll be informed that the new partition table is not currently in use, and so we need to run <kbd>sudo partprobe</kbd> to load the table into the kernel:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2b5f7d2f-b835-47cf-ad43-b091aa6ee533.png"/></p>
<ol start="5">
<li>Go back to one of your monitors. First, confirm the OSDs we are going to remove and remove the OSDs using the following <kbd>purge</kbd> commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo ceph osd tree</strong></pre>
<p style="padding-left: 60px">The preceding command gives the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/460b52da-291b-493c-a81c-13eec9ea2a1e.png" style="width:41.50em;height:14.92em;"/></p>
<p style="padding-left: 60px">Now, remove the logical OSD entry from the Ceph cluster—in this example, OSD 36:</p>
<pre style="padding-left: 60px"><strong>sudo ceph osd purge x --yes-i-really-mean-it</strong></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bfd1a0a0-6bd9-494f-8463-80ef62c5766c.png"/></p>
<ol start="6">
<li>Check the status of your Ceph cluster with the <kbd>ceph -s</kbd> command. You should now see that the OSD has been removed, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bf8d6dc8-19c7-46fb-9f15-96e5afb175fa.png" style="width:65.42em;height:37.92em;"/></p>
<p style="padding-left: 60px">Note that the number of OSDs has dropped, and that, because the OSDs have been removed from the CRUSH map, Ceph has now started to try and recover the missing data onto the remaining OSDs. It's probably a good idea not to leave Ceph in this state for too long to avoid unnecessary data movement.</p>
<ol start="3"/>
<ol start="7">
<li>Now issue the <kbd>ceph-volume</kbd> command to create the <kbd>bluestore</kbd> OSD using the following code. In this example, we will be storing the DB on a separate flash device, so we need to specify that option. Also, as per this book's recommendation, the OSD will be encrypted:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo ceph-volume lvm create --bluestore --data /dev/sd&lt;x&gt; --block.db /dev/sda&lt;ssd&gt; --dmcrypt</strong></pre>
<p style="padding-left: 60px"><span>The preceding command gives a lot of output, but if successful, we will end with the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e7524e7d-1fc6-4ab1-820d-ab55ceedfa30.png" style="width:34.67em;height:5.75em;"/></p>
<ol start="8">
<li>Check the status of <kbd>ceph</kbd> again with <kbd>ceph-s</kbd> to make sure that the new OSDs have been added and that Ceph is recovering the data onto them, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9ac2b0ed-a22f-4d55-89d7-6e0409fee71c.png" style="width:53.08em;height:33.00em;"/></p>
<p>Note that the number of misplaced objects is now almost zero because of the new OSDs that were placed in the same location in the CRUSH map before they were upgraded. Ceph now only needs to recover the data, not redistribute the data layout.</p>
<p>If further nodes need to be upgraded, wait for the back-filling process to complete and for the Ceph status to return to <kbd>HEALTH_OK</kbd>. Then the work can proceed on the next node.</p>
<p>As you can see, the overall procedure is very simple and is identical to the steps required to replace a failed disk.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about the new object store in Ceph called BlueStore. Hopefully, you have a better understanding of why it was needed and the limitations in the existing filestore design. You should also have a basic understanding of the inner workings of BlueStore and feel confident in how to upgrade your OSDs to BlueStore.</p>
<p>In the next chapter we will look at how Ceph storage can be exported via commonly used storage protocols to enable Ceph storage to be consumed by non-Linux clients.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What object store is the default when creating OSDs in Luminous and newer releases?</li>
<li>What database does BlueStore use internally?</li>
<li>What is the name of the process for moving data between levels in the database part of BlueStore?</li>
<li>What is the name of the method where small writes can be temporarily written to an SSD instead of an HDD?</li>
<li>How can you mount BlueFS and browse it as a standard Linux filesystem?</li>
<li>What's the default compression algorithm used in BlueStore?</li>
<li>Moving up a level in the BlueStore database increases the size by what multiplier?</li>
</ol>


            </article>

            
        </section>
    </body></html>