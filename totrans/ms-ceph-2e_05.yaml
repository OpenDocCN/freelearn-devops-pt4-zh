- en: Ceph and Non-Native Protocols
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ceph与非原生协议
- en: Years of development have enabled Ceph to build an extensive feature set, bringing
    high quality and performance storage to Linux. However, clients that don't run
    Linux (and that are therefore unable to natively talk to Ceph) have a limited
    scope as to where Ceph can be deployed. Recently, a number of new enhancements
    have been developed to allow Ceph to start to talk to some of these non-Linux-based
    clients, such as the **Internet Small Computer Systems Interface **(**iSCSI**)
    and **Network File System** (**NFS**). This chapter will look in detail at the
    various methods by which Ceph storage can be exported to clients and the strengths
    and weaknesses of each. In all methods, a Linux server is used as a proxy to translate
    the I/O requests from these clients into native Ceph I/Os, and as such, a working
    knowledge of how to use these protocols in Linux is beneficial. Making these proxy
    servers highly available will also be covered in this chapter, along with the
    difficulties of doing so.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 多年的开发使得Ceph构建了一个广泛的功能集，为Linux提供了高质量、高性能的存储。然而，不能运行Linux的客户端（因此无法与Ceph直接通信）在Ceph的部署范围上受限。最近，已经开发出一些新功能，允许Ceph与一些非Linux客户端开始进行通信，比如**互联网小型计算机系统接口**（**iSCSI**）和**网络文件系统**（**NFS**）。本章将详细介绍Ceph存储如何导出到客户端的不同方法，以及每种方法的优缺点。在所有方法中，都会使用Linux服务器作为代理，将这些客户端的I/O请求转换为原生Ceph
    I/O，因此，了解如何在Linux中使用这些协议将是有益的。本章还将介绍如何使这些代理服务器高可用，并探讨相关的困难。
- en: The two main storage types that will be looked at in this chapter will be file
    and block storage, as these are the most popular types of storage in legacy enterprise
    workloads.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点讨论两种主要的存储类型：文件存储和块存储，因为这两种存储类型在传统企业工作负载中最为常见。
- en: 'Briefly, we''ll cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 简要地，我们将在本章中涵盖以下主题：
- en: Block
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 块
- en: File
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件
- en: 'Examples:'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：
- en: Exporting Ceph RBDs via iSCSI
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过iSCSI导出Ceph RBD
- en: Exporting CephFS via Samba
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过Samba导出CephFS
- en: Exporting CephFS via NFS
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过NFS导出CephFS
- en: ESXi hypervisor
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ESXi虚拟化管理程序
- en: Clustering
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群
- en: Block
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 块
- en: Block-level storage mimics the type of storage that would have originally been
    provided by hard disks and, later, storage arrays. Typically, block storage is
    exported via storage arrays via fiber channel or iSCSI onto hosts where a local
    filesystem is then formatted onto the block device. In some cases, this filesytem
    may be of the clustered type, and can allow the block device to be presented across
    many hosts at the same time. It's important to note that even though block-based
    storage allows you to present it to multiple hosts, this should only be done if
    the filesystem supports it; otherwise, corruption of the filesystem is highly
    likely.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 块级存储模拟了最初由硬盘提供的存储类型，后来由存储阵列提供。通常，块存储通过光纤通道或iSCSI从存储阵列导出到主机，然后在块设备上格式化本地文件系统。在某些情况下，这个文件系统可能是集群类型的，允许在多个主机上同时呈现块设备。需要注意的是，尽管基于块的存储允许你将其呈现给多个主机，但只有在文件系统支持的情况下才应这样做，否则文件系统极有可能发生损坏。
- en: One use of block storage that has seen a massive expansion in recent years has
    been through the use of virtualization. Block storage is quite often presented
    to a hypervisor that's formatted with a filesystem. One or more virtual machines
    are then stored as files on this filesystem. This differs greatly from the native
    Ceph approach when using KVM as the hypervisor; as KVM directly supports Ceph **RADOS
    Block Devices** (**RBDs**), it stores each VM's disk directly as an RBD, removing
    the complexity and overheads associated with the hypervisor's filesystem.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，块存储在虚拟化技术中的应用有了巨大的扩展。块存储通常被呈现给格式化了文件系统的虚拟化管理程序。然后，一个或多个虚拟机以文件的形式存储在这个文件系统上。这与使用KVM作为虚拟化管理程序时的原生Ceph方式大相径庭；因为KVM直接支持Ceph
    **RADOS块设备**（**RBD**），它将每个虚拟机的磁盘直接存储为RBD，从而消除了虚拟化管理程序文件系统相关的复杂性和开销。
- en: Ceph RBDs, which are a type of block storage, can be exported via iSCSI to allow
    clients that speak iSCSI to consume Ceph storage. Since the release of Mimic,
    Ceph has had a basic level of support for configuring iSCSI exports of RBD images.
    The configuration of Ceph's iSCSI support is all managed through Ansible, which
    both installs the required software and exports the iSCSI devices.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Ceph RBD是块存储的一种类型，可以通过iSCSI导出，允许支持iSCSI的客户端使用Ceph存储。自Mimic版本发布以来，Ceph对RBD映像的iSCSI导出配置提供了基本支持。Ceph的iSCSI支持的配置全部通过Ansible管理，Ansible不仅安装所需的软件，还导出iSCSI设备。
- en: At the time of writing, there are currently still a few limitations that the
    reader should be aware of, mainly surrounding the **Highly Available** (**HA**)
    capabilities. The issues largely affect ESXi and clustering solutions where multiple
    hosts try and access the block device concurrently. At the time of writing, it
    is not recommended for you to use Ceph's iSCSI support for either of these use
    cases. For users who are interested in exploring the current compatibility further,
    it's recommended that they consult the upstream Ceph documentation and mailing
    lists.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在写作本文时，读者应注意目前仍然存在一些限制，主要涉及**高可用性**（**HA**）功能。这些问题主要影响ESXi和集群解决方案，其中多个主机试图同时访问块设备。在撰写本文时，不推荐您在这两种用例中使用Ceph的iSCSI支持。对于有兴趣进一步探索当前兼容性的用户，建议他们查阅上游Ceph文档和邮件列表。
- en: File
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文件
- en: As the name indicates, file storage is supported by some form of filesystem
    that stores files and directories. In the traditional storage scenario, file storage
    is normally provided via servers acting as *file servers* or through the use of
    **network-attached storage** (**NAS**). File-based storage can be provided over
    several protocols and can sit on several different types of filesystems.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名称所示，文件存储由某种形式的文件系统支持，文件系统用于存储文件和目录。在传统的存储场景中，文件存储通常通过充当*文件服务器*的服务器提供，或者通过使用**网络附加存储**（**NAS**）实现。文件存储可以通过多种协议提供，并可以存放在不同类型的文件系统上。
- en: The two most common file-access protocols are SMB and NFS, which are widely
    supported by many clients. SMB is traditionally seen as a Microsoft protocol,
    being the native file-sharing protocol in Windows, whereas NFS is seen as the
    protocol used on Unix-based infrastructures.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 两种最常见的文件访问协议是SMB和NFS，它们广泛被许多客户端支持。SMB传统上被视为微软协议，是Windows中的本地文件共享协议，而NFS则被认为是Unix基础架构中使用的协议。
- en: As we shall see later, both Ceph's RBDs and its own CephFS filesystem can be
    used as a basis to export file-based storage to clients. RBDs can be mounted on
    a proxy server where a local filesystem is then placed on top. From here, the
    exportation as NFS or SMB is very similar to any other server with local storage.
    When using CephFS, which in itself is a filesystem, there are direct interfaces
    to both NFS and SMB server software to minimize the number of levels in the stack.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们稍后将看到的，Ceph的RBD和其CephFS文件系统都可以作为导出文件存储到客户端的基础。RBD可以挂载在代理服务器上，随后在其上方放置本地文件系统。从这里开始，作为NFS或SMB导出的过程与任何其他具有本地存储的服务器非常相似。当使用CephFS时，作为一个文件系统，它有直接与NFS和SMB服务器软件的接口，以最小化堆栈中的层级数量。
- en: There are a number of advantages to exporting CephFS instead of a filesystem
    sitting on top of an RBD. These mainly center around simplifying the number of
    layers that I/Os have to pass through and the number of components in an HA setup.
    As was discussed earlier, most local filesystems can only be mounted on one server
    at a time, otherwise corruption will occur. Therefore, when designing an HA solution
    involving RBDs and local filesystems, care needs to be taken to ensure that the
    clustering solution won't try and mount the RBD and filesystem across multiple
    nodes. This is covered in more detail later in this chapter in the section on clustering.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 导出CephFS而不是将文件系统置于RBD之上的方式有许多优势。这些优势主要集中在简化I/O传输的层数和高可用（HA）配置中组件的数量。正如前面所讨论的，大多数本地文件系统一次只能在一个服务器上挂载，否则会发生损坏。因此，在设计涉及RBD和本地文件系统的HA解决方案时，必须确保集群解决方案不会尝试跨多个节点挂载RBD和文件系统。关于这一点，本章后面有关集群部分会详细讨论。
- en: 'There is, however, one possible reason for wanting to export RBDs formatted with
    local filesystems: the RBD component of Ceph is much simpler than CephFS in its
    operation and has been marked as stable for much longer than CephFS. While CephFS
    has proved to be very stable, thought should be given to the operational side
    of the solution, and you should ensure that the operator is happy managing CephFS.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一个可能的原因是希望导出格式化为本地文件系统的RBD：Ceph的RBD组件在操作上比CephFS更简单，并且比CephFS被标记为更稳定的时间更长。虽然CephFS已经证明非常稳定，但在考虑解决方案的操作性时，应该确保操作员能够顺利管理CephFS。
- en: To export CephFS via NFS, there are two possible solutions. One is to use the
    CephFS kernel client and mount the filesystem into the operating system, and then
    use the kernel-based NFS kernel server to export it to clients. Although this
    configuration should work perfectly fine, both the kernel-based NFS server and
    the CephFS client will typically rely on the operator to run a fairly recent kernel
    to support the latest features.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过NFS导出CephFS，有两种可能的解决方案。一个是使用CephFS内核客户端，将文件系统挂载到操作系统中，然后使用基于内核的NFS服务器将其导出到客户端。尽管这种配置应该可以正常工作，但基于内核的NFS服务器和CephFS客户端通常依赖操作员运行相对较新的内核以支持最新功能。
- en: A much better idea would be to use `nfs-ganesha`, which has support for directly
    communicating to CephFS filesystems. As Ganesha runs entirely in user space, there's
    no requirement for specific kernel versions, and the supported CephFS client functionality
    can keep up with the current state of the Ceph project. There are also several
    enhancements in Ganesha that the kernel-based NFS server doesn't support. Additionally,
    HA NFS should be easier to achieve with Ganesha over the kernel server.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的做法是使用`nfs-ganesha`，它支持直接与CephFS文件系统通信。由于Ganesha完全在用户空间中运行，因此不需要特定的内核版本，而且支持的CephFS客户端功能可以跟上Ceph项目的当前状态。Ganesha中还有一些内核NFS服务器不支持的增强功能。此外，使用Ganesha实现HA
    NFS应该比使用内核服务器更容易。
- en: Samba can be used to export CephFS as a Windows-compatible share. Like NFS,
    Samba also supports the ability to directly communicate with CephFS, and so in
    most cases, there should be no requirement to have to mount the CephFS filesystem
    into the OS first. A separate project CTDB can be used to provide HA of the CephFS-backed
    Samba shares.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Samba可以用于将CephFS导出为与Windows兼容的共享。与NFS类似，Samba也支持直接与CephFS通信，因此在大多数情况下，应该不需要先将CephFS文件系统挂载到操作系统中。可以使用一个单独的项目CTDB来为CephFS支持的Samba共享提供高可用性（HA）。
- en: Finally, it is worth noting that, although Linux clients can mount CephFS directly,
    it may still be preferable to export CephFS via NFS or SMB to them. We should
    do this because, given the way CephFS works, clients are in direct communication
    with the Ceph cluster, and in some cases, this may not be desirable because of
    security concerns. By reexporting CephFS via NFS, clients can consume the storage
    without being directly exposed to the Ceph cluster.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，值得注意的是，尽管Linux客户端可以直接挂载CephFS，但通过NFS或SMB导出CephFS可能仍然更为可取。我们之所以这么做，是因为，考虑到CephFS的工作方式，客户端直接与Ceph集群进行通信，在某些情况下，这可能由于安全问题而不可取。通过NFS重新导出CephFS，客户端可以在不直接暴露于Ceph集群的情况下使用存储。
- en: Examples
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例
- en: The following examples will demonstrate how to export RBDs as iSCSI devices,
    as well as how to export CephFS via NFS and Samba. All these examples assume you
    already have a working CephFS filesystem ready to export; if that is not the case,
    then please refer to [Chapter 5](a01c8234-61a1-4d8e-9393-33a7218cf49d.xhtml),
    *RADOS Pools and Client Access*, for instructions on how to deploy one.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例将演示如何将RBD导出为iSCSI设备，以及如何通过NFS和Samba导出CephFS。所有这些示例都假设您已经有一个准备好的可导出的CephFS文件系统；如果不是这种情况，请参考[第5章](a01c8234-61a1-4d8e-9393-33a7218cf49d.xhtml)，*RADOS池和客户端访问*，以获取如何部署CephFS的说明。
- en: They also assume you have a VM available to act as the proxy server. This could
    be a Ceph monitor VM for testing purposes, but this is not recommended for production
    workloads.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 它们还假设您有一个虚拟机可以作为代理服务器。这可以是一个用于测试目的的Ceph监视器虚拟机，但不建议用于生产工作负载。
- en: Exporting Ceph RBDs via iSCSI
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过iSCSI导出Ceph RBD
- en: iSCSI is a technology that allows you to export block devices over IP networking.
    As 10 G networking has been more widely adopted, iSCSI has become extremely popular
    and is now the dominant technology in the block storage scene.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: iSCSI是一种通过IP网络导出块设备的技术。随着10 G网络的广泛采用，iSCSI已经变得极为流行，现在成为了块存储领域的主流技术。
- en: The device that's exporting the block storage is called the iSCSI target and
    the client is called the iSCSI initiator, both of which are identified by an IQN
    name.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 导出块存储的设备称为 iSCSI 目标，客户端称为 iSCSI 启动器，二者都由 IQN 名称标识。
- en: At the time of writing, the iSCSI support in Ceph only works with Red Hat-derived
    distributions. Although the underlying components should all work across any Linux
    distribution, the glue that holds them together still requires a number of updates
    to improve compatibility. Therefore, this example will require a VM running CentOS
    for the iSCSI components to be installed on. If you're testing the functionality
    in the Vagrant and Ansible lab created in [Chapter 2](dd1d6803-6e40-4bfb-8150-b605bcc08d59.xhtml),
    *Deploying Ceph with Containers*, then you can modify the Vagrant file to provision
    an additional VM running CentOS.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在写作本文时，Ceph 中的 iSCSI 支持仅适用于基于 Red Hat 的发行版。尽管底层组件应当能够在任何 Linux 发行版上正常工作，但将它们连接在一起的粘合部分仍然需要进行一系列更新以提升兼容性。因此，本示例将需要一台运行
    CentOS 的虚拟机，以便安装 iSCSI 组件。如果你在 [第2章](dd1d6803-6e40-4bfb-8150-b605bcc08d59.xhtml)
    中创建的 Vagrant 和 Ansible 实验室中测试功能，*使用容器部署 Ceph*，则可以修改 Vagrant 文件，提供一个额外的运行 CentOS
    的虚拟机。
- en: The official package repository for the iSCSI components is only available via
    a full RHEL subscription. To obtain the packages for this example, they need to
    be downloaded from the Ceph's project build server.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: iSCSI 组件的官方包仓库仅通过完整的 RHEL 订阅提供。要获取此示例的包，必须从 Ceph 的项目构建服务器下载。
- en: 'The following links will take you to the recent builds of each package:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接将引导您到每个包的最新构建：
- en: '[https://shaman.ceph.com/repos/ceph-iscsi/master/](https://shaman.ceph.com/repos/ceph-iscsi/master/)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://shaman.ceph.com/repos/ceph-iscsi/master/](https://shaman.ceph.com/repos/ceph-iscsi/master/)'
- en: '[https://shaman.ceph.com/repos/kernel/](https://shaman.ceph.com/repos/kernel/)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://shaman.ceph.com/repos/kernel/](https://shaman.ceph.com/repos/kernel/)'
- en: '[https://shaman.ceph.com/repos/ceph-iscsi-cli/](https://shaman.ceph.com/repos/ceph-iscsi-cli/)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://shaman.ceph.com/repos/ceph-iscsi-cli/](https://shaman.ceph.com/repos/ceph-iscsi-cli/)'
- en: '[https://shaman.ceph.com/repos/ceph-iscsi-config/](https://shaman.ceph.com/repos/ceph-iscsi-config/)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://shaman.ceph.com/repos/ceph-iscsi-config/](https://shaman.ceph.com/repos/ceph-iscsi-config/)'
- en: '[https://shaman.ceph.com/repos/python-rtslib/](https://shaman.ceph.com/repos/python-rtslib/)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://shaman.ceph.com/repos/python-rtslib/](https://shaman.ceph.com/repos/python-rtslib/)'
- en: '[https://shaman.ceph.com/repos/tcmu-runner/](https://shaman.ceph.com/repos/tcmu-runner/)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://shaman.ceph.com/repos/tcmu-runner/](https://shaman.ceph.com/repos/tcmu-runner/)'
- en: 'On each page, look at the arch column as shown in the following screenshot.
    This is the directory that you''ll need to look in for the packages later:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一页上，查看架构列，如下所示的屏幕截图。这是您稍后需要查找包的目录：
- en: '![](img/5c95e85d-28f3-43ca-b2f3-9507858f91be.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c95e85d-28f3-43ca-b2f3-9507858f91be.png)'
- en: 'Click the latest (or whatever version you require) build number on the left,
    which will take you to the following page:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 点击左侧的最新（或您需要的任何版本）构建号，将带您进入以下页面：
- en: '![](img/e9d1af92-8489-47b5-a13f-ce450673695b.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9d1af92-8489-47b5-a13f-ce450673695b.png)'
- en: 'Click on the Repo URL link, which will take you to the repository directory
    tree. Browse to the correct arch type that you saw in the column earlier and you
    will be presented with the RPM to download, as shown in the following screenshot:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 点击 Repo URL 链接，这将带您进入仓库目录树。浏览到之前在架构列中看到的正确架构类型，您将看到可下载的 RPM，如下所示的截图所示：
- en: '![](img/85b3371c-bf31-4315-94d1-261d4a079cf8.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/85b3371c-bf31-4315-94d1-261d4a079cf8.png)'
- en: 'Copy the URL and then use `wget` to download the package, as shown in the following
    screenshot:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 复制 URL，然后使用 `wget` 下载该包，如以下截图所示：
- en: '![](img/2fd5f67a-48d6-47bf-9781-849741d3f7db.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fd5f67a-48d6-47bf-9781-849741d3f7db.png)'
- en: 'Repeat this for every URL listed previously. When you have finished, you should
    have the following packages:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对每个之前列出的 URL 都执行此操作。完成后，您应该拥有以下包：
- en: '![](img/bb2f0fc2-3417-47f3-a74d-860833ccfabb.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bb2f0fc2-3417-47f3-a74d-860833ccfabb.png)'
- en: 'Now, install all of the of RPMs by running the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过运行以下命令安装所有 RPM 包：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](img/91846933-79b8-49c9-8fe0-6ad0b7c3c1c4.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/91846933-79b8-49c9-8fe0-6ad0b7c3c1c4.png)'
- en: 'Now that the base iSCSI support is installed, we also require the Ceph packages
    to be installed using the following code:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，基础的 iSCSI 支持已安装，我们还需要使用以下代码安装 Ceph 包：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create a new repository file and add the Ceph RPM repositories using the following
    code:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的仓库文件，并使用以下代码添加 Ceph RPM 仓库：
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](img/8db8762c-3d23-41e3-91a6-9b17d296fd15.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8db8762c-3d23-41e3-91a6-9b17d296fd15.png)'
- en: 'Now add the Fedora EPEL repository, and install and update Ceph using the following
    code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在添加Fedora EPEL仓库，并使用以下代码安装和更新Ceph：
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Create the Ceph configuration directory, if it doesn''t already exist, using
    the following code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Ceph配置目录尚不存在，请使用以下代码创建：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Copy `ceph.conf` over from a Ceph monitor node using the following code:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码从Ceph监视节点复制`ceph.conf`：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Copy the Ceph `keyring` over using the following code:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码复制Ceph `keyring`：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Edit the Ceph iSCSI gateway configuration file using the following code:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码编辑Ceph iSCSI网关配置文件：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/7c69f778-6ec6-49c0-a9da-83fc8c071798.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c69f778-6ec6-49c0-a9da-83fc8c071798.png)'
- en: Make sure it looks like the code shown in the preceding screenshot. Note the
    addition on the bottom line to allow the testing of `ceph-iscsi` with only a single
    server. In a production setting, this line wouldn't be required as you would most
    likely have redundant iSCSI gateways.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 确保它看起来像前面的截图所示的代码。注意底部添加的行，它允许仅使用一个服务器来测试`ceph-iscsi`。在生产环境中，这一行不需要，因为你很可能会有冗余的iSCSI网关。
- en: 'Now enable and start the `ceph-iscsi` daemons using the following code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用以下代码启用并启动`ceph-iscsi`守护进程：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that the configuration stored in `iscsi-gateway.conf` is only to allow
    the `ceph-iscsi` services to start and connect to the Ceph cluster. The actual
    iSCSI configuration is stored centrally in RADOS objects.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，存储在`iscsi-gateway.conf`中的配置仅用于启动`ceph-iscsi`服务并连接到Ceph集群。实际的iSCSI配置集中存储在RADOS对象中。
- en: Now that the iSCSI daemons are running, the `gwcli` tool can be used to administer
    the iSCSI configuration and present the RBDs as iSCSI devices.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，iSCSI守护进程正在运行，可以使用`gwcli`工具管理iSCSI配置并将RBDs呈现为iSCSI设备。
- en: 'Once `gwcli` has started successfully, we can run the `ls` command to see the
    structure of the `ceph-iscsi` configuration as shown in the following screenshot:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦`gwcli`成功启动，我们可以运行`ls`命令查看`ceph-iscsi`配置的结构，如下截图所示：
- en: '![](img/17de2603-d910-4b24-a0fd-91f6ecd0307a.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17de2603-d910-4b24-a0fd-91f6ecd0307a.png)'
- en: The `gwcli` tool has connected to the Ceph cluster and retrieved the list of
    pools and other configuration. We can now configure the iSCSI.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`gwcli`工具已连接到Ceph集群并检索了池列表及其他配置。现在我们可以配置iSCSI了。'
- en: 'The first item to be configured is the iSCSI gateway, using the following code:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个要配置的项是iSCSI网关，使用以下代码：
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/9aaab51b-e6c6-42a1-8289-12cbad123f04.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9aaab51b-e6c6-42a1-8289-12cbad123f04.png)'
- en: 'Now, by entering the `iqn` that has been created, the IPs of all of the gateways
    can be added using the following code:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，输入已创建的`iqn`，可以使用以下代码将所有网关的IP添加进去：
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](img/f77b93d6-426e-4ef4-aae4-223a70fdc67b.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f77b93d6-426e-4ef4-aae4-223a70fdc67b.png)'
- en: Now we can create or add the RBDs. If the RBD already exists when running the
    `create` command, then `ceph-iscsi` will simply add the existing RBD; if no RBD
    of the given name exists, then a new RBD will be created. A good example of when
    using a preexisting RBD maybe required is when the RBD contains data or if we
    need to place the RBD data on an erasure-coded pool.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建或添加RBD。如果在运行`create`命令时RBD已经存在，那么`ceph-iscsi`将简单地添加现有RBD；如果没有给定名称的RBD，则会创建一个新的RBD。一个常见的例子是，当RBD包含数据或我们需要将RBD数据放置在纠删码池中时，可能需要使用预先存在的RBD。
- en: 'For this example, a 100 GB RBD called `iscsi-test` will be created in the RBD
    pool, as shown in the following code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，将在RBD池中创建一个100GB的RBD，名为`iscsi-test`，如以下代码所示：
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](img/bfbed4cc-4959-44e8-95d6-aecae382c339.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bfbed4cc-4959-44e8-95d6-aecae382c339.png)'
- en: 'Now the initiator `iqn` needs to be added and chap authentication assigned,
    as shown in the following code:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在需要添加发起者`iqn`并分配chap认证，如以下代码所示：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](img/6b8e90fd-6c03-4c34-a143-96e832f4bb57.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b8e90fd-6c03-4c34-a143-96e832f4bb57.png)'
- en: 'Finally, add the disks to the host as LUNs using the following code. The format
    of the target is `<rados pool>.<RBD name>`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用以下代码将磁盘添加到主机作为LUN。目标的格式是`<rados pool>.<RBD name>`：
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](img/c46b91f5-4201-4dff-8f17-1cbd1e75f7a4.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c46b91f5-4201-4dff-8f17-1cbd1e75f7a4.png)'
- en: The iSCSI target configuration is now complete and available to be added into
    any iSCSI initiator's target list. Once added and rescanned, the RBDs will show
    up as LUNS and can then be treated like normal block devices and formatted with
    any filesystem, as required.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: iSCSI目标配置现在已完成，可以添加到任何iSCSI发起者的目标列表中。添加并重新扫描后，RBD将作为LUN显示，然后可以像普通块设备一样处理，并根据需要格式化为任何文件系统。
- en: Exporting CephFS via Samba
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过Samba导出CephFS
- en: The Samba project was originally designed to allow clients and servers to talk
    to the Microsoft SMB protocol. It has since evolved to be able to act as a full
    Windows domain controller. As Samba can act as a file server for clients talking
    to the SMB protocol, it can be used to export CephFS to Windows clients.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Samba项目最初是为了允许客户端和服务器通过Microsoft SMB协议进行通信。它后来发展成可以充当完整的Windows域控制器。由于Samba可以作为文件服务器，支持通过SMB协议与客户端通信，因此它可以用来将CephFS导出给Windows客户端。
- en: There is a separate project called CTDB that's used in conjunction with Samba
    to create a failover cluster to provide highly available SMB shares. CTDB uses
    the concept of a recovery lock to detect and handle split-brain scenarios. Traditionally,
    CTDB has used an area of a clustered filesystem to store the recovery lock file;
    however, this approach does not work very well with CephFS because of the fact
    that the timings of the recovery sequence conflict with the timings of the OSDs
    and CephFS MDS failovers. Hence, a RADOS-specific recovery lock was developed
    that allowed CTDB to store recovery lock information directly in a RADOS object,
    which avoids the aforementioned issues.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 另有一个名为CTDB的独立项目，它与Samba配合使用，创建一个故障转移集群，以提供高度可用的SMB共享。CTDB使用恢复锁的概念来检测和处理脑裂情况。传统上，CTDB使用集群文件系统的某个区域来存储恢复锁文件；然而，这种方法在CephFS中效果不好，因为恢复序列的时序与OSD和CephFS
    MDS故障转移的时序冲突。因此，开发了一种RADOS特定的恢复锁，它允许CTDB直接将恢复锁信息存储在RADOS对象中，从而避免了上述问题。
- en: In this example, a two-proxy node cluster will be used to export a directory
    on CephFS as an SMB share that can be accessed from Windows clients. CTDB will
    be used to provide fail over functionality. This share will also make use of CephFS
    snapshots to enable the previous version's functionality in Windows File Explorer.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，将使用一个包含两个代理节点的集群，将CephFS上的目录导出为可供Windows客户端访问的SMB共享。CTDB将用于提供故障转移功能。此共享还将利用CephFS快照，以便在Windows文件资源管理器中启用上一个版本的功能。
- en: For this example, you will need two VMs that have functional networking and
    can reach your Ceph cluster. The VMs can either be manually created, deployed
    via Ansible in your lab, or installed on the Ceph monitors for testing the Samba
    software can be.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，您需要两台虚拟机，它们需要具备有效的网络连接，并能够访问您的Ceph集群。虚拟机可以是手动创建的、通过Ansible在您的实验室中部署的，或者可以在Ceph监视器上安装，用于测试Samba软件。
- en: 'Install the `ceph`, `ctdb`, and `samba` packages on both VMs using the following
    code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码在两台虚拟机上安装`ceph`、`ctdb`和`samba`包：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](img/d6391566-42f7-45e7-97a7-8c9f5cc7ac1e.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6391566-42f7-45e7-97a7-8c9f5cc7ac1e.png)'
- en: 'Copy `ceph.conf` over from a Ceph monitor node using the following code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码将`ceph.conf`从Ceph监视节点复制过来：
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Copy the Ceph keyring over from a monitor node using the following code:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码将Ceph密钥环从监视节点复制过来：
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Your Samba gateways should now be able to act as clients to your Ceph cluster.
    This can be confirmed by checking that you can query the Ceph clusters status.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您的Samba网关应该能够作为Ceph集群的客户端。这可以通过检查是否能够查询Ceph集群的状态来确认。
- en: As mentioned previously, CTDB has a Ceph plugin to store the recovery lock directly
    in a RADOS pool. In some Linux distributions, this plugin may not be distributed
    along with the Samba and CTDB packages; certainly, in Debian-based distributions,
    it is not currently included. To work around this and save on having to manually compile,
    we will borrow a precompiled version from another distribution.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，CTDB有一个Ceph插件，可以将恢复锁直接存储在RADOS池中。在某些Linux发行版中，可能不会随Samba和CTDB包一起分发此插件；在基于Debian的发行版中，目前没有包含该插件。为了解决这个问题并避免手动编译，我们将借用其他发行版中的预编译版本。
- en: 'Download the `samba-ceph` package from the SUSE repositories using the following
    code:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码从SUSE软件库下载` samba-ceph`包：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/79dfa3a0-72fb-4ac5-bc05-99f8ca92d192.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79dfa3a0-72fb-4ac5-bc05-99f8ca92d192.png)'
- en: 'Install a utility that will extract the contents of RPM packages using the
    following code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码安装一个实用工具，以提取RPM包的内容：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](img/667f4041-7de1-4cbb-a73e-0bff184f8793.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/667f4041-7de1-4cbb-a73e-0bff184f8793.png)'
- en: 'Use the `rpm2cpio` utility to extract the contents of the RPM package that
    has just been downloaded using the following code:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码使用`rpm2cpio`工具提取刚刚下载的RPM包的内容：
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](img/4db6600f-e1b7-489b-8822-af6c5d118223.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4db6600f-e1b7-489b-8822-af6c5d118223.png)'
- en: 'Finally, copy the CTDB RADOS helper into the `bin` folder on the VM using the
    following code:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用以下代码将CTDB RADOS助手复制到虚拟机的`bin`文件夹中：
- en: '[PRE20]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Make sure all of the steps are carried out on both VMs. Now all of the required
    software is installed, we can proceed with the configuration of Samba and CTDB.
    Both CTDB and Samba come with example contents in their configuration files. For
    the purpose of this example, only the bare minimum contents will be shown; it
    is left as an exercise for the reader if they wish to further explore the range
    of configuration options available:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在两个虚拟机上都执行了所有步骤。现在所有必需的软件已经安装完毕，我们可以继续配置 Samba 和 CTDB。CTDB 和 Samba 的配置文件中都包含示例内容。在本示例中，只显示最基本的内容；如果读者希望进一步探索可用的配置选项，可以自行研究：
- en: '[PRE21]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](img/8379bd02-099b-4329-8677-5a351ad9a6d0.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8379bd02-099b-4329-8677-5a351ad9a6d0.png)'
- en: '[PRE22]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](img/9bf59349-c9a5-4c4f-b43e-db971eacd9c7.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9bf59349-c9a5-4c4f-b43e-db971eacd9c7.png)'
- en: '[PRE23]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'On each line, enter the IP address of each node participating in the CTDB Samba
    cluster, as shown in the following screenshot:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一行中，输入参与 CTDB Samba 集群的每个节点的 IP 地址，如以下截图所示：
- en: '![](img/eae5a8b9-3bf0-4885-ae8d-7239e0a63035.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eae5a8b9-3bf0-4885-ae8d-7239e0a63035.png)'
- en: 'The last step is to create a Samba user that can be used to access the share.
    To do this, use the following code:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是创建一个 Samba 用户，用于访问共享。为此，请使用以下代码：
- en: '[PRE24]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](img/993a693b-25ce-4907-8916-72ee4195cebc.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/993a693b-25ce-4907-8916-72ee4195cebc.png)'
- en: 'Again, make sure this configuration is repeated across both Samba nodes. Once
    complete, the CTDB service can be started, which should hopefully form quorum
    and then launch Samba. You can start the CTDB service using the following code:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 再次确保在两个 Samba 节点上都执行此配置。一旦完成，可以启动 CTDB 服务，它应该能够形成选举并启动 Samba。您可以使用以下代码启动 CTDB
    服务：
- en: '[PRE25]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'After a few seconds, CTDB will start to mark the nodes as healthy; this can
    be confirmed by running the following code:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，CTDB 将开始标记节点为健康状态；可以通过运行以下代码来确认这一点：
- en: '[PRE26]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This should hopefully display a status similar to the following screenshot:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该显示一个与以下截图相似的状态：
- en: '![](img/75f34c45-0269-4b96-9d78-1bace6c6f488.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75f34c45-0269-4b96-9d78-1bace6c6f488.png)'
- en: It's normal for the status to be unhealthy for a short period after being started,
    but if the status stays in this state, check the CTDB logs located at `/var/log/ctdb`
    for a possible explanation as to what has gone wrong.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 启动后，状态短时间内不健康是正常的，但如果状态长时间保持这种状态，请检查位于 ` /var/log/ctdb` 的 CTDB 日志，以查看可能的错误原因。
- en: Once CTDB enters a healthy state, you should be able to access the CephFS share
    from any Windows client.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 CTDB 进入健康状态，您应该能够从任何 Windows 客户端访问 CephFS 共享。
- en: To provide true HA, you would need a mechanism to steer clients to the active
    node's IP addresses using something like a load balancer. This is outside the
    scope of this example.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供真正的高可用性（HA），您需要使用负载均衡器等机制将客户端引导到活动节点的 IP 地址。这超出了本示例的范围。
- en: Exporting CephFS via NFS
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过 NFS 导出 CephFS
- en: NFS is a file-sharing protocol that's supported on Linux, Windows, and ESXi
    operating systems. Being able to export CephFS filesystems as NFS shares therefore
    opens the door to being able to make use of CephFS across many different types
    of clients.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: NFS 是一个文件共享协议，支持 Linux、Windows 和 ESXi 操作系统。因此，将 CephFS 文件系统作为 NFS 共享进行导出，可以使
    CephFS 在多种不同类型的客户端之间使用。
- en: Ganesha is a user space NFS server that has a native CephFS plugin, so it is
    able to directly communicate with CephFS filesystems without having to mount them
    to the local server first. It also has support for storing its configuration and
    recovery information directly in RADOS objects, which helps to allow the NFS server
    to be run in a stateless fashion.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Ganesha 是一个用户空间的 NFS 服务器，具有原生的 CephFS 插件，因此它能够直接与 CephFS 文件系统进行通信，而不需要首先将其挂载到本地服务器。它还支持将其配置和恢复信息直接存储在
    RADOS 对象中，这有助于实现无状态运行 NFS 服务器。
- en: 'Go through the following steps to install and configure the export of CephFS
    via Ganesha:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤安装和配置通过 Ganesha 导出 CephFS：
- en: 'Using the following code, install the Ganesha PPA (Ganesha 2.7 was the newest
    release at the time of writing):'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码安装 Ganesha 的 PPA（在写作时 Ganesha 2.7 是最新版本）：
- en: '[PRE27]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](img/3025e77e-b6bf-4705-9178-8925e45c0ace.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3025e77e-b6bf-4705-9178-8925e45c0ace.png)'
- en: 'Install the PPA for `libntirpc-1.7`, which is required by Ganesha, using the
    following code:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码安装 `libntirpc-1.7` 的 PPA，这是 Ganesha 所需的：
- en: '[PRE28]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![](img/553a3ca1-2f6a-4e8d-8add-d3489ef143c6.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/553a3ca1-2f6a-4e8d-8add-d3489ef143c6.png)'
- en: 'Install Ganesha using the following code:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码安装 Ganesha：
- en: '[PRE29]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![](img/1076b3aa-c92c-4086-9ade-09bb10a82754.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1076b3aa-c92c-4086-9ade-09bb10a82754.png)'
- en: 'Copy `ceph.conf` over from a Ceph monitor node using the following code:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将 `ceph.conf` 从 Ceph 监视器节点复制过来：
- en: '[PRE30]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Copy the Ceph keyring over from a monitor node using the following code:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将 Ceph 密钥环从监视器节点复制过来：
- en: '[PRE31]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now that Ganesha is installed, it needs to be configured to point at your CephFS
    filesystem. A sample configuration file is provided with the Ganesha packages;
    you can use this file as a basis for this. Firstly, copy the sample `Ganesha Ceph
    config` file to become the main `Ganesha config` file using the following code:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 Ganesha 已经安装完成，需要配置它以指向你的 CephFS 文件系统。Ganesha 包中提供了一个示例配置文件，你可以使用这个文件作为基础。首先，使用以下代码将示例的
    `Ganesha Ceph 配置` 文件复制为主 `Ganesha 配置` 文件：
- en: '[PRE32]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The configuration file is well commented, but the following screenshot shows
    a condensed version with all of the necessary options configured. It''s recommended
    that the default configuration file is kept and options adjusted where necessary
    instead of pasting over the top, as the included comments are very useful in gaining
    a better understanding of the configuration options:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件中有详细注释，但以下截图展示了一个简化版，其中所有必要的选项都已配置。建议保留默认配置文件，并在需要的地方调整选项，而不是直接覆盖，因为包含的注释非常有助于更好地理解配置选项：
- en: '![](img/8b8d54aa-3a24-4c13-b7b1-2b5cff5bbda3.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b8d54aa-3a24-4c13-b7b1-2b5cff5bbda3.png)'
- en: The `path config` variable needs to be set to the root of the CephFS filesystem
    as CephFS doesn't currently correctly support exporting subdirectories via NFS.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`path config` 变量需要设置为 CephFS 文件系统的根目录，因为 CephFS 当前不正确地支持通过 NFS 导出子目录。'
- en: 'Now enable and start the `nfs-ganesha` service using the following code:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在使用以下代码启用并启动 `nfs-ganesha` 服务：
- en: '[PRE33]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: You should now be able to mount the NFS share into any compatible client. The
    NFS share name will be CephFs.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该能够将 NFS 共享挂载到任何兼容的客户端。NFS 共享的名称将是 CephFs。
- en: ESXi hypervisor
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ESXi 虚拟化平台
- en: A reasonably frequent requirement is to be able to export Ceph storage and consume
    it via VMware's ESXi hypervisor. ESXi supports iSCSI block storage that's formatted
    with its own VMFS clustered filesystem and file-based NFS storage. Both are fully
    functional and supported, meaning that it is normally a matter of user preference
    as to which is implemented or what's best supported by their storage array.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相当频繁的需求是能够导出 Ceph 存储并通过 VMware 的 ESXi 虚拟化平台进行使用。ESXi 支持使用其自身的 VMFS 集群文件系统格式化的
    iSCSI 块存储和基于文件的 NFS 存储。两者都完全可用且受支持，意味着通常由用户的偏好决定使用哪一种，或者哪一种存储阵列最受支持。
- en: When exporting Ceph storage to ESXi, there are a number of additional factors
    that may need to be taken into consideration when using Ceph as a storage provider
    and when deciding between iSCSI and NFS. As such, this section of this chapter
    is dedicated to explaining the additional factors that should be taken into consideration
    when presenting Ceph storage to ESXi.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在将 Ceph 存储导出到 ESXi 时，存在一些额外因素需要考虑，这些因素涉及将 Ceph 用作存储提供商时的使用以及在 iSCSI 和 NFS 之间做出选择时的决策。因此，本章的这一部分专门用于解释在将
    Ceph 存储呈现给 ESXi 时应考虑的额外因素。
- en: The first thing to consider is that ESXi was developed with enterprise storage
    arrays in mind, and a couple of the design decisions during its development have
    been made around the operation of these storage arrays. As discussed in the opening
    chapter, direct attached, fiber channel, and iSCSI arrays will have much lower
    latency than distributed network storage. With Ceph, an additional hop will be
    required, acting as the NFS or iSCSI proxy; this often results in a write latency
    that's several times that of a good block storage array.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 首先需要考虑的是，ESXi 是在考虑企业存储阵列的前提下开发的，在其开发过程中做出了一些设计决策，专门考虑了这些存储阵列的运行。如在开篇章节中所述，直接附加、光纤通道和
    iSCSI 阵列的延迟要比分布式网络存储低得多。对于 Ceph，需要额外的跳数，作为 NFS 或 iSCSI 代理；这通常会导致写入延迟是优秀块存储阵列的几倍。
- en: To assist with storage vendors' QOS attempts (ignoring VAAI accelerations for
    now), ESXi will break up any clone or migration operations into smaller 64 KB
    I/Os, with the reasoning being that a large number of parallel 64 KBs are easier
    to schedule for disk time than large multi MB I/Os, which would block disk operations
    for a longer time. Ceph, however, tends to favor larger I/O sizes, and so tends
    to perform worse when cloning or migrating VMs. Additionally, depending on the
    exportation method, Ceph may not provide read ahead, and so might harm sequential
    read performance.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了配合存储供应商的 QOS 尝试（暂时忽略 VAAI 加速），ESXi 会将任何克隆或迁移操作拆分为较小的 64 KB I/O，其原因是大量并行的 64
    KB I/O 比较容易调度磁盘时间，而大规模的多 MB I/O 会占用磁盘操作更长时间。然而，Ceph 更倾向于使用更大的 I/O 大小，因此在克隆或迁移虚拟机时性能较差。此外，根据导出方式的不同，Ceph
    可能不会提供预读功能，从而影响顺序读取性能。
- en: Another area in which care needs to be taken is in managing the impact of Ceph's
    PG locking. When accessing an object stored in Ceph, the PG containing that object
    is locked to preserve data consistency. All other I/Os to that PG have to queue
    until the lock is released. For most scenarios, this presents minimal issues;
    however, when exporting Ceph to ESXi, there are a number of things that ESXi does
    that can cause contention around this PG locking.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要注意的方面是管理 Ceph PG 锁的影响。当访问 Ceph 中存储的对象时，包含该对象的 PG 会被锁定以保持数据一致性。所有其他对该 PG
    的 I/O 操作必须排队，直到锁被释放。对于大多数场景，这个问题影响较小；但是，当将 Ceph 导出到 ESXi 时，ESXi 的一些操作可能会导致 PG
    锁的争用。
- en: As mentioned previously, ESXi migrates VMs by submitting the I/Os as 64 KB.
    It also tries to maintain a stream of 32 of these operations in parallel to keep
    performance acceptable. This causes issues when using Ceph as the underlying storage,
    as a high percentage of these 64 KB I/Os will all be hitting the same 4 MB object,
    which means that out of the 32 parallel requests, each one ends up being processed
    in an almost serial manner. RBD striping may be used to try and ensure that these
    highly parallel but also highly localized I/Os are distributed across a number
    of objects, but your mileage may vary. VAAI accelerations may help with some of
    the migration and cloning operations but, in some cases, these aren't always possible
    to use, and so ESXi will fall back to the default method.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，ESXi 通过将 I/O 提交为 64 KB 来迁移虚拟机。它还试图保持 32 个并行操作的流，以确保性能可接受。这在使用 Ceph 作为底层存储时会导致问题，因为这些
    64 KB 的 I/O 中有很大一部分会命中同一个 4 MB 对象，这意味着在 32 个并行请求中，每个请求几乎以串行方式处理。可以使用 RBD 条带化来确保这些高度并行但高度本地化的
    I/O 被分布到多个对象上，但实际效果可能会有所不同。VAAI 加速可能对某些迁移和克隆操作有所帮助，但在某些情况下，这些加速并不总是可用，因此 ESXi
    会回退到默认方法。
- en: In relation to VM migration, if you are using a VMFS over iSCSI over RBD configuration,
    you can also experience PG lock contention upon updating the VMFS metadata, which
    is stored in only a small area of the disk. The VMFS metadata will often be updated
    heavily when growing a thinly provisioned VMDK or writing into snapshotted VM
    files. PG lock contention can limit throughput when a number of VMs on the VMFS
    filesystem are all trying to update the VMFS metadata at once.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 与虚拟机迁移相关，如果您使用的是通过 iSCSI 访问 RBD 配置的 VMFS，您可能会在更新仅存储在磁盘小区域中的 VMFS 元数据时遇到 PG 锁竞争。当扩展精简配置的
    VMDK 或写入快照的虚拟机文件时，VMFS 元数据通常会进行大量更新。如果多个虚拟机同时尝试更新 VMFS 元数据，PG 锁竞争可能会限制吞吐量。
- en: At the time of writing, the official Ceph iSCSI support disables RBD caching.
    For certain operations, the lack of read-ahead caching has a negative impact on
    I/O performance. This is especially seen when you have to read sequentially through
    VMDK files, such as when you are migrating a VM between datastores or removing
    snapshots.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，官方的 Ceph iSCSI 支持禁用了 RBD 缓存。对于某些操作，缺少预读缓存会对 I/O 性能产生负面影响。这种情况在需要顺序读取
    VMDK 文件时尤为明显，例如在虚拟机在数据存储之间迁移或删除快照时。
- en: Regarding HA support, at the time of writing, the official Ceph iSCSI support
    only uses implicit ALUA to manage the active iSCSI paths. This causes issues if
    an ESXi host fails over to another path and other hosts in the same vSphere cluster
    stay on the original path. The long-term solution will be to switch to explicit
    ALUA, which allows the iSCSI initiator to control the active paths on the target,
    thereby ensuring that all hosts talk down the same path. The only current workaround
    to enable a full HA stack is to only run one VM per datastore.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 HA 支持，在撰写本文时，官方的 Ceph iSCSI 支持仅使用隐式 ALUA 来管理活动的 iSCSI 路径。如果 ESXi 主机故障切换到另一路径，而同一
    vSphere 集群中的其他主机仍保持在原路径，则会导致问题。长期解决方案将是切换到显式 ALUA，这样可以让 iSCSI 发起端控制目标端的活动路径，从而确保所有主机都通过相同的路径通信。目前启用完整
    HA 堆栈的唯一变通方法是每个数据存储只运行一个虚拟机。
- en: The NFS–XFS–RBD configuration shares a lot of the PG lock contention issues
    as the iSCSI configuration, and suffers from the contention caused by the XFS
    journal. The XFS journal is a small circular buffer measured in 10s of MBs, covering
    only a few underlying RADOS objects. As ESXi is sending sync writes via NFS, parallel
    writes to XFS queue up, waiting on journal writes to complete. Because XFS is
    not a distributed filesystem, extra steps need to be implemented when building
    an HA solution to manage the mounting of the RBDs and XFS filesystems.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: NFS–XFS–RBD 配置与 iSCSI 配置共享许多 PG 锁争用问题，并且受到 XFS 日志引发的争用影响。XFS 日志是一个小型的环形缓冲区，大小为数十
    MB，仅覆盖少数几个底层 RADOS 对象。由于 ESXi 通过 NFS 发送同步写入，XFS 的并行写入排队，等待日志写入完成。因为 XFS 不是分布式文件系统，所以在构建
    HA 解决方案时，需要采取额外措施来管理 RBD 和 XFS 文件系统的挂载。
- en: Finally, we have the NFS and CephFS method. As CephFS is a filesystem, it can
    be directly exported, meaning that there is one less layer than there is with
    the other two methods. Additionally, as CephFS is a distributed filesystem, it
    can be mounted across multiple proxy nodes at the same time, meaning that there
    are two fewer cluster objects to track and manage.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有 NFS 和 CephFS 方法。由于 CephFS 是一个文件系统，它可以直接导出，这意味着与其他两种方法相比少了一层。此外，由于 CephFS
    是一个分布式文件系统，它可以同时挂载到多个代理节点，这意味着需要跟踪和管理的集群对象减少了两个。
- en: It's also likely that a single CephFS filesystem will be exported via NFS, providing
    a single large ESXi datastore, meaning that there is no need to worry about migrating
    VMs between datastores, as is the case with RBDs. This greatly simplifies the
    operation, and works around a lot of the limitations that we've discussed so far.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 还可能通过 NFS 导出单个 CephFS 文件系统，提供一个大型的 ESXi 数据存储，这意味着不需要像使用 RBD 时那样担心在数据存储之间迁移虚拟机。这大大简化了操作，解决了我们迄今为止讨论的许多限制问题。
- en: Although CephFS still requires metadata operations, these are carried out in
    parallel far better than they are in the way metadata operations in XFS or VMFS
    are handled, and so there is only minimal impact on performance. The CephFS metadata
    pool can also be placed on flash storage to further increase performance. The
    way metadata updates are handled also greatly lowers the occurrence of PG locking,
    meaning that parallel performance on the datastore is not restricted.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 CephFS 仍然需要元数据操作，但这些操作的并行处理远远优于 XFS 或 VMFS 中的元数据操作，因此对性能的影响非常小。CephFS 的元数据池也可以放在闪存存储上，以进一步提高性能。元数据更新的处理方式也大大降低了
    PG 锁定的发生频率，这意味着数据存储上的并行性能不受限制。
- en: As mentioned previously in the NFS section, CephFS can be exported both directly
    via the Ganesha FSAL or by being mounted through the Linux kernel and then exported.
    For performance reasons, mounting CephFS via the kernel and then exporting is
    the current preferred method.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在 NFS 部分，CephFS 可以通过 Ganesha FSAL 直接导出，或者通过 Linux 内核挂载后再导出。出于性能考虑，目前推荐通过内核挂载并导出的方式。
- en: Before deciding on which method suits your environment the best, it is recommended
    that you investigate each method further and make sure that you are happy administering
    the solution.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定哪种方法最适合您的环境之前，建议您进一步研究每种方法，并确保您能够顺利管理该解决方案。
- en: Clustering
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群
- en: 'The aim of clustering is to take a single point of failure and, by letting
    it run across multiple servers, make the service more reliable. In theory this
    sounds relatively simple: if server A goes down, start the service on server B.
    In practice, however, there are several considerations that need to be taken into
    account; otherwise, there is a risk that availability will likely be worse than
    a single server, or even worse, that data corruption may occur. High availability
    is very hard to get right and very easy to get wrong.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 集群的目标是通过让单点故障跨多个服务器运行，从而提高服务的可靠性。理论上，这听起来相对简单：如果服务器A宕机，便在服务器B上启动服务。然而，在实践中，有几个需要考虑的因素；否则，存在可用性可能比单台服务器更差，甚至更糟的是可能发生数据损坏的风险。高可用性非常难以实现，但很容易出错。
- en: Split brain
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 脑裂
- en: The first issue that needs to be dealt with in clustering is the scenario where
    nodes of the cluster become disconnected and unaware of each other's status. This
    condition is known as split brain. In a two-node cluster, each node has no way
    of knowing whether the reason that it has lost communication with the other node
    is because the other node has gone offline or because there is some form of networking
    interruption. In the latter case, making the incorrect assumption and starting
    resources on both nodes would lead to data corruption. The way to work around
    split brain is to make sure a cluster always has an odd number of nodes; that
    way, at least two nodes should always be able to form quorum and agree that the
    third node is the one that has had the failure.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中需要解决的第一个问题是节点断开连接且无法互相知晓对方状态的情况。此状态被称为脑裂。在一个两节点的集群中，每个节点无法知道与另一个节点失去通信的原因是因为另一个节点已经宕机，还是因为某种形式的网络中断。在后者的情况下，如果做出错误的假设，并在两个节点上都启动资源，就会导致数据损坏。解决脑裂问题的方法是确保集群总是有奇数个节点；这样，至少两个节点总能形成仲裁并达成共识，确认哪个节点出现了故障。
- en: However, even when nodes have formed quorum, it isn't still safe to restart
    resources on the remaining nodes. Take the case where a node appears offline,
    possibly because of a networking partition, or maybe the server is under high
    load and stops responding in time. If the remaining nodes where to restart services
    on themselves, what would happen if and when this unresponsive node comes back
    online? In order to deal with this scenario, we need to ensure that the cluster
    can be 100% sure of the state of all of the nodes and resources at all times.
    This is accomplished with fencing.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使节点已经形成了仲裁，当剩余节点重新启动资源时，仍然不安全。以节点看似离线为例，可能是由于网络分区，或者是服务器负载过高，导致没有及时响应。如果剩余节点自己重新启动服务，如果这个无法响应的节点重新上线，可能会发生什么？为了应对这种情况，我们需要确保集群随时能够100%确认所有节点和资源的状态。这可以通过围栏机制来实现。
- en: Fencing
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 围栏
- en: Fencing is a process of restricting the running of resources unless there is
    a consistent view of the cluster state. It also plays a part in trying to return
    the cluster to a known state by controlling the power state of cluster nodes or
    other methods. As mentioned previously, if the cluster can't be sure of the current
    state of a cluster node, services cannot simply be restarted on other nodes, as
    there is no way of knowing whether the affected node is actually dead or is still
    running those resources. Unless configured to risk data consistency, the cluster
    will simply wait indefinitely until it can be sure of the state, and unless the
    affected node returns by itself, the cluster resources will remain offline.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 围栏是一个限制资源运行的过程，前提是集群状态有一致的视图。它还通过控制集群节点的电源状态或其他方法，帮助将集群恢复到已知状态。如前所述，如果集群不能确定集群节点的当前状态，其他节点无法简单地重新启动服务，因为无法知道受影响的节点是否已经真正宕机，或者仍在运行那些资源。除非配置了数据一致性风险，否则集群将无限期地等待，直到它可以确定状态，并且除非受影响的节点自行恢复，否则集群资源将保持离线状态。
- en: The solution is to employ fencing with a method such as **Shoot The Other Node
    In The Head** (**STONITH**), which is designed to be able to return the cluster
    to a normal state by manipulating an external control mechanism. The most popular
    approach is to use a server's bulit-in IPMI functionality to power cycle the node.
    As the server's IPMI is external to the operating system and usually connected
    to a different network than the server's LAN, it is highly unlikely that it would
    be affected by whatever has caused the server to appear offline. By power cycling
    the server and getting confirmation from IPMI that this has happened, the cluster
    can now be 100% certain that the the cluster resources are no longer running on
    that node. The cluster is then OK to restart resources on other nodes, without
    risk of conflict or corruption.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是使用 STONITH 等方法进行隔离，其设计目的是通过操作外部控制机制将集群恢复到正常状态。最流行的方法是使用服务器的内置 IPMI 功能来对节点进行电源循环。由于服务器的
    IPMI 是操作系统外部的，并且通常连接到与服务器 LAN 不同的网络，因此几乎不可能受到导致服务器脱机的问题的影响。通过对服务器进行电源循环并从 IPMI
    获得确认，集群现在可以完全确定集群资源不再在该节点上运行。然后，集群可以安全地在其他节点上重新启动资源，而无需担心冲突或损坏。
- en: Pacemaker and corosync
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pacemaker 和 corosync
- en: The most widely used clustering solution on Linux is the combination of pacemaker
    and corosync. Corosync is responsible for messaging between nodes and ensuring
    a consistent cluster state, and pacemaker is responsible for managing the resources
    on top of this cluster state. There are a large number of resource agents available
    for pacemaker that enable the clustering of a wide range of services, including
    a number of STONITH agents for common-server IPMIs.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Linux 上最广泛使用的集群解决方案是 pacemaker 和 corosync 的组合。Corosync 负责节点之间的消息传递和确保一致的集群状态，而
    pacemaker 则负责在此集群状态的基础上管理资源。pacemaker 提供了大量的资源代理，支持广泛的服务集群化，包括一些常见服务器 IPMI 的 STONITH
    代理。
- en: They can both be managed by a number of different client tools, the most common
    being `pcs` and `crmsh`. The following tutorial will focus on the `crmsh` toolset.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 它们都可以由多种不同的客户端工具管理，最常见的是 `pcs` 和 `crmsh`。以下教程将重点介绍 `crmsh` 工具集。
- en: Creating a highly available NFS share backed by CephFS
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建由 CephFS 支持的高可用 NFS 共享
- en: 'In this example, three VMs will be required to form the cluster nodes. Go through
    the following steps across all three VMs:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，需要三个 VM 才能形成集群节点。在所有三个 VM 上执行以下步骤：
- en: 'Install the `corosync`, `pacemaker`, and `cmrsh` toolsets using the following
    code:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码安装 `corosync`、`pacemaker` 和 `cmrsh` 工具集：
- en: '[PRE34]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](img/d6e6a39e-8699-4946-a58a-da97e66f9bc1.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6e6a39e-8699-4946-a58a-da97e66f9bc1.png)'
- en: 'Edit the `corosync` configuration file and change the bind address (`bindnetaddr`)
    to match the IP configured on the VM using the following code:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑 `corosync` 配置文件，并将绑定地址 (`bindnetaddr`) 更改为与 VM 上配置的 IP 匹配，使用以下代码：
- en: '[PRE35]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](img/de23c436-a02d-422e-a1ba-fcf079df3b39.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de23c436-a02d-422e-a1ba-fcf079df3b39.png)'
- en: 'Enable and start the `corosync` service using the code shown in the following
    screenshot:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码启用并启动 `corosync` 服务：
- en: '![](img/3d0d2041-cfc5-41e2-901c-2e1fb1a080d5.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d0d2041-cfc5-41e2-901c-2e1fb1a080d5.png)'
- en: 'After these steps have been completed on all nodes, check the status of the
    cluster. You should see that all three nodes have joined the cluster, as shown
    in the following screenshot:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有节点完成这些步骤后，检查集群的状态。您应该看到所有三个节点都已加入集群，如以下截图所示：
- en: '![](img/c9c1f00b-2ed8-48f0-bbe2-53a2419fb7a5.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9c1f00b-2ed8-48f0-bbe2-53a2419fb7a5.png)'
- en: Note that it says `No resources`. This is because, although the cluster is running
    and nodes have become members, no resources have yet to be configured. A virtual
    IP resource will be required, which is what NFS clients will connect to. A resource
    to control the Ganesha service will also be needed. Resources are managed by resource
    agents. These are normally scripts that contain a set of standard functions that
    pacemaker calls to start, stop, and monitor the resource. There are a large number
    of resource agents that are included with the standard pacemaker installation,
    but writing custom ones is not too difficult if required.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到显示的是`No resources`。这是因为，尽管集群正在运行，节点已成为成员，但尚未配置任何资源。需要一个虚拟 IP 资源，这是 NFS 客户端将要连接的资源。同时，还需要一个控制
    Ganesha 服务的资源。资源由资源代理管理。资源代理通常是一些脚本，包含一系列标准函数，pacemaker 会调用这些函数来启动、停止和监控资源。标准的
    pacemaker 安装包含了大量的资源代理，但如果需要，编写自定义的资源代理并不困难。
- en: 'As discussed at the start of this section, fencing and STONITH are essential
    parts of an HA cluster; however, when building test environments, it can be hard
    to implement STONITH. By default, if a STONITH configuration has not been configured,
    pacemaker will not let you start any resources, so for the purpose of this example, STONITH should
    be disabled with the following command:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如本节开始时所讨论的，围栏（fencing）和 STONITH 是高可用集群的核心部分；然而，在构建测试环境时，实施 STONITH 可能会很困难。默认情况下，如果没有配置
    STONITH 配置，pacemaker 将不允许你启动任何资源。因此，在这个示例中，应该使用以下命令禁用 STONITH：
- en: '[PRE36]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![](img/7d90e3df-332f-4ddd-b5bc-d1bffd49cf07.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d90e3df-332f-4ddd-b5bc-d1bffd49cf07.png)'
- en: 'Now that the cluster is ready to have resources created, let''s create the
    virtual IP resource using the following code:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在集群已经准备好创建资源，让我们使用以下代码创建虚拟 IP 资源：
- en: '[PRE37]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](img/47ee6909-4f9c-45e1-9224-afb01e9f96b9.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47ee6909-4f9c-45e1-9224-afb01e9f96b9.png)'
- en: From the preceding screenshot, you can see that the virtual IP has been started
    and is now running on node `nfs1`. If node `nfs1` becomes unavailable, then the
    cluster will try and keep the resource running by moving it to another node.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的截图中可以看到，虚拟 IP 已经启动，并且现在正在节点 `nfs1` 上运行。如果节点 `nfs1` 变得不可用，集群将尝试通过将资源迁移到其他节点来保持资源的运行。
- en: 'Now, as we did with the previous NFS section, let''s install the latest version
    of Ganesha by going through the following steps:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，和之前的 NFS 部分一样，让我们通过以下步骤安装 Ganesha 的最新版本：
- en: 'Install the Ganesha PPA using the following code ( `ganesha 2.7` was the newest
    release at the time of writing):'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码安装 Ganesha PPA（`ganesha 2.7` 是本文撰写时的最新版本）：
- en: '[PRE38]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![](img/3bc6be11-e006-481d-968f-e86bbc1b2bea.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3bc6be11-e006-481d-968f-e86bbc1b2bea.png)'
- en: 'Using the following code, install the PPA for `libntirpc-1.7`, which is required
    by Ganesha:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码安装 Ganesha 所需的 `libntirpc-1.7` PPA：
- en: '[PRE39]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![](img/2a2693e1-f32d-4a1f-97bb-6425d65f0c37.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a2693e1-f32d-4a1f-97bb-6425d65f0c37.png)'
- en: 'Install Ganesha using the following code:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码安装 Ganesha：
- en: '[PRE40]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![](img/bfd73d88-fee8-4455-8bdd-b6636061326d.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bfd73d88-fee8-4455-8bdd-b6636061326d.png)'
- en: 'Copy `ceph.conf` over from a Ceph monitor node using the following code:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码从 Ceph 监视节点复制 `ceph.conf`：
- en: '[PRE41]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Copy the Ceph keyring over from a monitor node using the following code:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码从监视节点复制 Ceph 密钥环：
- en: '[PRE42]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now that Ganesha is installed, the configuration can be applied. The same configuration
    can be used from the standalone Ganesha section, as shown in the following screenshot:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在 Ganesha 已经安装完成，可以应用配置。可以使用来自独立 Ganesha 部分的相同配置，如下图所示：
- en: '![](img/ee82a652-8120-4da9-8d8e-fce45cc0d6a4.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee82a652-8120-4da9-8d8e-fce45cc0d6a4.png)'
- en: Unlike in the standalone example, we must ensure that Ganesha is not set to
    run by itself, and only pacemaker should launch it.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 与独立示例不同，我们必须确保 Ganesha 不会单独运行，只有 pacemaker 才能启动它。
- en: 'Now that all of the configuration work is completed, the pacemaker resource
    can be added to control the running of Ganesha using the following code:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，所有配置工作已完成，可以使用以下代码将 pacemaker 资源添加到控制 Ganesha 的运行：
- en: '[PRE43]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Finally, we need to make sure that the Ganesha service is running on the same
    node as the virtual IP. We can do this by creating a group resource using the
    following code. A group resource ensures that all resources are run together on
    the same node, and that they''re started in the order in which they are defined:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要确保 Ganesha 服务与虚拟 IP 在同一节点上运行。我们可以通过使用以下代码创建一个组资源来实现这一点。组资源确保所有资源在同一节点上一起运行，并且按定义的顺序启动：
- en: '[PRE44]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, if we check the status of the cluster, we can see that the Ganesha service
    is now being run, and because of the grouping, it is running on the same node
    as the virtual IP, as shown in the following screenshot:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们检查集群状态，可以看到Ganesha服务正在运行，并且由于分组，它与虚拟IP运行在同一节点上，如下图所示：
- en: '![](img/9680f1f9-d481-44da-b679-762f4e25097f.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9680f1f9-d481-44da-b679-762f4e25097f.png)'
- en: NFS clients should now be able to connect to the virtual IP and map the NFS
    share. If a cluster node fails, the virtual IP and Ganesha service will migrate
    to another cluster node, and clients should only see a brief interruption to service.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: NFS客户端现在应该能够连接到虚拟IP并映射NFS共享。如果集群节点发生故障，虚拟IP和Ganesha服务将迁移到另一个集群节点，客户端应该只会看到短暂的服务中断。
- en: To check the failover capability, we can put the running cluster node into `standby`
    mode to force pacemaker to run the resources on another node.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查故障转移能力，我们可以将运行中的集群节点置为`standby`模式，强制Pacemaker将资源转移到另一个节点上。
- en: 'In the current example, the resources are running on node `nfs2`, so the command
    is as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前示例中，资源正在节点`nfs2`上运行，命令如下：
- en: '[PRE45]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '![](img/f91412ca-6a29-4d36-a824-ad336bd5e7c1.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f91412ca-6a29-4d36-a824-ad336bd5e7c1.png)'
- en: We can see now that node `nfs2` is now in `standby` mode and the resources have
    moved across to running on node `nfs3`.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以看到节点`nfs2`已经进入`standby`模式，资源已迁移并在节点`nfs3`上运行。
- en: Summary
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about the different storage protocols that exist
    and how they match to Ceph's capabilities. You also learned the protocols that
    are best suited for certain roles, and should be able to make informed decisions
    when selecting them.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了不同的存储协议以及它们如何与Ceph的功能相匹配。你还学会了哪些协议最适合某些角色，并且应该能够在选择时做出明智的决策。
- en: Having worked through the examples, you should also have a firm understanding
    of how to export Ceph storage via iSCSI, NFS, and SMB to enable non-native Ceph
    clients to consume Ceph storage.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些示例后，你应该也能清楚地理解如何通过iSCSI、NFS和SMB导出Ceph存储，使非本地Ceph客户端能够使用Ceph存储。
- en: Finally, you should also understand the requirements for being able to design
    and build a resilient failover cluster that can be used to deliver highly available
    Ceph storage to non-native clients.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你还应该了解设计和构建一个可用的故障转移集群的要求，能够为非本地客户端提供高可用的Ceph存储。
- en: In the next chapter we will look at the different types of RADOS pool types
    and the different types of Ceph storage which can be provisioned.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将讨论不同类型的RADOS池和可以配置的不同类型的Ceph存储。
- en: Questions
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Name the three storage protocols discussed in this chapter.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本章讨论的三种存储协议是什么？
- en: What storage protocol is typically used to provide block storage over an IP
    network?
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通常使用哪种存储协议通过IP网络提供块存储？
- en: Which storage protocol is primarily used by Windows clients?
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Windows客户端主要使用哪种存储协议？
- en: What's the user space NFS server called?
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户空间的NFS服务器叫什么？
- en: What two pieces of software are used to build a failover cluster?
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于构建故障转移集群的两个软件是什么？
- en: Why might you want to export CephFS via NFS to Linux clients?
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么你可能想通过NFS将CephFS导出到Linux客户端？
