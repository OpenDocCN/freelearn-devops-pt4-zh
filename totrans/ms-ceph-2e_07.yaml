- en: RADOS Pools and Client Access
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ceph provides a variety of different pool types and configurations. It also
    supports several different data-storage types to offer storage to clients. This
    chapter will look at the differences between replicated and erasure-coded pools,
    giving examples of the creation and maintenance of both. We will then move on
    to how to use these pools for the three data-storage methods: **RADOS Block Device** (**RBD**),
    object, and CephFS. Finally, we will finish with a look at how to take snapshots
    of the different types of storage methods. The following topics are covered in
    this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Pools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ceph storage types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RADOS pools are the core part of a Ceph cluster. Creating a RADOS pool is what
    drives the creation and distribution of the placement groups, which themselves
    are the autonomous part of Ceph. Two types of pools can be created, replicated,
    and erasure-coded, offering different usable capacities, durability, and performance.
    RADOS pools can then be used to provide different storage solutions to clients
    via RBS, CephFS, and RGW, or they can be used to enable tiered performance overlaying
    other RADOS pools.
  prefs: []
  type: TYPE_NORMAL
- en: Replicated pools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Replicated RADOS pools are the default pool type in Ceph; data is received
    by the primary OSD from the client and then replicated to the remaining OSDs.
    The logic behind the replication is fairly simple and requires minimal processing
    to calculate and replicate the data between OSDs. However, as the data is replicated
    in whole, there is a large write penalty, as the data has to be written multiple
    times across the OSDs. By default, Ceph will use a replication factor of 3x, so
    all data will be written three times; this does not take into account any other
    write amplification that may be present further down in the Ceph stack. This write
    penalty has two main drawbacks: It obviously puts further I/O load on your Ceph
    cluster, as there is more data to be written, and in the case of SSDs, these extra
    writes will wear out the flash cells more quickly. However, as we will see in
    the section, *Erasure-code pools*, for smaller I/Os, the simpler replication strategy
    actually results in lower total required operations—there is always a fixed 3x
    write penalty, no matter the I/O size.'
  prefs: []
  type: TYPE_NORMAL
- en: It should also be noted that although all replicas of an object are written
    to during a client write operation, when an object is read, only the primary OSD
    holding a copy of the object is involved. A client also only sends the write operation
    to the primary OSD, which then sends the operation to the remaining replicas.
    There are a number of reasons for this behavior, but they largely center around
    ensuring the consistency of reads.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, the default replication size is 3, with a required minimum size
    of two replicas to accept client I/O. Decreasing either of these values is not
    recommended, and increasing them will likely have minimal effects on increasing
    data durability, as the chance of losing three OSDs that all share the same PG
    is highly unlikely. As Ceph will prioritize the recovery of PGs that have the
    fewest copies, this further minimizes the risk of data loss, therefore, increasing
    the number of replica copies to four is only beneficial when it comes to improving
    data availability, where two OSDs sharing the same PG can be lost and allow Ceph
    to keep servicing client I/O. However, due to the storage overhead of four copies,
    it would be recommended to look at erasure coding at this point. With the introduction
    of NVMes, which due to their faster performance reduce rebuild times, using a
    replica size of 2 can still offer reasonable data durability.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a replicated pool, issue a command, such as the one in the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This would create a replicated pool with `128` placement groups, called `MyPool`.
  prefs: []
  type: TYPE_NORMAL
- en: Erasure code pools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ceph's default replication level provides excellent protection against data
    loss by storing three copies of your data on different OSDs. However, storing
    three copies of data vastly increases both the purchase cost of the hardware and
    the associated operational costs, such as power and cooling. Furthermore, storing
    copies also means that for every client write, the backend storage must write
    three times the amount of data. In some scenarios, either of these drawbacks may
    mean that Ceph is not a viable option.
  prefs: []
  type: TYPE_NORMAL
- en: Erasure codes are designed to offer a solution. Much like how RAID 5 and 6 offer
    increased usable storage capacity over RAID 1, erasure coding allows Ceph to provide
    more usable storage from the same raw capacity. However, also like the parity-based
    RAID levels, erasure coding brings its own set of disadvantages.
  prefs: []
  type: TYPE_NORMAL
- en: What is erasure coding?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Erasure coding** allows Ceph to achieve either greater usable storage capacity
    or increase resilience to disk failure for the same number of disks, versus the
    standard replica method. Erasure coding achieves this by splitting up the object
    into a number of parts and then also calculating a type of **cyclic redundancy
    check** (**CRC**), the erasure code, and then storing the results in one or more
    extra parts. Each part is then stored on a separate OSD. These parts are referred
    to as *K* and *M* chunks, where *K* refers to the number of data shards and *M* refers
    to the number of erasure code shards. As in RAID, these can often be expressed
    in the form *K+M,* or *4+2*, for example.'
  prefs: []
  type: TYPE_NORMAL
- en: In the event of an OSD failure that contains an object's shard, which is one
    of the calculated erasure codes, data is read from the remaining OSDs that store
    data with no impact. However, in the event of an OSD failure that contains the
    data shards of an object, Ceph can use the erasure codes to mathematically recreate
    the data from a combination of the remaining data and erasure code shards.
  prefs: []
  type: TYPE_NORMAL
- en: K+M
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The more erasure code shards you have, the more OSD failures you can tolerate
    and still successfully read data. Likewise, the ratio of *K* to *M* shards each
    object is split into has a direct effect on the percentage of raw storage that
    is required for each object.
  prefs: []
  type: TYPE_NORMAL
- en: A *3+1* configuration will give you 75% usable capacity but only allows for
    a single OSD failure, and so would not be recommended. In comparison, a three-way
    replica pool only gives you 33% usable capacity.
  prefs: []
  type: TYPE_NORMAL
- en: '*4+2* configurations would give you 66% usable capacity and allows for two
    OSD failures. This is probably a good configuration for most people to use.'
  prefs: []
  type: TYPE_NORMAL
- en: At the other end of the scale, *18+2* would give you 90% usable capacity and
    still allow for two OSD failures. On the surface, this sounds like an ideal option,
    but the greater total number of shards comes at a cost. A greater number of total
    shards has a negative impact on performance and also an increased CPU demand.
    The same 4 MB object that would be stored as a whole single object in a replicated
    pool would now be split into 20 x 200-KB chunks, which have to be tracked and
    written to 20 different OSDs. Spinning disks will exhibit faster bandwidth, measured
    in MBps with larger I/O sizes, but bandwidth drastically tails off at smaller
    I/O sizes. These smaller shards will generate a large amount of small I/O and
    cause an additional load on some clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, it''s important not to forget that these shards need to be spread across
    different hosts according to the CRUSH map rules: no shard belonging to the same
    object can be stored on the same host as another shard from the same object. Some
    clusters may not have a sufficient number of hosts to satisfy this requirement.
    If a CRUSH rule cannot be satisfied, the PGs will not become active, and any I/O
    destined for these PGs will be halted, so it''s important to understand the impact
    on a cluster''s health of making CRUSH modifications.'
  prefs: []
  type: TYPE_NORMAL
- en: Reading back from these high-chunk pools is also a problem. Unlike in a replica
    pool, where Ceph can read just the requested data from any offset in an object,
    in an erasure pool, all shards from all OSDs have to be read before the read request
    can be satisfied. In the *18+2* example, this can massively amplify the amount
    of required disk read ops, and average latency will increase as a result. This
    behavior is a side-effect that tends to only cause a performance impact with pools
    that use a lot of shards. A *4+2* configuration in some instances will get a performance
    gain compared to a replica pool, from the result of splitting an object into shards.
    As the data is effectively striped over a number of OSDs, each OSD has to write
    less data, and there are no secondary and tertiary replicas to write.
  prefs: []
  type: TYPE_NORMAL
- en: 'Erasure coding can also be used to improve durability rather than to maximize
    available storage space. Take, for example, a *4+4* pool: it has a storage efficiency
    of 50%, so it''s better than a 3x replica pool, yet it can sustain up to four
    OSD losses without data loss.'
  prefs: []
  type: TYPE_NORMAL
- en: How does erasure coding work in Ceph?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As with replication, Ceph has a concept of a primary OSD, which also exists when
    using erasure-coded pools. The primary OSD has the responsibility of communicating
    with the client, calculating the erasure shards, and sending them out to the remaining
    OSDs in the PG set. This is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1032823-151a-4466-bde2-59b24bea067e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If an OSD in the set is down, the primary OSD can use the remaining data and
    erasure shards to reconstruct the data, before sending it back to the client.
    During read operations, the primary OSD requests all OSDs in the PG set to send
    their shards. The primary OSD uses data from the data shards to construct the
    requested data, and the erasure shards are discarded. There is a fast read option
    that can be enabled on erasure pools, which allows the primary OSD to reconstruct
    the data from erasure shards if they return quicker than data shards. This can
    help to lower average latency at the cost of a slightly higher CPU usage. The
    following diagram shows how Ceph reads from an erasure-coded pool:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97d94d63-67ce-4fc9-b7b1-bb08b63ad9fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram shows how Ceph reads from an erasure pool when one of
    the data shards is unavailable. Data is reconstructed by reversing the erasure
    algorithm, using the remaining data and erasure shards:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d5c21792-0be1-42df-9b94-b32ed1e635bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Algorithms and profiles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of different erasure plugins you can use to create your erasure-coded
    pool.
  prefs: []
  type: TYPE_NORMAL
- en: Jerasure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The default erasure plugin in Ceph is the jerasure plugin, which is a highly-optimized open
    source erasure-coding library. The library has a number of different techniques
    that can be used to calculate the erasure codes. The default is Reed-Solomon, and
    this provides good performance on modern processors, which can accelerate the
    instructions that the technique uses.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also a number of other jerasure techniques that can be used, which
    all have a fixed number of *M* shards. If you are intending on only having two *M* shards,
    they can be a good candidate, as their fixed size means that optimizations are
    possible, lending to increased performance. Each optimized technique, aside from
    only supporting two erasure shards, also tends to have certain requirements around
    the total number of shards. Here is a brief description of each optimized technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '`reed_sol_van`: The default technique, complete flexibility on number of *k+m*
    shards, also the slowest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reed_sol_r6_op`: Optimized version of default technique for use cases where
    *m=2*. Although it is much faster than the unoptimized version, it''s not as fast
    as other versions. However, the number of *k* shards is flexible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cauchy_orig`: Better than the default, but it''s better to use `cauchy_good`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cauchy_good`: Middle-of-the-road performance while maintaining full flexibility
    of the shard configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`liberation`: Total number of shards must be equal to a prime number and *m=2*,
    so *3+2*, *5+2*, or *9+2* are all good candidates, excellent performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`liber8tion`: Total number of shards must be equal to *8* and *m=2*, only *6+2*
    is possible, but excellent performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`blaum_roth`: Total number of shards must be one less than a prime number and
    *m=2*, so the ideal is *4+2*, excellent performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As always, benchmarks should be conducted before storing any production data
    on an erasure-coded pool to identify which technique best suits your workload.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the jerasure profile should be preferred in most cases, unless another
    profile has a major advantage, as it offers well-balanced performance and is well-tested.
  prefs: []
  type: TYPE_NORMAL
- en: ISA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ISA library is designed to work with Intel processors and offers enhanced
    performance. It supports both the Reed-Solomon and Cauchy techniques.
  prefs: []
  type: TYPE_NORMAL
- en: LRC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the disadvantages of using erasure coding in a distributed storage system is
    that recovery can be very intensive on networking between hosts. As each shard
    is stored on a separate host, recovery operations require multiple hosts to participate
    in the process. When the CRUSH topology spans multiple racks, this can put pressure
    on the inter-rack networking links. The **Locally Repairable erasure Code** (**LRC**)
    erasure plugin adds an additional parity shard, which is local to each OSD node.
    This allows recovery operations to remain local to the node where an OSD has failed
    and remove the need for nodes to receive data from all other remaining shard-holding
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: However, the addition of these local recovery codes does impact the amount of
    usable storage for a given number of disks. In the event of multiple disk failures,
    the LRC plugin has to resort to using global recovery, as would happen with the
    jerasure plugin.
  prefs: []
  type: TYPE_NORMAL
- en: SHEC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The SHEC profile is designed with similar goals to the LRC plugin, in that it
    reduces the networking requirements during recovery. However, instead of creating
    extra parity shards on each node, SHEC shingles the shards across OSDs in an overlapping
    fashion. The shingle part of the plugin name represents the way the data distribution
    resembles shingled tiles on a roof of a house. By overlapping the parity shards
    across OSDs, the SHEC plugin reduces recovery resource requirements for both single
    and multiple disk failures.
  prefs: []
  type: TYPE_NORMAL
- en: Overwrite support in erasure-coded pools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although erasure-coded pool support has been in Ceph for several releases now,
    before the arrival of BlueStore in the Luminous release, it had not supported
    partial writes. This limitation meant that erasure pools could not directly be
    used with RBD and CephFS workloads. With the introduction of BlueStore in Luminous,
    it provided the groundwork for partial write support to be implemented. With partial
    write support, the number of I/O types that erasure pools can support almost matches
    replicated pools, enabling the use of erasure-coded pools directly with RBD and
    CephFS workloads. This dramatically lowers the cost of storage capacity for these
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: For full-stripe writes, which occur either for new objects or when the entire
    object is rewritten, the write penalty is greatly reduced. A client writing a
    4 MB object to a 4+2 erasure-coded pool would only have to write 6 MB of data,
    4 MB of data chunks, and 2 MB of erasure-coded chunks. This is compared to 12
    MB of data written in a replicated pool. It should, however, be noted that each
    chunk of the erasure stripe will be written to a different OSD. For smaller erasure
    profiles, such as *4+2*, this will tend to offer a large performance boost for
    both spinning disks and SSDs, as each OSD is having to write less data. However,
    for larger erasure stripes, the overhead of having to write to an ever-increasing
    number of OSDs starts to outweigh the benefit of reducing the amount of data to
    be written, particularly on spinning disks whose latency does not have a linear
    relationship to the I/O size.
  prefs: []
  type: TYPE_NORMAL
- en: Ceph's userspace clients, such as `librbd` and `libcephfs`, are clever enough
    to try to batch together smaller I/Os and submit a full stripe write if possible;
    this can help when the application residing previously is submitting sequential
    I/O but not aligned to the 4 MB object boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: Partial write support allows overwrites to be done to an object; this introduces
    a number of complexities, as, when a partial write is done, the erasure chunks
    also require updating to match the new object contents. This is very similar to
    the challenges faced by RAID 5 and 6, although having to coordinate this process
    across several OSDs in a consistent manor increases the complexity. When a partial
    write is performed, Ceph first reads the entire existing object off the disk,
    and then it must merge in memory the new writes, calculate the new erasure-coded
    chunks, and write everything back to the disk. So, not only is there both a read
    and a write operation involved, but each of these operations will likely touch
    several disks making up the erasure stripe. As you can see, a single I/O can end
    up having a write penalty several times higher than that of a replicated pool.
    For a *4+2* erasure-coded pool, a small 4 KB write could end up submitting 12
    I/Os to the disks in the cluster, not taking into account any additional Ceph
    overheads.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an erasure-coded pool
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's bring our test cluster up again and switch into super-user mode in Linux,
    so we don't have to keep prepending `sudo` to our commands.
  prefs: []
  type: TYPE_NORMAL
- en: Erasure-coded pools are controlled by the use of erasure profiles; these control how
    many shards each object is broken up into including the split between data and
    erasure shards. The profiles also include configuration to determine what erasure
    code plugin is used to calculate the hashes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following plugins are available to use:'
  prefs: []
  type: TYPE_NORMAL
- en: Jerasure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ISA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LRC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shingled Erasure Coding** (**SHEC**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To see a list of the erasure profiles, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see there is a `default` profile in the fresh installation of Ceph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/04266842-af2a-4313-839b-15d8cf69ac32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s see what configuration options it contains, using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `default` profile specifies that it will use the jerasure plugin with the
    Reed-Solomon error-correcting codes and will split objects into `2` data shards
    and `1` erasure shard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3cf99f8-389a-44a6-908d-f48074281008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is almost perfect for our test cluster; however, for the purpose of this
    exercise, we will create a new profile, using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see our new `example_profile` has been created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d829a26e-70ca-436d-a2d1-8fe412d219b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s create our erasure-coded pool with this profile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53812e80-e75b-4db6-a858-41fa2525fec5.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding command instructs Ceph to create a new pool called `ecpool` with `128` PGs.
    It should be an erasure-coded pool and should use the `example_profile` we previously
    created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create an object with a small text string inside it and then prove the
    data has been stored by reading it back:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'That proves that the erasure-coded pool is working, but it''s hardly the most
    exciting of discoveries:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f808d9cd-9a8c-4487-ab88-5ed84d88be1d.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's check whether we can see what's happening at a lower level.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, find out what PG is holding the object we just created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the preceding command tells us that the object is stored in PG `3.40` on
    OSDs `1`, `2`, and `0` in this example Ceph cluster. That''s pretty obvious, as
    we only have three OSDs, but in larger clusters, that is a very useful piece of
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c8c544c-1120-43d9-85a3-85989b886ea4.png)'
  prefs: []
  type: TYPE_IMG
- en: The PGs will likely be different on your test cluster, so make sure the PG folder
    structure matches the output of the preceding `ceph osd map` command.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using BlueStore, the file structure of the OSD is no longer viewable
    by default. However, you can use the following command on a stopped OSD to mount
    the BlueStore OSD as a Linux filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now look at the folder structure of the OSDs and see how the object
    has been split using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following examples are shown using filestore; if using BlueStore, replace
    the OSD path with the contents of the `/mnt` mount point from the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5e200a4-b91d-4e5e-a3ae-0e1a2f8da532.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8fee619e-8c24-4c96-b76e-ebf2efa9cc68.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87dd434f-0b55-418c-ba3f-02886db550bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice how the PG directory names have been appended to the shard number and
    that replicated pools just have the PG number as their directory name. If you
    examine the contents of the object files, you will see our text string that we
    entered into the object when we created it. However, due to the small size of
    the text string, Ceph has padded out the second shard with null characters and
    the erasure shard; hence, it will contain the same as the first. You can repeat
    this example with a new object that contains larger amounts of text to see how
    Ceph splits the text into the shards and calculates the erasure code.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting the 2147483647 error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This small section is included within the erasure-coding, rather than in [Chapter
    11](bf50389d-8256-4cbf-ae32-01a494ab4189.xhtml), *Troubleshooting*, of this book,
    as it''s commonly seen with erasure-coded pools and so is very relevant to this
    chapter. An example of this error is shown in the following screenshot, when running
    the `ceph health detail` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6fbcd7e7-fa12-465d-a4fe-49c8dc76d430.png)'
  prefs: []
  type: TYPE_IMG
- en: If you see `2147483647` listed as one of the OSDs for an erasure-coded pool,
    this normally means that CRUSH was unable to find a sufficient number of OSDs
    to complete the PG peering process. This is normally due to the number of K+M
    shards being larger than the number of hosts in the CRUSH topology. However, in
    some cases, this error can still occur even when the number of hosts is equal
    to or greater than the number of shards. In this scenario, it's important to understand
    how CRUSH picks OSDs as candidates for data placement. When CRUSH is used to find
    a candidate OSD for a PG, it applies the CRUSH map to find an appropriate location
    in the CRUSH topology. If the result comes back as the same as a previously-selected
    OSD, Ceph will retry to generate another mapping by passing slightly different
    values into the CRUSH algorithm. In some cases, if there is a similar number of
    hosts to the number of erasure shards, CRUSH may run out of attempts before it
    can suitably find the correct OSD mappings for all the shards. Newer versions
    of Ceph have mostly fixed these problems by increasing the CRUSH tuneable, `choose_total_tries`.
  prefs: []
  type: TYPE_NORMAL
- en: Reproducing the problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To aid our understanding of the problem in more detail, the following steps
    will demonstrate how to create an `erasure-code-profile` that will require more
    shards than our three-node cluster can support.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like earlier in the chapter, create a new erasure profile but modify the K/M
    parameters to be `k=3` and `m=1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now create a pool with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If we look at the output from `ceph -s`, we will see that the PGs for this
    new pool are stuck in the creating state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b122237f-d54b-43f4-9ea8-42b4f99ce063.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output of `ceph health detail` shows the reason, and we see the `2147483647` error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/058ca850-483c-48c4-8095-5a8c589faa9c.png)'
  prefs: []
  type: TYPE_IMG
- en: If you encounter this error and it is a result of your erasure profile being
    larger than your number of hosts or racks, depending on how you have designed
    your CRUSH map, then the only real solution is to either drop the number of shards
    or increase the number of hosts.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create an erasure-coded pool, issue a command, as shown in the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This would create an erasure-coded pool with 128 placement groups, called `MyECPool`,
    using the erasure-coding profile, called `MyECProfile`.
  prefs: []
  type: TYPE_NORMAL
- en: Although partial writes bring erasure-coded pools to near parity with replicated
    pools in terms of supported features, they still cannot store all the required
    data for RBDs. Therefore, when creating an RBD, you must place the RBD header
    object on a replicated RADOS pool and then specify that the data objects for that
    RBD should be stored in the erasure-coded pool.
  prefs: []
  type: TYPE_NORMAL
- en: Scrubbing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To protect against bit-rot, Ceph periodically runs a process called scrubbing
    to verify the data stored across the OSDs. The scrubbing process works at the
    PG level and compares the contents of each of the PGs across all of the participating
    OSDs to check that each OSD has identical contents. If an OSD is found to have
    an object copy that differs to the others or is even missing the object, the PG
    is marked as inconsistent. Inconsistent PGs can be repaired by instructing Ceph
    to repair the PG; this is covered in further detail in [Chapter 11](bf50389d-8256-4cbf-ae32-01a494ab4189.xhtml), *Troubleshooting*.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of scrubbing: normal and deep. Normal scrubbing simply
    checks for the existence of the object and that its metadata is correct; deep
    scrubbing is when the actual data is compared. Deep scrubbing tends to be much
    more I/O-intensive than normal scrubbing.'
  prefs: []
  type: TYPE_NORMAL
- en: Although BlueStore now supports checksums, the need for scrubbing is not completely
    redundant. BlueStore only compares the checksums against the data being actively
    read, and so for cold data that is very rarely written, data loss or corruption
    could occur and only the scrubbing process would detect this.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of scrubbing tuning options that are covered later in [Chapter
    9](5c0bf2de-c75e-4aa7-b3d9-8a1e038e3735.xhtml), *Tuning Ceph*; they influence
    the scheduling of when scrubbing takes place and the impact on client I/O.
  prefs: []
  type: TYPE_NORMAL
- en: Ceph storage types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although Ceph provides basic object storage via the RADOS layer, on its own
    this is not very handy, as the scope of applications that could consume RADOS
    storage directly is extremely limited. Therefore, Ceph builds on the base RADOS
    capabilities and provides higher-level storage types that can be more easily consumed
    by clients.
  prefs: []
  type: TYPE_NORMAL
- en: RBD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RBD for short, is how Ceph storage can be presented as standard Linux block
    devices. RBDs are composed of a number of objects, 4 MB by default, which are
    concatenated together. A 4 GB RBD would contain a 1,000 objects by default.
  prefs: []
  type: TYPE_NORMAL
- en: Thin provisioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Due to the way RADOS works, RBDs are thin provisioned;  that is to say, the
    underlying objects are only provisioned once data is written to the logical block
    address that corresponds to that object. There are no safeguards around this;
    Ceph will quite happily let you provision a 1 PB block device on a 1 TB disk,
    and, as long as you never place more than 1 TB of data on it, everything will
    work as expected. If used correctly, thin provisioning can greatly increase the
    usable capacity of a Ceph cluster as VMs, which are typically one of the main
    use cases for RBDs, likely have a large amount of whitespace contained within
    them. However care should be taken to monitor the growth of data on the Ceph cluster;
    if the underlying usable capacity is filled, the Ceph cluster will effectively
    go offline until space is freed.
  prefs: []
  type: TYPE_NORMAL
- en: Snapshots and clones
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RBDs support having snapshots taken of them. Snapshots are a read-only copy
    of the RBD image that persists its state from the point in time in which it was
    taken. Multiple snapshots can be taken to retain the RBDs history through time,
    if desired. The process of taking a snapshot of an RBD is extremely quick, and
    there is no performance penalty for reads going to the source RBD. However, when
    a write hits the source RBD for the first time, the existing contents of the object
    will be cloned for us by the snapshot, further I/O will have no further impact.
    This process is called **copy-on-write** and is a standard way of performing snapshots
    in storage products. It should be noted that this process is greatly accelerated
    in BlueStore, as a full object copy is not required as it was in filestore, although
    care should still be taken to make sure that RBDs that experience heavy write
    I/O are not left with open snapshots for long periods of time. As well as snapshots
    that require extra I/O during writes—as the copy-on-write process creates clones
    of the objects, additional cluster space is consumed—care should be taken to monitor
    space consumption when snapshots are in use.
  prefs: []
  type: TYPE_NORMAL
- en: During the removal of a snapshot, the PGs containing snapshot objects enter
    a snaptrim state. In this state, the objects that had been cloned as part of the
    copy-on-write process are removed. Again, on BlueStore, this process has much
    less impact on the cluster load.
  prefs: []
  type: TYPE_NORMAL
- en: RBDs also support snapshot layering; this is a process where a writable clone
    is made of an existing snapshot, which is itself a snapshot of an existing RBD.
    This process is typically used to create cloned VMs of master images; an initial
    RBD is created for a VM, to which an OS is installed. Snapshots are then taken
    throughout the master image's life to capture changes. These snapshots are then
    used as the basis for cloning new VMs. When an RBD snapshot is cloned initially,
    non objects of the objects in the RBD are required to be duplicated, as since
    they are identical to the source, they can simply be referenced by the clone.
    Once the cloned RBD starts getting data written to it, each object that is modified
    is then written out as a new object that belongs to the clone.
  prefs: []
  type: TYPE_NORMAL
- en: This process of object referencing means that a large number of VMs that share
    the same OS template will likely consume less space than if each VM was individually
    deployed to fresh RBDs. In some cases, it may be desired to force a full clone
    where all the RBDs objects are duplicated; this process in Ceph is called flattening
    a clone.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create a snapshot, called `snap1`, of a RBD image, called `test`, in
    the default RBD pool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Confirm that the snapshot has been created by viewing all snapshots of the
    RBD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/09816dc7-d16f-4ac6-a7a4-9895670f13c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For the snapshot to be cloned, it needs to be protected. As the clones are
    dependent on the snapshot, any modification to the snapshot would likely cause
    corruption in the clones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'View the info of the snapshot; it can be seen that the snapshot is now protected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/fd766cb7-32f3-42c5-a839-2003e0ab1409.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now a clone of the snapshot can be taken:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You can confirm the relationship of the clone to the snapshot by viewing the
    `rbd info` of the clone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/8a95445c-e8f1-4297-85ca-14a928663d78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Or you can do so by viewing the list of children of the snapshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/de66919a-b6fa-4e4e-a60e-18e58c14a65e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now flatten the clone; this will make it a completely independent RBD image
    no longer dependent on the snapshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/9711ece6-7bc2-4121-98cd-ac953ebcd144.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Confirm that the clone is now no longer attached to the snapshot; note the
    parent field is now missing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0250c040-ec7c-40c8-8741-308abd4c247d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Unprotect the snapshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally delete it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Object maps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As RBDs support thin provisioning and are composed of a large number of 4 MB
    objects, tasks such as determining what space the RBD is consuming, or cloning
    the RBD, would involve a large number of read requests to determine whether a
    certain object that is part of the RBD exists. To solve this problem, RBDs support
    object maps; these maps indicate which logical blocks of an RBD have been allocated
    and so greatly speed up the process of calculating which objects exist. The object
    map is stored as an object itself in the RADOS pool and should not be manipulated
    directly.
  prefs: []
  type: TYPE_NORMAL
- en: Exclusive locking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To try to prevent corruption from two clients writing to the same RBD at the
    same time, exclusive locking allows the client to acquire a lock to disallow any
    other client from writing to the RBD. It's important to note that clients can
    always request the lock to be transferred to themselves and so the lock is only
    to protect the RBD device itself; a non-clustered filesystem will still likely
    be corrupted if two clients try to mount it, regardless of the exclusive locking.
  prefs: []
  type: TYPE_NORMAL
- en: CephFS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CephFS is a POSIX-compatible filesystem that sits on top of RADOS pools. Being
    POSIX-compliment means that it should be able to function as a drop-in replacement
    for any other Linux filesystem and still function as expected. There is both a
    kernel and userspace client to mount the filesystem on to a running Linux system.
    The kernel client, although normally faster, tends to lag behind the userspace
    client in terms of supported features and will often require you to be running
    the latest kernel to take advantage of certain features and bug fixes. A CephFS
    filesystem can also be exported via NFS or Samba to non-Linux-based clients, both
    software have direct support for talking to CephFS. This subject will be covered
    in more detail in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: CephFS stores each file as one or more RADOS objects. If an object is larger
    than 4 MB, it will be striped across multiple objects. This striping behavior
    can be controlled by the use of XATTRs, which can be associated with both files
    and directories, and can control the object size, stripe width, and stripe count.
    The default striping policy effectively concatenates multiple 4 MB objects together,
    but by modifying the stripe count and width, a RAID 0 style striping can be achieved.
  prefs: []
  type: TYPE_NORMAL
- en: MDSes and their states
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CephFS requires an additional component to coordinate client access and metadata;
    this component is called the **Metadata** **Server**, or **MDS** for short. Although
    the MDS is used to serve metadata requests to and from the client, the actual
    data read and written still goes directly via the OSDs. This approach minimizes
    the impact of the MDS on the filesystem's performance for more bulk data transfers,
    although smaller I/O-intensive operations can start to be limited by the MDS performance.
    The MDS currently runs as a single-threaded process and so it is recommended that
    the MDS is run on hardware with the highest-clocked CPU as possible.
  prefs: []
  type: TYPE_NORMAL
- en: The MDS has a local cache for storing hot portions of the CephFS metadata to
    reduce the amount of I/O going to the metadata pool; this cache is stored in local
    memory for performance and can be controlled by adjusting the MDS cache memory-limit
    configurationoption, which defaults to 1 GB.
  prefs: []
  type: TYPE_NORMAL
- en: CephFS utilizes a journal stored in RADOS mainly for consistency reasons. The
    journal stores the stream of metadata updates from clients and then flushes them
    into the CephFS metadata store. If an MDS is terminated, the MDS that takes over
    the active role can then replay these metadata events stored in the journal. This
    process of replaying the journal is an essential part of the MDS becoming active
    and therefore will block until the process is completed. The process can be sped
    up by having a standby-replay MDS that is constantly replaying the journal that
    is ready to take over the primary active role in a much shorter amount of time.
    If you have multiple active MDSes, whereas a pure standby MDS can be a standby
    for any active MDS, standby-replay MDSes have to be assigned to a specific MDS
    rank.
  prefs: []
  type: TYPE_NORMAL
- en: 'As well as the active and replaying states, an MDS can also be in several other
    states; the ones you are likely to see in the ceph status are listed for reference
    for when operating a Ceph cluster with a CephFS filesystem. The states are split
    into two parts: the part on the left side of the colon shows whether the MDS is
    up or down. The part on the right side of the colon represents the current operational
    state:'
  prefs: []
  type: TYPE_NORMAL
- en: '`up:active`: This is the normal desired state, as long as one MDS is in this
    state, clients can access the CephFS filesystem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`up:standby`: This can be a normal state as long as one MDS is `up:active`.
    In this state, an MDS is online but not playing any active part in the CephFS
    infrastructure. It will come online and replay the CephFS journal in the event
    that the active MDS goes online.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`up:standby_replay`: Like the `up:standby` state, an MDS in this state is available
    to become active in the event of an active MDS going offline. However, a `standby_replay`
    MDS is continuously replaying the journal of MDS it has been configured to follow,
    meaning the failover time is greatly reduced. It should be noted that while a
    standby MDS can replace any active MDS, a `standby_replay` MDS can only replace
    the one it has been configured to follow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`up:replay`: In this state, an MDS has begun taking over the active role and
    is currently replaying the metadata stored in the CephFS journal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`up:reconnect`: If there were active client sessions active when the active
    MDS went online, the recovering MDS will try to re-establish client connections
    in this state until the client timeout is hit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although there are other states an MDS can be in, it is likely that during normal
    operations they will not be seen and so have not been included here. Please consult
    the official Ceph documentation for more details on all available states.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a CephFS filesystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To create a CephFS filesystem, two RADOS pools are required: one to store the
    metadata and another to store the actual data objects. Although technically any
    existing RADOS pools can be used, it''s highly recommended that dedicated pools
    are created. The metadata pool will typically contain only a small percentage
    of data when compared to the data pool and, so the number of PGs required when
    provisioning this pool can typically be set in the 64 - 128 range. The data pool
    should be provisioned much like an RBD pool and the number of PGs calculated to
    match the number of OSDs in the cluster and the share of data that the CephFS
    filesystem will store.'
  prefs: []
  type: TYPE_NORMAL
- en: At least one MDS will also need to be deployed, but it is recommended that,
    for any production deployment, at least two MDSes are deployed with one running
    as a standby or standby-replay.
  prefs: []
  type: TYPE_NORMAL
- en: 'Edit the `/etc/ansible/hosts` file and add the server that will hold the mds
    role. The following example is using the `mon2` VM from the test lab in [Chapter
    2](dd1d6803-6e40-4bfb-8150-b605bcc08d59.xhtml), *Deploying Ceph with Containers*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d106caf0-a351-43bd-9e28-cfe260bbbd5f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now run the Ansible playbook again and it will deploy the `mds`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dac4ad29-1645-4124-b9a0-c124bf6a8fe1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the playbook has finished running, check that the `mds` is up and running;
    this can be viewed via the `ceph-s` output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d112c02-1355-4cb9-8a5e-a9a42fe7b5df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Ansible should have provisioned data pools and metadata pools as part of the
    deployment process; this can be confirmed by running the following command from
    one of the monitor nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/803785b4-7cc2-4033-9b9b-bb9940a5037b.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding screenshot, we can see that pools 6 and 7 have been created
    for CephFS. If the pools have not been created, follow the steps at the start
    of this chapter on how to create RADOS pools. While the data pools may be created
    as erasure-coded pools, the metadata pool must be of the replicated type.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final step in creating a CephFS filesystem is to instruct Ceph to use the
    two created RADOS pools to build the filesystem. However, as in the previous steps,
    the Ansible deployment should have handled this. We can confirm by running the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'It will show the following if the CephFS filesystem has been created and is
    ready for service:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5fd5e8f1-28fd-496a-aace-4296852540ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If the CephFS filesystem was not created, use the following command to create
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the CephFS filesystem is active, it can be mounted to a client and
    used like any other Linux filesystem. When mounting a CephFS fileystem, the `cephx`
    user key needs to be passed via the mount command. This can be retrieved from
    the keyrings stored in the `/etc/ceph/` directory. In the following example, we
    will use the admin keyring; in production scenarios, it is recommended that a
    specific `cephx` user is created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/25b59cfe-f61a-475c-bf3c-64501f059b2d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The hashed key is what is required to mount the CephFS filesystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In this example, only a single monitor was specified; in production settings,
    it is recommended to supply all three monitor address in a comma-separated format
    to ensure failover.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is confirmation that the filesystem is mounted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48e8b038-cf30-4601-a695-ab8df9fca54e.png)'
  prefs: []
  type: TYPE_IMG
- en: How is data stored in CephFS?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand better how CephFS maps a POSIX-compatible filesystem over the
    top of an object store, we can look more closely at how Ceph maps file inodes
    to objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s look at a file called `test`, which is stored on a CephFS filesystem
    mounted under `/mnt/tmp`. The following command uses the familiar Unix `ls` command,
    but with some extra parameters to show more details, including the file inode
    number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cde14e6f-4e3f-4199-99e0-5927e18c382b.png)'
  prefs: []
  type: TYPE_IMG
- en: The output shows that the file is 1 G in size and that the inode number is the
    long number at the far left.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, by listing the objects stored in the CephFS data pool and greping for
    that number, we can find the object responsible for holding the filesystem details
    for that file. Before we can proceed; however, we need to convert the inode number
    that is stored in decimal into hex, as that is how CephFS stores the inode numbers
    as object names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db179865-0b16-46bf-bad7-173f7bb54c00.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can find the object in the pool; note that this may take a long time
    on a CephFS pool with lots of data, as it will be listing every object in the
    background:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/f1fdc352-b0c2-4c43-a853-7bb66f1bac50.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that 256 objects were found. By default, CephFS breaks larger files up
    into 4 MB objects, 256 of which would equal the size of the 1 G file.
  prefs: []
  type: TYPE_NORMAL
- en: The actual objects store the exact same data as the files viewable in the CephFS
    filesystem. If a text file is saved on a CephFS filesystem, its contents could
    be read by matching the underlying object to the inode number and using the `rados`
    command to download the object.
  prefs: []
  type: TYPE_NORMAL
- en: The `cephfs_metadata` pool stores all the metadata for the files stored on the
    CephFS filesystem; this includes values such as modified time, permissions, file
    names, and file locations in the directory tree. Without this metadata, the data
    objects stored in the data pool are literally just randomly-named objects; the
    data still exists but is fairly meaningless to human operators. The loss of CephFS
    metadata therefore does not lead to actual data loss, but still makes it more-or-less
    unreadable. Therefore, care should be taken to protect metadata pools just like
    any other RADOS pool in your Ceph cluster. There are some advanced recovery steps
    that may assist in metadata loss, which are covered in [Chapter 12](953d93b9-5cc3-4ca2-abea-24b7dc802c37.xhtml),
    *Disaster Recovery*.
  prefs: []
  type: TYPE_NORMAL
- en: File layouts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CephFS allows you to alter the way files are stored across the underlying objects
    by using settings that are known as file layouts. File layouts allow you to control
    the stripe size and width and also which RADOS pool the data objects will reside
    in. The file layouts are stored as extended attributes on files and directories.
    A new file or directory will inherit its parent's file layouts settings; however,
    further changes to a parent directory's layout will not affect existing files.
  prefs: []
  type: TYPE_NORMAL
- en: Adjusting the file striping will normally be done for performance reasons to
    increase the parallelism of reading larger files as a section of data will end
    up being spread across more OSDs. By default, there is no striping and a large
    file stored in CephFS will simply span across multiple objects of 4 MB in size.
  prefs: []
  type: TYPE_NORMAL
- en: File layouts can also be used to alter which data pool the objects for a file
    are stored in. This may be useful to allow different directories to be used for
    hot and cold data, where the hot files may reside on a 3x SSD pool and the cold
    files on an erasure-coded pool backed by spinning disks. A good example of this
    is possibly having a sub directory called `Archive/`, where users can copy files
    that are no longer expected to be in daily use. Any file copied into this directory
    would be stored on the erasure-coded pool.
  prefs: []
  type: TYPE_NORMAL
- en: 'File layouts can be viewed and edited by using the `setfattr` and `getfattr` tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/8fac0a4a-84c6-4797-b871-f5a09db3e044.png)'
  prefs: []
  type: TYPE_IMG
- en: It can be seen that the default file layout is storing the data objects for
    the test file in the `cephfs_data` pool. It can also be seen that the file is
    split into 4 MB objects and, due to the `stripe_unit` also being 4 MB and `stripe_count`
    being equal to 1, that no striping is being used.
  prefs: []
  type: TYPE_NORMAL
- en: Snapshots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CephFS also supports snapshots down to a per-directory level; the snapshot doesn't
    need to include the whole CephFS filesystem. Each directory on a CephFS filesystem
    contains a hidden `.snap` directory; when a new sub directory is created inside,
    a snapshot is effectively taken and the view inside this new sub directory will
    represent the state of the original directory at the point when the snapshot was
    taken.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple snapshots can be taken and browsed independently from each other, enabling
    the snapshots to be used as part of a short-term archiving scheme. One such use
    when CephFS is exported via Samba is to use the snapshot functionality to be exposed
    through the Windows Explorer previous versions tab.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, a test file is created, a snapshot taken, and then
    the file is modified. By examining the contents of the live and the file in the
    snapshot, we can see how CephFS snapshots present themselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5bee441-3b39-43e1-add0-e2942ce6c65a.png)'
  prefs: []
  type: TYPE_IMG
- en: Multi-MDS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A new feature of CephFS is the support for multiple active MDSes. Previously,
    it was only recommended to have a single active MDS with one or more standby,
    which for smaller CephFS deployments was more than adequate. However, in larger
    deployments, a single MDS could possibly start to become a limitation, especially
    due to the single-threaded limitation of MDSes. It should be noted that multiple
    active MDSes are purely for increased performance and do not provide any failover
    or high availability themselves; therefore, sufficient standby MDSes should always
    be provisioned.
  prefs: []
  type: TYPE_NORMAL
- en: When multiple active MDSes are present, the CephFS filesystem is split across
    each MDS so that the metadata requests are hopefully not all being handled by
    a single MDS anymore. This splitting process is done at a per-directory level
    and is dynamically adjusted based on the metadata request load. This splitting
    process involves the creation of new CephFS ranks; each rank requires a working
    MDS to allow it to become active.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, three active MDS servers are in use in the Ceph cluster.
    The primary MDS running rank 0 always hosts the CephFS root. The second MDS is
    serving metadata for the vertically-striped pattern directories, as their metadata
    load is significantly high. All other directories are still getting their metadata
    served by the primary MDS as they have little-to-no activity, with the exception
    of the directory containing Cat Gifs; this directory experiences an extremely
    high metadata request load and so has a separate rank and MDS assigned all to
    itself, as shown by the horizontal pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18fb1e0f-6860-43c1-99eb-109cd0c5d358.png)'
  prefs: []
  type: TYPE_IMG
- en: RGW
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **RADOS Gateway** (**RGW**) presents the Ceph native object store via a
    S3 or swift-compatible interface, which are the two most popular object APIs for
    accessing object storage, with S3 being the dominant one, mainly due to the success
    of Amazon's AWS S3\. This section of the book will primarily focus on S3.
  prefs: []
  type: TYPE_NORMAL
- en: RGW has recently been renamed to Ceph Object Gateway although both the previous
    names are still widely used.
  prefs: []
  type: TYPE_NORMAL
- en: The radosgw component of Ceph is responsible for turning S3 and swift API requests
    into RADOS requests. Although it can be installed alongside other components,
    for performance reasons, it's recommended to be installed on a separate server.
    The radosgw components are completely stateless and so lend themselves well to
    being placed behind a load balancer to allow for horizontal scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from storing user data, the RGW also requires a number of additional RADOS
    pools to store additional metadata. With the exception of the index pool, most
    of these pools are very lightly utilized and so can be created with a small amount
    of PGs, around 64 is normally sufficient. The index pools helps with the listing
    of bucket contents and so placing the index pool on SSDs is highly recommended.
    The data pool can reside on either spinning disks or SSDs, depending on the type
    of objects being stored, although object storage tends to be a fairly good match
    for spinning disks. Quite often, clients are remote and the latency of WAN connections
    offsets a lot of the gains to be had from SSDs. It should be noted that only the
    data pool should be placed on erasure-coded pools.
  prefs: []
  type: TYPE_NORMAL
- en: Handily, RGW will create the required pools the first time it tries to access
    them, reducing the complexity of installation somewhat. However, pools are created
    with their default settings, and it may be that you wish to create an erasure-coded
    pool for data-object storage. As long as no access has been made to the RGW service,
    the data pool should not exist after creation, and it can therefore be manually
    created as an erasure pool. As long as the name matches the intended pool name
    for the RGW zone, RGW will use this pool on first access, instead of trying to
    create a new one.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying RGW
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use the Ansible lab deployed in [Chapter 2](dd1d6803-6e40-4bfb-8150-b605bcc08d59.xhtml), *Deploying
    Ceph with Containers*, to deploy a RGW.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, edit the `/etc/ansible/hosts` file and add the `rgws` role to the `mon3`
    VM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9cddfdbb-6e1d-4cce-a20a-653d11e99d4c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We also need to update the `/etc/ansible/group_vars/ceph` file to add the `radosgw_address`
    variable; it will be set to `[::]`, which means bind to all IPv4 and IPv6 interfaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd34c35f-5784-47c3-bee1-16359030f8db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now run the Ansible playbook again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'After running, you should see it has successfully deployed the `RGW` component:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19f2f834-c7ed-42e8-aed6-87bb5aff3ae6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Viewing the Ceph status from a monitor node, we can check that the `RGW` service
    has registered with the Ceph cluster and is operational:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ec09db0-549e-4fa2-a135-d552b668b8ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that the RGW is active, a user account is required to interact with the
    S3 API, and this can be created using the `radosgw-admin` tool shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/66873c73-6500-431a-a675-ee5b4f6055bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Note the output from the command, particularly the `access_key` and `secret_key`,
    these are used with S3 clients to authenticate with the RGW.
  prefs: []
  type: TYPE_NORMAL
- en: 'To upload objects to our S3-capable Ceph cluster, we first need to create an
    S3 bucket. We will use the `s3cmd` tool to do this, which is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/33d77ab5-bdbb-4ba0-beac-ba6696e241ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that `s3cmd` is installed, it needs to be configured to point at our RGW
    server; it has a built-in configuration tool that can be used to generate the
    initial configuration. During the configuration wizard, it will prompt for the
    access key and secret that was generated when the user account was created, which
    is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/ce6a0468-5a10-4339-babd-f3be7af994ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The generated configuration will be pointing to Amazon''s S3 service; the generated
    configuration file needs to be edited and a few options modified. Edit the `.s3cfg`
    file in your Linux user''s home directory and make the following changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Comment out the `bucket_location` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a99c45d8-cdd4-4811-85ce-a2437a31c6f3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Change the `host_base` and `host_buckets` to match the address of the RGW:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99bf9dae-fdd0-4fe1-9150-a29b313452de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Save the file and quit back to the shell; `s3cmd` can now be used to manipulate
    your `s3` storage. The following example will create a `test` bucket where objects
    can be uploaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c6579b14-567d-4e53-87e8-0bb8eb7f6485.png)'
  prefs: []
  type: TYPE_IMG
- en: You now have a fully functional S3-compatible storage platform ready to explore
    the world of object storage.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about the differences between replicated and erasure-coded
    pools, and their strengths and weaknesses. Armed with this information, you should
    now be capable of making the best decision when it comes to deciding between replicated
    and erasure pools. You also have a more in-depth understanding of how erasure-coded
    pools function, which will aid planning and operations.
  prefs: []
  type: TYPE_NORMAL
- en: You should now feel confident in deploying Ceph clusters to provide block, file,
    and object storage, and be able to demonstrate regular administrative tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about librados and how to use it to make
    custom applications that talk directly to Ceph.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Name two different erasure-coding techniques.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the process called when an erasure-coded pool does a partial write to
    an object?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why might you choose an erasure-coded profile with two parity shards?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the process called to turn a cloned snapshot into a full-fat RBD image?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What Ceph daemon is required to run a CephFS filesystem?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why might you choose to run multiple active metadata servers over a single one
    in a CephFS filesystem?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What Ceph daemon is required to run a RGW?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What two APIs is Ceph's RGW capable of supporting?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
