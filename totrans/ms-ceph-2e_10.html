<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Monitoring Ceph</h1>
                </header>
            
            <article>
                
<p>When you are operating a Ceph cluster, it's important to monitor its health and performance. By monitoring Ceph, you can be sure that your cluster is running at full health and also be able to quickly react to any issues that may arise. By capturing and graphing performance counters, you will also have the data that's required to tune Ceph and observe the impact of your tuning on your cluster.</p>
<p>In this chapter, you will learn about the following topics:</p>
<ul>
<li>Why it is important to monitor Ceph</li>
<li>How to monitor Ceph's health by using the new built-in dashboard</li>
<li>What should be monitored</li>
<li>The states of PGs and what they mean</li>
<li>How to capture Ceph's performance counters with collectd</li>
<li>Example graphs using Graphite</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why it is important to monitor Ceph</h1>
                </header>
            
            <article>
                
<p>The most important reason to monitor Ceph is to ensure that the cluster is running in a healthy state. If Ceph is not running in a healthy state, be it because of a failed disk or for some other reason, the chances of a loss of service or data increase. Although Ceph is highly automated in recovering from a variety of scenarios, being aware of what is going on and when manual intervention is required is essential.</p>
<p>Monitoring isn't just about detecting failures; monitoring other metrics such as used disk space is just as essential as knowing when a disk has failed. If your Ceph cluster fills up, it will stop accepting I/O requests and will not be able to recover from future OSD failures.</p>
<p>Finally, monitoring both the operating systems and Ceph's performance metrics can help you spot performance issues or identify tuning opportunities.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What should be monitored</h1>
                </header>
            
            <article>
                
<p>The simple answer is everything, or, as much as you can. You can never predict what scenario may be forced upon you and your cluster, and having the correct monitoring and alerting in place can mean the difference between handing a situation gracefully or having a full-scale outage. A list of things that should be monitored in decreasing order of importance is as follows.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ceph health</h1>
                </header>
            
            <article>
                
<p>The most important thing to capture is the health status of Ceph. The main reporting item is the overall health status of the cluster, either <kbd>HEALTH_OK</kbd>, <kbd>HEALTH_WARN</kbd>, or <kbd>HEALTH_ERR</kbd>. By monitoring this state, you will be alerted any time Ceph itself thinks that something is not right. In addition to this, you may also want to capture the status of the PGs and number of degraded objects, as they can provide additional information as to what might be wrong without having to actually log on to a Ceph server and use the Ceph toolset to check the status.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Operating system and hardware</h1>
                </header>
            
            <article>
                
<p>It's also highly recommended that you capture the current status of the operating system running the Ceph software and also the status of the underlying hardware. Capturing things such as CPU and RAM usage will alert you to possible resource starvation before it potentially becomes critical. Also, long-term trending on this data can help to plan hardware choices for Ceph. Monitoring the hardware to capture hardware failures, such as disks, PSUs, and fans, is also highly recommended. Most server hardware is redundant and it may not be obvious that it is running in a degraded state <span>unless it is monitored</span>. In addition, monitoring network connections so that you can be sure that both NICs are available in bonded configuration are working is also a good idea.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Smart stats</h1>
                </header>
            
            <article>
                
<p>Using your operating system's smart monitoring tool suite to probe the health of the disks is also a good idea. They may help to highlight failing disks or ones with abnormal error rates. For SSDs, you can also measure the wear rate of the flash cells, which is a good indication of when the SSD is likely to fail. Finally, being able to capture the temperature of the disks will allow you to make sure that your servers are not overheating.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Network</h1>
                </header>
            
            <article>
                
<p>As Ceph relies on the network it runs over to be reliable, it can be beneficial to monitor network devices for errors and performance issues. Most network devices can be polled via SNMP to obtain this data. Since the Mimic release of Ceph, it automatically sends jumbo-sized frames in its heartbeats to try and catch scenarios where jumbo frames are not correctly configured across the network. However, it is also worth considering deploying your own jumbo-frame-checking monitoring to catch misconfigurations, as misconfigured jumbo frames can easily bring a Ceph cluster to its knees.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance counters</h1>
                </header>
            
            <article>
                
<p>By monitoring performance counters from both the operating system and Ceph, you are arming yourself with a wealth of knowledge to gain a better understanding of how your Ceph cluster is performing. If storage permits, it's worth trying to capture as many of these metrics as possible; you never know when the metrics will come in handy. It's quite often the case when diagnosing a problem that a metric that was previously thought to have no connection to the issue suddenly sheds light on the actual cause. The traditional approach of only monitoring key metrics is very limiting in this regard.</p>
<p>Most monitoring agents that run on Linux will allow you to capture a large array of metrics, from resource consumption to filesystem usage. It's worth spending time analyzing what metric you can collect and configuring them appropriately. Some of these monitoring agents will also have plugins for Ceph, which can pull out all of the performance counters from Ceph's various components, such as <kbd>osd</kbd> and <kbd>mon</kbd> nodes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Ceph dashboard</h1>
                </header>
            
            <article>
                
<p>Introduced in the Mimic release, Ceph now has an extremely useful dashboard based on the open ATTIC project. The dashboard in the initial Mimic release gives the Ceph operator the ability to monitor many aspects of their Ceph cluster that are needed on a daily basis. With preceding releases of Ceph, the dashboard has had further refinements and can now be used to manage some common tasks; over time, it is expected that the dashboard will continue to gain new features.</p>
<p>The dashboard is provided as a Ceph Mgr module and is included along with any dependencies in the standard Ceph installation. That means that all that is required to start using the Ceph dashboard is to simply enable the <kbd>mgr</kbd> module:</p>
<pre><strong>sudo ceph mgr module enable dashboard</strong></pre>
<p>SSL either needs to be disabled or a SSL certificate needs to be configured. Luckily, Ceph has a simple one-liner to get you started with a self-signed certificate:</p>
<pre><strong>sudo ceph dashboard create-self-signed-cert</strong></pre>
<p>It's recommended that you use a proper certificate in production deployments.</p>
<p>Lastly, a username and password are required to log in to the dashboard. Again, Ceph has a simple command to carry out this action:</p>
<pre><strong>sudo ceph dashboard set-login-credentials &lt;user&gt; &lt;password&gt;</strong></pre>
<p>Now you should be able to browse to <kbd>https://&lt;active mgr&gt;:8443</kbd> and log in with the credentials you have just created. I<span>n this is case,</span> <kbd>&lt;active mgr&gt;</kbd> is the Ceph node that is currently running the active mgr daemon; this can be seen via the <kbd>ceph -s</kbd> <span>Ceph status screen:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b5517428-b827-4ed0-9bc3-f736940dd212.png" style="width:61.67em;height:40.08em;"/></p>
<p>The first screen that's presented when you log into the dashboard gives an overview of the Ceph cluster's health and utilization.</p>
<p>Across the top of the page are a number of menus that allow you to view more detailed information about the Ceph cluster, such as details on the OSDs and PGs. The block menu allows you to view details on created RBD images and likewise the filesystems menu shows information about any CephFS filesystems.</p>
<p>The Object Gateway will show information about the RADOS Gateway, but requires configuration with a valid RGW user; please consult the official Ceph documentation for more information if required.</p>
<p>With the development future of the Ceph dashboard looking bright, it is highly recommended to deploy it for any Ceph cluster you manage. With future releases set to bring further enhancements around being able to manage your Ceph cluster from it, the dashboard will certainly become more and more useful over time. However, even in its current state, the ability for less knowledgeable administrators to easily be able to explore the current status of a running Ceph cluster is extremely useful.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">PG states – the good, the bad, and the ugly</h1>
                </header>
            
            <article>
                
<p>Each placement group in Ceph has one or more statuses assigned to it; normally, you want to see all your PGs with <kbd>active+clean</kbd> against them. Understanding what each state means can help us identify what is happening to PG and whether you need to take action.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The good ones</h1>
                </header>
            
            <article>
                
<p>The following states are indications of a healthy operating cluster, for which no action needs to be taken.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The active state</h1>
                </header>
            
            <article>
                
<p>The <kbd>active</kbd> state means that the PG is in full health, and it is capable of accepting client requests.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The clean state</h1>
                </header>
            
            <article>
                
<p>The <kbd>clean</kbd> state means that the PG's objects are being replicated the correct number of times and are all in a consistent state.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scrubbing and deep scrubbing</h1>
                </header>
            
            <article>
                
<p>Scrubbing means that Ceph checks the consistency of your data and is a normal background process. Scrubbing on its own is where Ceph checks that the objects and relevant metadata exists. When Ceph performs a deep scrub, it compares the contents of the objects and their replicas for consistency.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The bad ones</h1>
                </header>
            
            <article>
                
<p>The following states indicate that Ceph is not in full health, but shouldn't cause any immediate problems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The inconsistent state</h1>
                </header>
            
            <article>
                
<p>The <kbd>inconsistent</kbd> state means that during the scrub process, Ceph has found one or more objects that are inconsistent with its replicas. See <a href="bf50389d-8256-4cbf-ae32-01a494ab4189.xhtml">Chapter 11</a>, <em>Troubleshooting</em>, later in this book on how to deal with these errors.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The backfilling, backfill_wait, recovering, and recovery_wait states</h1>
                </header>
            
            <article>
                
<p>These states mean that Ceph is copying or migrating data from one OSD to another. This may possibly mean that this PG has less than the desired number of copies. If it's in the <kbd>wait</kbd> state, it means that due to throttles on each OSD, Ceph is limiting the number of concurrent operations to reduce the impact on client operations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The degraded state</h1>
                </header>
            
            <article>
                
<p>The <kbd>degraded</kbd> state means that the PG is missing or has out-of-date copies of one or more objects. These will normally be corrected by the recovery/backfill process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Remapped</h1>
                </header>
            
            <article>
                
<p>In order to become active, the PG is currently mapped to a different OSD or set of OSDs. This is likely to occur when OSD is down but has not been recovered to the remaining OSDs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Peering</h1>
                </header>
            
            <article>
                
<p>The peering state is part of the normal process of a PG becoming active and it should only be in this state briefly. It is listed in the bad section as a PG that remains in the peering state will block I/O.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The ugly ones</h1>
                </header>
            
            <article>
                
<p>These states are not ones you want to see. If you see any of these states, it's quite likely that client access to the cluster will be affected, and unless the situation can be fixed, data loss may occur.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The incomplete state</h1>
                </header>
            
            <article>
                
<p>An <kbd>incomplete</kbd> state means that Ceph is unable to find any valid copies of objects within the PG across any of the OSDs that are currently up in the cluster. This can either be that the objects are simply not there or the available objects are missing newer writes that may have occurred on now unavailable OSDs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The down state</h1>
                </header>
            
            <article>
                
<p>This will accompany the <kbd>incomplete</kbd> state. The PG is missing objects that are known to possibly be on unavailable OSDs and the PG cannot be started.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The backfill_toofull and recovery_toofull state</h1>
                </header>
            
            <article>
                
<p>Ceph has tried to recover your data, but your OSD disks are too full and it cannot continue. Extra OSDs are needed to fix this situation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monitoring Ceph with collectd</h1>
                </header>
            
            <article>
                
<p>Previously in this chapter, we covered what monitoring should be done around your entire Ceph infrastructure and also looked at the new, builtin Ceph dashboard. To gain further insights into the operation of your Ceph cluster and associated infrastructure, a more detailed monitoring setup is required. Although alert monitoring is out of scope of this book, we will now look at capturing the Ceph performance metrics with collectd, storing them in Graphite, and then finally creating a dashboard with graphs using Grafana. These captured metrics can then be used in the following chapter to help tune your Ceph cluster.</p>
<p>We will build this monitoring infrastructure on one of our monitor nodes in our test cluster. In a production cluster, it is highly recommended that it gets its own dedicated server.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Graphite</h1>
                </header>
            
            <article>
                
<p>Graphite is a time series database that excels in storing large amounts of metrics and has a mature query language, which can be used by applications to manipulate data.</p>
<p>We first need to install the required Graphite packages:</p>
<pre><strong>sudo apt-get install graphite-api graphite-carbon graphite-web</strong></pre>
<p>The preceding command gives the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-204 image-border" src="assets/488cb2df-39ad-4069-9510-6cc07aef46b6.png" style="width:63.17em;height:15.00em;"/></div>
<p>Edit the <kbd>/etc/graphite/storage-schemas.conf</kbd> <span>storage schemas file </span>and place the following into it:</p>
<pre>[carbon]<br/>pattern = ^carbon\.<br/>retentions = 60:90d<br/>[default_1min_for_1day]<br/>pattern = .*<br/>retentions = 60s:1d</pre>
<p>Now, we can create the graphite database by running the following command:</p>
<pre><strong>sudo graphite-manage syncdb</strong></pre>
<p>The preceding command will give the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-205 image-border" src="assets/dc018797-5e89-447f-bbbc-17b1d8a825ec.png" style="width:61.75em;height:6.83em;"/></div>
<div class="CDPAlignLeft CDPAlign">
<p>Set the password for the root user when prompted:</p>
</div>
<pre class="CDPAlignLeft CDPAlign"><strong>sudo apt-get install apache2 libapache2-mod-wsgi</strong></pre>
<p class="CDPAlignLeft CDPAlign"><span>The preceding command gives the following output:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-206 image-border" src="assets/d98b277c-89aa-43f5-a02f-be4fb9f33818.png" style="width:57.42em;height:13.42em;"/></div>
<p class="CDPAlignLeft CDPAlign">To stop the default Apache site from conflicting with the Graphite web service, we need to disable it by running the following command:</p>
<pre class="CDPAlignLeft CDPAlign"><strong>sudo a2dissite 000-default<br/></strong></pre>
<p class="CDPAlignLeft CDPAlign">The preceding command gives the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-207 image-border" src="assets/6158b809-8725-4c61-92a8-4ea5951a75e8.png" style="width:29.50em;height:6.00em;"/></div>
<p>We can now copy the Apache Graphite configuration into the Apache environment:</p>
<pre><strong>sudo cp /usr/share/graphite-web/apache2-graphite.conf<br/>/etc/apache2/sites-available<br/></strong><strong>sudo a2ensite apache2-graphite</strong></pre>
<p><span>The preceding commands give the following output:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-208 image-border" src="assets/b67de3db-4876-4bdb-ad1b-1001c51266f3.png" style="width:32.42em;height:6.42em;"/></div>
<p>Restart the Apache service:</p>
<pre><strong>sudo service apache2 reload</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Grafana</h1>
                </header>
            
            <article>
                
<p>We will edit the apt repository file and add the repository for Grafana:</p>
<pre><strong>sudo nano /etc/apt/sources.list.d/grafana.list</strong></pre>
<p>Place the following line into the file and save it:</p>
<pre><strong>deb https://packagecloud.io/grafana/stable/debian/ jessie main</strong></pre>
<p>Now run the following commands to retrieve the <kbd>gpg</kbd> key and update the package lists:</p>
<pre><strong>curl https://packagecloud.io/gpg.key | sudo apt-key add –</strong><br/><strong>sudo apt-get update</strong></pre>
<p>Install Grafana using the following command:</p>
<pre><strong>sudo apt-get install grafana</strong></pre>
<p>The preceding command gives the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-209 image-border" src="assets/1990df5e-78f0-4130-97d8-39dcc0c54b9e.png" style="width:61.75em;height:19.00em;"/></div>
<p>With the standard Vagrant configuration, you will not be able to connect to the HTTP port provided by Grafana. To access Grafana, we will need to port forward via <kbd>ssh port 3000</kbd> to our local machine.</p>
<p>An example of using PuTTY is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-210 image-border" src="assets/31fc51b4-0eab-477e-a122-028cf70d2aac.png" style="width:29.92em;height:29.50em;"/></div>
<p>Now, use <kbd>http://localhost:3000</kbd> in the URL. You should be taken to the Grafana home page. Navigate to data sources and then configure Grafana to poll our newly installed Graphite installation:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-211 image-border" src="assets/15f24153-3f71-479e-9096-d04d90cce146.png" style="width:31.17em;height:37.83em;"/></div>
<p class="CDPAlignLeft CDPAlign">If you get the green success bar when you click on the <span class="packt_screen">Save &amp; Test</span> button, then you have successfully installed and configured Graphite and Grafana.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">collectd</h1>
                </header>
            
            <article>
                
<p>Now that we have a shiny installation of Graphite and Grafana to look at, we need to put some data into it to be able to generate some graphs. collectd is a well-respected metric collection tool, which can output metrics to Graphite. The core collectd application is very minimal, and it relies on a series of plugins to collect metrics and forwards them onto applications such as Graphite for storage.</p>
<p>Before we start collecting metrics from our Ceph nodes, let's install collectd on the same VM where we installed Graphite and Grafana. We will do this to gain a better understanding of collectd and the process required to configure it. We will then use Ansible to install and configure collectd on all of our Ceph nodes, which would be the recommended approach if this was being rolled out in a production environment. We have the following code:</p>
<pre><strong>sudo apt-get install collectd-core</strong></pre>
<p>The preceding command will give the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-212 image-border" src="assets/0f642770-d575-41e3-ad62-3e4df0a4042e.png" style="width:57.25em;height:21.42em;"/></div>
<p>This will install collectd and a basic set of plugins for querying standard operating system resources. There is a sample configuration stored in the following location:</p>
<pre class="CDPAlignLeft CDPAlign"><strong>/usr/share/doc/collectd-core/examples/collectd.conf</strong></pre>
<p>It lists all of the core plugins and sample configuration options. It is worth reviewing this file to learn about the various plugins and their configuration options. For this example, however, we will start with an empty configuration file and configure a few basic resources:</p>
<ol>
<li>Create a new <kbd>collectd</kbd> configuration file using the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo nano /etc/collectd/collectd.conf</strong></pre>
<ol start="2">
<li>Add the following to it:</li>
</ol>
<pre><strong>       </strong>Hostname "ansible"<br/><br/>       LoadPlugin cpu<br/>       LoadPlugin df<br/>       LoadPlugin load<br/>       LoadPlugin memory<br/>       LoadPlugin write_graphite<br/><br/>       &lt;Plugin write_graphite&gt;<br/>          &lt;Node "graphing"&gt;<br/>              Host "localhost"<br/>              Port "2003"<br/>              Protocol "tcp"<br/>              LogSendErrors true<br/>              Prefix "collectd."<br/>              StoreRates true<br/>              AlwaysAppendDS false<br/>              EscapeCharacter "_"<br/>          &lt;/Node&gt;<br/>       &lt;/Plugin&gt;<br/><br/>       &lt;Plugin "df"&gt;<br/>         FSType "ext4"<br/>      &lt;/Plugin&gt;</pre>
<ol start="3">
<li>Restart the <kbd>collectd</kbd> service using the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo service collectd restart</strong></pre>
<ol start="4">
<li>Now, navigate back to Grafana and browse the dashboard's menu item. Click on the button in the middle of the screen to create a new dashboard:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-213 image-border" src="assets/9bc0bcbc-9a1b-4fe0-8c99-02258224a4e9.png" style="width:17.17em;height:10.67em;"/></div>
<ol start="5">
<li>Select <span class="packt_screen">Graph</span> to add a new graph to the dashboard. An example graph will now appear, which we will want to edit to replace with our own graphs. To do this, click on the graph title and a floating menu will appear:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-214 image-border" src="assets/aaee7386-fe72-40ab-9912-810b7e05ae6d.png" style="width:14.50em;height:7.08em;"/></div>
<ol start="6">
<li>Click on <span class="packt_screen">Edit</span> to go to the graph widget editing screen. From here, we can delete the fake graph data by selecting the <em>dustbin</em> icon, as shown in the following three-button menu box:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-215 image-border" src="assets/d0e5a750-326a-4385-b305-ac92ede696af.png" style="width:8.83em;height:4.83em;"/></div>
<ol start="7">
<li>Now, from the drop-down menu, change the panel data source to the Graphite source we have just added and click on the <kbd>Add query</kbd> button:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-216 image-border" src="assets/3b546ef4-57e5-4469-b7b1-d4bb86043723.png" style="width:24.58em;height:3.42em;"/></div>
<ol start="8">
<li>A query box will appear at the top of the editing panel. It will also have the three-button menu box, like before. From here, we can toggle the edit mode of the query editor by clicking on the button with the three horizontal lines:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-220 image-border" src="assets/6813b9b8-0701-4b84-a9ae-b5c86c0063a6.png" style="width:13.25em;height:8.75em;"/></div>
<p class="CDPAlignLeft CDPAlign">The <span class="packt_screen">Toggle Edit Mode</span> option switches the query editor between click and select mode, where you can explore the available metrics and build up basic queries and the text editor mode. The click and select mode is useful if you do not know the names of the metrics and only want to create basic queries. For more advanced queries, the text editor is required.</p>
<p class="CDPAlignLeft CDPAlign">We will first make a query for our graph using the basic editor mode and then switch to the text mode for the rest of this chapter to make it easier to copy the queries from this book.</p>
<p class="CDPAlignLeft CDPAlign">Let's first graph the system load of VM where we have installed <span class="packt_screen">collectd</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-221 image-border" src="assets/4caf3d8a-b599-45c4-8617-026be2452002.png" style="width:36.83em;height:4.75em;"/></div>
<p>This will now produce a graph, showing the system load.</p>
<p>By further clicking on the <span class="packt_screen">+</span> symbol, you can expand the query by applying different functions against the data. These could be used to add multiple data sources together or to find the average. We will cover this further in this chapter as we begin to craft some queries to analyze Ceph performance. Before we continue, let's switch the query editor mode to text mode to see what the query looks like:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-224 image-border" src="assets/b4d18c67-6af8-408f-aff4-77ecd678509c.png" style="width:22.67em;height:3.75em;"/></div>
<p>You can see that each leaf of the tree of metrics is separated by a dot. This is how the Graphite query language works.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying collectd with Ansible</h1>
                </header>
            
            <article>
                
<p>Now that we have confirmed that our monitoring stack is installed and working correctly, let's use Ansible to deploy collectd to all our Ceph nodes, so we can start monitoring it.</p>
<p>Switch to the <kbd>ansible</kbd> directory:</p>
<pre style="padding-left: 30px"><strong>cd /etc/ansible/roles</strong><br/><strong>git clone https://github.com/fiskn/Stouts.collectd</strong></pre>
<p>Edit your Ansible <kbd>site.yml</kbd> file and add the <kbd>collectd</kbd> role to the plays for your <kbd>mon</kbd> and <kbd>osd</kbd> nodes so that they look like the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-225 image-border" src="assets/7d5cd9a1-44e9-42fb-a7f3-b4a5d6323845.png" style="width:17.92em;height:8.83em;"/></div>
<p>Edit <kbd>group_vars/all</kbd> and enter the following:</p>
<p><img class="aligncenter size-full wp-image-226 image-border" src="assets/62a74bf3-b1d3-49c0-bec3-23374be862f0.png" style="width:56.00em;height:21.25em;"/></p>
<p>Now, run your <kbd>site.yml</kbd> playbook:</p>
<pre class="CDPAlignLeft CDPAlign"><strong>ansible-playbook -K site.yml</strong></pre>
<p class="CDPAlignLeft CDPAlign">The preceding command gives the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-228 image-border" src="assets/919339d4-9a37-48bc-abcb-af659d1f22ea.png" style="width:57.50em;height:15.42em;"/></div>
<p class="CDPAlignLeft CDPAlign">You should see from the status at the end that Ansible has deployed <kbd>collectd</kbd> to all your Ceph nodes, and it has configured the <kbd>collectd</kbd> Ceph plugin. In Grafana, you should now be able to see your Ceph nodes showing up as available metrics. The following is one of our monitor nodes:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-229 image-border" src="assets/798fee5c-ad98-4313-b060-90d786c8da5a.png" style="width:20.50em;height:3.50em;"/></div>
<p class="CDPAlignLeft CDPAlign">For example, we can now create a graph showing the number objects stored in the Ceph cluster. Create a new graph in Grafana and enter the following query:</p>
<pre class="CDPAlignLeft CDPAlign"><strong>collectd.mon1.ceph.mon.mon1.ceph_bytes.Cluster.numObject</strong></pre>
<p class="CDPAlignLeft CDPAlign">This will produce a graph like the following one:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-230 image-border" src="assets/df871fb2-0d6f-44a9-a95a-c531e2e838a7.png" style="width:58.50em;height:11.83em;"/></div>
<p class="mce-root"/>
<p>It's advised that you spend some time browsing through the available metrics so that you are familiar with them before proceeding to the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sample Graphite queries for Ceph</h1>
                </header>
            
            <article>
                
<p>Although you can generate some very useful graphs by simply selecting individual metrics, by harnessing the power of Graphite's functions to manipulate the metrics, graphs can be created, which offer a much more detailed insight into your Ceph cluster. The following Graphite queries are useful for generating common graphs and are also a good starting point so that you can create your own custom queries.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Number of Up and In OSDs</h1>
                </header>
            
            <article>
                
<p>It's very handy to be able to quickly glance at a dashboard and see how many OSDs are <kbd>Up</kbd> and <kbd>In</kbd>. The following two queries show these values:</p>
<pre><strong>maxSeries(collectd.mon*.ceph.mon.mon*.ceph_bytes.Cluster.numOsdIn)<br/><br/></strong><strong>maxSeries(collectd.mon*.ceph.mon.mon*.ceph_bytes.Cluster.numOsdUp)</strong></pre>
<p>Note the use of the <kbd>maxSeries</kbd> function, which allows data to be pulled from all the <kbd>mon</kbd> nodes and will take the highest value.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Showing the most deviant OSD usage</h1>
                </header>
            
            <article>
                
<p>Due to the way CRUSH places PGs on each OSD, there will never be a perfect balance of PGs per OSD. The following query will create a graph that will show the ten most deviant OSDs, so you can see if PG balancing would be beneficial. We have the following code:</p>
<pre style="padding-left: 60px"><strong>mostDeviant(10,collectd.osd*.df.var-lib-ceph-osd-ceph-   *.df_complex.used)</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Total number of IOPs across all OSDs</h1>
                </header>
            
            <article>
                
<p>This uses the <kbd>sumSeries</kbd> function and wildcards to add all the <kbd>op</kbd> metrics from every OSD together:</p>
<pre><strong>sumSeries(collectd.osd*.ceph.osd.*.ceph_rate.Osd.op)</strong></pre>
<p>There are also counters that will show read and write operations individually, named <kbd>opR</kbd> and <kbd>opW</kbd>, respectively.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Total MBps across all OSDs</h1>
                </header>
            
            <article>
                
<p>Similarly, there are also counters that show MBps for each OSD, such as the <kbd>op</kbd> counters; the <kbd>sumSeries</kbd> function can also be used. We have the following code:</p>
<pre><strong>sumSeries(collectd.osd*.ceph.osd.*.ceph_rate.Osd.{opInBytes,opOutBytes})</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cluster capacity and usage</h1>
                </header>
            
            <article>
                
<p>The following two queries show the total capacity of bytes in the cluster and the number of bytes used. They can be used to generate a pie chart in Grafana to show the percentage of used space. Note that these counters show the raw capacity before replication:</p>
<pre><strong>maxSeries(collectd.mon*.ceph.mon.mon*.ceph_bytes.Cluster.osdBytes)<br/><br/></strong><strong>maxSeries(collectd.mon*.ceph.mon.mon*.ceph_bytes.Cluster.osdBytesUsed)</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Average latency</h1>
                </header>
            
            <article>
                
<p>The following two queries can be used to graph the average latency of the cluster. Larger I/O sizes per operation will increase the average latency, as larger I/Os take longer to process. As such, these graphs will not give a clear picture of your cluster's latency if the average I/O size changes over time. We have the following code:</p>
<pre><strong>averageSeries(collectd.osd*.ceph.osd.*.ceph_latency.Osd.opWLatency)<br/><br/></strong><strong>averageSeries(collectd.osd*.ceph.osd.*.ceph_latency.Osd.opRLatency)</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Custom Ceph collectd plugins</h1>
                </header>
            
            <article>
                
<p>Although the standard collectd Ceph plugin does a good job of collecting all of Ceph's performance counters, it falls short of collecting all the required data to allow you to get a complete view of your cluster health and performance. This section will demonstrate how to use additional custom collectd plugins to collect the PG states, per pool performance stats, and more realistic latency figures:</p>
<ol>
<li>Jump on to one of your <kbd>mon</kbd> nodes via SSH and clone the following Git repository:</li>
</ol>
<pre style="padding-left: 60px"><strong>git clone https://github.com/grinapo/collectd-ceph</strong></pre>
<ol start="2">
<li>Create a <kbd>ceph</kbd> directory under the <kbd>collectd/plugins</kbd> directory:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo mkdir -p /usr/lib/collectd/plugins/ceph</strong></pre>
<ol start="3">
<li>Copy the <kbd>plugins</kbd> directory to <kbd>/usr/lib/collectd/plugins/ceph</kbd> using the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo cp -a collectd-ceph/plugins/*  <br/>/usr/lib/collectd/plugins/ceph/</strong></pre>
<ol start="4">
<li>Now, create a new <kbd>collectd</kbd> configuration file to enable the plugins:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo nano /etc/collectd/collectd.conf.d/ceph2.conf</strong></pre>
<ol start="5">
<li>Place the following configuration inside it and save the new file:</li>
</ol>
<pre style="padding-left: 60px"><strong> </strong>&lt;LoadPlugin "python"&gt;<br/> Globals true<br/> &lt;/LoadPlugin&gt;<br/><br/> &lt;Plugin "python"&gt;<br/> ModulePath "/usr/lib/collectd/plugins/ceph"<br/><br/> Import "ceph_pool_plugin"<br/> Import "ceph_pg_plugin"<br/> Import "ceph_latency_plugin"<br/> <br/> &lt;Module "ceph_pool_plugin"&gt;<br/> Verbose "True"<br/> Cluster "ceph"<br/> Interval "60"<br/> &lt;/Module&gt;<br/> &lt;Module "ceph_pg_plugin"&gt;<br/> Verbose "True"<br/> Cluster "ceph"<br/> Interval "60"<br/> &lt;/Module&gt;<br/> &lt;Module "ceph_latency_plugin"&gt;<br/> Verbose "True"<br/> Cluster "ceph"<br/> Interval "60"<br/> TestPool "rbd"<br/> &lt;/Module&gt;<br/> &lt;/Plugin&gt;</pre>
<p>The latency plugin uses a RADOS bench to determine the cluster latency; this means that it is actually running <span>RADOS</span> bench and will write data to your cluster. The <kbd>TestPool</kbd> parameter determines the target for the <span>RADOS</span> bench command. It is therefore recommended that on a production cluster, a separate small pool is created for this use.</p>
<div class="packt_tip">If you are trying to use these extra plugins on Kraken+ releases of Ceph, you will need to edit the <kbd>ceph_pg_plugin.py</kbd> file and modify the variable name on line 71 from <kbd>fs_perf_stat</kbd> to <kbd>perf_stat</kbd>.</div>
<ol start="6">
<li>Restart the <kbd>collectd</kbd> service:</li>
</ol>
<pre style="padding-left: 60px"><strong>service collectd restart</strong></pre>
<p style="padding-left: 60px">The average cluster latency can now be obtained with the following query:</p>
<pre style="padding-left: 60px"><strong>collectd.mon1.ceph-ceph.cluster.gauge.avg_latency</strong></pre>
<p>This figure is based on doing 64 KB writes, and so, unlike the OSD metrics, it will not change depending on the average client I/O size.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you learned the importance of monitoring your Ceph cluster and its supporting infrastructure. You should also have a good understanding of the various components that you should monitor and some example tools that can be used. We covered some of the PG states that, in conjunction with a monitoring solution will allow you to understand the current status of your Ceph cluster. Finally, we deployed a highly scalable monitoring system comprising collectd, Graphite, and Grafana, which will enable you to create professional looking dashboards to show the status and performance of your Ceph cluster.</p>
<p>In the next chapter, we will look at ways to tune the performance of you Ceph cluster, this leans heavily on being able to capture performance stats, which you should now be able to do following this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What port does the Ceph Dashboard run on?</li>
<li>What Ceph daemon is the Ceph Dashboard controlled by?</li>
<li>What does the inconsistent PG state mean?</li>
<li>What does the backfilling PG state mean?</li>
<li>What should you aim to monitor in your Ceph infrastructure?</li>
</ol>


            </article>

            
        </section>
    </body></html>