- en: Monitoring Ceph
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控 Ceph
- en: When you are operating a Ceph cluster, it's important to monitor its health
    and performance. By monitoring Ceph, you can be sure that your cluster is running
    at full health and also be able to quickly react to any issues that may arise.
    By capturing and graphing performance counters, you will also have the data that's
    required to tune Ceph and observe the impact of your tuning on your cluster.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在操作一个 Ceph 集群时，监控其健康和性能是很重要的。通过监控 Ceph，你可以确保集群处于良好的健康状态，并能够快速应对可能出现的问题。通过捕获并绘制性能计数器图表，你还将获得调整
    Ceph 所需的数据，并观察调整对集群的影响。
- en: 'In this chapter, you will learn about the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习以下主题：
- en: Why it is important to monitor Ceph
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么监控 Ceph 重要
- en: How to monitor Ceph's health by using the new built-in dashboard
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过使用新的内置仪表板监控 Ceph 的健康状态
- en: What should be monitored
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该监控的内容
- en: The states of PGs and what they mean
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PG 的状态及其含义
- en: How to capture Ceph's performance counters with collectd
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 collectd 捕获 Ceph 的性能计数器
- en: Example graphs using Graphite
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Graphite 的示例图表
- en: Why it is important to monitor Ceph
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么监控 Ceph 重要
- en: The most important reason to monitor Ceph is to ensure that the cluster is running
    in a healthy state. If Ceph is not running in a healthy state, be it because of
    a failed disk or for some other reason, the chances of a loss of service or data
    increase. Although Ceph is highly automated in recovering from a variety of scenarios,
    being aware of what is going on and when manual intervention is required is essential.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 监控 Ceph 最重要的原因是确保集群处于健康状态。如果 Ceph 没有运行在健康状态，无论是因为硬盘故障还是其他原因，服务或数据丢失的可能性都会增加。虽然
    Ceph 在从各种场景中恢复方面高度自动化，但了解正在发生的情况以及何时需要手动干预是至关重要的。
- en: Monitoring isn't just about detecting failures; monitoring other metrics such
    as used disk space is just as essential as knowing when a disk has failed. If
    your Ceph cluster fills up, it will stop accepting I/O requests and will not be
    able to recover from future OSD failures.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 监控不仅仅是检测故障；监控其他指标，如已使用的磁盘空间，与了解磁盘故障发生的时间同样重要。如果你的 Ceph 集群存储满了，它将停止接受 I/O 请求，并且无法从未来的
    OSD 故障中恢复。
- en: Finally, monitoring both the operating systems and Ceph's performance metrics
    can help you spot performance issues or identify tuning opportunities.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，监控操作系统和 Ceph 性能指标可以帮助你发现性能问题或识别调整机会。
- en: What should be monitored
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应该监控的内容
- en: The simple answer is everything, or, as much as you can. You can never predict
    what scenario may be forced upon you and your cluster, and having the correct
    monitoring and alerting in place can mean the difference between handing a situation
    gracefully or having a full-scale outage. A list of things that should be monitored
    in decreasing order of importance is as follows.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的答案是：所有内容，或者说，尽可能多地监控。你永远无法预测你的集群可能面临什么样的情况，设置正确的监控和告警系统可能意味着优雅地应对情况与发生大规模宕机之间的差异。应该监控的事项按重要性递减的顺序如下：
- en: Ceph health
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ceph 健康状态
- en: The most important thing to capture is the health status of Ceph. The main reporting
    item is the overall health status of the cluster, either `HEALTH_OK`, `HEALTH_WARN`,
    or `HEALTH_ERR`. By monitoring this state, you will be alerted any time Ceph itself
    thinks that something is not right. In addition to this, you may also want to
    capture the status of the PGs and number of degraded objects, as they can provide
    additional information as to what might be wrong without having to actually log
    on to a Ceph server and use the Ceph toolset to check the status.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是捕获 Ceph 的健康状态。主要的报告项是集群的整体健康状态，可能是 `HEALTH_OK`、`HEALTH_WARN` 或 `HEALTH_ERR`。通过监控这个状态，你将在
    Ceph 认为存在问题时及时收到警报。此外，你可能还需要捕获 PG 的状态和降级对象的数量，因为它们可以提供额外的信息，帮助你了解可能存在的问题，而无需实际登录
    Ceph 服务器并使用 Ceph 工具集来检查状态。
- en: Operating system and hardware
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作系统和硬件
- en: It's also highly recommended that you capture the current status of the operating
    system running the Ceph software and also the status of the underlying hardware.
    Capturing things such as CPU and RAM usage will alert you to possible resource
    starvation before it potentially becomes critical. Also, long-term trending on
    this data can help to plan hardware choices for Ceph. Monitoring the hardware
    to capture hardware failures, such as disks, PSUs, and fans, is also highly recommended.
    Most server hardware is redundant and it may not be obvious that it is running
    in a degraded state unless it is monitored. In addition, monitoring network connections
    so that you can be sure that both NICs are available in bonded configuration are
    working is also a good idea.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈建议您捕获运行 Ceph 软件的操作系统当前状态，以及底层硬件的状态。捕获诸如 CPU 和内存使用情况等信息将提醒您可能的资源短缺，在它变得关键之前。长期趋势分析这些数据还有助于规划
    Ceph 的硬件选择。还强烈建议监控硬件以捕获硬件故障，例如磁盘、电源单元和风扇。大多数服务器硬件都是冗余的，除非进行监控，否则很难察觉到它在降级状态下运行。此外，监控网络连接，以确保两块网卡在绑定配置中都处于正常工作状态，也是一个好主意。
- en: Smart stats
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 智能统计
- en: Using your operating system's smart monitoring tool suite to probe the health
    of the disks is also a good idea. They may help to highlight failing disks or
    ones with abnormal error rates. For SSDs, you can also measure the wear rate of
    the flash cells, which is a good indication of when the SSD is likely to fail.
    Finally, being able to capture the temperature of the disks will allow you to
    make sure that your servers are not overheating.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用操作系统的智能监控工具套件来探测磁盘的健康状况也是一个好主意。它们可以帮助您突出显示故障磁盘或错误率异常的磁盘。对于 SSD，您还可以测量闪存单元的磨损率，这可以很好地指示
    SSD 可能会发生故障的时间。最后，能够捕获磁盘的温度将帮助您确保服务器不会过热。
- en: Network
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络
- en: As Ceph relies on the network it runs over to be reliable, it can be beneficial
    to monitor network devices for errors and performance issues. Most network devices
    can be polled via SNMP to obtain this data. Since the Mimic release of Ceph, it
    automatically sends jumbo-sized frames in its heartbeats to try and catch scenarios
    where jumbo frames are not correctly configured across the network. However, it
    is also worth considering deploying your own jumbo-frame-checking monitoring to
    catch misconfigurations, as misconfigured jumbo frames can easily bring a Ceph
    cluster to its knees.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Ceph 依赖于其运行所依赖的网络的可靠性，因此监控网络设备的错误和性能问题是非常有益的。大多数网络设备可以通过 SNMP 进行轮询，以获取这些数据。自
    Mimic 版本发布以来，Ceph 会自动在其心跳中发送巨型帧，以尝试捕捉网络中未正确配置巨型帧的情况。然而，考虑部署您自己的巨型帧检查监控工具，以捕获配置错误的情况也是值得的，因为配置错误的巨型帧很容易使
    Ceph 集群陷入瘫痪。
- en: Performance counters
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能计数器
- en: By monitoring performance counters from both the operating system and Ceph,
    you are arming yourself with a wealth of knowledge to gain a better understanding
    of how your Ceph cluster is performing. If storage permits, it's worth trying
    to capture as many of these metrics as possible; you never know when the metrics
    will come in handy. It's quite often the case when diagnosing a problem that a
    metric that was previously thought to have no connection to the issue suddenly
    sheds light on the actual cause. The traditional approach of only monitoring key
    metrics is very limiting in this regard.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过同时监控操作系统和 Ceph 的性能计数器，您可以获得大量知识，帮助更好地理解 Ceph 集群的性能。如果存储条件允许，尽可能捕获更多这些指标是值得的；你永远不知道什么时候这些指标会派上用场。在诊断问题时，经常会遇到这种情况：一个曾经被认为与问题无关的指标，突然揭示出问题的实际原因。仅仅监控关键指标的传统方法在这方面是非常有限的。
- en: Most monitoring agents that run on Linux will allow you to capture a large array
    of metrics, from resource consumption to filesystem usage. It's worth spending
    time analyzing what metric you can collect and configuring them appropriately.
    Some of these monitoring agents will also have plugins for Ceph, which can pull
    out all of the performance counters from Ceph's various components, such as `osd`
    and `mon` nodes.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数运行在 Linux 上的监控代理都允许您捕获各种指标，从资源消耗到文件系统使用情况。值得花时间分析您可以收集的指标并适当配置它们。一些监控代理还将提供
    Ceph 插件，可以提取 Ceph 各个组件（如 `osd` 和 `mon` 节点）的所有性能计数器。
- en: The Ceph dashboard
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ceph 仪表板
- en: Introduced in the Mimic release, Ceph now has an extremely useful dashboard
    based on the open ATTIC project. The dashboard in the initial Mimic release gives
    the Ceph operator the ability to monitor many aspects of their Ceph cluster that
    are needed on a daily basis. With preceding releases of Ceph, the dashboard has
    had further refinements and can now be used to manage some common tasks; over
    time, it is expected that the dashboard will continue to gain new features.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在Mimic版本中引入的Ceph现在有一个基于开源ATTIC项目的极为实用的仪表板。初始的Mimic版本中的仪表板使Ceph运维人员能够监控日常所需的Ceph集群的许多方面。随着Ceph之前版本的发布，仪表板进行了进一步的改进，现在可以用来管理一些常见的任务；随着时间的推移，预计仪表板将继续增加新功能。
- en: 'The dashboard is provided as a Ceph Mgr module and is included along with any
    dependencies in the standard Ceph installation. That means that all that is required
    to start using the Ceph dashboard is to simply enable the `mgr` module:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 仪表板作为Ceph Mgr模块提供，并与任何依赖项一起包含在标准的Ceph安装中。这意味着，开始使用Ceph仪表板所需的唯一步骤是启用`mgr`模块：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'SSL either needs to be disabled or a SSL certificate needs to be configured.
    Luckily, Ceph has a simple one-liner to get you started with a self-signed certificate:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: SSL需要禁用，或者需要配置SSL证书。幸运的是，Ceph提供了一个简单的一行命令来帮助您启动自签名证书：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It's recommended that you use a proper certificate in production deployments.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐在生产部署中使用合适的证书。
- en: 'Lastly, a username and password are required to log in to the dashboard. Again,
    Ceph has a simple command to carry out this action:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，登录仪表板需要用户名和密码。同样，Ceph提供了一个简单的命令来执行此操作：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now you should be able to browse to `https://<active mgr>:8443` and log in
    with the credentials you have just created. In this is case, `<active mgr>` is
    the Ceph node that is currently running the active mgr daemon; this can be seen
    via the `ceph -s` Ceph status screen:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该能够浏览到`https://<active mgr>:8443`并使用刚才创建的凭据登录。在这种情况下，`<active mgr>`是当前运行活动mgr守护进程的Ceph节点；可以通过`ceph
    -s` Ceph状态屏幕查看此信息：
- en: '![](img/b5517428-b827-4ed0-9bc3-f736940dd212.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5517428-b827-4ed0-9bc3-f736940dd212.png)'
- en: The first screen that's presented when you log into the dashboard gives an overview
    of the Ceph cluster's health and utilization.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 登录仪表板时呈现的第一个屏幕提供了Ceph集群的健康状况和利用率概览。
- en: Across the top of the page are a number of menus that allow you to view more
    detailed information about the Ceph cluster, such as details on the OSDs and PGs.
    The block menu allows you to view details on created RBD images and likewise the
    filesystems menu shows information about any CephFS filesystems.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 页面顶部有多个菜单，允许您查看有关Ceph集群的更详细信息，如有关OSD和PG的详细信息。块菜单允许您查看已创建的RBD镜像的详细信息，同样，文件系统菜单显示有关任何CephFS文件系统的信息。
- en: The Object Gateway will show information about the RADOS Gateway, but requires
    configuration with a valid RGW user; please consult the official Ceph documentation
    for more information if required.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对象网关将显示有关RADOS网关的信息，但需要使用有效的RGW用户进行配置；如有需要，请查阅官方Ceph文档获取更多信息。
- en: With the development future of the Ceph dashboard looking bright, it is highly
    recommended to deploy it for any Ceph cluster you manage. With future releases
    set to bring further enhancements around being able to manage your Ceph cluster
    from it, the dashboard will certainly become more and more useful over time. However,
    even in its current state, the ability for less knowledgeable administrators to
    easily be able to explore the current status of a running Ceph cluster is extremely
    useful.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Ceph仪表板的未来发展前景看好，强烈推荐为您管理的任何Ceph集群部署它。未来版本预计将带来更多的增强功能，能够通过仪表板管理Ceph集群，随着时间的推移，仪表板将变得越来越有用。然而，即使在当前状态下，较不熟悉的管理员能够轻松查看正在运行的Ceph集群的当前状态也是非常有用的。
- en: PG states – the good, the bad, and the ugly
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PG状态——好、坏和丑
- en: Each placement group in Ceph has one or more statuses assigned to it; normally,
    you want to see all your PGs with `active+clean` against them. Understanding what
    each state means can help us identify what is happening to PG and whether you
    need to take action.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Ceph中的每个放置组（PG）都有一个或多个分配给它的状态；通常，您希望看到所有PG的状态为`active+clean`。理解每个状态的含义可以帮助我们识别PG发生了什么，是否需要采取行动。
- en: The good ones
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 好的状态
- en: The following states are indications of a healthy operating cluster, for which
    no action needs to be taken.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下状态表示一个健康运行的集群，此时无需采取任何措施。
- en: The active state
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活状态
- en: The `active` state means that the PG is in full health, and it is capable of
    accepting client requests.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`active` 状态意味着 PG 处于完全健康状态，能够接受客户端请求。'
- en: The clean state
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: clean 状态
- en: The `clean` state means that the PG's objects are being replicated the correct
    number of times and are all in a consistent state.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`clean` 状态意味着 PG 的对象已经按照正确的次数进行复制，且所有对象处于一致的状态。'
- en: Scrubbing and deep scrubbing
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scrubbing 和深度 Scrubbing
- en: Scrubbing means that Ceph checks the consistency of your data and is a normal
    background process. Scrubbing on its own is where Ceph checks that the objects
    and relevant metadata exists. When Ceph performs a deep scrub, it compares the
    contents of the objects and their replicas for consistency.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Scrubbing 意味着 Ceph 检查数据的一致性，这是一个正常的后台过程。单纯的 Scrubbing 是指 Ceph 检查对象及其相关的元数据是否存在。当
    Ceph 执行深度 Scrub 时，它会对比对象及其副本的内容来检查一致性。
- en: The bad ones
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不良状态
- en: The following states indicate that Ceph is not in full health, but shouldn't
    cause any immediate problems.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下状态表示 Ceph 并非完全健康，但不会立即导致问题。
- en: The inconsistent state
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不一致状态
- en: The `inconsistent` state means that during the scrub process, Ceph has found
    one or more objects that are inconsistent with its replicas. See [Chapter 11](bf50389d-8256-4cbf-ae32-01a494ab4189.xhtml),
    *Troubleshooting*, later in this book on how to deal with these errors.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`inconsistent` 状态意味着在 Scrub 过程中，Ceph 发现一个或多个对象与其副本不一致。请参阅本书后面的 [第11章](bf50389d-8256-4cbf-ae32-01a494ab4189.xhtml)，*故障排除*，了解如何处理这些错误。'
- en: The backfilling, backfill_wait, recovering, and recovery_wait states
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: backfilling、backfill_wait、recovering 和 recovery_wait 状态
- en: These states mean that Ceph is copying or migrating data from one OSD to another.
    This may possibly mean that this PG has less than the desired number of copies.
    If it's in the `wait` state, it means that due to throttles on each OSD, Ceph
    is limiting the number of concurrent operations to reduce the impact on client
    operations.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些状态意味着 Ceph 正在将数据从一个 OSD 复制或迁移到另一个 OSD。这可能意味着该 PG 的副本数少于期望的数量。如果处于 `wait` 状态，则表示由于每个
    OSD 上的限流，Ceph 限制了并发操作的数量，以减少对客户端操作的影响。
- en: The degraded state
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降级状态
- en: The `degraded` state means that the PG is missing or has out-of-date copies
    of one or more objects. These will normally be corrected by the recovery/backfill
    process.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`degraded` 状态意味着 PG 缺少或拥有过时副本的一个或多个对象。这些通常会通过恢复/回填过程得到纠正。'
- en: Remapped
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 已重新映射
- en: In order to become active, the PG is currently mapped to a different OSD or
    set of OSDs. This is likely to occur when OSD is down but has not been recovered
    to the remaining OSDs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了变为活动状态，PG 当前被映射到不同的 OSD 或一组 OSD 上。这通常发生在 OSD 宕机但尚未恢复到其余的 OSD 时。
- en: Peering
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Peering
- en: The peering state is part of the normal process of a PG becoming active and
    it should only be in this state briefly. It is listed in the bad section as a
    PG that remains in the peering state will block I/O.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Peering 状态是 PG 成为活动状态的正常过程的一部分，通常它只会短暂停留在此状态。如果 PG 长时间停留在 Peering 状态，将会阻塞 I/O
    操作，因此它被列在不良状态中。
- en: The ugly ones
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 丑陋状态
- en: These states are not ones you want to see. If you see any of these states, it's
    quite likely that client access to the cluster will be affected, and unless the
    situation can be fixed, data loss may occur.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这些状态是你不希望看到的。如果看到这些状态，很可能会影响客户端访问集群，并且除非问题得到修复，否则可能会发生数据丢失。
- en: The incomplete state
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: incomplete 状态
- en: An `incomplete` state means that Ceph is unable to find any valid copies of
    objects within the PG across any of the OSDs that are currently up in the cluster.
    This can either be that the objects are simply not there or the available objects
    are missing newer writes that may have occurred on now unavailable OSDs.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`incomplete` 状态意味着 Ceph 无法在当前集群中任何在线的 OSD 上找到 PG 内对象的有效副本。这可能是因为对象根本不存在，或者现有的对象缺少可能在现在不可用的
    OSD 上发生的较新写入。'
- en: The down state
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: down 状态
- en: This will accompany the `incomplete` state. The PG is missing objects that are
    known to possibly be on unavailable OSDs and the PG cannot be started.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这将伴随 `incomplete` 状态。PG 缺少已知可能位于不可用 OSD 上的对象，PG 无法启动。
- en: The backfill_toofull and recovery_toofull state
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: backfill_toofull 和 recovery_toofull 状态
- en: Ceph has tried to recover your data, but your OSD disks are too full and it
    cannot continue. Extra OSDs are needed to fix this situation.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Ceph 尝试恢复你的数据，但 OSD 磁盘已满，无法继续。需要更多的 OSD 来解决这个问题。
- en: Monitoring Ceph with collectd
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 collectd 监控 Ceph
- en: Previously in this chapter, we covered what monitoring should be done around
    your entire Ceph infrastructure and also looked at the new, builtin Ceph dashboard.
    To gain further insights into the operation of your Ceph cluster and associated
    infrastructure, a more detailed monitoring setup is required. Although alert monitoring
    is out of scope of this book, we will now look at capturing the Ceph performance
    metrics with collectd, storing them in Graphite, and then finally creating a dashboard
    with graphs using Grafana. These captured metrics can then be used in the following
    chapter to help tune your Ceph cluster.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章前面，我们讨论了在整个 Ceph 基础设施中应该进行哪些监控，并查看了新内置的 Ceph 仪表盘。为了更深入了解 Ceph 集群及其相关基础设施的操作，我们需要进行更详细的监控设置。虽然警报监控不在本书范围之内，但我们现在将看看如何使用
    collectd 捕获 Ceph 性能指标，存储到 Graphite 中，然后使用 Grafana 创建带有图表的仪表盘。这些捕获的指标可以在下一章中帮助调整
    Ceph 集群。
- en: We will build this monitoring infrastructure on one of our monitor nodes in
    our test cluster. In a production cluster, it is highly recommended that it gets
    its own dedicated server.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在测试集群的一个监控节点上构建这个监控基础设施。在生产集群中，强烈建议为其配置独立的服务器。
- en: Graphite
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Graphite
- en: Graphite is a time series database that excels in storing large amounts of metrics
    and has a mature query language, which can be used by applications to manipulate
    data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Graphite 是一个时间序列数据库，擅长存储大量的度量数据，并且具有成熟的查询语言，应用程序可以使用它来操作数据。
- en: 'We first need to install the required Graphite packages:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要安装所需的 Graphite 软件包：
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding command gives the following output:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将给出以下输出：
- en: '![](img/488cb2df-39ad-4069-9510-6cc07aef46b6.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/488cb2df-39ad-4069-9510-6cc07aef46b6.png)'
- en: 'Edit the `/etc/graphite/storage-schemas.conf` storage schemas file and place
    the following into it:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑 `/etc/graphite/storage-schemas.conf` 存储模式文件，并将以下内容放入其中：
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, we can create the graphite database by running the following command:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过运行以下命令来创建 Graphite 数据库：
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preceding command will give the following output:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将给出以下输出：
- en: '![](img/dc018797-5e89-447f-bbbc-17b1d8a825ec.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc018797-5e89-447f-bbbc-17b1d8a825ec.png)'
- en: 'Set the password for the root user when prompted:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示时设置 root 用户的密码：
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding command gives the following output:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将给出以下输出：
- en: '![](img/d98b277c-89aa-43f5-a02f-be4fb9f33818.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d98b277c-89aa-43f5-a02f-be4fb9f33818.png)'
- en: 'To stop the default Apache site from conflicting with the Graphite web service,
    we need to disable it by running the following command:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止默认的 Apache 站点与 Graphite Web 服务冲突，我们需要通过运行以下命令禁用它：
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The preceding command gives the following output:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将给出以下输出：
- en: '![](img/6158b809-8725-4c61-92a8-4ea5951a75e8.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6158b809-8725-4c61-92a8-4ea5951a75e8.png)'
- en: 'We can now copy the Apache Graphite configuration into the Apache environment:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将 Apache Graphite 配置复制到 Apache 环境中：
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding commands give the following output:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将给出以下输出：
- en: '![](img/b67de3db-4876-4bdb-ad1b-1001c51266f3.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b67de3db-4876-4bdb-ad1b-1001c51266f3.png)'
- en: 'Restart the Apache service:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 重启 Apache 服务：
- en: '[PRE9]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Grafana
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Grafana
- en: 'We will edit the apt repository file and add the repository for Grafana:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将编辑 apt 仓库文件并添加 Grafana 的仓库：
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Place the following line into the file and save it:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下行放入文件并保存：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now run the following commands to retrieve the `gpg` key and update the package
    lists:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在运行以下命令以获取 `gpg` 密钥并更新软件包列表：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Install Grafana using the following command:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令安装 Grafana：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding command gives the following output:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将给出以下输出：
- en: '![](img/1990df5e-78f0-4130-97d8-39dcc0c54b9e.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1990df5e-78f0-4130-97d8-39dcc0c54b9e.png)'
- en: With the standard Vagrant configuration, you will not be able to connect to
    the HTTP port provided by Grafana. To access Grafana, we will need to port forward
    via `ssh port 3000` to our local machine.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标准的 Vagrant 配置，你将无法连接到 Grafana 提供的 HTTP 端口。为了访问 Grafana，我们需要通过 `ssh port 3000`
    将端口转发到我们的本地机器。
- en: 'An example of using PuTTY is shown in the following screenshot:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用 PuTTY 的示例截图：
- en: '![](img/31fc51b4-0eab-477e-a122-028cf70d2aac.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/31fc51b4-0eab-477e-a122-028cf70d2aac.png)'
- en: 'Now, use `http://localhost:3000` in the URL. You should be taken to the Grafana
    home page. Navigate to data sources and then configure Grafana to poll our newly
    installed Graphite installation:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在 URL 中使用 `http://localhost:3000`。你应该能进入 Grafana 的主页。进入数据源设置，然后配置 Grafana
    来轮询我们新安装的 Graphite 安装：
- en: '![](img/15f24153-3f71-479e-9096-d04d90cce146.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/15f24153-3f71-479e-9096-d04d90cce146.png)'
- en: If you get the green success bar when you click on the Save & Test button, then
    you have successfully installed and configured Graphite and Grafana.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果点击 "Save & Test" 按钮时出现绿色成功条，说明你已经成功安装和配置了 Graphite 和 Grafana。
- en: collectd
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: collectd
- en: Now that we have a shiny installation of Graphite and Grafana to look at, we
    need to put some data into it to be able to generate some graphs. collectd is
    a well-respected metric collection tool, which can output metrics to Graphite.
    The core collectd application is very minimal, and it relies on a series of plugins
    to collect metrics and forwards them onto applications such as Graphite for storage.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了 Graphite 和 Grafana，可以开始向其中添加一些数据，以便生成图表。collectd 是一个备受尊敬的度量收集工具，可以将度量数据输出到
    Graphite。core collectd 应用程序非常简洁，它依赖于一系列插件来收集度量数据并将其转发到如 Graphite 之类的应用程序以进行存储。
- en: 'Before we start collecting metrics from our Ceph nodes, let''s install collectd
    on the same VM where we installed Graphite and Grafana. We will do this to gain
    a better understanding of collectd and the process required to configure it. We
    will then use Ansible to install and configure collectd on all of our Ceph nodes,
    which would be the recommended approach if this was being rolled out in a production
    environment. We have the following code:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始从 Ceph 节点收集度量数据之前，让我们在与 Graphite 和 Grafana 相同的虚拟机上安装 collectd。这样做是为了更好地理解
    collectd 及其配置过程。然后，我们将使用 Ansible 在所有 Ceph 节点上安装和配置 collectd，这是如果在生产环境中部署时推荐的方法。我们有以下代码：
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The preceding command will give the following output:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将输出以下内容：
- en: '![](img/0f642770-d575-41e3-ad62-3e4df0a4042e.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f642770-d575-41e3-ad62-3e4df0a4042e.png)'
- en: 'This will install collectd and a basic set of plugins for querying standard
    operating system resources. There is a sample configuration stored in the following
    location:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这将安装 collectd 和一组用于查询标准操作系统资源的基本插件。示例配置存储在以下位置：
- en: '[PRE15]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'It lists all of the core plugins and sample configuration options. It is worth
    reviewing this file to learn about the various plugins and their configuration
    options. For this example, however, we will start with an empty configuration
    file and configure a few basic resources:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 它列出了所有核心插件和示例配置选项。值得查看此文件以了解各种插件及其配置选项。然而，对于这个示例，我们将从一个空的配置文件开始，并配置一些基本资源：
- en: 'Create a new `collectd` configuration file using the following command:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令创建一个新的 `collectd` 配置文件：
- en: '[PRE16]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Add the following to it:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加以下内容：
- en: '[PRE17]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Restart the `collectd` service using the following command:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令重启 `collectd` 服务：
- en: '[PRE18]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, navigate back to Grafana and browse the dashboard''s menu item. Click
    on the button in the middle of the screen to create a new dashboard:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，返回 Grafana 并浏览仪表盘的菜单项。点击屏幕中间的按钮来创建一个新的仪表盘：
- en: '![](img/9bc0bcbc-9a1b-4fe0-8c99-02258224a4e9.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9bc0bcbc-9a1b-4fe0-8c99-02258224a4e9.png)'
- en: 'Select Graph to add a new graph to the dashboard. An example graph will now
    appear, which we will want to edit to replace with our own graphs. To do this,
    click on the graph title and a floating menu will appear:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 Graph 来向仪表盘添加一个新的图表。现在会出现一个示例图表，我们将编辑它以替换为我们自己的图表。为此，点击图表标题，会弹出一个浮动菜单：
- en: '![](img/aaee7386-fe72-40ab-9912-810b7e05ae6d.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aaee7386-fe72-40ab-9912-810b7e05ae6d.png)'
- en: 'Click on Edit to go to the graph widget editing screen. From here, we can delete
    the fake graph data by selecting the *dustbin* icon, as shown in the following
    three-button menu box:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 "Edit" 进入图表小部件编辑界面。在这里，我们可以通过选择 *dustbin* 图标删除虚假图表数据，如下图所示的三个按钮菜单框：
- en: '![](img/d0e5a750-326a-4385-b305-ac92ede696af.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0e5a750-326a-4385-b305-ac92ede696af.png)'
- en: 'Now, from the drop-down menu, change the panel data source to the Graphite
    source we have just added and click on the `Add query` button:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在下拉菜单中，将面板数据源更改为我们刚刚添加的 Graphite 数据源，并点击 `Add query` 按钮：
- en: '![](img/3b546ef4-57e5-4469-b7b1-d4bb86043723.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3b546ef4-57e5-4469-b7b1-d4bb86043723.png)'
- en: 'A query box will appear at the top of the editing panel. It will also have
    the three-button menu box, like before. From here, we can toggle the edit mode
    of the query editor by clicking on the button with the three horizontal lines:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个查询框将出现在编辑面板的顶部。它还将具有像以前一样的三个按钮菜单框。从这里，我们可以通过点击包含三条横线的按钮来切换查询编辑器的编辑模式：
- en: '![](img/6813b9b8-0701-4b84-a9ae-b5c86c0063a6.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6813b9b8-0701-4b84-a9ae-b5c86c0063a6.png)'
- en: The Toggle Edit Mode option switches the query editor between click and select
    mode, where you can explore the available metrics and build up basic queries and
    the text editor mode. The click and select mode is useful if you do not know the
    names of the metrics and only want to create basic queries. For more advanced
    queries, the text editor is required.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 切换编辑模式选项可以在点击选择模式和文本编辑模式之间切换。在点击选择模式下，你可以浏览可用的指标并构建基本查询；而文本编辑模式则适用于更高级的查询。如果你不熟悉指标名称并且只想创建基本查询，点击选择模式会很有用。对于更复杂的查询，需要使用文本编辑器。
- en: We will first make a query for our graph using the basic editor mode and then
    switch to the text mode for the rest of this chapter to make it easier to copy
    the queries from this book.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用基本编辑器模式为我们的图表创建查询，然后切换到文本模式，接下来的章节将使用文本模式，以便更容易复制书中的查询。
- en: 'Let''s first graph the system load of VM where we have installed collectd:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们绘制安装了 collectd 的虚拟机的系统负载图：
- en: '![](img/4caf3d8a-b599-45c4-8617-026be2452002.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4caf3d8a-b599-45c4-8617-026be2452002.png)'
- en: This will now produce a graph, showing the system load.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一个图表，显示系统负载。
- en: 'By further clicking on the + symbol, you can expand the query by applying different
    functions against the data. These could be used to add multiple data sources together
    or to find the average. We will cover this further in this chapter as we begin
    to craft some queries to analyze Ceph performance. Before we continue, let''s
    switch the query editor mode to text mode to see what the query looks like:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 通过进一步点击 + 符号，你可以通过对数据应用不同的函数来扩展查询。这些可以用来将多个数据源相加或计算平均值。我们将在本章中进一步探讨，当我们开始编写查询来分析
    Ceph 性能时。在继续之前，让我们将查询编辑模式切换为文本模式，看看查询的样子：
- en: '![](img/b4d18c67-6af8-408f-aff4-77ecd678509c.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4d18c67-6af8-408f-aff4-77ecd678509c.png)'
- en: You can see that each leaf of the tree of metrics is separated by a dot. This
    is how the Graphite query language works.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，每个指标树的叶子通过点号分隔。这就是 Graphite 查询语言的工作方式。
- en: Deploying collectd with Ansible
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Ansible 部署 collectd
- en: Now that we have confirmed that our monitoring stack is installed and working
    correctly, let's use Ansible to deploy collectd to all our Ceph nodes, so we can
    start monitoring it.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确认监控栈已安装并正常工作，让我们使用 Ansible 将 collectd 部署到所有 Ceph 节点，以便开始监控。
- en: 'Switch to the `ansible` directory:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 切换到 `ansible` 目录：
- en: '[PRE19]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Edit your Ansible `site.yml` file and add the `collectd` role to the plays
    for your `mon` and `osd` nodes so that they look like the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑你的 Ansible `site.yml` 文件，并将 `collectd` 角色添加到 `mon` 和 `osd` 节点的 play 中，使其如下所示：
- en: '![](img/7d5cd9a1-44e9-42fb-a7f3-b4a5d6323845.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d5cd9a1-44e9-42fb-a7f3-b4a5d6323845.png)'
- en: 'Edit `group_vars/all` and enter the following:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑 `group_vars/all` 并输入以下内容：
- en: '![](img/62a74bf3-b1d3-49c0-bec3-23374be862f0.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/62a74bf3-b1d3-49c0-bec3-23374be862f0.png)'
- en: 'Now, run your `site.yml` playbook:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，运行你的 `site.yml` playbook：
- en: '[PRE20]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The preceding command gives the following output:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令将给出以下输出：
- en: '![](img/919339d4-9a37-48bc-abcb-af659d1f22ea.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/919339d4-9a37-48bc-abcb-af659d1f22ea.png)'
- en: 'You should see from the status at the end that Ansible has deployed `collectd`
    to all your Ceph nodes, and it has configured the `collectd` Ceph plugin. In Grafana,
    you should now be able to see your Ceph nodes showing up as available metrics.
    The following is one of our monitor nodes:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 从最后的状态可以看出，Ansible 已经将 `collectd` 部署到所有的 Ceph 节点，并且配置了 `collectd` Ceph 插件。在
    Grafana 中，你现在应该能够看到 Ceph 节点作为可用指标出现。以下是我们的一个监控节点：
- en: '![](img/798fee5c-ad98-4313-b060-90d786c8da5a.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/798fee5c-ad98-4313-b060-90d786c8da5a.png)'
- en: 'For example, we can now create a graph showing the number objects stored in
    the Ceph cluster. Create a new graph in Grafana and enter the following query:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，现在我们可以创建一个图表，展示 Ceph 集群中存储的对象数量。在 Grafana 中创建一个新图表，并输入以下查询：
- en: '[PRE21]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This will produce a graph like the following one:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成如下图所示的图表：
- en: '![](img/df871fb2-0d6f-44a9-a95a-c531e2e838a7.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df871fb2-0d6f-44a9-a95a-c531e2e838a7.png)'
- en: It's advised that you spend some time browsing through the available metrics
    so that you are familiar with them before proceeding to the next section.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 建议你在继续下一节之前，花一些时间浏览可用的指标，以便熟悉它们。
- en: Sample Graphite queries for Ceph
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ceph 的示例 Graphite 查询
- en: Although you can generate some very useful graphs by simply selecting individual
    metrics, by harnessing the power of Graphite's functions to manipulate the metrics,
    graphs can be created, which offer a much more detailed insight into your Ceph
    cluster. The following Graphite queries are useful for generating common graphs
    and are also a good starting point so that you can create your own custom queries.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管您可以通过简单地选择单个指标生成一些非常有用的图表，但通过利用 Graphite 的功能来操作指标，可以创建提供更详细洞察的图表，帮助您更好地了解
    Ceph 集群。以下的 Graphite 查询对于生成常见图表非常有用，也是您创建自定义查询的一个良好起点。
- en: Number of Up and In OSDs
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OSD 的 Up 和 In 数量
- en: 'It''s very handy to be able to quickly glance at a dashboard and see how many
    OSDs are `Up` and `In`. The following two queries show these values:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 能够快速查看仪表板并查看有多少 OSD 是 `Up` 和 `In` 是非常方便的。以下两个查询展示了这些值：
- en: '[PRE22]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note the use of the `maxSeries` function, which allows data to be pulled from
    all the `mon` nodes and will take the highest value.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意 `maxSeries` 函数的使用，它允许从所有 `mon` 节点提取数据，并会取最高值。
- en: Showing the most deviant OSD usage
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 显示最偏差的 OSD 使用情况
- en: 'Due to the way CRUSH places PGs on each OSD, there will never be a perfect
    balance of PGs per OSD. The following query will create a graph that will show
    the ten most deviant OSDs, so you can see if PG balancing would be beneficial.
    We have the following code:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 CRUSH 方式将 PG 放置在每个 OSD 上，因此每个 OSD 上的 PG 数量永远不会完美平衡。以下查询将创建一个图表，展示十个最偏差的 OSD，您可以查看
    PG 平衡是否会有益。我们有以下代码：
- en: '[PRE23]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Total number of IOPs across all OSDs
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 所有 OSD 的 IOP 总数
- en: 'This uses the `sumSeries` function and wildcards to add all the `op` metrics
    from every OSD together:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这使用 `sumSeries` 函数和通配符将所有 OSD 的 `op` 指标加在一起：
- en: '[PRE24]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: There are also counters that will show read and write operations individually,
    named `opR` and `opW`, respectively.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 也有分别显示读操作和写操作的计数器，分别名为 `opR` 和 `opW`。
- en: Total MBps across all OSDs
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 所有 OSD 的总 MBps
- en: 'Similarly, there are also counters that show MBps for each OSD, such as the
    `op` counters; the `sumSeries` function can also be used. We have the following
    code:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，也有显示每个 OSD 的 MBps 的计数器，例如 `op` 计数器；也可以使用 `sumSeries` 函数。我们有以下代码：
- en: '[PRE25]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Cluster capacity and usage
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群容量和使用情况
- en: 'The following two queries show the total capacity of bytes in the cluster and
    the number of bytes used. They can be used to generate a pie chart in Grafana
    to show the percentage of used space. Note that these counters show the raw capacity
    before replication:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两个查询展示了集群中字节的总容量和已用字节数。它们可以用来在 Grafana 中生成一个饼图，显示已用空间的百分比。请注意，这些计数器显示的是在复制之前的原始容量：
- en: '[PRE26]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Average latency
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平均延迟
- en: 'The following two queries can be used to graph the average latency of the cluster.
    Larger I/O sizes per operation will increase the average latency, as larger I/Os
    take longer to process. As such, these graphs will not give a clear picture of
    your cluster''s latency if the average I/O size changes over time. We have the
    following code:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两个查询可以用来绘制集群的平均延迟图。如果每次操作的 I/O 大小较大，平均延迟将增加，因为较大的 I/O 需要更长时间来处理。因此，如果平均 I/O
    大小随时间变化，这些图表将无法清晰地展示集群的延迟。我们有以下代码：
- en: '[PRE27]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Custom Ceph collectd plugins
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义 Ceph collectd 插件
- en: 'Although the standard collectd Ceph plugin does a good job of collecting all
    of Ceph''s performance counters, it falls short of collecting all the required
    data to allow you to get a complete view of your cluster health and performance.
    This section will demonstrate how to use additional custom collectd plugins to
    collect the PG states, per pool performance stats, and more realistic latency
    figures:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管标准的 collectd Ceph 插件在收集 Ceph 的所有性能计数器方面表现良好，但它无法收集所有所需的数据，以便您全面了解集群的健康状况和性能。本节将展示如何使用额外的自定义
    collectd 插件来收集 PG 状态、每个池的性能统计信息以及更为真实的延迟数据：
- en: 'Jump on to one of your `mon` nodes via SSH and clone the following Git repository:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过 SSH 登录到您的一个 `mon` 节点，并克隆以下 Git 仓库：
- en: '[PRE28]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Create a `ceph` directory under the `collectd/plugins` directory:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `collectd/plugins` 目录下创建一个 `ceph` 目录：
- en: '[PRE29]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Copy the `plugins` directory to `/usr/lib/collectd/plugins/ceph` using the
    following command:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令将 `plugins` 目录复制到 `/usr/lib/collectd/plugins/ceph`：
- en: '[PRE30]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, create a new `collectd` configuration file to enable the plugins:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，创建一个新的 `collectd` 配置文件以启用插件：
- en: '[PRE31]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Place the following configuration inside it and save the new file:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下配置放入其中并保存新文件：
- en: '[PRE32]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The latency plugin uses a RADOS bench to determine the cluster latency; this
    means that it is actually running RADOS bench and will write data to your cluster.
    The `TestPool` parameter determines the target for the RADOS bench command. It
    is therefore recommended that on a production cluster, a separate small pool is
    created for this use.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟插件使用RADOS基准测试来确定集群的延迟；这意味着它实际上在运行RADOS基准测试并会向集群写入数据。`TestPool`参数确定了RADOS基准命令的目标。因此，建议在生产集群上为此用途创建一个单独的小池。
- en: If you are trying to use these extra plugins on Kraken+ releases of Ceph, you
    will need to edit the `ceph_pg_plugin.py` file and modify the variable name on
    line 71 from `fs_perf_stat` to `perf_stat`.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试在Kraken+版本的Ceph上使用这些额外的插件，你需要编辑`ceph_pg_plugin.py`文件，并在第71行修改变量名，从`fs_perf_stat`更改为`perf_stat`。
- en: 'Restart the `collectd` service:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重启`collectd`服务：
- en: '[PRE33]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The average cluster latency can now be obtained with the following query:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以通过以下查询获取平均集群延迟：
- en: '[PRE34]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This figure is based on doing 64 KB writes, and so, unlike the OSD metrics,
    it will not change depending on the average client I/O size.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图是基于进行64KB写入的，因此与OSD指标不同，它不会根据客户端I/O的平均大小发生变化。
- en: Summary
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned the importance of monitoring your Ceph cluster
    and its supporting infrastructure. You should also have a good understanding of
    the various components that you should monitor and some example tools that can
    be used. We covered some of the PG states that, in conjunction with a monitoring
    solution will allow you to understand the current status of your Ceph cluster.
    Finally, we deployed a highly scalable monitoring system comprising collectd,
    Graphite, and Grafana, which will enable you to create professional looking dashboards
    to show the status and performance of your Ceph cluster.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了监控Ceph集群及其支持基础设施的重要性。你还应该对需要监控的各种组件以及一些可用的示例工具有了很好的理解。我们讨论了一些PG状态，结合监控解决方案，可以帮助你了解Ceph集群的当前状态。最后，我们部署了一个高可扩展性的监控系统，包括collectd、Graphite和Grafana，这将使你能够创建专业的仪表盘，展示Ceph集群的状态和性能。
- en: In the next chapter, we will look at ways to tune the performance of you Ceph
    cluster, this leans heavily on being able to capture performance stats, which
    you should now be able to do following this chapter.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何调优Ceph集群的性能，这在很大程度上依赖于能够捕获性能统计数据，而你现在应该能够通过本章内容来做到这一点。
- en: Questions
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What port does the Ceph Dashboard run on?
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ceph Dashboard运行在哪个端口上？
- en: What Ceph daemon is the Ceph Dashboard controlled by?
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ceph Dashboard是由哪个Ceph守护进程控制的？
- en: What does the inconsistent PG state mean?
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不一致的PG状态意味着什么？
- en: What does the backfilling PG state mean?
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回填PG状态意味着什么？
- en: What should you aim to monitor in your Ceph infrastructure?
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你应该在Ceph基础设施中监控哪些内容？
