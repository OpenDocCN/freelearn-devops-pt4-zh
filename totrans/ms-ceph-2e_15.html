<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Disaster Recovery</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, you learned how to troubleshoot common Ceph problems, which, although they may be affecting the operation of the cluster, weren't likely to cause a total outage or data loss. This chapter will cover more serious scenarios, where the Ceph cluster is down or unresponsive. It will also cover various techniques to recover from data loss. It is to be understood that these techniques are more than capable of causing severe data loss themselves and should only be attempted as a last resort. If you have a support contract with your Ceph vendor or have a relationship with Red Hat, it is highly advisable to consult them first before carrying out any of the recovery techniques listed in this chapter.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Avoiding data loss</li>
<li>Using RBD mirroring to provide highly available block storage</li>
<li>Investigating asserts</li>
<li>Rebuilding monitor databases from OSDs</li>
<li>Extracting PGs from a dead OSD</li>
<li>Examining data from an offline Bluestore OSD</li>
<li>Recovering from lost objects or inactive PGs</li>
<li>Recovering from a failed CephFS filesystem</li>
<li>Rebuilding an RBD from dead OSDs</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is a disaster?</h1>
                </header>
            
            <article>
                
<p>To be able to recover from a disaster, you first have to understand and be able to recognize one. For the purpose of this chapter, we will assume that anything that leads to a sustained period of downtime is classed as a disaster. This will not cover scenarios where a failure happens that Ceph is actively working to recover from, or where it is believed that the cause is likely to be short-lived. The other type of disaster is one that leads to a permanent loss of data unless recovery of the Ceph cluster is possible. Data loss is probably the most serious issue, as the data may be irreplaceable or can cause serious harm to the future of the business.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Avoiding data loss</h1>
                </header>
            
            <article>
                
<p>Before starting to cover some recovery techniques, it is important to cover some points discussed in <a href="0f4119df-b421-4349-86c8-b68444743f8a.xhtml">Chapter 1</a>, <em>Planning for Ceph</em>. Disaster-recovery should be seen as a last resort; the recovery guides in this chapter should not be relied upon as a replacement for adhering to best practices.</p>
<p>First, make sure you have working and tested backups of your data; in the event of an outage, you will feel a million times more relaxed if you know that in the worst cases, you can fall back to backups. While an outage may cause discomfort for your users or customers, informing them that their data, which they had entrusted you with, is now gone is far worse. Also, just because you have a backup system in place, does not mean you should blindly put your trust in it. Regular test restores will mean that you will be able to rely on them when needed.</p>
<p>Make sure you follow some design principles also mentioned in <a href="0f4119df-b421-4349-86c8-b68444743f8a.xhtml">Chapter 1</a>, <em>Planning for Ceph</em>. Don't use configuration options, such as <kbd>nobarrier</kbd>, and strongly consider the replication level you use with in Ceph to protect your data. The chances of data loss are strongly linked to the redundancy level configured in Ceph, so careful planning is advised here.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What can cause an outage or data loss?</h1>
                </header>
            
            <article>
                
<p>The majority of outages and cases of data loss will be directly caused by the loss of a number of OSDs that exceed the replication level in a short period of time. If these OSDs do not come back online, either due to a software or hardware failure, and Ceph was not able to recover objects between OSD failures, these objects are now lost.</p>
<p>If an OSD has failed due to a failed disk, it is unlikely that recovery will be possible unless costly disk-recovery services are utilized, and there is no guarantee that any recovered data will be in a consistent state. This chapter will not cover recovering from physical disk failures and will simply suggest that the default replication level of 3 should be used to protect you against multiple disk failures.</p>
<p>If an OSD has failed due to a software bug, the outcome is possibly a lot more positive, but the process is complex and time-consuming. Usually an OSD, which, although the physical disk is in a good condition is unable to start, is normally linked to either a software bug or some form of corruption. A software bug may be triggered by an uncaught exception that leaves the OSD in a state that it cannot recover from. Corruption may occur after an unexpected loss of power, where the hardware or software was not correctly configured to maintain data consistency. In both cases, the outlook for the OSD itself is probably terminal, and if the cluster has managed to recover from the lost OSDs, it's best just to erase and reintroduce the OSD as an empty disk.</p>
<p>If the number of offline OSDs has meant that all copies of an object are offline, recovery procedures should be attempted to extract the objects from the failed OSDs, and insert them back into the cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RBD mirroring</h1>
                </header>
            
            <article>
                
<p>As mentioned, working backups are a key strategy in ensuring that a failure does not result in the loss of data. Starting with the Jewel release, Ceph introduced RBD mirroring, which allows you to asynchronously mirror an RBD from one cluster to another. Note the difference between Ceph's native replication, which is synchronous, and RBD mirroring. With synchronous replication, low latency between peers is essential, and asynchronous replication allows the two Ceph clusters to be geographically remote, as latency is no longer a factor.</p>
<p>By having a replicated copy of your RBD images on a separate cluster, you can dramatically reduce both your <strong>Recovery Time Objective</strong> (<strong>RTO</strong>) and <strong>Recovery Point Objective</strong> (<strong>RPO</strong>). The RTO is a measure of how long it takes from initiating recovery to when the data is usable. It is the worst-case measurement of time between each data point and describes the expected data loss. A daily backup would have an RPO of 24 hours; for example, potentially, any data written up to 24 hours since the last backup would be lost if you had to restore from a backup.</p>
<p class="mce-root"/>
<p>With RBD mirroring, data is asynchronously replicated to the target RBD, and so, in most cases, the RPO should be under a minute. As the target RBD is also a replica and not a backup that would need to be first restored, the RTO is also likely going to be extremely low. Additionally, as the target RBD is stored on a separate Ceph cluster, it offers additional protection for snapshots, which could also be impacted if the Ceph cluster itself experiences issues. At first glance, this makes RBD mirroring seem like the perfect tool to protect against data loss, and in most cases, it is a very useful tool. RBD mirroring is not a replacement for a proper backup routine though. In cases where data loss is caused by actions internal to the RBD, such as filesystem corruption or user error, these changes will be replicated to the target RBD. A separate isolated copy of your data is vital.</p>
<p>With that said, let's take a closer look at how RBD mirroring works.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The journal</h1>
                </header>
            
            <article>
                
<p>One of the key components in RBD mirroring is the journal. The RBD mirroring journal stores all writes to the RBD and notifies the client once they have been written. These writes are then written to the primary RBD image. The journal itself is stored as an RADOS object, prefixed similarly to how RBD images are. Separately, the remote <kbd>rbd-mirror</kbd> daemon polls the configured RBD mirrors and pulls the newly-written journal objects across to the target cluster and replays them in the target RBD.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The rbd-mirror daemon</h1>
                </header>
            
            <article>
                
<p>The <kbd>rbd-mirror</kbd> daemon is responsible for replaying the contents of the journal to a target RBD in another Ceph cluster. The <kbd>rbd-mirror</kbd> daemon only needs to run on the target cluster, unless you wish to replicate both ways, in which case, it will need to run on both clusters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring RBD mirroring</h1>
                </header>
            
            <article>
                
<p>In order to use the RBD mirroring functionality, we will require two Ceph clusters. We could deploy two identical clusters we have been using previously, but the number of VMs involved may exceed the capabilities of what most people's personal machines can run. Therefore, we will modify our vagrant and ansible configuration files to deploy two separate Ceph clusters, each with a single monitor and an OSD node.</p>
<p>The required <kbd>Vagrantfile</kbd> is very similar to the one used in <a href="dd1d6803-6e40-4bfb-8150-b605bcc08d59.xhtml">Chapter 2</a>, <em>Deploying Ceph with Containers</em>, to deploy your initial test cluster; the hosts part at the top should now look like this:</p>
<pre>nodes = [<br/> { :hostname =&gt; 'ansible', :ip =&gt; '192.168.0.40', :box =&gt; 'xenial64' },<br/>  { :hostname =&gt; 'site1-mon1', :ip =&gt; '192.168.0.41', :box =&gt; 'xenial64' },<br/>  { :hostname =&gt; 'site2-mon1', :ip =&gt; '192.168.0.42', :box =&gt; 'xenial64' },<br/>  { :hostname =&gt; 'site1-osd1',  :ip =&gt; '192.168.0.51', :box =&gt; 'xenial64', :ram =&gt; 1024, :osd =&gt; 'yes' },<br/>  { :hostname =&gt; 'site2-osd1',  :ip =&gt; '192.168.0.52', :box =&gt; 'xenial64', :ram =&gt; 1024, :osd =&gt; 'yes' }<br/>]</pre>
<p>For the Anisble configuration, we will maintain two separate Ansible configuration instances so that each cluster can be deployed separately. We will then maintain separate hosts files per instance, which we will specify when we run the playbook. To do this, we will not copy the <kbd>ceph-ansible</kbd> files into <kbd>/etc/ansible</kbd>, but keep them in the home directory by using the following command:</p>
<pre><strong>git clone https://github.com/ceph/ceph-ansible.git</strong></pre>
<pre><strong>cp -a ceph-ansible ~/ceph-ansible2</strong></pre>
<p>Create the same two files, called <kbd>all</kbd> and <kbd>Ceph</kbd>, in the <kbd>group_vars</kbd> directory as we did in <a href="dd1d6803-6e40-4bfb-8150-b605bcc08d59.xhtml">Chapter 2</a>, <em>Deploying Ceph with Containers</em>. This needs to be done in both copies of <kbd>ceph-ansible</kbd>:</p>
<ol>
<li>Create a hosts file in each <kbd>ansible</kbd> directory, and place the two hosts in each:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/80a1dcb8-1dde-46a6-bf07-eb98ed5fbb42.png" style="width:27.33em;height:12.25em;"/></p>
<p style="padding-left: 60px">The preceding screenshot is for the first host and the following screenshot is for the second host:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/574a9c43-bbb0-4d4f-9721-89d5e2359d0d.png" style="width:22.00em;height:9.33em;"/></p>
<ol start="2">
<li>Run the <kbd>site.yml</kbd> playbook under each <kbd>ceph-ansible</kbd> instance to deploy our two Ceph clusters:</li>
</ol>
<pre style="padding-left: 60px"><strong>ansible-playbook -K -i hosts site.yml</strong></pre>
<ol start="3">
<li>Adjust the replication level of the default pools to <kbd>1</kbd>, as our clusters only have <kbd>1</kbd> OSD. Run these commands on both clusters:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fb3d835f-5841-4341-8234-6f016fb8cb57.png" style="width:36.75em;height:5.08em;"/></p>
<ol start="4">
<li>Install the RBD mirroring daemon on both clusters:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo apt-get install rbd-mirror</strong></pre>
<p style="padding-left: 60px"><span>The following screenshot is the output for the preceding command:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/acb7ca25-6863-4aaf-9834-b873b912180b.png"/></p>
<ol start="5">
<li>Copy <kbd>ceph.conf</kbd> and  <kbd>keyring</kbd> from both clusters to each other.</li>
<li>Copy <kbd>ceph.conf</kbd> from <kbd>site1-mon1</kbd> to <kbd>site2-mon1</kbd> and call it <kbd>remote.conf</kbd>.</li>
<li>Copy <kbd>ceph.client.admin.keyring</kbd> from <kbd>site1-mon1</kbd> to <kbd>site2-mon1</kbd> and call it <kbd>remote.client.admin.keyring</kbd>.</li>
<li>Repeat the preceding two steps but this time copy the files from <kbd>site2-mon1</kbd> to <kbd>site1-mon1</kbd>.</li>
<li>Make sure the instances of <kbd>keyring</kbd> are owned by <kbd>ceph:ceph</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo chown ceph:ceph /etc/ceph/remote.client.admin.keyring</strong></pre>
<ol start="10">
<li>Tell Ceph that the pool called <kbd>rbd</kbd> should have the mirroring function enabled:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo rbd --cluster ceph mirror pool enable rbd image</strong></pre>
<ol start="11">
<li>Repeat this for the target cluster:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo rbd --cluster remote mirror pool enable rbd image</strong></pre>
<ol start="12">
<li>Add the target cluster as a peer of the pool mirroring configuration:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo rbd --cluster ceph mirror pool peer add rbd client.admin@remote</strong></pre>
<ol start="13">
<li>Run the same command locally on the second Ceph cluster:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo rbd --cluster ceph mirror pool peer add rbd client.admin@remote</strong></pre>
<ol start="14">
<li>Back on the first cluster, let's create a test RBD to use with our mirroring lab:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo rbd create mirror_test --size=1G</strong></pre>
<ol start="15">
<li>Enable the journaling feature on the RBD image:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo rbd feature enable rbd/mirror_test journaling</strong></pre>
<ol start="16">
<li>Enable mirroring for the RBD:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo rbd mirror image enable rbd/mirror_test</strong></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/16a0f6ec-843a-45b5-be72-ccfdca4d3953.png" style="width:42.33em;height:3.33em;"/></p>
<p>It's important to note that RBD mirroring works via a pull system. The <kbd>rbd-mirror</kbd> daemon needs to run on the cluster that you wish to mirror the RBDs to; it then connects to the source cluster and pulls the RBDs across. If you were intending to implement a two-way replication where each Ceph cluster replicates with each other, you would run the <kbd>rbd-mirror</kbd> daemon on both clusters. With this in mind, let's enable and start the <kbd>systemd</kbd> service for <kbd>rbd-mirror</kbd> on your target host:</p>
<pre><strong>sudo systemctl enable ceph-rbd-mirror@admin<br/></strong><strong>sudo systemctl start ceph-rbd-mirror@admin</strong></pre>
<p>The <kbd>rbd-mirror</kbd> daemon will now start processing all the RBD images configured for mirroring on your primary cluster.</p>
<p>We can confirm that everything is working as expected by running the following command on the target cluster:</p>
<pre><strong>sudo rbd --cluster remote mirror pool status rbd –verbose</strong></pre>
<p><span>The following screenshot is the output for the preceding command:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2f465a33-48f9-4a12-914a-d1b44678e841.png"/></p>
<p>In the previous screenshot, we can see that our <kbd>mirror_test</kbd> RBD is in a <kbd>up+replaying</kbd> state; this means that mirroring is in progress, and we can see via <kbd>entries_behind_master</kbd> that it is currently up to date.</p>
<p>Note the difference in the output of the RBD <kbd>info</kbd> commands on either of the clusters. On the source cluster, the primary status is <kbd>true</kbd>, which allows you to determine which cluster the RBD is the master state and can be used by clients. This also confirms that although we only created the RBD on the primary cluster, it has been replicated to the secondary one.</p>
<p>The source cluster is shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fd0c04ac-e659-4e7c-96b4-13eb3a62a0c3.png" style="width:59.42em;height:14.17em;"/></p>
<p>The target cluster is shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a9ed69e6-0fcf-475e-b8f0-34262a6be052.png" style="width:64.00em;height:15.08em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performing RBD failover</h1>
                </header>
            
            <article>
                
<p>Before we failover the RBD to the secondary cluster, let's map it, create a filesystem, and place a file on it, so we can confirm that the mirroring is working correctly. As of Linux kernel 4.11, the kernel RBD driver does not support the RBD journaling feature required for RBD mirroring; this means you cannot map the RBD using the kernel RBD client. As such, we will need to use the <kbd>rbd-nbd</kbd> utility, which uses the <kbd>librbd</kbd> driver in combination with Linux <kbd>nbd</kbd> devices to map RBDs via user space. Although there are many things that may cause Ceph to experience slow performance, here are some of the most likely causes:</p>
<pre><strong>sudo rbd-nbd map mirror_test</strong></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d6f22175-2f4f-4d32-84e3-db3a4e85a09a.png" style="width:32.92em;height:3.33em;"/></p>
<pre><strong>sudo mkfs.ext4 /dev/nbd0</strong></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9f3d999b-be44-4d27-9a3d-e737eb5c6c0e.png" style="width:37.92em;height:13.67em;"/></p>
<pre><strong>sudo mount /dev/nbd0 /mnt<br/></strong><strong>echo This is a test | sudo tee /mnt/test.txt<br/></strong><strong>sudo umount /mnt<br/></strong><strong>sudo rbd-nbd unmap /dev/nbd0<br/></strong><strong>Now lets demote the RBD on the primary cluster and promote it on the<br/>secondary<br/></strong><strong>sudo rbd --cluster ceph mirror image demote rbd/mirror_test<br/></strong><strong>sudo rbd --cluster remote mirror image promote rbd/mirror_test</strong></pre>
<p>Now, map and mount the RBD on the secondary cluster, and you should be able to read the test text file that you created on the primary cluster:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ac3321f6-14f1-47cc-9196-6c266177d5e5.png" style="width:30.17em;height:6.33em;"/></p>
<p>We can clearly see that the RBD has successfully been mirrored to the secondary cluster, and the filesystem content is just as we left it on the primary cluster.</p>
<div class="packt_infobox">If you try to map and mount the RBD on the cluster where the RBD is not in the primary state, the operation will just hang; this is because Ceph will not permit I/O to an RBD image in a non-master state.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RBD recovery</h1>
                </header>
            
            <article>
                
<p>In the event that a number of OSDs have failed, and you are unable to recover them via the <kbd>ceph-object-store</kbd> tool, your cluster will most likely be in a state where most, if not all, RBD images are inaccessible. However, there is still a chance that you may be able to recover RBD data from the disks in your Ceph cluster. There are tools that can search through the OSD data structure, find the object files relating to RBDs, and then assemble these objects back into a disk image, resembling the original RBD image.</p>
<p>In this section, we will focus on a tool by Lennart Bader to recover a test RBD image from our test Ceph cluster. The tool allows the recovery of RBD images from the contents of Ceph OSDs, without any requirement that the OSD is in a running or usable state. It should be noted that if the OSD has been corrupted due to an underlying filesystem corruption, the contents of the RBD image may still be corrupt. The RBD recovery tool can be found in the following GitHub repository: <a href="https://gitlab.lbader.de/kryptur/ceph-recovery" target="_blank">https://gitlab.lbader.de/kryptur/ceph-recovery</a>.</p>
<p>Before we start, make sure you have a small test RBD with a valid filesystem created on your Ceph cluster. Due to the size of the disks in the test environment that we created in <a href="dd1d6803-6e40-4bfb-8150-b605bcc08d59.xhtml">Chapter 2</a>, <em>Deploying Ceph with Containers</em>, it is recommended that the RBD is only a gigabyte in size.</p>
<p>We will perform the recovery on one of the monitor nodes, but in practice, this recovery procedure can be done from any node that can access the Ceph OSD disks. To access the disks, we need to make sure that the recovery server has sufficient space to recover the data.</p>
<p>In this example, we will mount the remote OSDs contents via <kbd>sshfs</kbd>, which allows you to mount remote directories over <kbd>ssh</kbd>. However in real life, there is nothing to stop you from physically inserting disks into another server or whatever method is required. The tool only requires to see the OSDs data directories:</p>
<ol>
<li>Clone the Ceph recovery tool from the Git repository:</li>
</ol>
<pre style="padding-left: 60px"><strong>git clone https://gitlab.lbader.de/kryptur/ceph-recovery.git</strong></pre>
<p style="padding-left: 60px"><span>The following screenshot is the output for the preceding command:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/af283c1a-0176-44c8-921b-a1e0e7f64888.png" style="width:43.75em;height:8.08em;"/></p>
<p class="mce-root"/>
<ol start="2">
<li>Make sure you have <kbd>sshfs</kbd> installed:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo apt-get install sshfs</strong></pre>
<p style="padding-left: 60px"><span>The following screenshot is the output for the preceding command:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e0e5f4f8-8f8d-4ea7-b891-3b73af8dcdba.png"/></p>
<ol start="3">
<li>Change into the cloned tool directory, and create the empty directories for each of the OSDs:</li>
</ol>
<pre style="padding-left: 60px"><strong>cd ceph-recovery<br/></strong><strong>sudo mkdir osds<br/></strong><strong>sudo mkdir osds/ceph-0<br/></strong><strong>sudo mkdir osds/ceph-1<br/></strong><strong>sudo mkdir osds/ceph-2</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Filestore</h1>
                </header>
            
            <article>
                
<p>For filestore, we can simply mount each remote OSD to the directories that we have just created. Note that you need to make sure your OSD directories match your actual test cluster:</p>
<pre><strong>sudo sshfs vagrant@osd1:/var/lib/ceph/osd/ceph-0 osds/ceph-0<br/></strong><strong>sudo sshfs vagrant@osd2:/var/lib/ceph/osd/ceph-2 osds/ceph-2<br/></strong><strong>sudo sshfs vagrant@osd3:/var/lib/ceph/osd/ceph-1 osds/ceph-1</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">BlueStore</h1>
                </header>
            
            <article>
                
<p>As Bluestore does not store the objects in a native Linux filesystem, we can't just mount the filesystems. However, the <kbd>ceph-object-store</kbd> tool allows you to mount the contents of a BlueStore OSD as a <kbd>fuse</kbd> filesystem. </p>
<p>On each OSD node, create a directory under the <kbd>/mnt</kbd> folder to mount each OSD on that node:</p>
<pre><strong>mkdir /mnt/osd-0</strong></pre>
<p>Now mount the BlueStore OSD to this new directory:</p>
<pre><strong>ceph-objectstore-tool --op fuse --data-path /var/lib/ceph/osd/ceph-0 --mountpoint /mnt/osd-0</strong></pre>
<p><span>The following screenshot is the output for the preceding command:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6f27079b-d5c0-4eb1-bd2b-559ee2788071.png" style="width:70.33em;height:4.33em;"/></p>
<p>The BlueStore OSD is now mounted as a <kbd>fuse</kbd> filesystem to the <kbd>/mnt/osd-0</kbd> directory. However, it will only remain mounted while the <kbd>ceph-object-store</kbd> command is running. So if you wish to mount multiple OSDs or manually browse through the directory tree, open additional SSH sessions to the Ceph node. The following is a screenshot showing the contents of the <kbd>/mnt/osd-0</kbd> directory from a new SSH session:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/eea946d5-22b9-48bd-ab4e-0304c43d382d.png"/></p>
<p>When you have finished with the OSD, simply use <em>Ctrl</em> + <em>C</em> on the SSH session running the <kbd>ceph-objectstore-tool</kbd> command to unmount.</p>
<p>Now we can mount the fuse-mounted OSDs to our management server like we did with filestore:</p>
<pre><strong>sudo sshfs vagrant@osd1:/mnt/osd-0 osds/ceph-0<br/></strong><strong>sudo sshfs vagrant@osd2:/mnt/osd-1 osds/ceph-1<br/></strong><strong>sudo sshfs vagrant@osd3:/mnt/osd-2 osds/ceph-2</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RBD assembly – filestore</h1>
                </header>
            
            <article>
                
<p>Now we can use the tool to scan the OSD directories and compile a list of the RBDs that are available. The only parameter needed for this command is the location where the OSDs are mounted. In this case, it is in a directory called <kbd>osds</kbd>. The results will be listed in the VM directory:</p>
<pre><strong>sudo ./collect_files.sh osds</strong></pre>
<p><span>The following screenshot is the output for the preceding command:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/03846663-92d1-421a-b9e8-1d850a466aec.png" style="width:37.25em;height:9.92em;"/></p>
<p>If we look inside the VM directory, we can see that the tool has found our test RBD image. Now that we have located the image, the next step is to assemble various objects located on the OSDs. The three parameters for this command are the name of the RBD image found in the previous step, the size of the image, and the destination for the recovered image file. The size of the image is specified in bytes, and it is important that it is at least as big as the original image; it can be bigger, but the RBD will not recover if the size is smaller:</p>
<pre><strong>sudo ./assemble.sh vms/test.id 1073741824 .</strong></pre>
<p><span>The following screenshot is the output for the preceding command:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fb8634ea-7537-4ca6-8c1e-85ec5ed09712.png"/></p>
<p>The RBD will now be recovered from the mounted OSD contents to the specified image file. Depending on the size of the image, it may take a while, and a progress bar will show you its progress.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RBD assembly – BlueStore</h1>
                </header>
            
            <article>
                
<p>The RBD assembly script will not work with BlueStore OSDs as BlueStore stores the RBD objects with a slightly different naming convention. An updated script is provided in the following steps to aid with RBD recovery.</p>
<p>Download the script to assist with the recovery of RBDs from BlueStore OSDs:</p>
<pre><strong>wget https://raw.githubusercontent.com/fiskn/assemble_bluestore_rbd/master/assemble_bluestore.sh</strong><br/><strong>chmod +x ./assemble_bluestore.sh</strong></pre>
<p>Run the recovery script with three parameters, where first is the RBD image hash name, the second is the RBD size in bytes, and the third is the output filename.</p>
<p>The following example is from a 10 GB test RBD:</p>
<pre><strong>./assemble_bluestore.sh 7d8ad6b8b4567 1073741824 text.img </strong></pre>
<p><span>The following screenshot is the output for the preceding command:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9730a343-be26-4597-96f9-efc388c15e15.png"/></p>
<p>The RBD image should now be recovered.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Confirmation of recovery</h1>
                </header>
            
            <article>
                
<p>Once completed, we can run a file system called <kbd>fsck</kbd> on the image to make sure that it has been recovered correctly. In this case, the RBD was formatted with <kbd>ext4</kbd>, so we can use the <kbd>e2fsck</kbd> tool to check the image:</p>
<pre><strong>sudo e2fsck test.raw</strong></pre>
<p><span>The following screenshot is the output for the preceding command:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7a9343f3-db9d-452e-b4f1-54b2af36e6cf.png" style="width:35.25em;height:4.92em;"/></p>
<p>Excellent, the image file is clean, which means that there is now a very high chance that all our data has been recovered successfully.</p>
<p>Now we can finally mount the image as a loopback device to access our data. If the command returns no output, we have successfully mounted it:</p>
<pre><strong>sudo mount -o loop test.raw /mnt</strong></pre>
<p>You can see that the image is successfully mounted as a loop device:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/544665dd-0be5-4140-a6ea-bf07f3219bd1.png" style="width:39.83em;height:13.92em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RGW Multisite</h1>
                </header>
            
            <article>
                
<p>Ceph also supports the ability to run two or more RADOS Gateway Zones to provide high availability of the S3 and swift-compatible storage across multiple sites. Each zone is backed by a completely separate Ceph cluster, meaning that it is extremely unlikely that any hardware of software failure can take the service completely offline. When using RGW in a multisite configuration, it's important to note that data is eventually consistent, so that data is not guaranteed to be in the same state in every zone immediately after it has been written.</p>
<p>For more information on RGW multi-site configurations, please consult the official Ceph documentation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CephFS recovery</h1>
                </header>
            
            <article>
                
<p>Unlike RBDs, which are simply a concatenation of objects, CephFS requires consistent data in both the data and metadata pools. It also requires a healthy CephFS journal; if any of these data sources have issues, CephFS will go offline and may not recover. This section of the chapter will look at recovering CephFS to an active state and then further recovery steps in the scenario that the metadata pool is corrupt or incomplete.</p>
<p>There are a number of conditions where CephFS may go offline but will not result in any permanent data loss; these are often caused by transient events in the Ceph cluster but shouldn't result in any long-term data loss, and in most cases CephFS should automatically recover.</p>
<p>As CephFS sits on RADOS, barring any software bugs in CephFS, any data loss or corruption should only occur in the instance where there has been a data loss occurrence in the RADOS layer, perhaps due to multiple OSD failures leading to the loss of a PG.</p>
<p>The loss of objects or PGs from the data pool will not take the CephFS filesystem offline, but will result in access requests to the affected files to return zeroes. This will likely cause any applications higher up the stack to fail and, due to the semi-random nature of files or parts of files, which map to PGs, the result would likely mean that the CephFS filesystem is largely usable. The best case in this scenario would be to try to recover the RADOS pool PGs as seen later in this chapter.</p>
<p>The loss of objects or PGs from the metadata pool will take the CephFS filesystem offline and it will not recover without manual intervention. It is important to point out that the actual data contents are unaffected by metadata loss, but the objects storing this data would be largely meaningless without the metadata. Ceph has a number of tools that can be used to recover and rebuild metadata, which may enable you to recover from metadata loss. However, as has been mentioned several times throughout this book, prevention is better than cure and as such, these tools should not been seen as a standard recovery mechanism, but only to be used as a last resort when recovery from regular backups have failed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the disaster</h1>
                </header>
            
            <article>
                
<p>To create the scenario where a CephFS filesystem has lost or corrupted its metadata pool, we will simply delete all objects in the metadata pool. This example will use the filesystem deployed in <a href="a01c8234-61a1-4d8e-9393-33a7218cf49d.xhtml">Chapter 5</a>, <em>RADOS Pools and Client Access</em>, but the procedure should be identical to any other deployed CephFS filesystem.</p>
<p>First, let's switch to the root account and mount the CephFS filesystem:</p>
<pre><strong>sudo -i</strong><br/><strong>mount -t ceph 192.168.0.41:/ /mnt/cephfs -o name=admin,secret=AQC4Q85btsqTCRAAgzaNDpnLeo4q/c/q/0fEpw==</strong></pre>
<p>Place a few test files on the CephFS filesystem that we will later attempt to recover:</p>
<pre><strong>echo "doh" &gt; /mnt/cephfs/doh</strong><br/><strong><span>echo "ray" &gt; /mnt/cephfs/ray<br/></span><span>echo "me" &gt; /mnt/cephfs/me</span></strong></pre>
<p>Now that we have the test files in place, let's delete all the objects in the metadata pool to simulate a loss of the metadata pool:</p>
<pre><strong>rados purge cephfs_metadata --yes-i-really-really-mean-it</strong></pre>
<p><span>The following screenshot is the output for the preceding command:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/43d3be7e-aacc-4c02-ae94-944b07580388.png" style="width:41.92em;height:6.08em;"/></p>
<p>Let's restart the <kbd>mds</kbd> daemon; trigger the failure:</p>
<pre><strong>systemctl restart ceph-mds@*</strong></pre>
<p>If we now check the CephFS status with the <kbd>ceph -s</kbd> command, we can see that <kbd>mds</kbd> has detected metadata damage and taken the filesystem offline:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f09752de-1e15-4ebe-8ac7-822be060c44b.png" style="width:25.25em;height:15.33em;"/></p>
<p>To get more information on the damage, we can run the following command. Check the CephFS journal:</p>
<pre><strong>cephfs-journal-tool journal inspect</strong></pre>
<p><span>The following screenshot is the output for the preceding command:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f3f4ab78-93f3-4b3b-8d5f-190cf2cb03f8.png" style="width:25.42em;height:5.08em;"/></p>
<p>Yep, that's severely damaged, as expected.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CephFS metadata recovery</h1>
                </header>
            
            <article>
                
<p>Normally it would be suggested to export the journal for safe-keeping to minimize data loss, but in this case as we know we can safely just reset it straight away:</p>
<pre><strong><span class="n">cephfs</span><span class="o">-</span><span class="n">journal</span><span class="o">-</span><span class="n">tool</span> <span class="n">journal</span> <span class="n">reset</span></strong></pre>
<p><span>The following screenshot is the output for the preceding command:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ddb9cda4-559a-46f9-88e3-9723db0aaf32.png" style="width:33.75em;height:7.75em;"/></p>
<p>The next command resets the RADOS state of the filesystem to allow the recovery process to rebuild from a consistent state:</p>
<pre><strong>ceph fs reset cephfs --yes-i-really-mean-it</strong></pre>
<p>Next, the MDS tables are reset to enable them to be generated from scratch. These tables are stored as objects in the metadata pool. The following commands create new objects:</p>
<pre style="padding-left: 30px"><strong> cephfs-table-tool all reset session</strong><br/><strong> cephfs-table-tool all reset snap</strong><br/><strong> cephfs-table-tool all reset inode</strong></pre>
<p><span>The following screenshot is the output for the preceding command:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4cd27b59-123f-4db7-a783-b055cf9eb70b.png" style="width:23.42em;height:19.25em;"/></p>
<p>Reset the CephFS journal:</p>
<pre><strong>cephfs-journal-tool --rank=0 journal reset</strong></pre>
<p>Finally, create the root inodes and prepare for data-object discovery:</p>
<pre><strong>cephfs-data-scan init</strong></pre>
<p><span>Now that the state of CephFS has been fully reset, scans of the data pool can be undertaken to rebuild the metadata from the available data objects. This is a three-stage process using the following three commands. The first command scans through the data pool, finds all the extents that make up each file, and stores this as temporary data. Information, such as creation time and file size is also calculated and stored. The second stage then searches through this temporary data and rebuilds inodes into the metadata pool. Finally the linking of the inodes occurs:</span></p>
<pre><strong>cephfs-data-scan scan_extents cephfs_data</strong><br/><strong>cephfs-data-scan scan_inodes cephfs_data</strong><br/><strong>cephfs-data-scan scan_links</strong></pre>
<div class="packt_tip packt_infobox">The scan inodes and extents commands can take an extremely long time to run on large filesystems. The operations can be run in parallel to speed the process up; check out the official Ceph documentation for more information.</div>
<p>Once the process is complete, check that the CephFS filesystem is now in a healthy state:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e9579401-5845-4773-a473-28a62880372b.png" style="width:27.67em;height:13.75em;"/></p>
<p>We should also now be able to browse the filesystem from the mount point where we mounted it at the start of this section:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0042412f-2cd6-4f84-bf19-4d45169a27a7.png" style="width:28.25em;height:10.50em;"/></p>
<p>Note that although the recovery tools have managed to locate the files and rebuild some of their metadata, information such as their name has been lost and hence placed inside the <kbd>lost+found</kbd> directory. By examining the contents of the files, we could identify which file is which and rename it to the original filename.</p>
<p>In practice, although we have restored the CephFS filesystem, the fact that we are missing the files' original names and directory location likely means recovery is only partially successful. It should also be noted that the recovered filesystem may not be stable and it is highly recommended that any salvaged files be recovered before the filesystem is trashed and rebuilt. This is a disaster-recovery process that should only be used after ruling out restoring from backups.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Lost objects and inactive PGs</h1>
                </header>
            
            <article>
                
<p>This section of the chapter will cover the scenario where a number of OSDs have gone offline in a short period of time, leaving some objects with no valid replica copies. It's important to note that there is a difference between an object that has no remaining copies and an object that has a remaining copy, but it is known that another copy has had more recent writes. The latter is normally seen when running the cluster with <kbd>min_size</kbd> set to <kbd>1</kbd>.</p>
<p>To demonstrate how to recover an object that has an out-of-date copy of data, let's perform a series of steps to break the cluster:</p>
<ol>
<li>Set <kbd>min_size</kbd> to <kbd>1</kbd>; hopefully by the end of this example, you will see why you don't ever want to do this in real life:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo ceph osd pool set rbd min_size 1</strong></pre>
<p style="padding-left: 60px"><span>The following screenshot is the output for the preceding command:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/680a90b5-efe5-4664-acf9-7157cb8cd463.png" style="width:55.00em;height:5.00em;"/></p>
<ol start="2">
<li>Create a test object that we will make later make Ceph believe is lost:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo rados -p rbd put lost_object logo.png<br/></strong><strong>sudo ceph osd set norecover<br/></strong><strong>sudo ceph osd set nobackfill</strong></pre>
<p style="padding-left: 60px">These two flags make sure that when the OSDs come back online after making the write to a single OSD, the changes are not recovered. Since we are only testing with a single option, we need these flags to simulate the condition in real life, where it's likely that not all objects can be recovered in sufficient time before the OSD, when the only copy goes offline for whatever reason.</p>
<ol start="3">
<li>Shut down two of the OSD nodes, so only one OSD is remaining. Since we have set <kbd>min_size</kbd> to <kbd>1</kbd>, we will still be able to write data to the cluster. You can see that the Ceph status shows that the two <span>OSDs </span>are now down:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/02b12728-2ab5-435a-846e-f5cfc5ac05f6.png"/></p>
<ol start="4">
<li>Write to the object again, the write will go to the remaining OSD:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo rados -p rbd put lost_object logo.png</strong></pre>
<ol start="5">
<li>Shut down the remaining <span>OSDs</span>; once it has gone offline, power back the remaining two <span>OSDs</span>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3dc3fc96-c9ea-410d-83c9-0f22e11fdd7b.png" style="width:57.33em;height:23.33em;"/></div>
<p style="padding-left: 60px">You can see that Ceph knows that it already has an unfound object even before the recovery process has started. This is because during the peering phase, the PG containing the modified object knows that the only valid copy is on <kbd>osd.0</kbd>, which is now offline.</p>
<ol start="6">
<li>Remove the <kbd>nobackfill</kbd> and <kbd>norecover</kbd> flags, and let the cluster try to perform recovery. You will see that even after the recovery has progressed, there will be one PG in a degraded state, and the unfound object warning will still be present. This is a good thing, as Ceph is protecting your data from corruption. Imagine what would happen if a 4 MB chunk of an RBD that contained a database suddenly went back in time.</li>
</ol>
<p>If you try to read or write to our test object, you will notice the request will just hang; this is Ceph protecting your data. There are three ways to fix this problem. The first solution and the most ideal one is to get a valid copy of this object back online; this could either be done by bringing <kbd>osd.0</kbd> online, or by using the <kbd>objectstore</kbd> tool to export and import this object into a healthy OSD. For the purpose of this section, let's assume that neither of those options is possible. Before we cover the remaining two options, let's investigate further to uncover what is going on under the hood.</p>
<p>Run the Ceph health detail to find out which PG is having the problem:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4985dfe4-dd65-4f14-a96a-38319e8e59ee.png" style="width:51.83em;height:7.42em;"/></div>
<p>In this case, it's <kbd>pg 0.31</kbd>, which is in a degraded state, because it has an unfound object. Let's query the <kbd>pg</kbd>:</p>
<pre><strong>ceph pg 0.31 query</strong></pre>
<p><span>The following screenshot is the output for the preceding command:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8a0cc1a9-ac7a-4928-88c5-924fcc9e84b9.png" style="width:32.67em;height:14.17em;"/></p>
<p>Look for the recovery section; we can see that Ceph has tried to probe <kbd>"osd": "0"</kbd> for the object, but it is down. It has tried to probe <kbd>"osd": "1"</kbd> for the object, but for whatever reason it was of no use, we know the reason is that it is an out-of-date copy.</p>
<p>Now, let's look into some more detail on the missing object:</p>
<pre><strong>sudo ceph pg 0.31 list_missing</strong></pre>
<p><span>The following screenshot is the output for the preceding command:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/721776ae-9340-4867-a3cb-04b3690a84cf.png" style="width:27.17em;height:30.83em;"/></div>
<p>The <kbd>need</kbd> and <kbd>have</kbd> lines reveal the reason. We have <kbd>epoch 383'5</kbd>, but the valid copy of the object exists in <kbd>398'6</kbd>; this is why <kbd>min_size=1</kbd> is bad. You might be in a situation where you only have a single valid copy of an object. If this was caused by a disk failure, you would have bigger problems.</p>
<p>To recover from this, we have two options: we can either choose to use the older copy of the object or simply delete it. It should be noted that if this object is new and an older copy does not exist on the remaining <span>OSDs</span>, it will also delete the object.</p>
<p>To delete the object, run this:</p>
<pre><strong>ceph pg 0.31 mark_unfound_lost delete</strong></pre>
<p>To revert it, run this:</p>
<pre><strong>ceph pg 0.31 mark_unfound_lost revert</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recovering from a complete monitor failure</h1>
                </header>
            
            <article>
                
<p>In the unlikely event that you lose all of your monitors, all is not lost. You can rebuild the monitor database from the contents of the OSDs with the use of <kbd>ceph-objectstore-tool</kbd>.</p>
<p>To set the scenario, we will assume that an event has occurred and has corrupted all three monitors, effectively leaving the Ceph cluster inaccessible. To recover the cluster, we will shut down two of the monitors and leave a single failed monitor running. We will then rebuild the monitor database, overwrite the corrupted copy, and restart the monitor to bring the Ceph cluster back online.</p>
<p>The <kbd>objectstore</kbd> tool needs to be able to access every OSD in the cluster to rebuild the monitor database; in this example, we will use a script, which will connect via <kbd>ssh</kbd> to access the OSD data. As the OSD data is not accessible by every user, we will use the root user to log into the OSD hosts. By default, most Linux distributions will not allow remote, password-based root logins, so ensure you have copied your public <kbd>ssh</kbd> key to the root users on some remote OSD nodes.</p>
<p>The following script will connect to each of the OSD nodes specified in the hosts variable, and it will extract the data required to build the monitor database:</p>
<pre><strong>#!/bin/bash<br/></strong><strong>hosts="osd1 osd2 osd3"<br/></strong><strong>ms=/tmp/mon-store/<br/></strong><strong>mkdir $ms<br/></strong><strong># collect the cluster map from OSDs<br/></strong><strong>for host in $hosts; do<br/></strong><strong> echo $host<br/></strong><strong> rsync -avz $ms root@$host:$ms<br/></strong><strong> rm -rf $ms<br/></strong><strong> ssh root@$host &lt;&lt;EOF<br/></strong><strong> for osd in /var/lib/ceph/osd/ceph-*; do<br/></strong><strong> ceph-objectstore-tool --data-path \$osd --op update-mon-db --mon-store-path $ms<br/></strong><strong> done<br/></strong><strong>EOF<br/></strong><strong> rsync -avz root@$host:$ms $ms<br/></strong><strong>done</strong></pre>
<p>This will generate the following contents in the <kbd>/tmp/mon-store</kbd> directory:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0cca07f6-fdc4-4f29-a8e8-041ae47b8c4d.png" style="width:24.33em;height:3.50em;"/></p>
<p>We also need to assign new permissions via the <kbd>keyring</kbd>:</p>
<pre><strong>sudo ceph-authtool /etc/ceph/ceph.client.admin.keyring --create-keyring --gen-key -n client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *'</strong></pre>
<p><span>The following screenshot is the output for the preceding command:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/10feb3c6-60c8-478b-86dc-b78a329c1e95.png"/></p>
<pre><strong>sudo ceph-authtool /etc/ceph/ceph.client.admin.keyring --gen-key -n mon. --cap mon 'allow *'<br/></strong><strong>sudo cat /etc/ceph/ceph.client.admin.keyring</strong></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/35b21c1e-aa9b-4cec-8c54-609dd0eb3dc7.png" style="width:38.58em;height:11.08em;"/></p>
<p>Now that the monitor database is rebuilt, we can copy it to the monitor directory, but before we do so, let's take a backup of the existing database:</p>
<pre><strong>sudo mv /var/lib/ceph/mon/ceph-mon1/store.db /var/lib/ceph/mon/ceph-mon1/store.bak</strong></pre>
<p>Now, copy the rebuilt version:</p>
<pre><strong>sudo mv /tmp/mon-store/store.db /var/lib/ceph/mon/ceph-mon1/store.db<br/></strong><strong>sudo chown -R ceph:ceph /var/lib/ceph/mon/ceph-mon1</strong></pre>
<p>If you try to start the monitor now, it will get stuck in a probing state, as it tries to probe for other monitors. This is Ceph trying to avoid a split-brain scenario; however, in this case, we want to force it to form a quorum and go fully online. To do this, we need to edit <kbd>monmap</kbd>, remove the other monitors, and then inject it back into the monitors database:</p>
<pre><strong>sudo ceph-mon -i mon1 --extract-monmap /tmp/monmap</strong></pre>
<p>Check the contents of <kbd>monmap</kbd>:</p>
<pre><strong>sudo monmaptool /tmp/monmap –print</strong></pre>
<p><span>The following screenshot is the output for the preceding command:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6b94da93-d24d-438c-afb3-23d1d79c8a56.png" style="width:29.75em;height:9.83em;"/></p>
<p>You will see that there are three <kbd>mons</kbd> present, so let's remove two of them:</p>
<pre><strong>sudo monmaptool /tmp/monmap --rm noname-b<br/></strong><strong>sudo monmaptool /tmp/monmap --rm noname-c</strong></pre>
<p>Now, check again to make sure they are completely gone:</p>
<pre><strong>sudo monmaptool /tmp/monmap –print</strong></pre>
<p><span>The following screenshot is the output for the preceding command:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5f0a16e4-2500-4f5f-bd48-d7ad9b961af2.png" style="width:34.42em;height:8.83em;"/></p>
<pre><strong>sudo ceph-mon -i mon1 --inject-monmap /tmp/monmap</strong></pre>
<p>Restart all your OSDs, so they rejoin the cluster; then you will be able to successfully query the cluster status and see that your data is still there:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d1eb22c8-c9fd-478e-9029-788cdbb74f97.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the Ceph object-store tool</h1>
                </header>
            
            <article>
                
<p>Hopefully, if you have followed best practices, your cluster is running with three replicas and is not configured with any dangerous configuration options. Ceph, in most cases, should be able to recover from any failure.</p>
<p>However, in the scenario where a number of OSDs go offline, a number of PGs and/or objects may become unavailable. If you are unable to reintroduce these OSDs back into the cluster to allow Ceph to recover them gracefully, the data in those PGs is effectively lost. However, there is a possibility that the OSD is still readable to use the <kbd>objectstore</kbd> tool to recover the PGs contents. The process involves exporting the PGs from the failed OSDs and then importing the PGs back into the cluster. The <kbd>objectstore</kbd> tool requires that the OSDs' internal metadata is still in a consistent state, so full recovery is not guaranteed.</p>
<p>In order to demonstrate the use of the <kbd>objectstore</kbd> tool, we will shut down two of our three test cluster OSDs, and then recover the missing PGs back into the cluster. In real life, it's unlikely you would be facing a situation where every single PG from the failed OSDs is missing, but for demonstration purposes, the required steps are the same:</p>
<ol>
<li>Set the pool size to <kbd>2</kbd>, so we can make sure that we lose all the copies of some PGs when we stop the OSD service:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/023010d3-2bab-462a-a54e-eaee3b8ffe27.png" style="width:34.83em;height:3.67em;"/></div>
<ol start="2">
<li>Shut down two of the OSD services, and you will see from the Ceph status screen that the number of PGs will go offline:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8e86c55f-84bc-4853-85e5-87ae38a2d9c3.png"/></p>
<ol start="3">
<li>Running a Ceph health detail will also show which PGs are in a degraded state:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/43a5b418-5f23-48c5-9549-f8836f399cd7.png" style="width:34.25em;height:14.58em;"/></p>
<p style="padding-left: 60px">The stale PGs are the ones that no longer have a surviving copy, and it can be seen that the acting OSD is the one that was shut down.</p>
<p style="padding-left: 60px">If we use <kbd>grep</kbd> to filter out just the stale PGs, we can use the resulting list to work out what PGs we need to recover. If the OSDs have actually been removed from the cluster, the PGs will be listed as incomplete rather than stale.</p>
<ol start="4">
<li>Check the OSD to make sure the PG exists in it:</li>
</ol>
<pre style="padding-left: 60px"><strong>ceph-objectstore-tool --op list-pgs --data-path /var/lib/ceph/osd/ceph-0</strong></pre>
<p style="padding-left: 60px"><span>The following screenshot is the output for the preceding command:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c2f4b88f-9d71-4e1f-9381-f38af27cd549.png" style="width:55.58em;height:13.75em;"/></p>
<ol start="5">
<li>Use the <kbd>objectstore</kbd> tool to export the <kbd>pg</kbd> to a file. As the amount of data in our test cluster is small, we can just export the data to the OS disk. In real life, you probably want to consider connecting additional storage to the server. USB disks are ideal for this, as they can easily be moved between servers as part of the recovery process:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo ceph-objectstore-tool --op export --pgid 0.2a --data-path /var/lib/ceph/osd/ceph-2 --file 0.2a_export</strong></pre>
<p style="padding-left: 60px"><span>The following screenshot is the output for the preceding command:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7ff2b4b2-413c-46c2-9aa1-9b29c935605d.png"/></p>
<p style="padding-left: 60px">If you experience an assert while running the tool, you can try running it with the <kbd>--skip-journal-replay</kbd> flag, which will skip replaying the journal into the OSD. If there was any outstanding data in the journal, it will be lost. But this may allow you to recover the bulk of the missing PGs that would have otherwise been impossible. And repeat this until you have exported all the missing PGs.</p>
<ol start="6">
<li>Import the missing PGs back into an operating OSD. While we could import the PGs into an existing OSD, it is much safer to perform the import on a new OSD, so we don't risk further data loss. For this, create a directory-based OSD on the disk used by the failed OSD. It's highly recommended in a real disaster scenario that the data would be inserted into an OSD running on a separate disk, rather than using an existing OSD. This is done so that there is no further risk to any data in the Ceph cluster.</li>
</ol>
<p style="padding-left: 60px">Also, it doesn't matter that the PGs that are being imported are all inserted into the same temporary OSD. As soon as Ceph discovers the objects, it will recover them to the correct location in the cluster.</p>
<ol start="7">
<li>Create a new empty folder for the OSD:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo mkdir /var/lib/ceph/osd/ceph-2/tmposd/</strong></pre>
<ol start="8">
<li>Use <kbd>ceph-disk</kbd> or <kbd>ceph-volume</kbd> to prepare the directory for Ceph:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo ceph-disk prepare  /var/lib/ceph/osd/ceph-2/tmposd/</strong></pre>
<ol start="9">
<li>Change the ownership of the folder to the <kbd>ceph</kbd> user and the group:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo chown -R ceph:ceph /var/lib/ceph/osd/ceph-2/tmposd/</strong></pre>
<ol start="10">
<li>Activate the OSD to bring it online:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo ceph-disk activate  /var/lib/ceph/osd/ceph-2/tmposd/</strong></pre>
<ol start="11">
<li>Set the weight of the OSD to stop any objects from being backfilled into it:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo ceph osd crush reweight osd.3 0</strong></pre>
<ol start="12">
<li>Proceed with the PG import, specifying the temporary OSD location and the PG files that we exported earlier:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo ceph-objectstore-tool --op import --data-path /var/lib/ceph/osd/ceph-3 --file 0.2a_export</strong></pre>
<p style="padding-left: 60px">The following screenshot is the output for the preceding command:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d99e0733-217c-47b3-ad51-6e1f9becc339.png"/></p>
<ol start="13">
<li>Repeat this for every PG that you exported previously. Once complete, reset file ownership and restart the new temp OSD:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo chown -R ceph:ceph /var/lib/ceph/osd/ceph-2/tmposd/<br/></strong><strong>sudo systemctl start ceph-osd@3</strong></pre>
<ol start="14">
<li>Check the Ceph status output, you will see that your PGs are now active, but in a degraded state. In the case of our test cluster, there are not sufficient OSDs to allow the objects to recover to the correct amount of copies. If there were more OSDs in the cluster, the objects would then be backfilled around the cluster and would recover to full health with the correct number of copies:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2dbf57f8-5e7b-4825-ada7-e60441f96a32.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Investigating asserts</h1>
                </header>
            
            <article>
                
<p>Assertions are used in Ceph to ensure that, during the execution of the code, any assumptions that have been made about the operating environment remain true. These assertions are scattered throughout the Ceph code and are designed to catch any conditions that may go on to cause further problems if the code is not stopped.</p>
<p>If you trigger an assertion in Ceph, it's likely that some form of data has a value that is unexpected. This may be caused by some form or corruption or unhandled bug.</p>
<p>If an OSD causes an assert and refuses to restart, the usual recommended approach would be to destroy the OSD, recreate it, and then let Ceph backfill objects back to it. If you have a reproducible failure scenario, it is probably also worth filing a bug in the Ceph bug tracker.</p>
<p>As mentioned, OSDs can fail either due to hardware or software faults in either the stored data or OSD code. Software faults are much more likely to affect multiple OSDs at once; if your OSDs have become corrupted due to a power outage, it's highly likely more than one OSD will be affected. In the case where multiple OSDs are failing with asserts and they are causing one or more PGs in the cluster to be offline, simply recreating the OSDs is not an option. The OSDs that are offline contain all the three copies of the PG, so recreating the OSDs would make any form of recovery impossible and result in permanent data loss.</p>
<p>Before attempting the recovery techniques in this chapter, such as exporting and importing PGs, investigation into the asserts should be done. Depending on your technical ability and how much downtime you can tolerate before you need to start focusing on other recovery steps, investigating the asserts may not result in any success. By investigating the assert and looking through the Ceph source referenced by the assert, it may be possible to identify the cause of the assert. If this is possible, a fix can be implemented in the Ceph code to avoid the OSD asserting. Don't be afraid to reach out to the community for help on these matters.</p>
<p>In some cases, the OSD corruption may be so severe that even the <kbd>objectstore</kbd> tool itself may assert when trying to read from the OSD. This will limit the recovery steps outlined in this chapter, and trying to fix the reason behind the assert might be the only option. Although by this point, it is likely that the OSD has sustained heavy corruption, and recovery may not be possible.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example assert</h1>
                </header>
            
            <article>
                
<p>The following assert was taken from the Ceph user's mailing list:</p>
<pre><strong>2017-03-02 22:41:32.338290 7f8bfd6d7700 -1 osd/ReplicatedPG.cc: In function 'void ReplicatedPG::hit_set_trim(ReplicatedPG::RepGather*, unsigned int)' thread 7f8bfd6d7700 time 2017-03-02 22:41:32.335020<br/><br/></strong><strong>osd/ReplicatedPG.cc: 10514: FAILED assert(obc)<br/><br/><br/></strong><strong>ceph version 0.94.7 (d56bdf93ced6b80b07397d57e3fa68fe68304432)<br/></strong><strong> 1: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x85) [0xbddac5]<br/></strong><strong> 2: (ReplicatedPG::hit_set_trim(ReplicatedPG::RepGather*, unsigned int)+0x75f) [0x87e48f]<br/></strong><strong> 3: (ReplicatedPG::hit_set_persist()+0xedb) [0x87f4ab]<br/></strong><strong> 4: (ReplicatedPG::do_op(std::tr1::shared_ptr&lt;OpRequest&gt;&amp;)+0xe3a) [0x8a0d1a]<br/></strong><strong> 5: (ReplicatedPG::do_request(std::tr1::shared_ptr&lt;OpRequest&gt;&amp;, ThreadPool::TPHandle&amp;)+0x68a) [0x83be4a]<br/></strong><strong> 6: (OSD::dequeue_op(boost::intrusive_ptr&lt;PG&gt;, std::tr1::shared_ptr&lt;OpRequest&gt;, ThreadPool::TPHandle&amp;)+0x405) [0x69a5c5]<br/></strong><strong> 7: (OSD::ShardedOpWQ::_process(unsigned int, ceph::heartbeat_handle_d*)+0x333) [0x69ab33]<br/></strong><strong> 8: (ShardedThreadPool::shardedthreadpool_worker(unsigned int)+0x86f) [0xbcd1cf]<br/></strong><strong> 9: (ShardedThreadPool::WorkThreadSharded::entry()+0x10) [0xbcf300]<br/></strong><strong> 10: (()+0x7dc5) [0x7f8c1c209dc5]<br/></strong><strong> 11: (clone()+0x6d) [0x7f8c1aceaced]</strong></pre>
<p>The top part of the assert shows the function from where the assert was triggered and also the line number and file where the assert can be found. In this example, the <kbd>hit_set_trim</kbd> function is apparently the cause of the assert. We can look into the <kbd>ReplicatedPG.cc</kbd> file around line 10,514 to try to understand what might have happened. Note the version of the Ceph release (0.94.7), as the line number in GitHub will only match if you are looking at the same version.</p>
<p>From looking at the code, it appears that the returned value from the <kbd>get_object_context</kbd> function call is directly passed to the <kbd>assert</kbd> function. If the value is zero <span>–</span> indicating the object containing the hit-set to be trimmed could not be found <span>–</span> the OSD will assert. From this information, there is a chance that investigation could be done to work out why the object is missing and recover it. Or the <kbd>assert</kbd> command could be commented out to see whether it allows the OSD to continue functioning. In this example, allowing the OSD to continue processing will likely not cause an issue, but in other cases, an assert may be the only thing stopping more serious corruption from occurring. If you don't 100% understand why something is causing an assert, and the impact of any potential change you might make, seek help before continuing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you learned how to troubleshoot Ceph when all looks lost. If Ceph is unable to recover PGs itself, you now understand how to manually rebuild PGs from failed OSDs. You can also rebuild the monitor's database if you lose all of your monitor nodes but still have access to your OSDs. You explored the process of recreating RBDs from the raw data remaining on your OSDs. Finally, you configured two separate Ceph clusters and configured replication between them using RBD mirroring to provide a failover option, should you encounter a complete Ceph cluster failure.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What Ceph daemon allows RBDs to be replicated to another Ceph cluster?</li>
<li>True or false: RBDs by default are just a concatenated string of objects.</li>
<li>What tool can be used to export or import a PG from or to an OSD?</li>
<li>True or false: An unfound object status means that data has been lost forever.</li>
<li>What is the main disadvantage you are left with after rebuilding CephFS metadata?</li>
</ol>


            </article>

            
        </section>
    </body></html>