["```\nnodes = [\n { :hostname => 'ansible', :ip => '192.168.0.40', :box => 'xenial64' },\n  { :hostname => 'site1-mon1', :ip => '192.168.0.41', :box => 'xenial64' },\n  { :hostname => 'site2-mon1', :ip => '192.168.0.42', :box => 'xenial64' },\n  { :hostname => 'site1-osd1',  :ip => '192.168.0.51', :box => 'xenial64', :ram => 1024, :osd => 'yes' },\n  { :hostname => 'site2-osd1',  :ip => '192.168.0.52', :box => 'xenial64', :ram => 1024, :osd => 'yes' }\n]\n```", "```\ngit clone https://github.com/ceph/ceph-ansible.git\ncp -a ceph-ansible ~/ceph-ansible2\n```", "```\nansible-playbook -K -i hosts site.yml\n```", "```\nsudo apt-get install rbd-mirror\n```", "```\nsudo chown ceph:ceph /etc/ceph/remote.client.admin.keyring\n```", "```\nsudo rbd --cluster ceph mirror pool enable rbd image\n```", "```\nsudo rbd --cluster remote mirror pool enable rbd image\n```", "```\nsudo rbd --cluster ceph mirror pool peer add rbd client.admin@remote\n```", "```\nsudo rbd --cluster ceph mirror pool peer add rbd client.admin@remote\n```", "```\nsudo rbd create mirror_test --size=1G\n```", "```\nsudo rbd feature enable rbd/mirror_test journaling\n```", "```\nsudo rbd mirror image enable rbd/mirror_test\n```", "```\nsudo systemctl enable ceph-rbd-mirror@admin sudo systemctl start ceph-rbd-mirror@admin\n```", "```\nsudo rbd --cluster remote mirror pool status rbd –verbose\n```", "```\nsudo rbd-nbd map mirror_test\n```", "```\nsudo mkfs.ext4 /dev/nbd0\n```", "```\nsudo mount /dev/nbd0 /mnt echo This is a test | sudo tee /mnt/test.txt sudo umount /mnt sudo rbd-nbd unmap /dev/nbd0 Now lets demote the RBD on the primary cluster and promote it on the\nsecondary sudo rbd --cluster ceph mirror image demote rbd/mirror_test sudo rbd --cluster remote mirror image promote rbd/mirror_test\n```", "```\ngit clone https://gitlab.lbader.de/kryptur/ceph-recovery.git\n```", "```\nsudo apt-get install sshfs\n```", "```\ncd ceph-recovery sudo mkdir osds sudo mkdir osds/ceph-0 sudo mkdir osds/ceph-1 sudo mkdir osds/ceph-2\n```", "```\nsudo sshfs vagrant@osd1:/var/lib/ceph/osd/ceph-0 osds/ceph-0 sudo sshfs vagrant@osd2:/var/lib/ceph/osd/ceph-2 osds/ceph-2 sudo sshfs vagrant@osd3:/var/lib/ceph/osd/ceph-1 osds/ceph-1\n```", "```\nmkdir /mnt/osd-0\n```", "```\nceph-objectstore-tool --op fuse --data-path /var/lib/ceph/osd/ceph-0 --mountpoint /mnt/osd-0\n```", "```\nsudo sshfs vagrant@osd1:/mnt/osd-0 osds/ceph-0 sudo sshfs vagrant@osd2:/mnt/osd-1 osds/ceph-1 sudo sshfs vagrant@osd3:/mnt/osd-2 osds/ceph-2\n```", "```\nsudo ./collect_files.sh osds\n```", "```\nsudo ./assemble.sh vms/test.id 1073741824 .\n```", "```\nwget https://raw.githubusercontent.com/fiskn/assemble_bluestore_rbd/master/assemble_bluestore.sh\nchmod +x ./assemble_bluestore.sh\n```", "```\n./assemble_bluestore.sh 7d8ad6b8b4567 1073741824 text.img \n```", "```\nsudo e2fsck test.raw\n```", "```\nsudo mount -o loop test.raw /mnt\n```", "```\nsudo -i\nmount -t ceph 192.168.0.41:/ /mnt/cephfs -o name=admin,secret=AQC4Q85btsqTCRAAgzaNDpnLeo4q/c/q/0fEpw==\n```", "```\necho \"doh\" > /mnt/cephfs/doh\necho \"ray\" > /mnt/cephfs/ray\necho \"me\" > /mnt/cephfs/me\n```", "```\nrados purge cephfs_metadata --yes-i-really-really-mean-it\n```", "```\nsystemctl restart ceph-mds@*\n```", "```\ncephfs-journal-tool journal inspect\n```", "```\ncephfs-journal-tool journal reset\n```", "```\nceph fs reset cephfs --yes-i-really-mean-it\n```", "```\n cephfs-table-tool all reset session\n cephfs-table-tool all reset snap\n cephfs-table-tool all reset inode\n```", "```\ncephfs-journal-tool --rank=0 journal reset\n```", "```\ncephfs-data-scan init\n```", "```\ncephfs-data-scan scan_extents cephfs_data\ncephfs-data-scan scan_inodes cephfs_data\ncephfs-data-scan scan_links\n```", "```\nsudo ceph osd pool set rbd min_size 1\n```", "```\nsudo rados -p rbd put lost_object logo.png sudo ceph osd set norecover sudo ceph osd set nobackfill\n```", "```\nsudo rados -p rbd put lost_object logo.png\n```", "```\nceph pg 0.31 query\n```", "```\nsudo ceph pg 0.31 list_missing\n```", "```\nceph pg 0.31 mark_unfound_lost delete\n```", "```\nceph pg 0.31 mark_unfound_lost revert\n```", "```\n#!/bin/bash hosts=\"osd1 osd2 osd3\" ms=/tmp/mon-store/ mkdir $ms # collect the cluster map from OSDs for host in $hosts; do\n echo $host rsync -avz $ms root@$host:$ms rm -rf $ms ssh root@$host <<EOF for osd in /var/lib/ceph/osd/ceph-*; do ceph-objectstore-tool --data-path \\$osd --op update-mon-db --mon-store-path $ms done EOF\n rsync -avz root@$host:$ms $ms done\n```", "```\nsudo ceph-authtool /etc/ceph/ceph.client.admin.keyring --create-keyring --gen-key -n client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *'\n```", "```\nsudo ceph-authtool /etc/ceph/ceph.client.admin.keyring --gen-key -n mon. --cap mon 'allow *' sudo cat /etc/ceph/ceph.client.admin.keyring\n```", "```\nsudo mv /var/lib/ceph/mon/ceph-mon1/store.db /var/lib/ceph/mon/ceph-mon1/store.bak\n```", "```\nsudo mv /tmp/mon-store/store.db /var/lib/ceph/mon/ceph-mon1/store.db sudo chown -R ceph:ceph /var/lib/ceph/mon/ceph-mon1\n```", "```\nsudo ceph-mon -i mon1 --extract-monmap /tmp/monmap\n```", "```\nsudo monmaptool /tmp/monmap –print\n```", "```\nsudo monmaptool /tmp/monmap --rm noname-b sudo monmaptool /tmp/monmap --rm noname-c\n```", "```\nsudo monmaptool /tmp/monmap –print\n```", "```\nsudo ceph-mon -i mon1 --inject-monmap /tmp/monmap\n```", "```\nceph-objectstore-tool --op list-pgs --data-path /var/lib/ceph/osd/ceph-0\n```", "```\nsudo ceph-objectstore-tool --op export --pgid 0.2a --data-path /var/lib/ceph/osd/ceph-2 --file 0.2a_export\n```", "```\nsudo mkdir /var/lib/ceph/osd/ceph-2/tmposd/\n```", "```\nsudo ceph-disk prepare  /var/lib/ceph/osd/ceph-2/tmposd/\n```", "```\nsudo chown -R ceph:ceph /var/lib/ceph/osd/ceph-2/tmposd/\n```", "```\nsudo ceph-disk activate  /var/lib/ceph/osd/ceph-2/tmposd/\n```", "```\nsudo ceph osd crush reweight osd.3 0\n```", "```\nsudo ceph-objectstore-tool --op import --data-path /var/lib/ceph/osd/ceph-3 --file 0.2a_export\n```", "```\nsudo chown -R ceph:ceph /var/lib/ceph/osd/ceph-2/tmposd/ sudo systemctl start ceph-osd@3\n```", "```\n2017-03-02 22:41:32.338290 7f8bfd6d7700 -1 osd/ReplicatedPG.cc: In function 'void ReplicatedPG::hit_set_trim(ReplicatedPG::RepGather*, unsigned int)' thread 7f8bfd6d7700 time 2017-03-02 22:41:32.335020 osd/ReplicatedPG.cc: 10514: FAILED assert(obc) ceph version 0.94.7 (d56bdf93ced6b80b07397d57e3fa68fe68304432)\n 1: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x85) [0xbddac5] 2: (ReplicatedPG::hit_set_trim(ReplicatedPG::RepGather*, unsigned int)+0x75f) [0x87e48f] 3: (ReplicatedPG::hit_set_persist()+0xedb) [0x87f4ab] 4: (ReplicatedPG::do_op(std::tr1::shared_ptr<OpRequest>&)+0xe3a) [0x8a0d1a] 5: (ReplicatedPG::do_request(std::tr1::shared_ptr<OpRequest>&, ThreadPool::TPHandle&)+0x68a) [0x83be4a] 6: (OSD::dequeue_op(boost::intrusive_ptr<PG>, std::tr1::shared_ptr<OpRequest>, ThreadPool::TPHandle&)+0x405) [0x69a5c5] 7: (OSD::ShardedOpWQ::_process(unsigned int, ceph::heartbeat_handle_d*)+0x333) [0x69ab33] 8: (ShardedThreadPool::shardedthreadpool_worker(unsigned int)+0x86f) [0xbcd1cf] 9: (ShardedThreadPool::WorkThreadSharded::entry()+0x10) [0xbcf300] 10: (()+0x7dc5) [0x7f8c1c209dc5] 11: (clone()+0x6d) [0x7f8c1aceaced]\n```"]