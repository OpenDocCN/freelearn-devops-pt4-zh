<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">GitLab Vision - the Whole Toolchain in One Application</h1>
                </header>
            
            <article>
                
<p>This chapter is intended to provide more insight into the background against which GitLab has arisen. The product was literally created to help solve a number of problems that the Agile movement experienced. We will talk about the history of development methodologies and the rise of Agile as the dominant way to develop software. <span><span>The Agile methodologies</span></span> spilled over to the traditional operations department, which then led to the DevOps movement. Finally, we will summarize a number of tools that are part of the DevOps way of working.</p>
<p>In this chapter, we will cover following topics:</p>
<ul>
<li class="mce-root"><span>The Agile Manifesto</span></li>
<li class="mce-root"><strong>Extreme Programming</strong> (<strong>XP</strong>)</li>
<li class="mce-root"><span>The DevOps movement</span></li>
<li class="mce-root"><span>The toolchain</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements </h1>
                </header>
            
            <article>
                
<div>
<p><span>To follow along with the instructions in this chapter, please download the Git repository with examples available from GitHub at <a href="https://github.com/PacktPublishing/Mastering-GitLab-12/tree/master/Chapter09" target="_blank">https://github.com/PacktPublishing/Mastering-GitLab-12/tree/master/Chapter09</a></span>.<a href="https://github.com/PacktPublishing/Mastering-GitLab-12/tree/master/Chapter08"/></p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Agile Manifesto</h1>
                </header>
            
            <article>
                
<p>Some people in the 1990s had a problem with the classic engineering mindset of comparing software development to build engineering. Instead of trying to keep the requirements stable and not let them get out of hand through requirements creep or scope creep, they looked for a process that did not depend on the stability of requirement. Those people came up with a number of different ideas in response, and those methods are commonly known as lightweight methods. All of these form the Agile movement, together with lean manufacturing methods, and have grown very popular over time.</p>
<p>Now, what is it about Agile that everyone says nowadays? For some, it's way of life and of looking at things. In IT, according to Google, it is a method of project management, used especially for software development, that is characterized by the division of tasks into short phases of work and the frequent reassessment and adaptation of plans.</p>
<p>Officially, Agile is not a method, but a collective term. Agile was born from several other methods to develop products (mostly software). Examples include XP, Scrum, the <strong>Dynamic Systems Development Method</strong> (<strong>DSDM</strong>), Adaptive Software Development, and Crystal. These methods share the common characteristic that they all aim for less bureaucracy during product and software development, and embrace change. In the 1980s and 1990s, these separate methods were developed by various experts, who eventually set up the Agile Manifesto in 2001. These were mainly intended to prevent problems with the application of traditional waterfall methods.</p>
<p>All of the big names from different Agile disciplines eventually decided to come together informally and discuss ways to help IT improve. The Agile Manifesto was drawn up during this meeting of 17 software developers. It took place from <span>February </span>11 to February 13, 2001, at The Lodge in Snowbird, Utah. The name Agile was also chosen here. Word has it that the name lightweight method was on the table, but Agile was eventually chosen.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The initial model – waterfall</h1>
                </header>
            
            <article>
                
<p>When people talk about the origins of the waterfall model, it is often said that W. W. Royce introduced it in 1970 in the paper, <em>Managing the Development of Large Software Systems</em> (<a href="http://www-scf.usc.edu/~csci201/lectures/Lecture11/royce1970.pdf">http://www-scf.usc.edu/~csci201/lectures/Lecture11/royce1970.pdf</a>). Royce himself actually believed in the iterative approach to software development and did not even use the term waterfall. Royce described the waterfall model as a method he thought was too extreme – and even an proposition doomed to fail: <em>"I believe in this concept, but the implementation described above is risky and invites failure"</em>, Royce wrote.</p>
<p>In 1970, Royce thought that the waterfall method had to be seen as an initial concept, as he felt there were errors in the method. He published a document examining how the initial concept could be developed into a repeated method. In this enhanced model, there was more feedback between each phase than the previous phase, as we now often see in the current methods. Annoyingly for Royce, only the initial method got attention, and the criticism he had on this method was largely ignored.</p>
<p>Royce's intention was to transform the model from the paper into an iterating model; still, the original method has been widely used and idealized. However, people who oppose this model think it is too basic and has no real practical use. The following diagram illustrates the <span>waterfall model:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/523b8e48-5f0e-40aa-8f99-7567fb2faaf7.png" style="width:22.08em;height:26.25em;"/></p>
<p>The waterfall model consists of the following phases:</p>
<ul>
<li><strong>Definition study/analysis (Requirements/analysis)</strong>: In this phase, the only goal is to search for requirements. Some research is done to clarify the purpose of the software.</li>
<li><strong>Basic design (Design)</strong>: In this phase, what has emerged during the first phase becomes clearer. The customer wishlist is put on paper and the user interface of the program is already being considered. Generally speaking, in this phase, it is recorded what the future system must do.</li>
</ul>
<ul>
<li><strong>Technical design/detail design (Design)</strong>: A prototype or minimal program can already be built using the basic design. During this phase, consideration is given to the possibilities of achieving the desired functionality technically. The options are already grouped in modules, functions, or programs.</li>
<li><strong>Construction/implementation (Coding)</strong>: In the construction phase, the actual source code is written for the program.</li>
<li><strong>To test (Testing)</strong>: In the testing phase, it is checked whether the software is built properly according to the design. Here, errors can also emerge that have already been <span>made in earlier stages. In the theoretical model, this should not happen.</span></li>
<li><strong>Integration (Testing)</strong>: The system should now be ready and tested. However, it should also work with other pieces of software or hardware. There are special tests for this that make sure the integration works.</li>
<li><strong>Management and maintenance (Maintenance)</strong>: In order to ensure that the system continues to operate and function according to specification, maintenance will have to be carried out.</li>
</ul>
<p>To summarize, the waterfall model consists of different phases. Each phase has its own level that also determines the sequence. The highest level is executed first before the following, lower phases. This is equal to the natural effect of a waterfall, hence the name.</p>
<p>To mitigate the cons of the original method, several enhanced forms were developed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Royce's model</h1>
                </header>
            
            <article>
                
<p>Royce's model describes a different waterfall model that can go back to previous phases. Often, it will become apparent at a particular phase that something went wrong in a previous phase (this will most commonly surface in testing phases). It should then be possible to go back to a previous phase easily. 99% of the time, changes to the design have to be made – nobody is perfect, so this model is more realistic. Nevertheless, Royce continued to emphasize the importance of good documentation for proper phase transitions.</p>
<p><span>The following diagram illustrates Royce's model</span><span>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/98c6319d-e8ff-465a-9e5d-59ba7af3a9e1.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The sashimi model</h1>
                </header>
            
            <article>
                
<div>
<p><span>The sashimi model, designed by Peter Degrace, is one of the models discussed in the book, <em>Wicked </em></span><em><span>Problems, Righteous Solutions: A Catalog of Modern Software Engineering Paradigms</span></em><span>. This model assumes the same phases as the waterfall model, but that they can overlap (even more than once). This way of working means that fewer resources are wasted. In the following diagram, you can see how the phases can overlap. What matters is that there are no hard endings of phases or gateways. You can see the current time as an example. Another aspect of this figure is that, in contrast to the waterfall model, the lead time is also included in the model. This is to indicate that you can already start designing, even if the analysis is not yet complete. It also means that you can go back to the analysis in the design phase.</span></p>
<p><span>The following figure illustrates the</span><span> sashimi model:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/67b2349c-bad9-4f02-8d16-7c30d294be81.png" style="width:27.83em;height:22.92em;"/></p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The V-model</h1>
                </header>
            
            <article>
                
<p>The V-model shows the phase transitions within software development, which offers the possibility of QA. For every phase transition, the developers and the customer make quality agreements about, for example, the designs. The V shape illustrates that at the bottom of the shape, after real implementation, there will be an ever-growing understanding of the problem that is being solved, and that initial ideas and requirements are tested against reality.</p>
<p><span>The following diagram illustrates the V-model </span><span>model:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/51d17d27-69f6-4e91-99ad-46a086571321.png" style="width:38.42em;height:22.17em;"/></p>
<p>The sequential way of working that is presented here in these methods fits classic engineering methods. For a long time during the 1970s and 1980s, it was believed that software engineering as a discipline should work in a similar way to how engineers build skyscrapers. Skyscrapers are built by first creating a detailed architectural plan before construction workers can carry out the building phase. To create an architecture and a plan in such a way, you need a very clear understanding of what needs to be built. What is important here is that the upfront requirements are clearly outlined and do not change along the way. If the expectations of real-world engineering projects changed as rapidly as they do in software, however, nothing would get built.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DSDM</h1>
                </header>
            
            <article>
                
<p>The <span>DSDM</span> is derived from the linear <strong>Software Development Methodology</strong> <em>(</em><strong>SDM</strong>) model, which originated in the 1990s. It was an answer to the problem that, in linear methods, the functionality to be developed is often available too late as a whole. If the functionality to be delivered can be divided into sub-functionalities, these sub-functionalities can be delivered separately. We call this incremental system development or step-by-step development. In the beginning you want to find out which functionality can be split off. This means that the first two phases from the waterfall approach are completed, but only when the partial functionalities are clear can these be developed step by step in parallel.</p>
<p>DSDM is more than just applying iterations at development stages. All of the remaining phases after analysis are iterative. In addition, the division of phases looks slightly different. In contrast to the linear development model, it is also possible to go back to previous phases.</p>
<p>The phases within DSDM are structured as follows:</p>
<ul>
<li>Feasibility study</li>
<li>Business analysis</li>
<li>Functional model iteration</li>
<li>Design and construction iteration</li>
<li>Implementation</li>
</ul>
<p>Timeboxing is one of the core practices used in DSDM to control each stage, which is combined with better prioritization with MoSCoW. We will explain this in the following section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Timeboxing</h1>
                </header>
            
            <article>
                
<p>In order to ensure that a project can be of service to the organization in time, timeboxes are used. For example, the functionality with the highest priority should always be delivered within a timebox. If there is time and space left, then there will be room for functionalities with a lower priority. A timebox is a time interval in which an intermediate product is delivered. During the project, the precise functionality is refined further. Due to growing insights and changing circumstances, the specifications of a functionality can also change.</p>
<p>A timebox is prepared, in which must-haves, should-haves, and (possibly) could-haves are defined. This layout allows you to create room for manoeuvre without affecting the end time of the timebox. In other words, in the case of changing insight or emergencies, you will be able to re-prioritize. This may be at the expense of should-have and could-have system requirements. By applying MoSCoW, you make these choices explicit.</p>
<p>With this technique, you can keep constant focus on functional requirements with the highest priority, monitor your time and budget, and still be able to act when insights change. Using this technique enables you to give priority to the system requirements that give a company the most benefit, and lower the priority for requirements that were derived from situations that may never occur. This also makes systems simpler in design—something that improves their maintainability.</p>
<p>The classification of requirements and wishes based on the MoSCoW classification also helps users to visualize the support for a project within an organization. There are now authors who think that you should consider projects as clusters of micro-projects. Every micro-project is a requirement or a wish with a certain priority. In many projects, the support from stakeholders starts to crumble when people are confronted with changes. If a project includes many must-haves, this risk will be smaller than when it largely consists of would-<span>h</span>aves.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">MoSCoW</h1>
                </header>
            
            <article>
                
<p>It often happens that a project is hindered by too many wishes from the user organization. However, a development strategy that provides feedback to the customer can prevent over-demanding the development organization. DSDM counteracts this by dividing the functional requirements and wishes into a number of categories in which the priority is indicated for each functionality.</p>
<p>DSDM uses the MoSCoW rules to determine the priorities for requirements and wishes.</p>
<p>MoSCoW stands for:</p>
<ul>
<li><strong>Must-have</strong>: This category has the highest priority, is guaranteed to be delivered, and counts as the engine of the information system.</li>
<li><strong>Should-have</strong>: A necessary requirement where a (temporary) workaround is possible.</li>
<li><strong>Could-have</strong>: A requirement with a clear added value, but without it there is still a usable system.</li>
<li><strong>Would-have (or want to have but won't have this time around)</strong>: This requirement can be missed, although it does not mean that it is not relevant; in the next increment, it can be a <strong>must-have</strong>.</li>
</ul>
<p>Nowadays, DSDM is not extensively used. The last decade has seen other methods gain a lot more popularity, especially Agile methods such as Scrum, which we will discuss next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scrum</h1>
                </header>
            
            <article>
                
<p>The next lightweight model we will touch upon in this book is called Scrum, which, as of 2018, has been widely adopted outside the IT realm. It is not a method such as DSDM and can be better described as a framework. It uses the paradigm of a sports team (rugby, to be exact), where a group of people work together to achieve a goal.<strong> </strong>In the rugby game, a scrum drives the ball into the game. The scrum group consists of five to eight players who operate as a unit. In the IT world, it is a group of people who create business value through close cooperation and coordination.</p>
<p>In Rugby, each player has a unique position; they play both roles in attack and defense, and they work as a team to get the ball to the other side. It can be compared to a situation in IT, where the degree of success of a scrum team depends on the different disciplines within the team and how they work together and coordinate with each other.</p>
<p>The Rugby comparison originates from a 1986 article from the Harvard Business Review, <span><em>The New Product Development Game</em>, where the authors Takeuchi and Nonaka introduced the term scrum in the context of product development. They argued that it would bring more speed and flexibility, and they based it on case studies done in several industries, notably the automotive industries.</span></p>
<p>In the early 1990s, Ken Schwaber and Jeff Sutherland started using scrum techniques in their companies, and eventually in 1995, they presented a paper describing the Scrum framework at a software design conference.</p>
<p>Scrum sets out the following values:</p>
<ul>
<li><strong>Commitment</strong>: The members must fully commit themselves to <span>the project</span>; it is not a part-time job.</li>
<li><strong>Focus</strong>: embers should focus on what needs to be done in each sprints.</li>
<li><strong>Openness<span> (Transparency)</span></strong>: People must keep each other well informed about progress and possible problems.</li>
<li><strong>Respect</strong>: Members must respect those with a different background and expertise and trust each other's good intent.</li>
<li><strong>Guts</strong>: Members must have the courage to say things, ask questions, and come up with new solutions.</li>
</ul>
<p>Scrum works with multidisciplinary teams who prefer to work in one room so that consultation is easy. The team is supervised by a scrum master, who has a facilitating role. The product owner is the customer or a client, or a representative thereof. He or she specifies the desired results, usually in the form of user stories. These user stories are kept in a list, the product backlog, or the work stock. The product owner sorts the work stock for priority. The most important user stories are at the top.</p>
<p>In Scrum, you work in sprints or iterations. These usually last from about a week to a month, with a duration of two weeks as the most common. Sprints are timeboxed. In other words, it is certain in advance how long a Sprint will last for and when it will end. At the beginning of a sprint, the user stories for that Sprint are determined and recorded in the sprint backlog.</p>
<p>Sprints provide results that are as tangible as possible. This means that the software development will provide usable code, including integration, tests, and documentation, that is understandable for the customer or end user.</p>
<p>At the end of a sprint, a sprint review takes place, where the result is shown to the product owner. In addition, an evaluation takes place within the team.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Crystal methods</h1>
                </header>
            
            <article>
                
<p>A Crystal method is a lightweight method with characteristics such as emphasis on people instead of processes and products, fast communication (preferably by working together in one room), the quick delivery of products, frequent and automatic testing, and regular evaluations.</p>
<p>Unlike some other software development processes, Crystal is not a software development method, but a collection of methods and processes. This collection is called the Crystal Family. Crystal was invented and described by Alistair Cockburn. Each member of the Crystal Family is indicated with a color representing the weight of a method, where the following applies: the darker the color, the heavier the method. The color of the method is chosen on the basis of the size and severity of the project. The size is determined by the number of people participating in the project, and the severity is determined by the risk that <span><span>choosing the method</span></span> could cause systemic damage. The colors are, like real crystals, sorted from light to dark. Crystal clear is the smallest and lightest, followed by yellow, orange, orange web, red, maroon, blue, violet, and so on.</p>
<p>Although the Crystal methods differ from each other, they do have some similarities, which is why they are also called a family. They have three common priorities: safety, efficiency, and usability. Furthermore, they also have common characteristics, the three most important of which are the frequent delivery of (intermediate) products, feedback on improvements, and good communication.</p>
<p>We have already discussed a team-based approach to product development with certain process steps, and now with Crystal methods, a lightweight Agile method that focuses on people within the team itself. There are also more radical methods that focus not only on people and process, but also on tooling and technical quality. We will discuss these XP<strong> </strong>methods in the following section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">XP</h1>
                </header>
            
            <article>
                
<p>One of the most important subcultures of the Agile movement is XP. The main founders of XP are Kent Beck, Ken Auer, Ward Cunningham, Martin Fowler, and Ron Jeffries. They developed XP during the <strong>Chrysler Comprehensive Compensation</strong> <span>(<strong>C3</strong>)</span> system project in 1996. It is very popular nowadays, and this is reflected in the culture of software development but also in its ways of working and the toolset it uses. We believe GitLab is, in many ways, the tool once imagined for XP. It is, in fact, so profound that we have decided to dedicate an entire chapter to it to explain its relevance for GitLab.</p>
<p>XP takes its name from the fact that a number of proven development principles (so-called best practices) are carried through to the extreme. The optimal power of XP stems from the application of the 12 best practices of software development. The best practices are grouped into four groups: fine feedback, continuous process, shared knowledge, and the well-being of the developers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fine-scale feedback</h1>
                </header>
            
            <article>
                
<p>One of the most important principles in XP is the usage of feedback mechanisms and trying to keep the feedback loops as small as possible. This starts at the planning stage, because feedback from a customer at this stage can already limit wasted time.</p>
<p>The fine-scale feedback group in XP includes four practices: planning game, pair programming, <strong>test-driven development</strong><span> (</span><strong>TDD</strong><span>)</span>, and whole team. We will discuss the feedback loop for each practice in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Planning game</h1>
                </header>
            
            <article>
                
<p>The planning takes place at the beginning of each iteration and consists of both release planning and iteration planning.</p>
<p>Release planning determines which functionality will be realized in which release. Both the developers and the users are present. </p>
<p>Release planning consists of the following three phases:</p>
<ul>
<li><strong>Exploration phase</strong>: Here, the users make a shortlist of the most important requirements for the new system. This happens in the form of user stories.</li>
<li><strong>Commitment phase</strong>: Here, it is decided which user stories will be included in the next release and when this release will be.</li>
<li><strong>Steering phase</strong>: Here, the plan can be modified, and new stories added and others removed.</li>
</ul>
<p>Iteration planning is when user stories included in the release schedule for an upcoming Sprint are divided into tasks for the developers. No users are involved, only developers.</p>
<p>Iteration planning also consists of three phases; they are as follows:</p>
<ul>
<li><strong>Exploration phase</strong>: Here, the stories are translated into tasks, which are then written on cards called task cards.</li>
<li><strong>Commitment phase</strong>: Here, the time taken to realize the tasks is estimated and the tasks are assigned to the developers (pairs).</li>
<li><strong>Steering phase</strong>: Here, the tasks are carried out, and the result is compared with the original time schedule of the user story.</li>
</ul>
<p>The purpose of this kind of planning is to ensure that a product can be delivered. It is not so much about delivering exact data as about delivering the product.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Release planning</h1>
                </header>
            
            <article>
                
<p>During this part of the planning game, customers and developers try to find out what will be included in the next release of the software and when this will take place. The focus is on creating user stories.</p>
<p>This part of the game consists of the following three phases:</p>
<ul>
<li><strong>Exploration phase</strong>: This is the process of gathering requirements and estimating the amount of time it will take to realize them. Activities in this phase includes the following:
<ul>
<li><strong>Writing a story (user story)</strong>: Here, the users come up with a problem or wish; during a consultation, the developers will try to fully understand this problem. On this basis, a user story is written. This is done by the users, indicating what they expect from a system. It is important that developers do not interfere.</li>
<li><strong>Estimating the user story (estimating a story)</strong>: The developers estimate how much time it will take to make this. The developers can now also designate short examinations, called spikes, to investigate parts of the problem or the solution direction. These spikes are used to achieve better time estimates and are thrown away as soon as the problem and/or the solution is clear to everyone.</li>
<li><strong>Splitting a user story</strong>: In this phase, the story must be completely clear and all ambiguities must be cleared up before iterative planning can be started. If the developers cannot give a time estimate for the story due to lack of clarity, the story must be split. If the users have described all their wishes, they can continue with the concept of decision-making, which is known as the commitment phase.</li>
</ul>
</li>
<li><strong>Commitment phase</strong>: In this phase, we will find out what the costs are, what the benefits are, and what schedule consequences they have. We create four different lists based on the way we sort the items, which are as follows:
<ul>
<li><strong>Sorting by value</strong>: Users put the user stories in order of what they consider important. They make the following three stacks:
<ul>
<li><strong>Critical</strong>: Without these stories, the system cannot work or has no value.</li>
<li><strong>Important</strong>: User stories that are important to the company.</li>
<li><strong>Nice to have</strong>: User stories in which less important characteristics are realized.</li>
</ul>
</li>
<li><strong>Sorting according to risk</strong>: Here, the developers give an estimate of the risks and sort the story accordingly. <span>All values ​​for the user story are added together, giving the user story a accumulated risk value of low (0-1), medium (2-4), or high (5-6</span>). The following is an example of this:</li>
</ul>
</li>
</ul>
<p style="padding-left: 150px">Completeness (do we have all the details about the table?) </p>
<ul>
<li style="list-style-type: none">
<ul>
<li style="list-style-type: none">
<ul>
<li>Full (0)</li>
<li>Incomplete (1)</li>
<li>Unknown (2)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p style="padding-left: 150px">Vulnerability (are changes likely?): </p>
<ul>
<li style="list-style-type: none">
<ul>
<li style="list-style-type: none">
<ul>
<li>Low (0)</li>
<li>Medium (1)</li>
<li>High (2)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p style="padding-left: 150px">Difficulty (how difficult is it to realize?): </p>
<ul>
<li style="list-style-type: none">
<ul>
<li style="list-style-type: none">
<ul>
<li>Simple (0)</li>
<li>Standard (1)</li>
<li>Difficult (2)</li>
</ul>
</li>
</ul>
</li>
</ul>
<ol>
<li style="list-style-type: none">
<ul>
<li><strong>Determining the development speed (velocity)</strong>: Here, the developers determine at what speed they can execute a project and sort items accordingly.</li>
<li><strong>Scope</strong>: Here, it is determined which user stories will be realized in the coming release. This is the final sort. On this basis, the release date is determined. The sort should be according to the value for the users (business value).</li>
</ul>
</li>
</ol>
<ul>
<li><strong>Steering phase</strong>: In this phase, the developers can steer the process together with the users. In other words, they can still change something, whether that be an individual user's story, or the importance of another particular story.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Iteration planning</h1>
                </header>
            
            <article>
                
<p>Depending on the speed of the team, it can be determined how many story points the team can do per iteration. Iterations can last from one to three weeks. The focus here is on creating tasks and prioritizing them. The iterations also have the same phases as the earlier release planning, as follows:</p>
<ul>
<li><strong>Exploration phase</strong>: During the research phase of iteration planning, the user stories are divided into tasks, and how long a task will take is estimated. The main activities in this phase include the following:
<ul>
<li>Translating the user story into tasks and writing them on task cards.</li>
<li>Adding or splitting tasks; in other words, if the developer cannot properly estimate how long a task will last because it is too big or too small, something will need to be altered.</li>
<li>Estimating the task, where an estimation of a task's execution is produced.</li>
</ul>
</li>
<li><strong>Commitment phase</strong>: In the assignment phase of iteration planning, the following tasks are distributed among the developers:<br/>
<ul>
<li>A developer (programmer) accepts a task: Each developer takes a task for which they then become responsible.</li>
<li>The developer gives a time schedule: Because the developer is now responsible, they are now best able to give a time estimate.</li>
<li>The effective working time is determined, outlining the number of hours that a developer or programmer can develop during an iteration. (For example, in a 40-hour working week, in which five hours of meetings are held, the effective working time becomes 35 hours.)</li>
<li>Balancing: Once all the tasks have been assigned, the number of hours each developer has received is also compared to how many hours they actually have available (this is also known as the load factor). The tasks may then be redistributed to ensure that each developer has roughly the same amount of work. If a developer has too much work, something will have to shift.</li>
</ul>
</li>
<li class="mce-root"><strong>Steering phase</strong>: The execution of tasks is done during the execution, or steering phase, of iteration planning. There is a bit of a game element to this, but the following steps are advised:
<ul>
<li><strong>Taking a task card</strong>: Here, the developer gets a card with the description of one of the tasks they have registered for.</li>
<li><strong>Finding a partner</strong>: The developer looks for a partner to develop the task with.</li>
<li><strong>Designing the task</strong>: If necessary, in this stage, a (short) design will be made.</li>
<li><strong>Writing the unit test</strong>: Before any programming, all tests must be ready. Preferably, these are automatic tests because they often have to be done every time source code is checked in.</li>
<li><strong>Writing the code</strong>: Here, the programmer or developer makes the program.</li>
<li><strong>Testing the program</strong>: The unit tests are performed.</li>
<li><strong>Refactoring</strong>: In this step, the refactoring rules are applied and the developers ensure their code meets the standards.</li>
<li><strong>Doing functional and integration tests</strong>: After the unit test, all possible other tests are run, such as integration tests, regression tests, and so on. The code must be adjusted in this step until the test succeeds.</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pair programming</h1>
                </header>
            
            <article>
                
<p>XP states that, ultimately, everything revolves around code when software is crafted. So, if it is a good thing for developers to review code together, software should always be developed in pairs. In other words, pair programming can be defined by two people working behind one computer. Pair programming is often considered one of the most extreme and controversial aspects of XP because it is thought to be slower. However, research has shown that peer review and code inspection are the most powerful weapons against bugs—and these are much more powerful than systematic testing. These techniques are only used sparsely and often drum up great resistance among the programmers themselves, as well as managers who are afraid of an increase in working hours.</p>
<p>By enforcing the rule that all software development is carried out in pairs, which also change composition regularly, a collective sense of ownership arises, and peer review and code inspection become a natural part of the software process. As a result, the system ultimately delivered no longer consists of a collection of pieces of code tied together with strings, which are poorly maintainable.</p>
<p>There is another advantage to this way of working: there are always at least two people who fully understand every piece of code. The transference of knowledge to new colleagues happens more naturally, and a continuous training on the job takes place.</p>
<p>Some of the benefits of pair programming include the following:</p>
<ul>
<li><strong>Better quality code</strong>: Activities such as reading code out loud and discussing the thought process behind it helps others to understand its complexity, as well as giving developers the opportunity to clarify any details and prevent irrevocable choices from being made.</li>
<li><strong>Better knowledge-sharing within a team</strong>: This is particularly useful when one of the developers is not yet familiar with the software component cooperates with someone who is.</li>
<li><strong>Improved knowledge transfer</strong>: This is helped by developers automatically learning new techniques and skills from experienced team members.</li>
<li><strong>Less management overhead</strong>:<strong> </strong>This is aided by less individual control because developers are working in teams of two or more.</li>
<li><strong>Continued focus</strong>: Pair programming can be particularly helpful if one member of the pair has their work interrupted for any reason.</li>
</ul>
<p>So, are there any drawbacks to pair programming? Currently, it is not known exactly what the costs and benefits of pair programming are, but initial research indicates that the duration of a task increases by an average of 15% when a pair is working over an individual. Whether that cost can be justified by higher code quality is debatable.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Test Driven Development</h1>
                </header>
            
            <article>
                
<p>In <strong>Test Driven Development</strong> (<strong>TDD</strong>), testing is carried out before any programming. TDD relies on the premise that if testing is good, the test code should be written before a line of code (functionality) is.</p>
<p>Within XP, the writing of automatic unit tests occupies an important place, as writing unit tests is done before an actual program is started. In TDD, the programmer makes one or two tests, writes a piece of the program, makes an additional test case, reworks the program until the new test passes, the designs a new test, and so on.</p>
<p>The advantage of this process is that the programmer is obliged to think about the functionality and the exceptions that their program should take into account; they think about what the program should do first and how the program will work second. In other words, the tests capture the required functionality. It is therefore important that every program only has enough functionality to make the test work. If all tests pass, the program meets the previously-defined requirements, which are defined in the written and successful unit tests.</p>
<p>When refactoring is required, the already-written unit tests are therefore a guarantee that changes will not cause undesirable side effects in the operation of a program.</p>
<p>Should it be necessary to expand a program with new functionality, the first thing to do is to start writing new unit tests that will define the new functionality to be written. This new functionality is only realized when both the new and the old unit tests all succeed.</p>
<p>If a bug is found at a later stage during a functional test, writing a unit test that brings this bug to light is the first port of call. A bug is therefore not a fault in the program, but rather the result of an insufficient test.</p>
<p>To summarize, a software development project that is carried out using TDD does the following:</p>
<ul>
<li>Starts with one unit test that describes one feature of the program</li>
<li>Runs the test, which should ultimately fail because there is no code</li>
<li>Utilizes the minimum amount of code needed to make a test pass</li>
<li>Rewrites code to make it simpler</li>
<li>Repeats the process with more tests</li>
</ul>
<p>Using this approach, the defect rate should go down after time, despite more time being needed to get things started. Most teams report that once they have reached the end stages of a project, the upfront testing cost is paid back and they work quicker in a project's final phases. Code that is developed this way tends to be of higher quality than otherwise, because for testing to work, you are forced to create code that is high in cohesion but low in coupling. This keeps code that works on the same behavior and keeps properties in the same class, as well as keeping modules as isolated as possible with clear interfaces to other code.</p>
<p>This approach may sound quite simple, but in practice, it's quite hard, as developers may forget to run tests. However, this problem can be easily fixed by setting up a project template in a CI/CD environment with pre-configured tests, where tests are run at every commit or push. If there are software engineers who are prone to overdo testing, it is a good idea to agree on the number of tests, and how far they will go, beforehand. Don't test constructs that are simple, such as accessors, for instance. On the other hand, be careful not to over-simplify your tests, such as by creating tests but no assertions.</p>
<p>Team culture and agreements are very important for testing. If some team members aren't on board, you will have conflicts. Also ensure that any test-templating, automation, or suites are well supported, or your tests will break. (This also means that several people should have knowledge of these products.)</p>
<p>In essence, TDD can really improve the quality of your software. This is especially true if all tests are automated via a continuous process, which we will discuss in the following section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Continuous processes</h1>
                </header>
            
            <article>
                
<p>Continuous processes are a group of processes that are envisioned to run all the time with no interruptions. There is also no need for batching, which often slows down XP as a whole.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Continuous integration</h1>
                </header>
            
            <article>
                
<p>If integration and integration tests are important, then code should be integrated as often as possible – preferably, several times a day. This will prevent your team from working with different copies locally and encourages them to work alongside each other. Any integration problems will also become immediately visible. GitLab CI was created for this reason, and was introduced in <a href="384dcfd9-ef7f-470d-89dc-3af7502a2d09.xhtml">Chapter 1</a>, <em>Introducing the GitLab Architecture</em>.</p>
<p>One of the key reasons CI is used is to prevent integration problems, which can occur if developers work on their own for too long. Imagine the phenomenon of integration hell, where at the last minute before release, a developer merges a big chunk of code that then introduces conflicts.</p>
<p><span>Continuous integration has always been coupled with TDD in the XP world. Before integration tests are run, it helps if code is thoroughly tested locally, preferably by using unit tests. This way of testing code in your local environment helps uncover bugs before they break other people's code</span><span>. Note that you can also hide features that are not yet complete by using feature toggles, which disable certain behaviors in code.</span></p>
<div>
<p><span>In some cases, build servers are used for other parts of the software</span> <strong>Quality Assurance</strong><span> (<strong>QA</strong>) process, including running additional security tests, measuring performance, and even generating documentation. This behavior of shifting responsibilities to the build server means that a lot of QA work that was traditionally done after development work can instead be performed during development, with the bonus of immediate feedback. This feedback loop is a big driver of the continuous process of developing a software product, with the other being automation.</span></p>
</div>
<p><span>Automation has been further extended to create</span> <strong>Continuous Delivery</strong> <span>(<strong>CD</strong>), by making the deployment of software part of the automation. To make this possible and run quickly without issue, the code in the main trunk or branch should always be in a state so that it can be deployed.</span></p>
<p> </p>
<p> </p>
<p>Every element of building a software product that can also be automated is eligible to be part of the CI process, especially when it's particularly <span>complex</span>. Automating these stages is one of the reasons CI/CD exists.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Refactoring</h1>
                </header>
            
            <article>
                
<p>An important technique that differentiates XP from traditional development methods is refactoring, which is the continuous rewriting of program code in small, precisely-measured steps without affecting any visible functionality. In short, refactoring adds nothing to the functionality, but simplifies the design. By regularly executing rewriting steps, the overall effect is often astonishing.</p>
<p>In the meantime, some 70 rewrite rules have been discovered and documented. They carry names such as introduce null object, replace temp with query, and replace conditional with polymorphism. The preconditions for the successful application of refactoring is that there are unit tests available that can be carried out automatically after every rewriting step to ensure that the functionality has not changed. For example, for Smalltalk, there is now a refactoring browser, with which rewriting rules can be applied automatically and without the user having to worry too much about accuracy. Refactoring is often used in preparation for implementing an extension or a change in functionality.</p>
<p>What is not meant with refactoring is the rewriting of code, bug-fixing, or changing the user interface. Another danger of refactoring is that with the absence of good automated tests, you may introduce regression errors.</p>
<p>After some time and experience using this technique, teams report considerable improvements in the length of the code, less duplication, better coupling and cohesion, and reduced cyclomatic complexity. For people new to your software, this makes it easier to learn. For teams, it helps to think collectively about the general design of a project, and to understand why certain decisions have been made. Usually, this also relies on the introduction of certain reusable components and modules.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Short iterations</h1>
                </header>
            
            <article>
                
<p>Software is regularly delivered to the customer for review in releases of limited size; if short iteration strokes are good, you should therefore make them very short. We're talking seconds, minutes, or hours, instead of weeks, months, and years. An average iteration of XP takes two weeks, although according to <a href="http://www.extremeprogramming.org/">extremeprogramming.org</a>, it can vary from one to three weeks.</p>
<p>The XP cycle consists of six phases: Exploration, Planning, Iterations to Release, Production, Maintenance, and Death.</p>
<p>For Agile projects, an iteration is a specific time period during which development takes place. This is called timeboxing. This period varies from project to project, but is usually between one and four weeks, and is often defined for each project. A typical Agile approach would be that a project consists of several iterations, with a short phase of planning at the beginning and a closing phase at the end.</p>
<p>Iterations are mostly classified in work weeks that start on a Monday and end on a Friday. After a while, the fixed term of the iteration makes it easier to assess how long a project will take.</p>
<p>The iteration timebox in the Scrum methodology is called a Sprint, which is of course a <span>reference to</span> <span>R</span>ugby. In XP, they are called weekly cycles. To most people, the word iteration means repetition or even multiple repetitions; when used in the Agile context, it means a repeated process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Everybody owns the code</h1>
                </header>
            
            <article>
                
<p>Note that every developer has equal rights to all aspects of a program's code. If the design is good, make it part of everyone's daily work and improve the design step by step as soon as the need arises. If architecture is so important, let everyone work on developing it. This concept encourages everyone to contribute and take responsibility.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Shared understanding</h1>
                </header>
            
            <article>
                
<p>The values in a group are mostly to do with perception. To be efficient and effective as a group, you have to agree on certain points and share values and a common understanding.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Coding standards</h1>
                </header>
            
            <article>
                
<p>To have a shared understanding you need to have some rules. There are coding standards that are known and used by everyone. It really helps if source is consistently formatted. That way everyone can read it and change it. Speak the same language in your code files.  It will also help in ensuring collective ownership of the codebase.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Simple design</h1>
                </header>
            
            <article>
                
<p>If the code belongs to all <span>developers </span>in a group and everyone can change everything, it should be possible for them to do so. Keep the design as simple as possible. XP works a lot with the <strong>Keep It Short and Simple</strong> (<strong>KISS</strong>) principle. In other words, for a system to be easy to change, the design should be as simple as possible. This is easier said than done, however.</p>
<p>Traditional development methods have learned to think ahead and to always think about functionality in a design that might have to be realized in the future, but these methods are based on the assumption that costs for changes increase exponentially. This is why XP is always trying to choose the simplest design to enable a functionality that must now be realized. Ideally, any future expansions can be implemented with XP without the usual extra costs. It also turns out that, all too often in the realization of a well thought-out design, it does not reflect current requirements. This may happen either because certain details have been overlooked during analysis and design, or because requirements have been adjusted. With XP, design does not come first, but instead follows the code.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">System metaphor</h1>
                </header>
            
            <article>
                
<p>All team members, including developers and users or customers, share a common view on the system (known as a metaphor); everyone must be able to describe the system in simple words. The use of naming conventions should also contribute to this.</p>
<p>As we have discussed, the final takeaway is that when working with XP, the human element is still the most important one. Is everyone talking about the same things, for example? Do they all think the right priorities are set? Are they able to create software that works and is understandable? This human element is also one of the driving forces behind DevOps, the spillover of Agile thinking from software development to IT operations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The DevOps movement</h1>
                </header>
            
            <article>
                
<p>The term DevOps originated in Belgium around the end of the last decade, as a result of the so-called DevOps days. These days were meant to bring together IT experts from both development and management operations. A DevOps team was initially defined as a multidisciplinary team that is fully responsible for the management and CD of a service. Think of Amazon and Google as companies that use these kinds of teams; they release dozens of changes every day.</p>
<p>This way of working is not yet standardized in big organizations—ITIL and PRINCE2 still reign, and the Information Technology (IT) department is desperately trying to deliver services with value. The way in which these services are provided is difficult to maintain in the current situation where IT is still often seen as a cost item. This is caused by the following:</p>
<ul>
<li>The way of organizing organizations as a collection of independent silo's</li>
<li>A focus on creating process excess (too many rules set in stone)</li>
<li>Not defining clear <strong>Key Performance Indicators</strong> (<strong>KPIs</strong>) for measuring performance.</li>
</ul>
<p>The technologies in these silos are not compatible, whereas for a successful business-IT alignment, one coherent chain is needed.</p>
<p>In addition, we are now seeing that customers are increasingly asking for the fast delivery of a new functionality. This includes the quick resolution of incidents, short lines of communication, and excellent quality requirements in their IT organization. Using the old ways of organizing IT in an organization, processes, working methods, attitude, behavior, and the required performance and results are not being sufficiently realized. A famous quote attributed to Albert Einstein, that "<em>the definition of insanity is doing the same thing over and over again and expecting a different result"</em>, seems to increasingly apply to IT. It is time, therefore, to fundamentally reconsider the setup of its organizations.</p>
<p>The following diagram illustrates the DevOps process:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d888abfd-d7aa-4880-aeb7-2464c5b1c5fc.png" style="width:26.00em;height:14.92em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">History of the movement</h1>
                </header>
            
            <article>
                
<p>The term DevOps still causes a lot of confusion in many places. As a movement, it is still young, but it is largely based on common sense and experiences from the past. DevOps teams appeared from an effort by companies to respond to changes in the market. The new DevOps approach has been further developed with the aim of releasing higher-quality software to customers faster and more frequently. A brief timeline of DevOps is as follows:</p>
<ul>
<li><strong>2007</strong>: During the migration of a data center for the Belgian government, Patrick Debois is frustrated by the many conflicts between developers and system administrators. This makes him think.</li>
<li><strong>2008</strong>: At the Agile Conference in Toronto, software developer Andrew Shafer is poised to give a session about Agile infrastructure. He decides to skip it because he thought there were no attendees, but Debois was going to attend. Later, Debois tracks down Shafer for a wide-ranging hallway conversation. Based on their talk, they form the Agile Systems Administration Group.</li>
<li><strong>2009</strong>: Two Flickr employees, John Allspaw and Paul Hammond, make the case to test, build, and deploy responsive, fresh software in a bid to make operations and development integrated and transparent. The first DevOps days take place in Gent, Belgium. The conference takes place on October 30 with an impressive collection of developers, system administrators, experts, and others. When the conference ends, ongoing discussions move over to T<span>witter. To create a memorable hashtag, Debois shortens the name to #DevOps</span><span>.</span></li>
</ul>
<ul>
<li><strong>2010</strong>: This is when the first ever DevOps days were organized in the US, carried out with the help of John Willis (author of the famous book, <em>The Phoenix Project</em>), along with early proponents of DevOps. The event soon becomes a global series of conferences that are community-organized and are the major force driving the DevOps community forward.</li>
<li><strong>2011</strong>: The community of DevOps starts to use open source tools, such as Vagrant, that can leverage technologies such as Chef and Puppet.</li>
<li><strong>2012</strong>: The presentation development segment grows rapidly and becomes focused on innovation. There are now various DevOps days that suddenly pop up in a number of countries.</li>
<li><strong>2014</strong>: Some of the biggest companies worldwide begin to use the DevOps method in their organization, including LEGO and Nordstrom.</li>
</ul>
<p>Today, DevOps is embraced across the world by a number of businesses; small, big, and private businesses benefit from DevOps. DevOps can bring out the best results in the long-run for any business and contribute to its success.</p>
<p>However, an organization is not able to switch over to DevOps quickly – changing processes in an organization can have a major impact on its culture and needs time. A good way to find out where you might be in this journey is to use a maturity model. When using a model to represent reality, you can start to simplify the problem, instead of being overloaded by the amount of solutions and tools that are available. If you know where you are in the maturity model, you can determine where you want to be, and then plan your journey.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Four Quadrant Model</h1>
                </header>
            
            <article>
                
<p><span>The original maturity model is the Capability Maturity Model invented at <strong>Carnegie Mellon University</strong> (<strong>CMU</strong>). It is a bit heavy to fully utilize, so simplified, more lightweight versions of it are preferable. One such version is the Four Quadrant Model put forward by Brian Dawson (<a href="https://techbeacon.com/devops/how-map-your-devops-journey">https://techbeacon.com/devops/how-map-your-devops-journey</a>). It is derived from real-world DevOps transformations and offers a flexible way to assess maturity.</span></p>
<p><span>In the Four Quadrant Model, the values ​​on the <em>x</em> axis consist of the different phases in the cycle of software development. You can recognize the <strong>Software Development Life Cycle</strong> (<strong>SDLC</strong>) in this:</span></p>
<ul>
<li>Define</li>
<li>Plan</li>
<li>Code</li>
</ul>
<ul>
<li>Build</li>
<li>Integrate</li>
<li>Test</li>
<li>Release</li>
<li>Deploy</li>
<li>Operate</li>
</ul>
<p>You see there is quite some overlap with the DevOps lifecycle phases that are proposed by GitLab.</p>
<p><span>The cycle is divided into two halves: <strong>Agile Upstream</strong> (which includes a definition, planning, coding, and building) and <strong>Agile Downstream</strong> (which includes integration, testing, release, implementation, and methods such as continuous deployment and continuous delivery).</span></p>
<p><span>On the <em>y</em> axis, there is the level of adoption of Agile and DevOps practices in an organization. At the lower end, there is the team level, which moves on to the workgroup level, and finally the enterprise level. In the original CMMI model, there are usually different levels of maturity. The following figure illustrates the 4 Quadrant model:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a30b2b34-6140-4070-9823-631035dc5b50.png" style="width:28.25em;height:14.00em;"/></p>
<p class="mce-root"><span>Agile Upstream means that in a software life cycle, the development side of the product is done with Agile methodologies. Agile Downstream is all about the deployment and operational side of the SDLC.</span></p>
<p><span>Each team must strive to implement the Four Quadrant Model because it enables them to innovate faster, increase productivity, respond to market changes, gain a competitive advantage, and increase employee satisfaction and retention.</span></p>
<p><span>Another way of measuring maturity is to look at competencies. How strong are you in certain aspects, for example? </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Four levels of competence</h1>
                </header>
            
            <article>
                
<p class="mce-root">One way to test competence is by using the model mentioned by Mike Kavis' paper in Forbes. He describes a model that is based on the <em>Four stages of learning</em> used by Noel Burch in the 1970s.</p>
<p class="mce-root">The basic idea is that an individual goes through the following four stages while acquiring a new skill:</p>
<ul>
<li><strong>Stage 1: unconscious incompetence</strong>: A person may not recognize that they need a certain skill. The first step in changing or growing is to recognize this deficit and to acknowledge the skill as is.</li>
<li><strong>Stage 2: conscious incompetence</strong>: Gradually, it becomes apparent that a necessary skill is lacking. This is learned by making mistakes.</li>
<li><strong>Stage 3: conscious competence</strong>: After several iterations or tries, the person acquires the skill and knowingly applies it. It won't succeed every time, and doing so takes serious effort.</li>
<li><strong>Stage 4: unconscious competence</strong>: The skill has become so natural or logical, that it can be applied unconsciously. It can even by taught to others.</li>
</ul>
<p class="mce-root">Davis claims that this model can be applied to organizations that are trying to grasp the DevOps concept. It is not scientifically proven, but it can be valuable to make the following comparison:</p>
<ul>
<li><strong>Stage 1</strong>: <strong>Nothing there</strong>: The organization initially seems averse to change. The term DevOps is described as a hype and is not applicable. This usually means that people don't really understand what DevOps is about. People are trapped in the old silo thoughts and think that development should take over operations, or vice versa.</li>
<li><strong>Stage 2</strong>: <strong>Recognition</strong>: Finally, it has sunk in that something has to change. In this stage, there will be mistakes. For instance, automation is introduced but the development silo may still think it is responsible for writing everything. A new silo (the DevOps silo) emerges, where developers are only creating automation for operations. These developers are not knowledgeable in networking  or security and compliance or other operational issues. Similar problems occur if the Operations department silo is converted to 'the' DevOps engineer.  With limited knowledge about engineering, untested and unmanageable shell scripts may appear. However, at this stage, an organization is still learning and will eventually proceed to the next stage if the inevitable growing pains are managed.</li>
</ul>
<ul>
<li><strong>Stage 3</strong>: <strong>Coming of age</strong>: After learning from their mistake, the management of an organization has taken up interest and recognized the added value of changing people and processes. In the previous stage, the aim was to integrate the silos of development and operations. Now that this has succeeded through trial and error, cooperation has expanded to include legal departments, compliance, and audit. The first signs of productivity are visible, with the creation of specialized platforms, a framework, or a template for deploying standardized enterprise applications from idea to production. Platforms begin to have everything baked in, such as compliance and quality control.</li>
<li><strong>Stage 4</strong>: <strong>100% business driven</strong>: At this stage, multiple business units in an organization deploy several times a day and are able to easily enhance the process and share their knowledge via the platform. In the most optimal form, the business unit is in complete control and has become a multi-disciplined team that can advise and collaborate with dedicated platform specialists.</li>
</ul>
<p>Of course, these models are quite theoretical, but they can help in the process of organizational change. Fortunately, several tools have emerged to help organizations bridge these gaps between the stages of maturity, which we'll discuss in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The toolchain</h1>
                </header>
            
            <article>
                
<p>Although we have learned that DevOps is more than just tools, there are a number of tools that are commonly used in the enterprise, such as the following:</p>
<ul>
<li><strong>A source code repository</strong>: Computer source code has become a very valuable asset. It is usually stored in a repository with advanced version management features. The repository manages the many versions of code that are checked in, so developers can collaborate on each other's projects. This concept is not new and has been around for 30 years, but is a big part of continuous integration because it is where the source code is kept. Popular source code repository tools include the following:
<ul>
<li>Git on the client</li>
<li>GitLab</li>
<li>GitHub</li>
<li>Subversion</li>
<li>TFS</li>
<li>CVS</li>
</ul>
</li>
</ul>
<p style="padding-left: 60px">All of these repository tools are explained in detail in<span> </span><a href="d98d2cb3-53e4-4544-be20-4e618ae24e7b.xhtml">Chapter 4</a><span>, <em>Configuring GitLab from the Terminal</em>, <a href="f5893a6a-9076-45e6-8042-92c8304a0033.xhtml">Chapter 5</a>, <em>Importing Your Project from GitHub to GitLab</em>, <a href="632675c5-19ce-4c2f-8441-a8232ba70f1a.xhtml">Chapter 6</a>, <em>Migrating From CVS</em>, and <a href="6f1e3f0c-5dcc-47c1-a928-62caeebeca40.xhtml">Chapter 7</a>, <em>Switching from SVN</em></span>.</p>
<ul>
<li><strong>Build server</strong>: Building software used to be done on the individual developer's workstation, but for the CI pipeline, a dedicated build server is used to compile source code from the source code repository into executable artefacts. Modern build servers do not just build, but also provide advance testing functions. Popular tools include the following:
<ul>
<li><strong>GitLab Runners</strong>: The build tool for GitLab.</li>
<li><strong>Jenkins</strong>: A fork of the Hudson project and a CI platform. This platform is primarily intended for the repeated execution and monitoring of build tasks, as well as the automated building and testing of applications. The many freely available plugins make it very easy to further expand the functionality of Jenkins. This software is only available as a distributed service to use on the cloud and is tightly integrated with GitHub as a source code repository.</li>
</ul>
</li>
<li><strong>Configuration management</strong>: F<span>or CI/CD, you need to control the environment where it takes place. For this, there are configuration management tools that describe and automate large parts of your infrastructure. Popular tools include the following:</span>
<ul>
<li><strong>Puppet</strong>: Management software which can control large numbers of servers. This concerns both the management of configuration files (the settings of servers) and the management of the installed software (packages). It uses a declarative language and has a steep learning curve.</li>
<li><strong>Chef</strong>: Also configuration management software, Chef supports slightly fewer platforms than Puppet and is not a declarative language. Chef uses pure Ruby code that indicates what you want to do on your servers. You have more freedom to create your own program data structures and functions. It is used by GitLab to manage the omnibus package.</li>
</ul>
</li>
</ul>
<ul>
<li><strong>Virtual infrastructure</strong>: Infrastructure on which software runs has always been virtual, and an operating system is already several layers of abstraction. In the cloud, virtual infrastructure is an extra layer of abstraction that represents entire machines (such as networks, nodes, and storage). There is also an orchestration layer that manages the infrastructure. This provides easy up- and down-scaling and can use all resources efficiently. The first real, large-scale virtual infrastructure that became available as a service was Amazon Web Services. The other major tech companies soon followed with Google Cloud and Microsoft Azure. These infrastructures can be managed with their own orchestration tools, but also have APIs that can be used by configuration management tools, including the following:
<ul>
<li>Ansible</li>
<li>Puppet</li>
<li>Chef</li>
<li>Google Cloud</li>
</ul>
</li>
</ul>
<div>
<p style="padding-left: 60px"><span>Governments often have special requirements regarding their data. This is called data sovereignty and</span> this <span>is why specialist clouds have arisen for governments. According to Garter,</span> <span>these clouds could be the next legacy system, after government infrastructure was moved like-for-like to the cloud without being decomposed into elastic, efficient, and cost-effective cloud components.</span></p>
<p style="padding-left: 60px"><span>Not everyone is able to run their software and data on public clouds, even if they have special agreements. If you run a private or hybrid cloud, for example, you are essentially using abstractions that exist on the internet in your own data center. Even without the elasticity of Amazon or Azure, it can be very beneficial to apply cloud techniques yourself. The accompanying automation tools make integration with existing systems easier, and a lot less people are needed for managing the system. There are also private clouds; for example, VMware has vCloud. It is quite easy to extend your existing VMware infrastructure to create cloud-like environments.</span></p>
</div>
<ul>
<li><strong>Test automation</strong>: Testing is all about ensuring confidence in your product. When the product reaches deployment time in your pipeline, it should be tested for certain defects automatically before it has reached that point. There are several tools available to perform all kinds of testing and integrate nicely with a lot of other pipeline products; they include the following:
<ul>
<li><strong>Selenium</strong>: Selenium is an application that allows you to automate browsers. What you do with this depends on your goal. You can automate repetitive administrative tasks, but Selenium is also used for browser testing.</li>
<li><strong>Cucumber</strong>: This is a test tool for <strong>Behavior-Driven Development</strong> (<strong>BDD</strong>). The primary goal of BDD is to let people communicate, and close the gap between technical and business people. You can write tests in a human-readable format.</li>
<li><strong>Apache JMeter</strong>: JMeter is an open source tool that performs load, performance, and stress tests. It is a simple but effective application where different types of scripts show exactly what the result of the test is. These scripts are used for HTTP websites and provide a simulated test environment. In addition to applications, JMeter is also suitable for checking services on the web and various databases.</li>
<li>You can also utilize GitLab Runners and write your own tests.</li>
</ul>
</li>
<li><strong>Pipeline orchestration</strong>: The pipeline refers to an automated number of steps to get your code from inception to production after it has been checked into version control. It's based on the idea of a manufacturing assembly line.</li>
</ul>
<p style="padding-left: 60px">To manage things along the way in a CI/CD process, pipeline orchestration tools were introduced. Some of these tools include:</p>
<ol>
<li style="list-style-type: none">
<ul>
<li><strong>Kubernetes</strong>: Kubernetes is, essentially, a platform for the roll-out and management of containers on a large scale. Kubernetes, Greek for helmsman or pilot, is the second name for the project, which originally saw the light of day in the big halls of Google as Project Seven of Nine. Project Seven of Nine was an external version of Borg, the task scheduler that drives the services of Google, and the operation of which was a Google secret for a long time.</li>
<li>Built as an extension to the Docker API, orchestration using Swarm also became popular a couple of years ago. It can easily convert a loose group of Docker containers in a managed virtual Docker engine. This makes it very easy to start running container workloads at scale from scratch.</li>
<li><strong>Mesos/Marathon Apache</strong>: Mesos is a distributed kernel and is the backbone of DC/OS. It abstracts CPU, memory, storage, and other computer resolutions. It has APIs for resource management, planning in data centers, and cloud environments. It can scale up to 10,000 nodes. It can therefore be extremely suitable for large production clusters. It supports container orchestration with Marathon.</li>
</ul>
</li>
</ol>
<div>
<p><span>All of the aforementioned tools can be integrated with GitLab, and you can use GitLab for all parts of a pipeline. You can use runners for testing, building, or deploying your product, and you can utilize Kubernetes to orchestrate your workloads.</span></p>
<p><span>It's up to you which part of the pipeline is used in GitLab, but it can support you in all stages of the DevOps life cycle, illustrated as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/caf5c152-1771-4cf1-a29a-8d18153158b6.png" style="width:64.00em;height:7.08em;"/></p>
</div>
<p class="mce-root">We have now explained the basic setup of a CD pipeline in DevOps. GitLab offers close to 100% of all the stages, but can integrate with existing components as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter was intended to provide more background on the origins and development of GitLab. A tool does not arise from the void. In the 1990s, it became clear that in different parts of the world, people came to the same conclusion: linear software development is not the right approach for all projects. The solution to this problem has finally reached the operations department after 10 years through DevOps. DevOps is a way of working and a culture with accompanying tools for which GitLab has been built. In the next chapter, we will see how GitLab can contribute to a better DevOps experience.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What is an SDLC?</li>
<li>How many participants were at the Agile Manifesto conference in Utah?</li>
<li>When was the waterfall model first mentioned?</li>
<li>Where was XP programming born?</li>
<li>What does MoSCoW mean?</li>
<li>Where and when did the first DevOps days take place?</li>
<li>What is Agile Upstream?</li>
<li>Name two configuration management tools.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><em>The Agile Maturity Model</em>: <a href="https://info.thoughtworks.com/rs/thoughtworks2/images/agile_maturity_model.pdf">https://info.thoughtworks.com/rs/thoughtworks2/images/agile_maturity_model.pdf</a></li>
<li><em>DevOps maturity model</em><q><em>:</em></q> <a href="https://techbeacon.com/devops/how-map-your-devops-journey">https://techbeacon.com/devops/how-map-your-devops-journey</a></li>
<li><em>What is DevOps?</em> <a href="http://radar.oreilly.com/2012/06/what-is-devops.html">http://radar.oreilly.com/2012/06/what-is-devops.html</a></li>
<li><em>The Agile Developer's Handbook</em>, by <em>Paul Flewelling</em><q>: </q><a href="https://www.packtpub.com/web-development/agile-developers-handbook">https://www.packtpub.com/web-development/agile-developers-handbook</a></li>
<li><em>DevOps: Continuous Delivery, Integration, and Deployment with DevOps</em>, by <em>Sricharan Vadapalli</em>: <a href="https://www.packtpub.com/virtualization-and-cloud/devops-continuous-delivery-integration-and-deployment-devops">https://www.packtpub.com/virtualization-and-cloud/devops-continuous-delivery-integration-and-deployment-devops</a></li>
<li><em>Practical DevOpsm</em>, by <em>Joakim Verona</em>:<q> </q><a href="https://www.packtpub.com/in/networking-and-servers/practical-devops">https://www.packtpub.com/in/networking-and-servers/practical-devops</a></li>
<li><em>Wicked Problems, Righteous Solutions: A Catalogue of Modern Software Engineering Paradigms</em>, by <span><em>DeGrace</em>, <em>Peter</em>, and <em>Stahl</em>, <em>Leslie Hulet</em>, </span>pp. 116, 117, 127. Reprinted with permission of Prentice Hall, Englewood Cliffs, New Jersey, 1990.</li>
<li><span><em>Managing the development of large systems: Concepts and techniques</em></span>, by W. W. Royce<span> In: 9th International Conference on Software Engineering. ACM. 1970. p. 328-38.</span></li>
</ul>


            </article>

            
        </section>
    </body></html>