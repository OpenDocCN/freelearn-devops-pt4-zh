<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Autoscaling GitLab CI Runners</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we were able to scale GitLab Runners using the Kubernetes executor. Depending on your requirements of how many jobs should be able to run concurrently, the number of available Runners in a Kubernetes cluster can go up or down. Having a big number of runners available can be very costly. Even if they were to be turned off, they would still cost money. It's much better to have them created on demand and destroyed when they are no longer needed.</p>
<p>There is another GitLab Runner executor that can behave in this elastic way and dynamically add or remove Runner instances, and this is known as the Docker Machine executor. We will show you what this looks like from an architectural point of view, explain some of its settings, and provide you with some examples of running the Docker Machine executor with the VirtualBox driver and the Amazon EC2 driver.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li><span>Runner client architecture</span></li>
<li><span>Setting up the environment</span></li>
<li><span>Configuring the GitLab Runner</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You can find the code file for this chapter in this book's GitHub repository at <a href="https://github.com/PacktPublishing/Mastering-GitLab-12/tree/master/Chapter17" target="_blank">https://github.com/PacktPublishing/Mastering-GitLab-12/tree/master/Chapter17</a>.<a href="https://github.com/PacktPublishing/Mastering-GitLab-12/tree/master/CHP16"/></p>
<p>The other requirements for this chapter are as follows:</p>
<ul>
<li>Docker Machine is automatically installed with the Docker software distributions for macOS or Windows. If you don't have it, you can find it at the following link: <a href="https://github.com/docker/machine/releases/">https://github.com/docker/machine/releases/</a>.</li>
<li>You need a Linux bastion host with up-to-date patches.</li>
<li>Access to the Docker registry image is required (<a href="https://hub.docker.com/_/registry">https://hub.docker.com/_/registry</a>).</li>
<li>Access to the MinIO Docker images is required (<a href="https://hub.docker.com/r/minio/minio">https://hub.docker.com/r/minio/minio</a>).</li>
<li>You need VirtualBox installed on the bastion host (<a href="http://www.virtualbox.org">http://www.virtualbox.org</a>).</li>
<li>You need an AWS account, which will be used for scaling with the EC2 infrastructure.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Runner client architecture</h1>
                </header>
            
            <article>
                
<p>First, we will describe the architecture of this solution. Expanding on the architecture that was put forward in previous chapters, we have a GitLab instance with a GitLab CI that receives a request from a GitLab Runner that's running a dedicated host. This can be either a local virtual machine or an instance in the cloud. <span>The Runner is equipped with the Docker Machine program. </span></p>
<p><span>The Docker Machine executor type is basically a GitLab Runner that executes D</span>ocker Machine <span>commands. With Docker Machine, you can create virtual hosts that run the Docker Engine. You can control these hosts with it </span>and create new virtual machines with Docker Engine installed, which in turn can run GitLab Runner container instances. This is explained in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3775cd7a-9448-4495-85fc-3e9d21452cb6.png" style="width:44.08em;height:56.50em;"/></p>
<p>In the preceding diagram, you can see the <strong>Docker Machine</strong> component and that it can instantiate multiple runners. There are also two other components in the diagram called a <strong>Caching server</strong> (which can store dependencies for builds) and the <strong>Docker registry proxy </strong>(which can cache Docker images from places such as the Docker Hub). Both components will be explained in more detail in the <em>Configuring the Runner</em> section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the environment</h1>
                </header>
            
            <article>
                
<p>To enable a Docker Machine-based Runner, the following steps have to be performed:</p>
<ol>
<li>Prepare a bastion host as the host where Docker will create new machines.</li>
<li>Deploy the GitLab Runner software on this machine.</li>
<li>Install Docker Machine.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing a bastion host</h1>
                </header>
            
            <article>
                
<p>For this example, we chose my macOS-based machine. This can be a Linux virtual machine or your laptop—<span>a</span>ny machine that can run a recent version of the GitLab Runner software. The only function this host will have is executing the GitLab Runner software with the Docker Machine executor. It should be tightly secured as a bastion to withstand attacks since it can control multiple Runner instances through the <kbd>docker-machine</kbd> commands, and that makes it a target.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying the GitLab Runner software</h1>
                </header>
            
            <article>
                
<p>On macOS, we use the Homebrew package manager. To install the Runner software, you can execute the following command:</p>
<pre><strong>brew install gitlab-runner</strong></pre>
<p>After that, you can <kbd>register</kbd> the Runner, as shown in <a href="4e94f5e4-336d-42ae-af76-477949936356.xhtml">Chapter 15</a>, <em>Installing and Configuring GitLab Runners</em>:</p>
<pre><strong>gitlab-runner register</strong></pre>
<div class="packt_infobox">Choose the Docker Machine executor when you're asked for an executor in the registration process.</div>
<p>After the Runner has been registered, don't start it just yet—we need to edit the <kbd>config.toml</kbd> configuration file that's located in <kbd>~/.gitlab-runner/config.toml</kbd> on macOS. We will do that in the <em>Configuring the Runner</em> section.</p>
<p>First, though, we need to install the Docker Machine binary before we configure the Runner in order to start the Docker Machine executor.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Docker Machine</h1>
                </header>
            
            <article>
                
<p>If you've installed Docker on macOS or on Windows, you will already have the binary installed. You can test the installation by using the following command:</p>
<pre class="p1"><strong><span class="s1">$ docker-machine -v<br/></span><span class="s1">docker-machine version 0.16.1, build cce350d7</span></strong></pre>
<p>You can create new Docker hosts with this tool. They can be created on your local machine or network, but also in the cloud with the help of big providers such as Microsoft, Amazon, and Google. Docker Machine has plugins for many systems. The following is a list of them:</p>
<ul>
<li>All VMware products</li>
<li>Virtualbox</li>
<li>Microsoft Hyper-V</li>
<li>Digitial Ocean</li>
<li>Amazon Web Services (EC2)</li>
<li>Microsoft Azure</li>
<li>Exoscale</li>
<li>Google Computing Engine</li>
<li>Scaleway</li>
<li>IBM Softlayer</li>
<li>Rackspace</li>
<li>OpenStack</li>
<li>Linode</li>
</ul>
<p>If you are running Linux, you can download and install Docker Machine from <a href="https://github.com/docker/machine/releases/">https://github.com/docker/machine/releases/</a>.</p>
<p>If you look at the Dockerfile of GitLab Runner on <a href="https://hub.docker.com/r/gitlab/gitlab-runner/dockerfile">https://hub.docker.com/r/gitlab/gitlab-runner/dockerfile</a>, which is used to build the default container, there is a line that says the following:</p>
<pre><strong>wget -q https://github.com/docker/machine/releases/download/v0.7.0/docker-machine-Linux-x86_64 -O /usr/bin/docker-machine &amp;&amp; \</strong><br/><strong>     chmod +x /usr/bin/docker-machine</strong></pre>
<p>The Docker Machine binary is installed directly in this container image and is used by the GitLab Runner software. When you have verified that the Docker Machine binary is available, the next step is to configure the Runner.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring the Runner</h1>
                </header>
            
            <article>
                
<p><span>Now that you've installed the Runner software and Docker Machine, it's time to edit the Runner configuration file. On macOS, you can find the <kbd>config.toml</kbd> file in <kbd>~/.gitlab-runner/config.toml</kbd>. It is in your home directory because the Runner runs in the user space on macOS.</span></p>
<p><span>Now, we will take a look at some of the configuration options you can specify in the <kbd>config.toml</kbd> file that are specifically for the autoscaling Runner.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Off-peak time mode configuration</h1>
                </header>
            
            <article>
                
<p>Most organizations don't have the need for 24/7 capacity since they don't need to use CI runners all of the time. The most work is done during work hours in a regular work week, and at the weekends, there's less of a need for software to be built. In this situation, it makes no sense to have machines sitting idle, waiting for jobs. By specifying a schedule with the <kbd>OffPeakPeriods</kbd> option, you can specify these times of lower productivity. During those times, the parameters to control the creation of the runner's capacity are different. You specify them by putting <kbd>OffPeak</kbd> in front of it. Therefore, <kbd>IdleCount</kbd> becomes <kbd>OffpeakIdleCount</kbd>, <kbd>IdleTime</kbd> becomes <kbd>OffPeakIdletime</kbd>, and so on. The functionality of the algorithm stays the same.</p>
<p class="mce-root">In the following schedule (which is common), you can see off-peak times on weekdays during the night, evening, and the entire weekend:</p>
<pre>[runners.machine]<br/>   OffPeakPeriods = [<br/>     "* * 0-9,18-23 * * mon-fri *",<br/>     "* * * * * sat,sun *"<br/>   ]</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Distributed runners caching</h1>
                </header>
            
            <article>
                
<p><span>GitLab Runners have a built-in caching mechanism. It can be set on a global level, as well as for an individual project.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting the cache globally </h1>
                </header>
            
            <article>
                
<p>You can set a path in your <kbd>config.toml</kbd> configuration file so that it will cache every job:</p>
<pre style="padding-left: 30px">[runners.cache]<br/> Path = "/node_modules"</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting the cache at the project level</h1>
                </header>
            
            <article>
                
<p>You can set which path is to be cached in the <kbd>.gitlab-ci.yml</kbd> file for the project itself:</p>
<pre> cache:<br/> paths:<br/> - node_modules/</pre>
<p>The preceding settings apply to the context of just a single Runner host. If we use autoscaling, we need a way to have this cache shared by all of the runners to help gain speed. We can use external storage such as an S3 bucket to act as a cache. We just have to add the <kbd>[runners.cache.s3]</kbd> part to the <kbd>config.toml</kbd> file of the runners:</p>
<pre>     [runners.cache.s3]<br/>       ServerAddress = "s3-website-us-east-1.amazonaws.com"<br/>       BucketName = "joustie-gitlab-runner-cache"<br/>       AccessKey = "xxx"<br/>       SecretKey= "xxxx"<br/>       Insecure = false</pre>
<p>If this is your first time doing this, it will try to get the <kbd>cache.zip</kbd> file from the S3 storage bucket. However, if there is no file, it will complain and continue:</p>
<pre style="padding-left: 30px"><strong>Checking cache for default...</strong><br/><strong> FATAL: file does not exist</strong></pre>
<p>After the build, which populates the <kbd>node_modules</kbd> directory with dependencies, the contents of that directory is zipped and sent to the S3 storage bucket:</p>
<pre style="padding-left: 30px"><strong>Creating cache default...</strong><br/><strong> node_modules/: found 5728 matching files          </strong><br/><strong> Uploading cache.zip to https://joustie-gitlab-runner-cache.s3.amazonaws.com/project/14/default</strong><br/><strong> Created cache</strong><br/><strong> Job succeeded</strong></pre>
<p>If we retry the job, we will find that there's now a <kbd>cache.zip</kbd> file in S3, and it will be used instead of downloading all of those node dependencies again:</p>
<pre><strong>Checking cache for default...</strong><br/><strong> Downloading cache.zip from https://joustie-gitlab-runner-cache.s3.amazonaws.com/project/14/default</strong><br/><strong> Successfully extracted cache</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Distributed container registry mirroring</h1>
                </header>
            
            <article>
                
<p>Another situation that can slow down building considerably is that the runners continuously download Docker containers from the internet. It is a much better idea to create a proxy for that. In <kbd>runners.machine</kbd>, you can specify which <kbd>engine-registry-mirror</kbd> should be used. If this is used on your local network, this saves a lot of traffic. Here, you see the section as I used it in our example project:</p>
<pre style="padding-left: 30px">  MachineOptions = [<br/> "engine-registry-mirror=http://192.168.1.10:6000"<br/> ]</pre>
<p>In the most basic way, the Docker Machine executor uses <kbd>docker-machine</kbd> to spawn new instances of the GitLab Runner container.</p>
<p>You can combine this with other features, such as shared caching and using a dedicate container registry to facilitate large amounts of instances.</p>
<p>If you enable these settings for your runner, you need to deploy a caching server and a registry mirroring service, which we will show you in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing and running a proxy container registry and a caching server</h1>
                </header>
            
            <article>
                
<p>The two extra machines are necessary to help with performance when your plan is to deploy an entire elastic fleet of GitLab Runners. If you have more than five runners that can operate simultaneously, you will already gain an advantage when running a registry proxy and a caching server. An extra feature you get is that a bit of high availability is introduced in your architecture: you are able to do builds when your internet connection is flaky or offline.</p>
<p>For a proxy container registry, you need to have a proxy that implements the <span>Docker Registry HTTP API V2, which we will install in the next section.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Proxy container registry</h1>
                </header>
            
            <article>
                
<p>There is a convenient Docker container readily available to fulfill this role. You can start this Docker container immediately by using the following code:</p>
<pre><strong>docker run -d -p 6000:5000 -e REGISTRY_PROXY_REMOTEURL=https://registry-1.docker.io  --name runner-registry registry:2</strong></pre>
<p><span>If you create such a registry proxy and check the log file when the runners start a job, you will find that the proxy serves the Runner by fetching and caching images:</span></p>
<pre><strong><span>$ docker logs registry -f<br/><br/></span><span>172.17.0.1 - - [25/May/2019:14:28:40 +0000] "GET /v2/ HTTP/1.1" 200 2 "" "docker/18.09.6 go/go1.10.8 git-commit/481bc77 kernel/4.14.116-boot2docker os/linux arch/amd64 UpstreamClient(Go-http-client/1.1)"<br/>...</span></strong></pre>
<div class="packt_infobox">There isn't much to configure, but you can find more information here: <a href="https://hub.docker.com/_/registry">https://hub.docker.com/_/registry</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Caching server</h1>
                </header>
            
            <article>
                
<p>There are two options for creating a caching server. You can either get an S3 bucket in Amazon or another cloud provider or run an object storage service yourself such as MinIO, which can be found at <a href="https://min.io">https://min.io</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating an S3 bucket in Amazon Web Services</h1>
                </header>
            
            <article>
                
<p>Log in to the Amazon Web Services console and find the S3 dashboard via <span class="packt_screen">Services</span> | <span class="packt_screen">S3</span>. Click on <span class="packt_screen">Create bucket</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c07dc11e-b1f5-484d-ba44-923b4af697eb.png"/></p>
<p>We have named it <kbd>joustie-gitlab-runner-cache</kbd> and left the rest as the default:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/18cb12bc-c41e-4dd1-adb3-7a5137854437.png"/></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating your own MinIO server</h1>
                </header>
            
            <article>
                
<p>This is also conveniently available as a Docker image. It is recommended to run this on a dedicated host because the storage that's involved can grow quite large, and you don't want this service to take down any other service that is running on that host.</p>
<p>Run the container with the following command:</p>
<pre><strong>docker run -it -p 9005:9000 -v ~/.minio:/root/.minio -v /s3:/export --name caching-server minio/minio:latest server /export</strong></pre>
<p>Take note of the <kbd>/s3</kbd> volume that is mounted in the container, which will serve as the directory that stores the cached objects.</p>
<p>The output of the preceding command will appear after some time:</p>
<pre><strong>latest: Pulling from minio/minio</strong><br/><strong> e7c96db7181b: Already exists</strong><br/><strong> 94d4d681d0f2: Pull complete</strong><br/><strong> 664c3f016f88: Pull complete</strong><br/><strong> b3235cce6961: Pull complete</strong><br/><strong> Digest:sha256:244c711462a69303c0aa4f8d7943ba8b36dd55246e29da44c6653e39eaa42e70</strong><br/><strong> Status: Downloaded newer image for minio/minio:latest</strong></pre>
<p>The container image will be downloaded.</p>
<p>After that, the MinIO container will start and report the location it uses, as well as <kbd>AccesKey</kbd> and <kbd>SecretKey</kbd>, which are to be used by the runners:</p>
<pre><strong>Endpoint:  http://172.17.0.4:9000  http://127.0.0.1:9000</strong><br/><strong> AccessKey: xxx</strong><br/><strong> SecretKey: xxx</strong></pre>
<p>We will demonstrate the usage of the cache by building a Node.js example project. It contains a <kbd>node_modules</kbd> directory, which we specify as a cached location in the <kbd>.gitlab-ci.yml</kbd> file.</p>
<p>When you build the Node.js project using a GitLab Runner, it will report its use of the cache in the Runner job log file:</p>
<pre><strong>node_modules/: found 5728 matching files          </strong><br/><strong> Uploading cache.zip to http://192.168.178.82:9005/joustie-gitlab-runner-cache/project/14/default</strong><br/><strong> Created cache</strong><br/><strong> Job succeeded</strong></pre>
<p>When we looked on the dedicated machine where the MinIO container was running, we found the following directory structure when we typed in the <kbd>tree</kbd> command in <kbd>/s3</kbd> (this is the directory that's used by the MinIO Docker container to store objects):</p>
<pre><strong>$tree</strong><br/><strong> .</strong><br/><strong> └── joustie-gitlab-runner-cache</strong><br/><strong>     └── project</strong><br/><strong>         └── 14</strong><br/><strong>             └── default</strong><br/> <br/><strong> 3 directories, 1 file</strong></pre>
<p><span>We ran the GitLab Runner job again, which found <kbd>cache.zip</kbd> in the MinIO bucket.</span></p>
<p><span>This was an example of using a single runner. However, you may want to use these options to scale your Runner instances. Let's look at this in the next section.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scaling your runners</h1>
                </header>
            
            <article>
                
<p>In the previous section, we configured the software and prepared our environment so that we could scale up and down the number of runners while also providing some shared services like a registry proxy and a caching server. Now, let's look at two examples. We will run jobs on a Runner that's been configured to use VirtualBox and one that's been configured to use Amazon Web Services. VirtualBox is the open source virtualization solution from Oracle and can be found at <a href="https://www.virtualbox.org/">https://www.virtualbox.org/</a>.</p>
<p>Almost all of the Runner configuration files (the <kbd>config.toml</kbd> file) can be identical; we only change the machine driver part. Let's start with the VirtualBox option.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using Docker Machines with a local VirtualBox instance</h1>
                </header>
            
            <article>
                
<p>We start with the local <kbd>gitLab-runner</kbd> service with the <kbd>config</kbd> file for VirtualBox:</p>
<pre class="code highlight js-syntax-highlight plaintext solarized-dark"><strong><span class="line">brew services start gitlab-runner</span></strong></pre>
<p>After a few seconds, we will see that VirtualBox spins up a number of virtual machines:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/daf1c152-8dfd-42bd-b88b-caa67f7d22fe.png" style="width:27.50em;height:5.92em;"/></p>
<p><span>When we started a build of the event manager project in <a href="1eceee2e-41d6-41cc-95eb-3d225f25f3fa.xhtml">Chapter 10</a>, <em>Create Your Product, Verify, and Package it</em></span>, we <span>saw that it needs more runners (five parallel jobs) to run the pipeline. Therefore, we will start more machines:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3e8fdca2-9509-4534-a302-adb39c6f12c6.png" style="width:29.42em;height:14.75em;"/></p>
<p><span>When the build has finished, after</span> <kbd>IdleTime</kbd> <span>has gone by, the number of machines will be reduced:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b15110be-a3ce-4c39-a857-d2457f5f17cb.png" style="width:29.58em;height:3.42em;"/></p>
<p>The VirtualBox driver is an excellent choice if you already have some servers with VirtualBox installed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using docker machines that have been created on Amazon Web Services (EC2)</h1>
                </header>
            
            <article>
                
<p>If you change the machine driver from VirtualBox to Amazon EC2 and restart the runners, Docker Machine will spin up runners in Amazon if you have your credentials saved in your home directory or inserted in your shell environment. If not, then you will need to save them in the <kbd>config.toml</kbd> file.</p>
<p>After some time, the runners will appear in the EC2 web interface:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/eb8d180d-96ea-4ea7-9b4f-dceb79b5ecf2.png" style="width:47.83em;height:10.25em;"/></p>
<p><span>When those runners are started, you can change your Docker context to the one in Amazon so that you can run Docker commands and control those machines:</span></p>
<pre><strong>eval $(docker-machine env runner-fatr91wm-elastic-runner-1558647049-87941946)</strong></pre>
<p>When you access the logs of the Runner on your bastion host, you will see that it will scale down the number of machines to <kbd>IdleCount</kbd>:</p>
<pre class="code highlight js-syntax-highlight plaintext solarized-dark"><strong><span class="line">$docker logs  runner-fatr91wm</span>
<span class="line">gitlab-runner[69056]:  WARNING: Removing machine : Too many idle machines </span></strong></pre>
<p>If we start the same job from earlier (event manager project), we will see that many jobs are queued following the start of the build pipeline:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/02f62556-6b64-4240-902c-3976150881b2.png" style="width:25.58em;height:23.42em;"/></p>
<p class="mce-root"/>
<p class="CDPAlignLeft CDPAlign">The <kbd>docker-machine ls</kbd> co<span>mmand will show us that many EC2 instances are started on AWS by the autoscaling GitLab Runner:</span></p>
<pre><strong>$ docker-machine ls</strong><br/><strong> NAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORS</strong><br/><strong> runner-fatr91wm-elastic-runner-1558702358-7e149fd0 * amazonec2 Running tcp://3.86.53.66:2376 v18.09.6 </strong><br/><strong> runner-fatr91wm-elastic-runner-1558702588-f8114aa8 - amazonec2 Running tcp://34.207.177.20:2376 </strong><br/><strong> runner-fatr91wm-elastic-runner-1558702591-6e1b6fd9 - amazonec2 Running tcp://34.235.132.231:2376 </strong><br/><strong> runner-fatr91wm-elastic-runner-1558702594-dd75b49b - amazonec2 Running tcp://35.175.240.226:2376 </strong><br/><br/></pre>
<p><span>You can also view new Runner instances in the AWS web console:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bafb055f-28c8-4d73-986b-3c58622efce5.png"/></p>
<p><span>After successfully completing some of the jobs in the pipeline, the Runner will scale back down again:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5fc5c3d4-fc5c-48b5-bda9-7d36ab44b19d.png"/></p>
<p><span>As you can see, it's quite simple to change the Docker Machine driver in the Runner configuration file and get the same behavior. The Runner scaled up virtual machines in VirtualBox are used to run Docker containers, as well as Amazon Web Services.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we explained the autoscaling feature of GitLab Runners. Like in Kubernetes, it gives you the option of creating Runner instances on demand and scaling back in times of less need. Under the hood, it uses Docker Machine to manage these replicas. There are several drivers available that instantly allow runners to be created on a big list of platforms.</p>
<p>In the next chapter, we will take a look at the options for monitoring all of these instances.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What Docker feature is used by the <kbd>docker-machine</kbd> executor?</li>
<li>What additional servers are recommended when you use autoscaling?</li>
<li>In what file is the runner's distributed cache saved?</li>
<li>What is the name of the object storage that was used in this chapter?</li>
<li>What is the name of the configuration file of a GitLab Runner?</li>
<li>What is the name of the Docker image that's used as a registry cache?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><em>Getting Started with Containerization</em>, by <em>Gabriel N. Schenker</em>, <em>Hideto Saito</em>, <em>Hui-Chuan Chloe Lee</em>, and <em>Ke-Jou Carol Hsu</em>: <a href="https://www.packtpub.com/in/virtualization-and-cloud/getting-started-containerization">https://www.packtpub.com/in/virtualization-and-cloud/getting-started-containerization</a></li>
<li><em>Mastering Docker <span>–</span> Third Edition</em>, by <em>Russ McKendrick</em> and <em>Scott Gallagher</em>: <a href="https://www.packtpub.com/in/virtualization-and-cloud/mastering-docker-third-edition">https://www.packtpub.com/in/virtualization-and-cloud/mastering-docker-third-edition</a></li>
<li><em>Getting Started with Oracle VM VirtualBox</em>, by <em>Pradyumna Dash</em>: <a href="https://www.packtpub.com/in/virtualization-and-cloud/getting-started-oracle-vm-virtualbox">https://www.packtpub.com/in/virtualization-and-cloud/getting-started-oracle-vm-virtualbox</a></li>
<li><em>Hands-On AWS System Administration</em>, by <em>Glauber Gallego</em>, <em>Daniel Stori</em>, and <em>Satyajit Das</em>: <a href="https://www.packtpub.com/in/virtualization-and-cloud/hands-aws-system-administration">https://www.packtpub.com/in/virtualization-and-cloud/hands-aws-system-administration</a></li>
</ul>


            </article>

            
        </section>
    </body></html>