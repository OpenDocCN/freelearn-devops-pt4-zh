<html><head></head><body>
		<div id="_idContainer047">
			<h1 id="_idParaDest-225"><em class="italic"><a id="_idTextAnchor225"/>Chapter 12</em>: Implementing Container Networking Concepts</h1>
			<p>Container network isolation leverages network namespaces to provide separate network stacks for each container. Without a container runtime, managing network interfaces across multiple namespaces would be complex. Podman provides flexible network management that allows users to customize how containers communicate with external containers and other containers inside the same host. </p>
			<p>In this chapter, we will learn about the common configuration practices for managing container networking, along with the differences between rootless and rootfull containers.</p>
			<p>In this chapter, we're going to cover the following main topics: </p>
			<ul>
				<li>Container networking and Podman setup</li>
				<li>Interconnecting two or more containers</li>
				<li>Exposing containers outside our underlying host</li>
				<li>Rootless container network behavior</li>
			</ul>
			<h1 id="_idParaDest-226"><a id="_idTextAnchor226"/>Technical requirements</h1>
			<p>To complete this chapter, you will need a machine with a working Podman installation. As we mentioned in <a href="B17908_03_epub.xhtml#_idTextAnchor068"><em class="italic">Chapter 3</em></a>, <em class="italic">Running the First Container</em>, all the examples in this book can be executed on a Fedora 34 system or later but can be reproduced on your <strong class="bold">operating system</strong> (<strong class="bold">OS</strong>) of choice. The examples in this chapter will be related to both Podman v3.4.z and Podman v4.0.0 since they provide different network implementations.</p>
			<p>A good understanding of the topics that were covered in <a href="B17908_04_epub.xhtml#_idTextAnchor083"><em class="italic">Chapter 4</em></a>, <em class="italic">Managing Running Containers</em>, <a href="B17908_05_epub.xhtml#_idTextAnchor101"><em class="italic">Chapter 5</em></a>, <em class="italic">Implementing Storage for the Container's Data</em>, and <a href="B17908_09_epub.xhtml#_idTextAnchor167"><em class="italic">Chapter 9</em></a>, <em class="italic">Pushing Images to a Container Registry</em>, will help you grasp the container networking topics we'll be covering.</p>
			<p>You must also have a good understanding of basic networking concepts to understand topics such as routing, the IP protocol, DNS, and firewalling.</p>
			<h1 id="_idParaDest-227"><a id="_idTextAnchor227"/>Container networking and Podman setup</h1>
			<p>In this section, we'll cover Podman's networking implementation and how to configure networks. Podman 4.0.0 introduced an important change to the network stack. However, Podman 3 is still widely used in the community. For this reason, we will cover both implementations. </p>
			<p>Podman 3 leverages the <strong class="bold">Container Network Interface</strong> (<strong class="bold">CNI</strong>) to manage local networks that are <a id="_idIndexMarker1085"/>created on the host. The CNI provides a standard set of <a id="_idIndexMarker1086"/>specifications and libraries to create and configure plugin-based network interfaces in a container environment.</p>
			<p>CNI specifications were created for Kubernetes to provide a network configuration format that's used by the container runtime to set up the defined plugins, as well as an execution protocol between plugin binaries and runtimes. The great advantage of this plugin-based approach is that vendors and communities can develop third-party plugins that satisfy the CNI's specifications.</p>
			<p>The Podman 4 network stack is based on a brand new project called <strong class="bold">Netavark</strong>, a container-native <a id="_idIndexMarker1087"/>networking implementation completely written in Rust and designed to work with Podman. Rust is a great programming language for developing system and network components thanks to its efficient memory management and high performance, similar to the C programming language. Netavark provides better support for dual-stack networking (IPv4/IPv6) and inter-container DNS resolution, along with a tighter bond with the Podman project development roadmap.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Users upgrading from Podman 3 to Podman 4 will continue to use CNI by default and preserve their previous configuration. New Podman 4 installations will use Netavark by default. Users can revert to the CNI network backend by upgrading the <strong class="source-inline">network_backend</strong> field in the <strong class="source-inline">/usr/share/containers/containers.conf</strong> file.</p>
			<p>In the next subsection, we'll focus on the CNI configuration that's used by Podman 3 to orchestrate container networking. </p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor228"/>CNI configuration quick start</h2>
			<p>A typical CNI configuration file defines a list of plugins and their related configuration. The following example shows <a id="_idIndexMarker1088"/>the default CNI configuration of a fresh Podman installation on Fedora:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Chapter12/podman_cni_conf.json</p>
			<p class="source-code">  "cniVersion": "0.4.0",</p>
			<p class="source-code">  "name": "podman",</p>
			<p class="source-code">  "plugins": [</p>
			<p class="source-code">    {</p>
			<p class="source-code">      "type": "bridge",</p>
			<p class="source-code">      "bridge": "cni-podman0",</p>
			<p class="source-code">      "isGateway": true,</p>
			<p class="source-code">      "ipMasq": true,</p>
			<p class="source-code">      "hairpinMode": true,</p>
			<p class="source-code">      "ipam": {</p>
			<p class="source-code">        "type": "host-local",</p>
			<p class="source-code">        "routes": [{ "dst": "0.0.0.0/0" }],</p>
			<p class="source-code">        "ranges": [</p>
			<p class="source-code">          [</p>
			<p class="source-code">            {</p>
			<p class="source-code">              "subnet": "10.88.0.0/16",</p>
			<p class="source-code">              "gateway": "10.88.0.1"</p>
			<p class="source-code">            }</p>
			<p class="source-code">          ]</p>
			<p class="source-code">        ]</p>
			<p class="source-code">      }</p>
			<p class="source-code">    },</p>
			<p class="source-code">    {</p>
			<p class="source-code">      "type": "portmap",</p>
			<p class="source-code">      "capabilities": {</p>
			<p class="source-code">        "portMappings": true</p>
			<p class="source-code">      }</p>
			<p class="source-code">    },</p>
			<p class="source-code">    {</p>
			<p class="source-code">      "type": "firewall"</p>
			<p class="source-code">    },</p>
			<p class="source-code">    {</p>
			<p class="source-code">      "type": "tuning"</p>
			<p class="source-code">    }</p>
			<p class="source-code">  ]</p>
			<p class="source-code">}</p>
			<p>As we can see, the <strong class="source-inline">plugins</strong> list in this file contains a set of plugins that are used by the runtime to orchestrate container networking. </p>
			<p>The CNI community curates a <a id="_idIndexMarker1089"/>repository of reference plugins that can be <a id="_idIndexMarker1090"/>used by container runtimes. CNI reference <a id="_idIndexMarker1091"/>plugins are organized into <strong class="bold">interface-creating</strong>, <strong class="bold">IP address management</strong> (<strong class="bold">IPAM</strong>), and <strong class="bold">Meta</strong> plugins. Interface-creating plugins can make use of IPAM and Meta plugins. </p>
			<p>The following non-exhaustive list describes the most commonly used interface-creating plugins:</p>
			<ul>
				<li><strong class="source-inline">bridge</strong>: This plugin creates a dedicated Linux bridge on the host for the network. Container interfaces are attached to the managed bridge to communicate between each other and with the external systems. This plugin is currently supported by Podman and by the <strong class="source-inline">podman network</strong> CLI tools and is the default interface-creating plugin that's configured when Podman is installed or a new network is created.</li>
				<li><strong class="source-inline">ipvlan</strong>: This plugin allows you to attach an IPVLAN interface to the container. The IPVLAN solution is an alternative to the traditional Linux bridge networking solution for containers, where a single parent interface is shared across multiple sub-interfaces, each with an IP address. This plugin is currently supported by Podman but you can still manually create and edit the CNI configuration file if necessary.</li>
				<li><strong class="source-inline">macvlan</strong>: This plugin allows a MACVLAN configuration, which is an approach similar to IPVLAN with one <a id="_idIndexMarker1092"/>main difference: in this configuration, each container sub-interface also gets a MAC address. This plugin is currently supported by Podman and by the <strong class="source-inline">podman network</strong> CLI tools.</li>
				<li><strong class="source-inline">host-device</strong>: This plugin allows you to directly pass an existing interface into a container. This is currently not supported by Podman.</li>
			</ul>
			<p>CNI IPAM plugins are related to <a id="_idIndexMarker1093"/>the IP address management inside containers. There are only three reference IPAM plugins:</p>
			<ul>
				<li><strong class="source-inline">dhcp</strong>: This plugin lets you execute a daemon on the host that manages the <strong class="source-inline">dhcp</strong> leases on behalf of the running containers. It also implies that a running <strong class="source-inline">dhcp</strong> server is already running on the host network.</li>
				<li><strong class="source-inline">host-local</strong>: This plugin is used to allocate IP addresses to containers using a defined address range. The allocation data is stored in the host filesystem. It is optimal for local container execution and is the default IPAM plugin that's used by Podman in the network bridge.</li>
				<li><strong class="source-inline">static</strong>: This is a basic plugin that manages a discrete list of static addresses that are assigned to containers.</li>
			</ul>
			<p>NI Meta plugins are used to configure specific behaviors in the host, such as tuning, firewall rules, and port <a id="_idIndexMarker1094"/>mapping, and are executed as chained plugins along with the interface-creating plugins. The current Meta plugins that are maintained in the reference plugins repository are as follows:</p>
			<ul>
				<li><strong class="source-inline">portmap</strong>: This plugin is used to manage port mapping between the container and the host. It applies <a id="_idIndexMarker1095"/>configuration using the host firewall (<strong class="source-inline">iptables</strong>) and is <a id="_idIndexMarker1096"/>responsible for creating <strong class="bold">Source NAT</strong> (<strong class="bold">SNAT</strong>) and <strong class="bold">Destination Nat</strong> (<strong class="bold">DNAT</strong>) rules. This plugin is enabled by default in Podman.</li>
				<li><strong class="source-inline">firewall</strong>: This plugin configures firewall rules to allow container ingress and egress traffic. It's enabled by default in Podman.</li>
				<li><strong class="source-inline">tuning</strong>: This plugin <a id="_idIndexMarker1097"/>customizes system tuning (using <strong class="source-inline">sysctl</strong> parameters) and interface attributes in the network namespace. It's enabled by default in Podman.</li>
				<li><strong class="source-inline">bandwidth</strong>: This plugin can be used to configure traffic rate limiting on containers using the Linux traffic control subsystem.</li>
				<li><strong class="source-inline">sbr</strong>: This plugin <a id="_idIndexMarker1098"/>is used to configure <strong class="bold">source-based routing</strong> (<strong class="bold">SBR</strong>) on interfaces.<p class="callout-heading">Important Note</p><p class="callout">On a Fedora system, all the CNI plugin binaries are located in the <strong class="source-inline">/usr/libexec/cni</strong> folder and are provided by the <strong class="source-inline">containernetworking-plugins</strong> package, installed as a Podman dependency.</p></li>
			</ul>
			<p>Going back to the CNI configuration example, we can see that the default Podman configuration uses a <strong class="source-inline">bridge</strong> plugin with <strong class="source-inline">host-local</strong> IP address management and that the <strong class="source-inline">portmap</strong>, <strong class="source-inline">tuning</strong>, and <strong class="source-inline">firewall</strong> plugins are chained together with it.</p>
			<p>In the default network that was created for Podman, the subnet that's been allocated for container networking is <strong class="source-inline">10.88.0.0/16</strong> and the bridge, called <strong class="source-inline">cni-podman0</strong>, acts as the default gateway to containers on <strong class="source-inline">10.88.0.1</strong>, implying that all outbound traffic from a container <a id="_idIndexMarker1099"/>is directed to the bridge's interface.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">This configuration is applied to rootfull containers only. Later in this chapter, we'll learn that Podman uses a different networking approach for rootless containers to overcome the user's limited privileges. We will see that this approach has many limitations on host interfaces and IP address management.</p>
			<p>Now, let's see what happens on the host when a new rootfull container is created.</p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor229"/>Podman CNI walkthrough</h2>
			<p>In this subsection, we will investigate the most peculiar network events that occur when a new container is created <a id="_idIndexMarker1100"/>when CNI is used as a network backend.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">All the examples in this subsection are executed as the <strong class="source-inline">root</strong> user. Ensure that you clean up the existing running containers to have a clearer view of the network interfaces and firewall rules.</p>
			<p>We will try to run an example using the Nginx container and map its default internal port, <strong class="source-inline">80/tcp</strong>, to the host port, <strong class="source-inline">8080/tcp</strong>.</p>
			<p>Before we begin, we want to verify the current host's IP configuration:</p>
			<p class="source-code"># ip addr show</p>
			<p class="source-code">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group</p>
			<p class="source-code">default qlen 1000</p>
			<p class="source-code">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</p>
			<p class="source-code">    inet 127.0.0.1/8 scope host lo</p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">    inet6 ::1/128 scope host </p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000</p>
			<p class="source-code">    link/ether 52:54:00:a9:ce:df brd ff:ff:ff:ff:ff:ff</p>
			<p class="source-code">    altname enp0s5</p>
			<p class="source-code">    altname ens5</p>
			<p class="source-code">    inet 192.168.121.189/24 brd 192.168.121.255 scope global dynamic noprefixroute eth0</p>
			<p class="source-code">       valid_lft 3054sec preferred_lft 3054sec</p>
			<p class="source-code">    inet6 fe80::2fb:9732:a0d9:ac70/64 scope link noprefixroute        valid_lft forever preferred_lft forever</p>
			<p class="source-code">3: <strong class="bold">cni-podman0</strong>: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state <strong class="bold">DOWN</strong> group default qlen 1000</p>
			<p class="source-code">    link/ether de:52:45:ae:1a:7f brd ff:ff:ff:ff:ff:ff</p>
			<p class="source-code">    inet <strong class="bold">10.88.0.1/16</strong> brd 10.88.255.255 scope global cni-podman0</p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">    inet6 fe80::dc52:45ff:feae:1a7f/64 scope link </p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p>Along with the host's main interface, <strong class="source-inline">eth0</strong>, we can see a <strong class="source-inline">cni-podman0</strong> bridge interface with <a id="_idIndexMarker1101"/>an address of <strong class="source-inline">10.88.0.1/16</strong>. Also, notice that the bridge's state is set to <strong class="source-inline">DOWN</strong>.</p>
			<p class="callout-heading">Important</p>
			<p class="callout">If the host that's being used for the test is a fresh install and Podman has never been executed before, the <strong class="source-inline">cni-podman0</strong> bridge interface will not be listed. This is not a problem – it will be created when a rootfull container is created for the first time.</p>
			<p>If no other container is running on the host, we should see no interface attached to the virtual bridge. To verify this, we are going to use the <strong class="source-inline">bridge link show</strong> command, whose output is expected to be empty:</p>
			<p class="source-code"># bridge link show cni-podman0</p>
			<p>Looking at the firewall rules, we do not expect to see rules related to containers in the <strong class="source-inline">filter</strong> and <strong class="source-inline">nat</strong> tables:</p>
			<p class="source-code"># iptables -L</p>
			<p class="source-code"># iptables -L -t nat </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The output of the preceding commands has been omitted for the sake of brevity, but it is worth noting that the <strong class="source-inline">filter</strong> table should already contain two CNI-related chains named <strong class="source-inline">CNI-ADMIN</strong> and <strong class="source-inline">CNI-FORWARD</strong>.</p>
			<p>Finally, we want to inspect the routing rules for the <strong class="source-inline">cni-podman0</strong> interface:</p>
			<p class="source-code"># ip route show dev cni-podman0 </p>
			<p class="source-code">10.88.0.0/16 proto kernel scope link src 10.88.0.1 linkdown</p>
			<p>This command says that <a id="_idIndexMarker1102"/>all traffic going to the <strong class="source-inline">10.88.0.0/16</strong> network goes through the <strong class="source-inline">cni-podman0</strong> interface.</p>
			<p>Let's run our Nginx container and see what happens to the network interfaces, routing, and firewall configuration: </p>
			<p class="source-code"># podman run -d -p 8080:80 \</p>
			<p class="source-code">  --name net_example docker.io/library/nginx</p>
			<p>The first and most interesting event is a new network interface being created, as shown in the output of the <strong class="source-inline">ip addr show</strong> command:</p>
			<p class="source-code"># ip addr show</p>
			<p class="source-code">[...omitted output...]</p>
			<p class="source-code">3: cni-podman0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state <strong class="bold">UP</strong> group default qlen 1000</p>
			<p class="source-code">    link/ether de:52:45:ae:1a:7f brd ff:ff:ff:ff:ff:ff</p>
			<p class="source-code">    inet 10.88.0.1/16 brd 10.88.255.255 scope global cni-podman0</p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">    inet6 fe80::dc52:45ff:feae:1a7f/64 scope link </p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">5: <strong class="bold">vethcf8b2132@if2</strong>: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue <strong class="bold">master cni-podman0</strong> state UP group default </p>
			<p class="source-code">    link/ether b6:4c:1d:06:39:5a brd ff:ff:ff:ff:ff:ff link-netns <strong class="bold">cni-df380fb0-b8a6-4f39-0d19-99a0535c2f2d</strong></p>
			<p class="source-code">    inet6 fe80::90e3:98ff:fe6a:acff/64 scope link </p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p>This new interface is part of a <strong class="bold">veth pair</strong> (see <strong class="source-inline">man 4 veth</strong>), a couple of virtual Ethernet devices that act like a local tunnel. Veth pairs are native Linux kernel virtual interfaces that don't depend <a id="_idIndexMarker1103"/>on a container runtime and can be applied to use cases that go beyond container execution.</p>
			<p>The interesting part of veth pairs is that they can be spawned across multiple network namespaces and that a packet that's sent to one side of the pair is immediately received on the other side.</p>
			<p>The <strong class="source-inline">vethcf8b2132@if2</strong> interface is linked to a device that resides in a network namespace named <strong class="source-inline">cni-df380fb0-b8a6-4f39-0d19-99a0535c2f2d</strong>. Since Linux offers us the option to inspect network namespaces using the <strong class="source-inline">ip netns</strong> command, we can check if the namespace exists and inspect its network stack:</p>
			<p class="source-code"># ip netns</p>
			<p class="source-code">cni-df380fb0-b8a6-4f39-0d19-99a0535c2f2d (id: 0)</p>
			<p class="callout-heading">Hint</p>
			<p class="callout">When a new network namespace is created, a file with the same name under <strong class="source-inline">/var/run/netns/</strong> is created. This file has also the same inode number that's pointed to by the symlink under <strong class="source-inline">/proc/&lt;PID&gt;/ns/net</strong>. When the file is opened, the returned file descriptor gives access to the namespace.</p>
			<p>The preceding command confirms that the network namespace exists. Now, we want to inspect the network interfaces that have been defined inside it:</p>
			<p class="source-code"># ip netns exec cni-df380fb0-b8a6-4f39-0d19-99a0535c2f2d ip addr show</p>
			<p class="source-code">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</p>
			<p class="source-code">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</p>
			<p class="source-code">    inet 127.0.0.1/8 scope host lo</p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">    inet6 ::1/128 scope host </p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">2: <strong class="bold">eth0@if5</strong>: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default </p>
			<p class="source-code">    link/ether fa:c9:6e:5c:db:ad brd ff:ff:ff:ff:ff:ff link-netnsid 0</p>
			<p class="source-code">    inet 10.88.0.3/16 brd 10.88.255.255 scope global eth0</p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">    inet6 fe80::f8c9:6eff:fe5c:dbad/64 scope link </p>
			<p class="source-code">       valid_lft forever preferred_lft forever </p>
			<p>Here, we executed an <strong class="source-inline">ip addr show</strong> command that's nested inside the <strong class="source-inline">ip netns exec</strong> command. The output <a id="_idIndexMarker1104"/>shows us an interface that is on the other side of our veth pair. This also tells us something valuable: the container's IPv4 address, set to <strong class="source-inline">10.88.0.3</strong>.</p>
			<p class="callout-heading">Hint</p>
			<p class="callout">If you're curious, the container IP configuration, when using Podman's default network with the <strong class="source-inline">host-local</strong> IPAM plugin, is persisted to the <strong class="source-inline">/var/lib/cni/networks/podman</strong> folder. Here, a file named after the assigned IP address is created and written with the container-generated ID. </p>
			<p class="callout">If a new network is created and used by a container, its configuration will be persisted in the <strong class="source-inline">/var/lib/cni/networks/&lt;NETWORK_NAME&gt;</strong> folder.</p>
			<p>We can also inspect the container's routing tables:</p>
			<p class="source-code"># ip netns exec cni-df380fb0-b8a6-4f39-0d19-99a0535c2f2d ip route</p>
			<p class="source-code">default via 10.88.0.1 dev eth0 </p>
			<p class="source-code">10.88.0.0/16 dev eth0 proto kernel scope link src 10.88.0.3</p>
			<p>All the outbound traffic that's directed to the external networks will go through the <strong class="source-inline">10.88.0.1</strong> address, which has been assigned to the <strong class="source-inline">cni-podman0</strong> bridge.</p>
			<p>When a new container is created, the <strong class="source-inline">firewall</strong> and <strong class="source-inline">portmapper</strong> CNI plugins apply the necessary rules in the <a id="_idIndexMarker1105"/>host filter and NAT tables. In the following code, we can see the rules that have been applied to the container IP address in the <strong class="source-inline">nat</strong> table, where SNAT, DNAT, and masquerading rules have been applied:</p>
			<p class="source-code"># iptables -L -t nat -n | grep -B4 10.88.0.3</p>
			<p class="source-code">Chain POSTROUTING (policy ACCEPT)</p>
			<p class="source-code">target     prot opt source               destination         </p>
			<p class="source-code">CNI-HOSTPORT-MASQ  all  --  0.0.0.0/0            0.0.0.0/0            /* CNI portfwd requiring masquerade */</p>
			<p class="source-code">CNI-fb51a7bfa5365a8a89e764fd  all  --  10.88.0.3            0.0.0.0/0            /* name: "podman" id: "a5054cca3436a7bc4dbf78fe4b901ceef0569ced24181d2e7b118232123a5f e3" */</p>
			<p class="source-code">--</p>
			<p class="source-code">Chain CNI-DN-fb51a7bfa5365a8a89e76 (1 references)</p>
			<p class="source-code">target     prot opt source               destination         </p>
			<p class="source-code">CNI-HOSTPORT-SETMARK  tcp  --  10.88.0.0/16         0.0.0.0/0            tcp dpt:8080</p>
			<p class="source-code">CNI-HOSTPORT-SETMARK  tcp  --  127.0.0.1            0.0.0.0/0            tcp dpt:8080</p>
			<p class="source-code"><strong class="bold">DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:8080 to:10.88.0.3:80</strong></p>
			<p>The bolder line shows a DNAT rule in a custom chain named <strong class="source-inline">CNI-DN-fb51a7bfa5365a8a89e76</strong>. This rule says that all the TCP packets whose destination is the <strong class="source-inline">8080/tcp</strong> port on the host should be redirected to the <strong class="source-inline">10.88.0.3:80</strong> port, which is the network socket <a id="_idIndexMarker1106"/>that's exposed by the container. This rule matches the<strong class="source-inline">–p 8080:80</strong> option that we passed during container creation.</p>
			<p>But how does the container communicate with the external world? Let's inspect the <strong class="source-inline">cni-podman0</strong> bridge again while looking for notable changes:</p>
			<p class="source-code"># bridge link show cni-podman0</p>
			<p class="source-code">5: vethcf8b2132@eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 master cni-podman0 state forwarding priority 32 cost 2</p>
			<p>The aforementioned interface is connected to the virtual bridge, which also happens to have an IP address assigned to it (<strong class="source-inline">10.88.0.1</strong>) that acts as the default gateway for all the containers.</p>
			<p>Let's try to trace the path of an ICMP packet from the container to a well-known host, <strong class="source-inline">1.1.1.1</strong> (Cloudflare public DNS). To do so, we must run the <strong class="source-inline">traceroute</strong> utility from the container network's namespace using the <strong class="source-inline">ip netns exec</strong> command:</p>
			<p class="source-code"># ip netns exec cni-df380fb0-b8a6-4f39-0d19-99a0535c2f2d traceroute -I 1.1.1.1</p>
			<p class="source-code">traceroute to 1.1.1.1 (1.1.1.1), 30 hops max, 60 byte packets</p>
			<p class="source-code">1  _gateway (10.88.0.1)  0.071 ms  0.025 ms  0.003 ms</p>
			<p class="source-code">2  192.168.121.1 (192.168.121.1)  0.206 ms  0.195 ms  0.189 ms</p>
			<p class="source-code">3  192.168.1.1 (192.168.1.1)  5.326 ms  5.323 ms  5.319 ms</p>
			<p class="source-code">4  192.168.50.6 (192.168.50.6)  17.598 ms  17.595 ms  17.825 ms</p>
			<p class="source-code">5  192.168.50.5 (192.168.50.5)  17.821 ms  17.888 ms  17.882 ms</p>
			<p class="source-code">6  10.177.21.173 (10.177.21.173)  17.998 ms  17.772 ms  24.777 ms</p>
			<p class="source-code">7  185.210.48.42 (185.210.48.42)  25.963 ms  7.604 ms  7.702 ms</p>
			<p class="source-code">8  185.210.48.43 (185.210.48.43)  7.906 ms  10.344 ms  10.984 ms</p>
			<p class="source-code">9  185.210.48.77 (185.210.48.77)  12.212 ms  12.030 ms  12.983 ms</p>
			<p class="source-code">10  1.1.1.1 (1.1.1.1)  12.524 ms  12.160 ms  12.649 ms</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The traceroute program could be installed on the host by default. To install it on Fedora, run the <strong class="source-inline">sudo dnf install traceroute</strong> command.</p>
			<p>The preceding output shows a series of <strong class="bold">hops</strong>, which are a way to count the number of routes that a packet must pass to reach a destination. In this example, we have a total of 10 hops, which is necessary to reach the target node. The first hop goes through the container's default gateway (<strong class="source-inline">10.88.0.1</strong>), moving to the host's network stack. </p>
			<p>The second hop is the host's default gateway (<strong class="source-inline">192.168.121.1</strong>), which is assigned to a virtual bridge in a hypervisor and connected to our lab's host VM. </p>
			<p>The third hop is a private <a id="_idIndexMarker1107"/>network default gateway (<strong class="source-inline">192.168.1.1</strong>) that's assigned to a physical router that's connected to the lab's hypervisor network.</p>
			<p>This demonstrates that all the traffic goes through the <strong class="source-inline">cni-podman0</strong> bridge interface.</p>
			<p>We can create more than one network, either using Podman native commands or our favorite editor to manage JSON files directly.</p>
			<p>Now that we've <a id="_idIndexMarker1108"/>explored CNI's implementation and configuration details, let's look at the new Netavark implementation in Podman 4. </p>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor230"/>Netavark configuration quick start</h2>
			<p>Podman's 4.0.0 release <a id="_idIndexMarker1109"/>introduced Netavark as the default <a id="_idIndexMarker1110"/>network backend. The advantages of Netavark are as follows:</p>
			<ul>
				<li>Support for dual IPv4/IPv6 stacks</li>
				<li>Support for DNS native resolution using the <strong class="bold">aardvark-dns</strong> companion project</li>
				<li>Support for rootless containers</li>
				<li>Support for different firewall implementations, including iptables, firewalld, and nftables</li>
			</ul>
			<p>The configuration files that are used by Netavark are not very different from the ones that were shown for CNI. Netavark still uses JSON format to configure networks; files are stored under the <strong class="source-inline">/etc/containers/networks</strong> path for rootfull containers and the <strong class="source-inline">~/.local/share/containers/storage/networks</strong> path for rootless containers.</p>
			<p>The following configuration file shows an example network that's been created and managed under Netavark:</p>
			<p class="source-code">[</p>
			<p class="source-code">     {</p>
			<p class="source-code">          "name": "netavark-example",</p>
			<p class="source-code">          "id": "d98700453f78ea2fdfe4a1f77eae9e121f3cbf4b6160dab89edf9ce23c b924d7",</p>
			<p class="source-code">          "driver": "bridge",</p>
			<p class="source-code">          "network_interface": "podman1",</p>
			<p class="source-code">          "created": "2022-02-17T21:37:59.873639361Z",</p>
			<p class="source-code">          "subnets": [</p>
			<p class="source-code">               {</p>
			<p class="source-code">                    "subnet": "10.89.4.0/24",</p>
			<p class="source-code">                    "gateway": "10.89.4.1"</p>
			<p class="source-code">               }</p>
			<p class="source-code">          ],</p>
			<p class="source-code">          "ipv6_enabled": false,</p>
			<p class="source-code">          "internal": false,</p>
			<p class="source-code">          "dns_enabled": true,</p>
			<p class="source-code">          "ipam_options": {</p>
			<p class="source-code">               "driver": "host-local"</p>
			<p class="source-code">          }</p>
			<p class="source-code">     }</p>
			<p class="source-code">]</p>
			<p>The first noticeable <a id="_idIndexMarker1111"/>element is the more compact size of the configuration file compared to a CNI configuration. The following fields are defined:</p>
			<ul>
				<li><strong class="source-inline">name</strong>: The name of the network.</li>
				<li><strong class="source-inline">id</strong>: The unique network ID.</li>
				<li><strong class="source-inline">driver</strong>: This specifies the kind of network driver that's being used. The default is <strong class="source-inline">bridge</strong>. Netavark also supports MACVLAN drivers.</li>
				<li><strong class="source-inline">network_interface</strong>: This is the name of the network interface associated with the network. If <strong class="source-inline">bridge</strong> is the configured driver, this will be the name of the Linux bridge. In the preceding example, a bridge is created called <strong class="source-inline">podman1</strong>.</li>
				<li><strong class="source-inline">created</strong>: The network creation timestamp.</li>
				<li><strong class="source-inline">subnets</strong>: This provides a list of subnet and gateway objects. Subnets are assigned automatically. However, when you're creating a new network with Podman, users can provide a custom CIDR. Netavark allows you to manage multiple subnets and gateways on a network.</li>
				<li><strong class="source-inline">ipv6_enabled</strong>: Native support for IPv6 in Netavark can be enabled or disabled with this boolean.</li>
				<li><strong class="source-inline">internal</strong>: This boolean is used to configure a network for internal use only and to block external routing.</li>
				<li><strong class="source-inline">dns_enabled</strong>: This boolean enables DNS resolution for the network and is served by the <strong class="source-inline">aardvark-dns</strong> daemon.</li>
				<li><strong class="source-inline">ipam_options</strong>: This object defines a series of <strong class="source-inline">ipam</strong> parameters. In the preceding example, the only option is the kind of IPAM driver, <strong class="source-inline">host-local</strong>, which behaves in a way similar to the CNI host-local plugin.</li>
			</ul>
			<p>The default <a id="_idIndexMarker1112"/>Podman 4 network, named <strong class="source-inline">podman</strong>, implements a bridge driver (the bridge's name is <strong class="source-inline">podman0</strong>). Here, DNS support is disabled, similar to what happens with the default CNI configuration. </p>
			<p>Netavark is also an executable binary that's installed by default in the <strong class="source-inline">/usr/libexec/podman/netavark</strong> path. It has a simple <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>) that implements the <strong class="source-inline">setup</strong> and <strong class="source-inline">teardown</strong> commands, applying the network configuration <a id="_idIndexMarker1113"/>to a given network namespace (see <strong class="source-inline">man netavark</strong>).</p>
			<p>Now, let's look at the effects of creating a new container with Netavark.</p>
			<h2 id="_idParaDest-231"><a id="_idTextAnchor231"/>Podman Netavark walkthrough</h2>
			<p>Like CNI, Netavark manages the creation of network configurations in the container network namespace <a id="_idIndexMarker1114"/>and the host network namespace, including the creation of veth pairs and the Linux bridge that's defined in the config file. </p>
			<p>Before the first container is created in the default Podman network, no bridges are created and the host interfaces are the only ones available, along with the loopback interface: </p>
			<p class="source-code"># ip addr show</p>
			<p class="source-code">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</p>
			<p class="source-code">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</p>
			<p class="source-code">    inet 127.0.0.1/8 scope host lo</p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">    inet6 ::1/128 scope host </p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000</p>
			<p class="source-code">    link/ether 52:54:00:9a:ea:f4 brd ff:ff:ff:ff:ff:ff</p>
			<p class="source-code">    altname enp0s5</p>
			<p class="source-code">    altname ens5</p>
			<p class="source-code">    inet 192.168.121.15/24 brd 192.168.121.255 scope global dynamic noprefixroute eth0</p>
			<p class="source-code">       valid_lft 3293sec preferred_lft 3293sec</p>
			<p class="source-code">    inet6 fe80::d0fb:c0d1:159e:2d54/64 scope link noprefixroute </p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p>Let's run a new Nginx container and see what happens:</p>
			<p class="source-code"># podman run -d -p 8080:80 \</p>
			<p class="source-code">  --name nginx-netavark </p>
			<p class="source-code">  docker.io/library/nginx</p>
			<p>When the container is <a id="_idIndexMarker1115"/>started, the <strong class="source-inline">podman0</strong> bridge and a veth interface appear:</p>
			<p class="source-code"># ip addr show</p>
			<p class="source-code">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</p>
			<p class="source-code">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</p>
			<p class="source-code">    inet 127.0.0.1/8 scope host lo</p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">    inet6 ::1/128 scope host </p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000</p>
			<p class="source-code">    link/ether 52:54:00:9a:ea:f4 brd ff:ff:ff:ff:ff:ff</p>
			<p class="source-code">    altname enp0s5</p>
			<p class="source-code">    altname ens5</p>
			<p class="source-code">    inet 192.168.121.15/24 brd 192.168.121.255 scope global dynamic noprefixroute eth0</p>
			<p class="source-code">       valid_lft 3140sec preferred_lft 3140sec</p>
			<p class="source-code">    inet6 fe80::d0fb:c0d1:159e:2d54/64 scope link noprefixroute </p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">3: veth2772d0ea@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master podman0 state UP group default qlen 1000</p>
			<p class="source-code">    link/ether fa:a3:31:63:21:60 brd ff:ff:ff:ff:ff:ff link-netns netns-61a5f9f9-9dff-7488-3922-165cdc6cd320</p>
			<p class="source-code">    inet6 fe80::f8a3:31ff:fe63:2160/64 scope link </p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">8: podman0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000</p>
			<p class="source-code">    link/ether ea:b4:9d:dd:2c:d1 brd ff:ff:ff:ff:ff:ff</p>
			<p class="source-code">    inet 10.88.0.1/16 brd 10.88.255.255 scope global podman0</p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">    inet6 fe80::24ec:30ff:fe1a:2ca8/64 scope link </p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p>There are no particular changes for the end user in terms of network namespaces, mixing context between version management, firewall rules, or routing compared to the CNI walkthrough provided previously. </p>
			<p>Again, a network <a id="_idIndexMarker1116"/>namespace in the host is created for the <strong class="source-inline">nginx-netavark</strong> container. Let's inspect the contents of the network namespace:</p>
			<p class="source-code"># ip netns exec netns-61a5f9f9-9dff-7488-3922-165cdc6cd320 ip addr show</p>
			<p class="source-code">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</p>
			<p class="source-code">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</p>
			<p class="source-code">    inet 127.0.0.1/8 scope host lo</p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">    inet6 ::1/128 scope host </p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">2: eth0@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000    link/ether ae:9b:7f:07:3f:16 brd ff:ff:ff:ff:ff:ff link-netnsid 0</p>
			<p class="source-code">    inet 10.88.0.4/16 brd 10.88.255.255 scope global eth0</p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">    inet6 fe80::ac9b:7fff:fe07:3f16/64 scope link </p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p>Once again, it is possible to find the internal IP address that's been assigned to the container.</p>
			<p>If the container is executed in rootless mode, the bridge and veth pairs will be created in a rootless network namespace. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The rootless network namespace can be inspected in Podman 4 with the <strong class="source-inline">podman unshare --rootless-netns</strong> command.</p>
			<p class="callout">Users running Podman 3 and CNI can use the <strong class="source-inline">--rootless-cni</strong> option to obtain the same results.</p>
			<p>In the next subsection, we will <a id="_idIndexMarker1117"/>learn how to manage and customize container networks with the CLI tools that are offered by Podman.</p>
			<h2 id="_idParaDest-232"><a id="_idTextAnchor232"/>Managing networks with Podman</h2>
			<p>The <strong class="source-inline">podman network</strong> command provides the necessary tools for managing container networks. The following <a id="_idIndexMarker1118"/>subcommands <a id="_idIndexMarker1119"/>are available:</p>
			<ul>
				<li><strong class="source-inline">create</strong>: Creates a new network</li>
				<li><strong class="source-inline">connect</strong>: Connects to a given network</li>
				<li><strong class="source-inline">disconnect</strong>: Disconnects from a network</li>
				<li><strong class="source-inline">exists</strong>: Checks if a network exists</li>
				<li><strong class="source-inline">inspect</strong>: Dumps the CNI configuration of a network</li>
				<li><strong class="source-inline">prune</strong>: Removes unused networks</li>
				<li><strong class="source-inline">reload</strong>: Reloads container firewall rules</li>
				<li><strong class="source-inline">rm</strong>: Removes a given network</li>
			</ul>
			<p>In this section, you will learn how to create a new network and connect a container to it. For Podman 3, all the generated CNI config files are written to the <strong class="source-inline">/etc/cni/net.d</strong> folder in the host. </p>
			<p>For Podman 4, all the generated Netavark config files for rootfull networks are written to <strong class="source-inline">/etc/containers/networks</strong>, while the config files for rootless networks are written to <strong class="source-inline">~/.local/share/containers/storage/networks</strong>.</p>
			<p>The following command creates a new network called <strong class="source-inline">example1</strong>:</p>
			<p class="source-code"># podman network create \</p>
			<p class="source-code">  --driver bridge \</p>
			<p class="source-code">  --gateway "10.89.0.1" \</p>
			<p class="source-code">  --subnet "10.89.0.0/16" example1</p>
			<p>Here, we provided subnet and gateway information, along with the driver type that corresponds <a id="_idIndexMarker1120"/>to the CNI interface-creating plugin. The <a id="_idIndexMarker1121"/>resulting network configuration is written in the aforementioned paths according to the kind of network backend and can be inspected with the <strong class="source-inline">podman network inspect</strong> command.</p>
			<p>The following output shows the configuration for a CNI network backend:</p>
			<p class="source-code"># podman network inspect example1</p>
			<p class="source-code">[</p>
			<p class="source-code">    {</p>
			<p class="source-code">        "cniVersion": "0.4.0",</p>
			<p class="source-code">        "name": "example1",</p>
			<p class="source-code">        "plugins": [</p>
			<p class="source-code">            {</p>
			<p class="source-code">                <strong class="bold">"bridge": "cni-podman1"</strong>,</p>
			<p class="source-code">                "hairpinMode": true,</p>
			<p class="source-code">                "ipMasq": true,</p>
			<p class="source-code">                "ipam": {</p>
			<p class="source-code">                    "ranges": [</p>
			<p class="source-code">                        [</p>
			<p class="source-code">                            {</p>
			<p class="source-code">                                "gateway": "10.89.0.1",</p>
			<p class="source-code">                                <strong class="bold">"subnet": "10.89.0.0/16"</strong></p>
			<p class="source-code">                            }</p>
			<p class="source-code">                        ]</p>
			<p class="source-code">                    ],</p>
			<p class="source-code">                    "routes": [</p>
			<p class="source-code">                        {</p>
			<p class="source-code">                            "dst": "0.0.0.0/0"</p>
			<p class="source-code">                        }</p>
			<p class="source-code">                    ],</p>
			<p class="source-code">                    "type": "host-local"</p>
			<p class="source-code">                },</p>
			<p class="source-code">                "isGateway": true,</p>
			<p class="source-code">                "type": "bridge"</p>
			<p class="source-code">            },</p>
			<p class="source-code">            {</p>
			<p class="source-code">                "capabilities": {</p>
			<p class="source-code">                    "portMappings": true</p>
			<p class="source-code">                },</p>
			<p class="source-code">                "type": "portmap"</p>
			<p class="source-code">            },</p>
			<p class="source-code">            {</p>
			<p class="source-code">                "backend": "",</p>
			<p class="source-code">                "type": "firewall"</p>
			<p class="source-code">            },</p>
			<p class="source-code">            {</p>
			<p class="source-code">                "type": "tuning"</p>
			<p class="source-code">            },</p>
			<p class="source-code">            {</p>
			<p class="source-code">                "capabilities": {</p>
			<p class="source-code">                    "aliases": true</p>
			<p class="source-code">                },</p>
			<p class="source-code">                "domainName": "dns.podman",</p>
			<p class="source-code">                "type": "dnsname"</p>
			<p class="source-code">            }</p>
			<p class="source-code">        ]</p>
			<p class="source-code">    }</p>
			<p class="source-code">]</p>
			<p>The new network CNI configuration shows that a bridge called <strong class="source-inline">cni-podman1</strong> will be created for this network and that containers will allocate IPs from the <strong class="source-inline">10.89.0.0/16</strong> subnet.</p>
			<p>The other fields of the configuration are pretty similar to the default one, except for the <strong class="source-inline">dnsname</strong> plugin (project's repository: <a href="https://github.com/containers/dnsname">https://github.com/containers/dnsname</a>), which is used <a id="_idIndexMarker1122"/>to enable internal container name <a id="_idIndexMarker1123"/>resolution. This feature provides an advantage in cross-container communication that we will look at in the next subsection.</p>
			<p>The following output shows the generated configuration for a Netavark network backend:</p>
			<p class="source-code"># podman network inspect example1</p>
			<p class="source-code">[</p>
			<p class="source-code">     {</p>
			<p class="source-code">          "name": "example1",</p>
			<p class="source-code">          "id": "a8ca04a41ef303e3247097b86d9048750e5f1aa819ec573b0e5f78e3cc8a 971b",</p>
			<p class="source-code">          "driver": "bridge",</p>
			<p class="source-code">          "network_interface": "podman1",</p>
			<p class="source-code">          "created": "2022-02-18T17:56:28.451701452Z",</p>
			<p class="source-code">          "subnets": [</p>
			<p class="source-code">               {</p>
			<p class="source-code">                    "subnet": "10.89.0.0/16",</p>
			<p class="source-code">                    "gateway": "10.89.0.1"</p>
			<p class="source-code">               }</p>
			<p class="source-code">          ],</p>
			<p class="source-code">          "ipv6_enabled": false,</p>
			<p class="source-code">          "internal": false,</p>
			<p class="source-code">          "dns_enabled": true,</p>
			<p class="source-code">          "ipam_options": {</p>
			<p class="source-code">               "driver": "host-local"</p>
			<p class="source-code">          }</p>
			<p class="source-code">     }</p>
			<p class="source-code">]</p>
			<p>Notice that the <a id="_idIndexMarker1124"/>bridge naming convention with <a id="_idIndexMarker1125"/>Netavark is slightly different since it uses the <strong class="source-inline">podmanN</strong> pattern, with <em class="italic">N &gt;= 0</em>.</p>
			<p>To list all the existing networks, we can use the <strong class="source-inline">podman network ls</strong> command:</p>
			<p class="source-code"># podman network ls</p>
			<p class="source-code">NETWORK ID   NAME     VERSION  PLUGINS</p>
			<p class="source-code">2f259bab93aa podman   0.4.0    bridge,portmap,firewall,tuning</p>
			<p class="source-code">228b48a56dbc example1 0.4.0    bridge,portmap,firewall,tuning,dnsname</p>
			<p>The preceding output shows the name, ID, CNI version, and active plugins of each active network.</p>
			<p>On Podman 4, the output is slightly more compact since there are no CNI plugins to be shown:</p>
			<p class="source-code"># podman network ls</p>
			<p class="source-code">NETWORK ID    NAME        DRIVER</p>
			<p class="source-code">a8ca04a41ef3  example1    bridge</p>
			<p class="source-code">2f259bab93aa  podman      bridge</p>
			<p>Now, it's time <a id="_idIndexMarker1126"/>to spin up a container that's attached <a id="_idIndexMarker1127"/>to the new network. The following code creates a PostgreSQL database that's attached to the <strong class="source-inline">example1</strong> network:</p>
			<p class="source-code"># podman run -d -p 5432:5432 \</p>
			<p class="source-code">  --network example1 \</p>
			<p class="source-code">  -e POSTGRES_PASSWORD=password \</p>
			<p class="source-code">  --name postgres \</p>
			<p class="source-code">  docker.io/library/postgres</p>
			<p class="source-code">533792e9522fc65371fa6d694526400a3a01f29e6de9b2024e84895f354e d2bb</p>
			<p>The new container receives an address from the <strong class="source-inline">10.89.0.0/16</strong> subnet, as shown by the <strong class="source-inline">podman inspect</strong> command:</p>
			<p class="source-code"># podman inspect postgres --format '{{.NetworkSettings.Networks.example1.IPAddress}}'</p>
			<p class="source-code">10.89.0.3</p>
			<p>When we're using the CNI network backend, we can double-check this information by looking at the contents of the new <strong class="source-inline">/var/lib/cni/networks/example1</strong> folder:</p>
			<p class="source-code"># ls -al /var/lib/cni/networks/example1/</p>
			<p class="source-code">total 20</p>
			<p class="source-code">drwxr-xr-x. 2 root root 4096 Jan 23 17:26 .</p>
			<p class="source-code">drwxr-xr-x. 5 root root 4096 Jan 23 16:22 ..</p>
			<p class="source-code">-rw-r--r--. 1 root root   70 Jan 23 16:26 10.89.0.3</p>
			<p class="source-code">-rw-r--r--. 1 root root    9 Jan 23 16:57 last_reserved_ip.0</p>
			<p class="source-code">-rwxr-x---. 1 root root    0 Jan 23 16:22 lock</p>
			<p>Looking at the content of the <strong class="source-inline">10.89.0.3</strong> file, we find the following:</p>
			<p class="source-code"># cat /var/lib/cni/networks/example1/10.89.0.3</p>
			<p class="source-code">533792e9522fc65371fa6d694526400a3a01f29e6de9b2024e84895f354 ed2bb</p>
			<p>The file holds the <a id="_idIndexMarker1128"/>container ID of our <strong class="source-inline">postgres</strong> container, which <a id="_idIndexMarker1129"/>is used to track the mapping with the assigned IP address. As we mentioned previously, this behavior is managed by the <strong class="source-inline">host-local</strong> plugin, the default IPAM choice for Podman networks.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The Netavark network backend tracks IPAM configuration in the <strong class="source-inline">/run/containers/networks/ipam.db</strong> file for rootfull containers.</p>
			<p>We can also see that a new Linux bridge has been created (notice the <strong class="source-inline">cni-</strong> prefix that is used for CNI network backends):</p>
			<p class="source-code"># ip addr show cni-podman1</p>
			<p class="source-code">8: cni-podman1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000</p>
			<p class="source-code">    link/ether 56:ed:1d:a9:53:54 brd ff:ff:ff:ff:ff:ff</p>
			<p class="source-code">    inet 10.89.0.1/16 brd 10.89.255.255 scope global cni-podman1</p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">    inet6 fe80::54ed:1dff:fea9:5354/64 scope link </p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p>The new device is connected to one peer of the PostgreSQL container's veth pair:</p>
			<p class="source-code"># bridge link show</p>
			<p class="source-code">10: vethf03ed735@eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 master cni-podman1 state forwarding priority 32 cost 2 </p>
			<p class="source-code">20: veth23ee4990@eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 master cni-podman0 state forwarding priority 32 cost 2</p>
			<p>Here, we <a id="_idIndexMarker1130"/>can see that <strong class="source-inline">vethf03ed735@eth0</strong> is <a id="_idIndexMarker1131"/>attached to the <strong class="source-inline">cni-podman1</strong> bridge. The interface has the following configuration:</p>
			<p class="source-code"># ip addr show vethf03ed735</p>
			<p class="source-code">10: vethf03ed735@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master cni-podman1 state UP group default </p>
			<p class="source-code">    link/ether 86:d1:8c:c9:8c:2b brd ff:ff:ff:ff:ff:ff link-netns cni-77bfb1c0-af07-1170-4cc8-eb56d15511ac</p>
			<p class="source-code">    inet6 fe80::f889:17ff:fe83:4da2/64 scope link </p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p>The preceding output also shows that the other side of the veth pair is located in the container's network namespace – that is, <strong class="source-inline">cni-77bfb1c0-af07-1170-4cc8-eb56d15511ac</strong>. We can inspect the container's network configuration and confirm the IP address that's been allocated from the new subnet:</p>
			<p class="source-code"># ip netns exec cni-77bfb1c0-af07-1170-4cc8-eb56d15511ac ip addr show</p>
			<p class="source-code">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</p>
			<p class="source-code">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</p>
			<p class="source-code">    inet 127.0.0.1/8 scope host lo</p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">    inet6 ::1/128 scope host </p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">2: eth0@if10: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default </p>
			<p class="source-code">    link/ether ba:91:9e:77:30:a1 brd ff:ff:ff:ff:ff:ff link-netnsid 0</p>
			<p class="source-code">    inet <strong class="bold">10.89.0.3/16</strong> brd 10.89.255.255 scope global eth0</p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">    inet6 fe80::b891:9eff:fe77:30a1/64 scope link </p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The network namespace naming pattern for the Netavark backend in Podman 4 is <strong class="source-inline">netns-&lt;UID&gt;</strong>.</p>
			<p>It is possible to <a id="_idIndexMarker1132"/>connect a running container to <a id="_idIndexMarker1133"/>another network without stopping and restarting it. In this way, the container will keep an interface attached to the original network and a second interface, attached to the new network, will be created. This feature, which is useful for use cases such as reverse proxies, can be achieved with the <strong class="source-inline">podman network connect</strong> command. Let's try to run a new <strong class="source-inline">net_example</strong> container:</p>
			<p class="source-code"># podman run -d -p 8080:80 --name net_example docker.io/library/nginx </p>
			<p class="source-code"># podman network connect example1 net_example</p>
			<p>To verify <a id="_idIndexMarker1134"/>that the container has been attached <a id="_idIndexMarker1135"/>to the new network, we can run the <strong class="source-inline">podman inspect</strong> command and look at the networks:</p>
			<p class="source-code"># podman inspect net_example</p>
			<p class="source-code">[...omitted output...]</p>
			<p class="source-code">            "Networks": {</p>
			<p class="source-code">                "example1": {</p>
			<p class="source-code">                    "EndpointID": "",</p>
			<p class="source-code">                    "Gateway": "10.89.0.1",</p>
			<p class="source-code">                    <strong class="bold">"IPAddress": "10.89.0.10"</strong>,</p>
			<p class="source-code">                    "IPPrefixLen": 16,</p>
			<p class="source-code">                    "IPv6Gateway": "",</p>
			<p class="source-code">                    "GlobalIPv6Address": "",</p>
			<p class="source-code">                    "GlobalIPv6PrefixLen": 0,</p>
			<p class="source-code">                    "MacAddress": "fa:41:66:0a:25:45",</p>
			<p class="source-code">                    "NetworkID": "example1",</p>
			<p class="source-code">                    "DriverOpts": null,</p>
			<p class="source-code">                    "IPAMConfig": null,</p>
			<p class="source-code">                    "Links": null</p>
			<p class="source-code">                },</p>
			<p class="source-code">                "podman": {</p>
			<p class="source-code">                    "EndpointID": "",</p>
			<p class="source-code">                    "Gateway": "10.88.0.1",</p>
			<p class="source-code">                   <strong class="bold"> "IPAddress": "10.88.0.7"</strong>,</p>
			<p class="source-code">                    "IPPrefixLen": 16,</p>
			<p class="source-code">                    "IPv6Gateway": "",</p>
			<p class="source-code">                    "GlobalIPv6Address": "",</p>
			<p class="source-code">                    "GlobalIPv6PrefixLen": 0,</p>
			<p class="source-code">                    "MacAddress": "ba:cd:eb:8d:19:b5",</p>
			<p class="source-code">                    "NetworkID": "podman",</p>
			<p class="source-code">                    "DriverOpts": null,</p>
			<p class="source-code">                    "IPAMConfig": null,</p>
			<p class="source-code">                    "Links": null</p>
			<p class="source-code">                }</p>
			<p class="source-code">            }</p>
			<p class="source-code">[…omitted output...]</p>
			<p>Here, we can see that the container now has two interfaces attached to the <strong class="source-inline">podman</strong> and <strong class="source-inline">example1</strong> networks, with IP addresses allocated from each network's subnet. </p>
			<p>To disconnect a container from a network, we can use the <strong class="source-inline">podman network disconnect</strong> command:</p>
			<p class="source-code"># podman network disconnect example1 net_example</p>
			<p>When a network <a id="_idIndexMarker1136"/>is not necessary anymore and is <a id="_idIndexMarker1137"/>disconnected from running containers, we can delete it with the <strong class="source-inline">podman network rm</strong> command:</p>
			<p class="source-code"># podman network rm example1</p>
			<p class="source-code">example1</p>
			<p>The command's output shows the list of removed networks. Here, the network's CNI configuration is removed from the host's <strong class="source-inline">/etc/cni/net.d</strong> directory.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">If the network has associated containers that are either running or have been stopped, the previous message will fail with <strong class="source-inline">Error: "example1" has associated containers with it</strong>. To work around this issue, remove or disconnect the associated containers before using the command.</p>
			<p>The <strong class="source-inline">podman network rm</strong> command <a id="_idIndexMarker1138"/>is useful when we <a id="_idIndexMarker1139"/>need to remove a specific network. To remove all unused networks, the <strong class="source-inline">podman network prune</strong> command is a better choice: </p>
			<p class="source-code"># podman network prune</p>
			<p class="source-code">WARNING! This will remove all networks not used by at least one container.</p>
			<p class="source-code">Are you sure you want to continue? [y/N] y</p>
			<p class="source-code">example2</p>
			<p class="source-code">db_network</p>
			<p>In this section, we learned about the CNI specification and how Podman leverages its interface to simplify container networking. In a multi-tier or microservices scenario, we need to let containers communicate with each other. In the next section, we will learn how to manage container-to-container communication.</p>
			<h1 id="_idParaDest-233"><a id="_idTextAnchor233"/>Interconnecting two or more containers</h1>
			<p>Using our knowledge from the previous section, we should be aware that two or more containers that have <a id="_idIndexMarker1140"/>been created inside the same network can reach each other on the same subnet without the need for external routing. </p>
			<p>At the same time, two or more containers that belong to different networks will be able to reach each other on different subnets by routing packets through their networks.</p>
			<p>To demonstrate this, let's create a couple of <strong class="source-inline">busybox</strong> containers in the same default network:</p>
			<p class="source-code"># podman run -d --name endpoint1 \</p>
			<p class="source-code">  --cap-add=net_admin,net_raw busybox /bin/sleep 10000</p>
			<p class="source-code"># podman run -d --name endpoint2 \</p>
			<p class="source-code">  --cap-add=net_admin,net_raw busybox /bin/sleep 10000</p>
			<p>In our lab, the two containers have <strong class="source-inline">10.88.0.14</strong> (<strong class="source-inline">endpoint1</strong>) and <strong class="source-inline">10.88.0.15</strong> (<strong class="source-inline">endpoint2</strong>) as their addresses. These two addresses are subject to change and can be collected using the methods illustrated previously with the <strong class="source-inline">podman inspect</strong> or the <strong class="source-inline">nsenter</strong> commands.</p>
			<p>Regarding capabilities <a id="_idIndexMarker1141"/>customization, we added the <strong class="source-inline">CAP_NET_ADMIN</strong> and <strong class="source-inline">CAP_NET_RAW</strong> capabilities to let the containers run commands such as <strong class="source-inline">ping</strong> or <strong class="source-inline">traceroute</strong> seamlessly.</p>
			<p>Let's try to run a <strong class="source-inline">traceroute</strong> command from <strong class="source-inline">endpoint1</strong> to <strong class="source-inline">endpoint2</strong> to see the path of a packet:</p>
			<p class="source-code"># podman exec -it endpoint1 traceroute 10.88.0.14</p>
			<p class="source-code">traceroute to 10.88.0.14 (10.88.0.14), 30 hops max, 46 byte packets</p>
			<p class="source-code">1  10.88.0.14 (10.88.0.14)  0.013 ms  0.004 ms  0.002 ms</p>
			<p>As we can see, the packet stays on the internal network and reaches the node without additional hops.</p>
			<p>Now, let's create a new network, <strong class="source-inline">net1</strong>, and connect a container called <strong class="source-inline">endpoint3</strong> to it:</p>
			<p class="source-code"># podman network create --driver bridge --gateway "10.90.0.1" --subnet "10.90.0.0/16" net1</p>
			<p class="source-code"># podman run -d --name endpoint3 --network=net1 --cap-add=net_admin,net_raw busybox /bin/sleep 10000</p>
			<p>The container in our lab gets an IP address of <strong class="source-inline">10.90.0.2</strong>. Let's see the network path from <strong class="source-inline">endpoint1</strong> to <strong class="source-inline">endpoint3</strong>:</p>
			<p class="source-code"># podman exec -it endpoint1 traceroute 10.90.0.2</p>
			<p class="source-code">traceroute to 10.90.0.2 (10.90.0.2), 30 hops max, 46 byte packets</p>
			<p class="source-code">1  host.containers.internal (10.88.0.1)  0.003 ms  0.001 ms  0.006 ms</p>
			<p class="source-code">2  10.90.0.2 (10.90.0.2)  0.001 ms  0.002 ms  0.002 ms</p>
			<p>This time, the packet has traversed the <strong class="source-inline">endpoint1</strong> container's default gateway (<strong class="source-inline">10.88.0.1</strong>) and reached the <strong class="source-inline">endpoint3</strong> container, which is routed from the host to the associated <strong class="source-inline">net1</strong> Linux bridge.</p>
			<p>Connectivity across containers <a id="_idIndexMarker1142"/>in the same host is very easy to manage and understand. However, we are still missing an important aspect for container-to-container communication: DNS resolution. </p>
			<p>Let's learn how to leverage this feature with Podman networks.</p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor234"/>Container DNS resolution</h2>
			<p>Despite its many configuration caveats, DNS resolution is a very simple concept: a service is <a id="_idIndexMarker1143"/>queried to provide the IP address associated with a given <a id="_idIndexMarker1144"/>hostname. The amount of information that can be provided by a DNS server is far richer than this, but we want to focus on simple IP resolution in this example. </p>
			<p>For example, let's imagine a scenario where a web application running on a container named <strong class="source-inline">webapp</strong> needs read/write access to a database running on a second container named <strong class="source-inline">db</strong>. DNS resolution enables <strong class="source-inline">webapp</strong> to query for the <strong class="source-inline">db</strong> container's IP address before contacting it.</p>
			<p>Previously, we learned that Podman's default network does not provide DNS resolution, while new user-created networks have DNS resolution enabled by default. On a CNI network backend, the <strong class="source-inline">dnsname</strong> plugin automatically configures a <strong class="source-inline">dnsmasq</strong> service, which is started when containers are connected to the network, to provide DNS resolution. On a Netavark network backend, the DNS resolution is delivered by <strong class="source-inline">aarvark-dns</strong>.</p>
			<p>To test this feature, we are going to reuse the <strong class="bold">students</strong> web application that we illustrated in <a href="B17908_10_epub.xhtml#_idTextAnchor193"><em class="italic">Chapter 10</em></a>, <em class="italic">Troubleshooting and Monitoring Containers</em>, since it provides an adequate client-server example with a minimal REST service and a database backend based on PostgreSQL.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">The source code is available in this book's GitHub repository at <a href="https://github.com/PacktPublishing/Podman-for-DevOps/tree/main/Chapter10/students">https://github.com/PacktPublishing/Podman-for-DevOps/tree/main/Chapter10/students</a>.</p>
			<p>In this example, the web application simply prints some output in JSON as the result of an HTTP GET that <a id="_idIndexMarker1145"/>triggers a query to a PostgreSQL database. For our <a id="_idIndexMarker1146"/>demonstration, we will run both the database and the web application on the same network.</p>
			<p>First, we must create the PostgreSQL database pod while providing a generic username and password:</p>
			<p class="source-code"># podman run -d \</p>
			<p class="source-code">   --network net1 --name db \</p>
			<p class="source-code">   -e POSTGRES_USER=admin \</p>
			<p class="source-code">   -e POSTGRES_PASSWORD=password \</p>
			<p class="source-code">   -e POSTGRES_DB=students \</p>
			<p class="source-code">   postgres</p>
			<p>Next, we must restore the data from the SQL dump in the <strong class="source-inline">students</strong> folder to the database:</p>
			<p class="source-code"># cd Chapter10/students</p>
			<p class="source-code"># cat db.sql | podman exec -i db psql -U admin students</p>
			<p>If you haven't already built it in the previous chapters, then you need to build the <strong class="source-inline">students</strong> container image and run it on the host:</p>
			<p class="source-code"># buildah build -t students .</p>
			<p class="source-code"># podman run -d \</p>
			<p class="source-code">   --network net1 \</p>
			<p class="source-code">   -p 8080:8080 \</p>
			<p class="source-code">   --name webapp \</p>
			<p class="source-code">   students \</p>
			<p class="source-code">   students <strong class="bold">-host db -port 5432</strong> \</p>
			<p class="source-code">   -username admin -password</p>
			<p>Notice the highlighted part of the command: the <strong class="source-inline">students</strong> application accepts the <strong class="source-inline">-host</strong>, <strong class="source-inline">-port</strong>, <strong class="source-inline">-username</strong>, and <strong class="source-inline">-password</strong> options to customize the database's endpoints and credentials.</p>
			<p>We did not provide any IP address in the host field. Instead, the Postgres container name, <strong class="source-inline">db</strong>, along with the default <strong class="source-inline">5432</strong> port, were used to identify the database.</p>
			<p>Also, notice that the <strong class="source-inline">db</strong> container <a id="_idIndexMarker1147"/>was created without any kind of port <a id="_idIndexMarker1148"/>mapping: we expect to directly reach the database over the <strong class="source-inline">net1</strong> container network, where both containers were created.</p>
			<p>Let's try to call the <strong class="source-inline">students</strong> application API and see what happens: </p>
			<p class="source-code"># curl localhost:8080/students {"Id":10149,"FirstName":"Frank","MiddleName":"Vincent","LastName":"Zappa","Class":"3A","Course":"Composition"}</p>
			<p>The query worked fine, meaning that the application successfully queried the database. But how did this happen? How did it resolve the container IP address by only knowing its name? In the next section, we'll look at the different behaviors on CNI and Netavark network backends.</p>
			<h3>DNS resolution on a CNI network backend</h3>
			<p>On Podman 3 or Podman 4 with a CNI backend, the <strong class="source-inline">dnsname</strong> plugin is enabled in the <strong class="source-inline">net1</strong> network <a id="_idIndexMarker1149"/>and a dedicated <strong class="source-inline">dnsmasq</strong> service <a id="_idIndexMarker1150"/>is spawned that is in charge of resolving container names to their assigned IP addresses. Let's start by finding the container's IP addresses first:</p>
			<p class="source-code"># podman inspect db --format '{{.NetworkSettings.Networks.net1.IPAddress}}'</p>
			<p class="source-code"><strong class="bold">10.90.0.2</strong></p>
			<p class="source-code"># podman inspect webapp --format '{{.NetworkSettings.Networks.net1.IPAddress}}'</p>
			<p class="source-code"><strong class="bold">10.90.0.3</strong></p>
			<p>We want to look for <strong class="source-inline">dnsmasq</strong> processes running on the system:</p>
			<p class="source-code"># ps aux | grep dnsmasq</p>
			<p class="source-code"><strong class="bold">root        2703  0.0  0.0  26436  2384 ?        S    16:16   0:00 /usr/sbin/dnsmasq -u root --conf-file=/run/containers/cni/dnsname/net1/dnsmasq.conf</strong></p>
			<p class="source-code">root        5577  0.0  0.0   6140   832 pts/0    S+   22:00   0:00 grep --color=auto dnsmasq</p>
			<p>The preceding <a id="_idIndexMarker1151"/>output shows an instance of the <strong class="source-inline">dnsmasq</strong> process <a id="_idIndexMarker1152"/>running with a config file that's been created under the <strong class="source-inline">/run/containers/cni/dnsname/net1/</strong> directory. Let's inspect its contents:</p>
			<p class="source-code"># ls -al /run/containers/cni/dnsname/net1/</p>
			<p class="source-code">total 12</p>
			<p class="source-code">drwx------. 2 root root 120 Jan 25 16:16 .</p>
			<p class="source-code">drwx------. 3 root root  60 Jan 25 16:16 ..</p>
			<p class="source-code">-rw-r--r--. 1 root root  30 Jan 25 16:28 addnhosts</p>
			<p class="source-code">-rwx------. 1 root root 356 Jan 25 16:16 dnsmasq.conf</p>
			<p class="source-code">-rwxr-x---. 1 root root   0 Jan 25 16:16 lock</p>
			<p class="source-code">-rw-r--r--. 1 root root   5 Jan 25 16:16 pidfile</p>
			<p><strong class="source-inline">/run/containers/cni/dnsname/net1/dnsmasq.conf</strong> defines the <strong class="source-inline">dnsmasq</strong> configuration:</p>
			<p class="source-code"># cat /run/containers/cni/dnsname/net1/dnsmasq.conf </p>
			<p class="source-code">## WARNING: THIS IS AN AUTOGENERATED FILE</p>
			<p class="source-code">## AND SHOULD NOT BE EDITED MANUALLY AS IT</p>
			<p class="source-code">## LIKELY TO AUTOMATICALLY BE REPLACED.</p>
			<p class="source-code">strict-order</p>
			<p class="source-code">local=/dns.podman/</p>
			<p class="source-code">domain=dns.podman</p>
			<p class="source-code">expand-hosts</p>
			<p class="source-code">pid-file=/run/containers/cni/dnsname/net1/pidfile</p>
			<p class="source-code">except-interface=lo</p>
			<p class="source-code">bind-dynamic</p>
			<p class="source-code">no-hosts</p>
			<p class="source-code">interface=cni-podman1</p>
			<p class="source-code">addn-hosts=/run/containers/cni/dnsname/net1/addnhosts</p>
			<p>The process listens <a id="_idIndexMarker1153"/>on the <strong class="source-inline">cni-podman1</strong> interface (the <strong class="source-inline">net1</strong> network bridge, which has an IP address of <strong class="source-inline">10.90.0.1</strong>) and is <a id="_idIndexMarker1154"/>authoritative for the <strong class="source-inline">dns.podman</strong> domain. The host's records are kept in the <strong class="source-inline">/run/containers/cni/dnsname/net1/addnhosts</strong> file, which contains the following:</p>
			<p class="source-code"># cat /run/containers/cni/dnsname/net1/addnhosts </p>
			<p class="source-code">10.90.0.2  db</p>
			<p class="source-code">10.90.0.3  webapp</p>
			<p>When a container in the <strong class="source-inline">net1</strong> network attempts DNS resolution, it uses its <strong class="source-inline">/etc/resolv.conf</strong> file to find out the DNS server to direct the query to. The file's content in the <strong class="source-inline">webapp</strong> container is as follows:</p>
			<p class="source-code"># podman exec -it webapp cat /etc/resolv.conf</p>
			<p class="source-code">search dns.podman</p>
			<p class="source-code">nameserver 10.90.0.1</p>
			<p>This shows that the container contacts the <strong class="source-inline">10.90.0.1</strong> address (which is also the container default gateway and the <strong class="source-inline">cni-podman1</strong> bridge) to query hostname resolution.</p>
			<p>The search domain allows processes to search for a <strong class="bold">Fully Qualified Domain Name</strong> (<strong class="bold">FQDN</strong>). In the <a id="_idIndexMarker1155"/>preceding example, <strong class="source-inline">db.dns.podman</strong> would be resolved correctly by the DNS service. The search domain for a CNI network configuration can be customized by editing the related config file under <strong class="source-inline">/etc/cni/net.d/</strong>. The default configuration for the <strong class="source-inline">dnsname</strong> plugin in the <strong class="source-inline">net1</strong> config is as follows:</p>
			<p class="source-code">{</p>
			<p class="source-code">         "type": "dnsname",</p>
			<p class="source-code">         <strong class="bold">"domainName": "dns.podman"</strong>,</p>
			<p class="source-code">         "capabilities": {</p>
			<p class="source-code">            "aliases": true</p>
			<p class="source-code">         }</p>
			<p class="source-code">      }</p>
			<p>When you update the <strong class="source-inline">domainName</strong> field to a new value, the changes are not effective immediately. To <a id="_idIndexMarker1156"/>regenerate the updated <strong class="source-inline">dnsmasq.conf</strong>, all <a id="_idIndexMarker1157"/>the containers in the network must be stopped to let the <strong class="source-inline">dnsname</strong> plugin clean up the current network configuration. When containers are restarted, the <strong class="source-inline">dnsmasq</strong> configuration is regenerated accordingly.</p>
			<h3>DNS resolution on a Netavark network backend</h3>
			<p>If the preceding example <a id="_idIndexMarker1158"/>was executed on Podman 4 <a id="_idIndexMarker1159"/>with a Netavark network backend, the <strong class="source-inline">aardvark-dns</strong> daemon would be responsible for container resolution in a similar way to <strong class="source-inline">dnsmasq</strong>.</p>
			<p>The <strong class="source-inline">aardvark-dns</strong> project is a companion project of Netavark written in Rust. It is a lightweight authoritative DNS service that can work on both IPv4 A records and IPv6 AAAA records.</p>
			<p>When a new network with DNS resolution enabled is created, a new <strong class="source-inline">aardvark-dns</strong> process is created, as shown in the following code:</p>
			<p class="source-code"># ps aux | grep aardvark-dns</p>
			<p class="source-code">root        9115  0.0  0.0 344732  2584 pts/0    Sl   20:15   0:00 /usr/libexec/podman/aardvark-dns --config /run/containers/networks/aardvark-dns -p 53 run</p>
			<p class="source-code">root       10831  0.0  0.0   6400  2044 pts/0    S+   23:36   0:00 grep --color=auto aardvark-dns</p>
			<p>The process listens on port <strong class="source-inline">53/udp</strong> of the host network namespace for rootfull containers and on port <strong class="source-inline">53/udp</strong> of the rootless network namespace for rootless containers.</p>
			<p>The output of the <strong class="source-inline">ps</strong> command also shows the default configuration path – the <strong class="source-inline">/run/containers/networks/aardvark-dns</strong> directory – where the <strong class="source-inline">aardvark-dns</strong> process <a id="_idIndexMarker1160"/>stores the resolution configurations <a id="_idIndexMarker1161"/>under different files, named after the associated network. For example, for the <strong class="source-inline">net1</strong> network, we will find content similar to the following:</p>
			<p class="source-code"># cat /run/containers/networks/aardvark-dns/net1</p>
			<p class="source-code">10.90.0.1</p>
			<p class="source-code">dc7fff2ef78e99a2a1a3ea6e29bfb961fc07cd6cf71200d50761e25df30 11636 10.90.0.2  db,dc7fff2ef78e</p>
			<p class="source-code">10c7bbb7006c9b253f9ebe1103234a9af41dced8f12a6d94b7fc46a9a97 5d8cc 10.90.0.2  webapp,10c7bbb7006c</p>
			<p>The file stores IPv4 addresses (and IPv6 addresses, if present) for every container. Here, we can see the containers' names and short IDs resolved to the IPv4 addresses.</p>
			<p>The first line tells us the address where <strong class="source-inline">aardvark-dns</strong> is listening for incoming requests. Once again, it corresponds to the default gateway address for the network.</p>
			<p>Connecting containers across the same network allows for fast and simple communication across different services running in separate network namespaces. However, there are use cases where containers must share the same network namespace. Podman offers a solution to achieve this goal easily: Pods.</p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor235"/>Running containers inside a Pod</h2>
			<p>The concept <a id="_idIndexMarker1162"/>of a Pod comes from the Kubernetes architecture. According to <a id="_idIndexMarker1163"/>the official upstream documentation, "<em class="italic">A Pod ... is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers</em>."</p>
			<p>A Pod is also the smallest deployable unit in Kubernetes scheduling. All the containers inside a Pod share the same network, UTC, IPC, and (optionally) PID namespace. This means that all the services running on the different containers can refer to each other as <strong class="bold">localhost</strong>, while external containers will continue to contact the Pod's IP address. A Pod receives one IP address that is shared across all the containers.</p>
			<p>There are many adoption <a id="_idIndexMarker1164"/>use cases. A very common one is sidecar <a id="_idIndexMarker1165"/>containers: in this case, a reverse proxy or an OAuth proxy runs alongside the main container to provide authentication or service mesh functionalities.</p>
			<p>Podman provides the basic tooling for manipulating Pods with the <strong class="source-inline">podman pod</strong> command. The following example shows how to create a basic Pod with two containers and demonstrates network namespace sharing across the two containers in the Pod.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">To understand the following example, stop and remove all the running containers and Pods and start with a clean environment.</p>
			<p><strong class="source-inline">podman pod create</strong> initializes a new, empty Pod from scratch:</p>
			<p class="source-code"># podman pod create --name example_pod</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">When a new, empty Pod is created, Podman also creates an <strong class="source-inline">infra</strong> container, which is used to initialize the namespaces when the Pod is started. This container is based on the <strong class="source-inline">k8s.gcr.io/pause</strong> image for Podman 3 and a locally-built <strong class="source-inline">podman-pause</strong> image for Podman 4.</p>
			<p>Now, we can create two basic <strong class="source-inline">busybox</strong> containers inside the Pod:</p>
			<p class="source-code"># podman create --name c1 --pod example_pod busybox sh -c 'sleep 10000'</p>
			<p class="source-code"># podman create --name c2 --pod example_pod busybox sh -c 'sleep 10000'</p>
			<p>Finally, we can start the Pod (and its associated containers) with the <strong class="source-inline">podman pod start</strong> command:</p>
			<p class="source-code"># podman pod start example_pod</p>
			<p>Here, we have a <a id="_idIndexMarker1166"/>running Pod with two containers (plus an infra one) running. To verify <a id="_idIndexMarker1167"/>its status, we can use the <strong class="source-inline">podman pod ps</strong> command:</p>
			<p class="source-code"># podman pod ps</p>
			<p class="source-code">POD ID        NAME         STATUS      CREATED        INFRA ID      # OF CONTAINERS</p>
			<p class="source-code">8f89f37b8f3b  example_pod  Degraded    8 minutes ago  95589171284a  4</p>
			<p>With the <strong class="source-inline">podman pod top</strong> command, we can see the resources that are being consumed by each container in the Pod:</p>
			<p class="source-code"># podman pod top example_pod</p>
			<p class="source-code">USER        PID         PPID        %CPU        ELAPSED          TTY         TIME        COMMAND</p>
			<p class="source-code">root        1           0           0.000       10.576973703s  ?           0s          sleep 1000 </p>
			<p class="source-code">0           1           0           0.000       10.577293395s  ?           0s          /catatonit -P </p>
			<p class="source-code">root        1           0           0.000       9.577587032s   ?           0s          sleep 1000 </p>
			<p>After creating the Pod, we can inspect the network's behavior. First, we will see that only one network namespace has been created in the system:</p>
			<p class="source-code"># ip netns</p>
			<p class="source-code">netns-17b9bb67-5ce6-d533-ecf0-9d7f339e6ebd (id: 0)</p>
			<p>Let's check the IP configuration for this namespace and its related network stack:</p>
			<p class="source-code"># ip netns exec netns-17b9bb67-5ce6-d533-ecf0-9d7f339e6ebd ip addr show</p>
			<p class="source-code">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</p>
			<p class="source-code">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</p>
			<p class="source-code">    inet 127.0.0.1/8 scope host lo</p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">    inet6 ::1/128 scope host </p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">2: eth0@if15: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default </p>
			<p class="source-code">    link/ether a6:1b:bc:8e:65:1e brd ff:ff:ff:ff:ff:ff link-netnsid 0</p>
			<p class="source-code">    inet <strong class="bold">10.88.0.3/16</strong> brd 10.88.255.255 scope global eth0</p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">    inet6 fe80::a41b:bcff:fe8e:651e/64 scope link </p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p>To verify that the <strong class="source-inline">c1</strong> and <strong class="source-inline">c2</strong> containers share the same network namespace and are running with an IP address of <strong class="source-inline">10.88.0.3</strong>, we can run the same <strong class="source-inline">ip addr show</strong> command inside the containers using the <strong class="source-inline">podman exec</strong> command:</p>
			<p class="source-code"># podman exec -it c1 ip addr show</p>
			<p class="source-code"># podman exec -it c2 ip addr show</p>
			<p>These two containers <a id="_idIndexMarker1168"/>are expected to return the same output <a id="_idIndexMarker1169"/>as the <strong class="source-inline">netns-17b9bb67-5ce6-d533-ecf0-9d7f339e6ebd</strong> network namespace.</p>
			<p>The example pod can be stopped and removed with the <strong class="source-inline">podman pod stop</strong> and <strong class="source-inline">podman pod rm</strong> commands, respectively:</p>
			<p class="source-code"># podman pod stop example_pod</p>
			<p class="source-code"># podman pod rm example_pod</p>
			<p>We will cover pods in more detail in <a href="B17908_14_epub.xhtml#_idTextAnchor257"><em class="italic">Chapter 14</em></a>, <em class="italic">Interacting with systemd and Kubernetes</em>, where we will also discuss name resolution and multi-pod orchestration.</p>
			<p>In this section, we focused on communication across two or more containers inside the same host or Pod, regardless <a id="_idIndexMarker1170"/>of the number and type of networks involved. However, containers <a id="_idIndexMarker1171"/>are a platform where you can run services that are generally accessed by the external world. For this reason, in the next section, we will investigate the best practices that can be applied to expose containers outside their hosts and make their services accessible to other clients/consumers.</p>
			<h1 id="_idParaDest-236"><a id="_idTextAnchor236"/>Exposing containers outside our underlying host</h1>
			<p>Container adoption in an enterprise company or a community project could be a hard thing to do that <a id="_idIndexMarker1172"/>could require time. For this reason, we may not have all the required services running as containers during our adoption journey. This is why exposing containers outside our underlying host could be a nice solution for interconnecting services that live in containers to services that run in the legacy world.</p>
			<p>As we briefly saw earlier in this chapter, Podman uses two different networking stacks, depending on the container: rootless or rootfull.</p>
			<p>Even though the underlying mechanism is slightly different, depending on if you are using a rootless or a rootfull container, Podman's command-line options for exposing network ports are the same for both container types.</p>
			<p class="callout-heading">Good to Know</p>
			<p class="callout">Note that the example we are going to see in this section will be executed as a root user. This choice was necessary because the main objective of this section is to show you some of the firewall configurations that could be mandatory for exposing a container service to the outside world.</p>
			<p>Exposing a container starts with Port Publishing activities. We'll learn what this is in the next section.</p>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor237"/>Port Publishing</h2>
			<p>Port Publishing consists of instructing Podman to create a temporary mapping between the container's ports and <a id="_idIndexMarker1173"/>some random or custom host's ports.</p>
			<p>The option to instruct Podman to publish a port is really simple – it consists of adding the <strong class="source-inline">-p</strong> or <strong class="source-inline">--publish</strong> option to the <strong class="source-inline">run</strong> command. Let's see how it works:</p>
			<p class="source-code">-p=ip:hostPort:containerPort</p>
			<p>The previous option publishes a container's port, or range of ports, to the host. When we are specifying ranges for <strong class="source-inline">hostPort</strong> or <strong class="source-inline">containerPort</strong>, the number must be equal for both ranges.</p>
			<p>We can even omit <strong class="source-inline">ip</strong>. In that case, the port will be bound on all the IPs of the underlying host. If we do not set the host port, the container's port will be randomly assigned a port on the host.</p>
			<p>Let's look at an example of the port publishing option:</p>
			<p class="source-code"># podman run -dt -p 80:80/tcp docker.io/library/httpd</p>
			<p class="source-code">Trying to pull docker.io/library/httpd:latest...</p>
			<p class="source-code">Getting image source signatures</p>
			<p class="source-code">Copying blob 41c22baa66ec done  </p>
			<p class="source-code">Copying blob dcc4698797c8 done  </p>
			<p class="source-code">Copying blob d982c879c57e done  </p>
			<p class="source-code">Copying blob a2abf6c4d29d done  </p>
			<p class="source-code">Copying blob 67283bbdd4a0 done  </p>
			<p class="source-code">Copying config dabbfbe0c5 done  </p>
			<p class="source-code">Writing manifest to image destination</p>
			<p class="source-code">Storing signatures</p>
			<p class="source-code">ea23dbbeac2ea4cb6d215796e225c0e7c7cf2a979862838ef4299d410c90 ad44</p>
			<p>As you can see, we have told Podman to run a container starting from the <strong class="source-inline">httpd</strong> base image. Then, we allocated a pseudo-tty (<strong class="source-inline">-t</strong>) in detached mode (<strong class="source-inline">-d</strong>) before setting the port mapping to bind the underlying host port, <strong class="source-inline">80</strong>, to port <strong class="source-inline">80</strong> of the container.</p>
			<p>Now, we can use the <strong class="source-inline">podman port</strong> command to see the actual mapping: </p>
			<p class="source-code"># podman ps</p>
			<p class="source-code">CONTAINER ID  IMAGE                           COMMAND           CREATED        STATUS            PORTS               NAMES</p>
			<p class="source-code">ea23dbbeac2e  docker.io/library/httpd:latest  httpd-foreground  3 minutes ago  Up 3 minutes ago  0.0.0.0:80-&gt;80/tcp  ecstatic_chaplygin</p>
			<p class="source-code"># podman port ea23dbbeac2e</p>
			<p class="source-code">80/tcp -&gt; 0.0.0.0:80</p>
			<p>First, we requested the list of running containers and then passed the correct container ID to the <strong class="source-inline">podman port</strong> command. We can check if the mapping is working properly like so:</p>
			<p class="source-code"># curl localhost:80</p>
			<p class="source-code">&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;</p>
			<p>Here, we executed a <strong class="source-inline">curl</strong> command from the host system and it worked – the <strong class="source-inline">httpd</strong> process running in the container just replied to us.</p>
			<p>If we have multiple ports and <a id="_idIndexMarker1174"/>we do not care about their assignment on the underlying host system, we can easily leverage the<strong class="source-inline">–P</strong> or <strong class="source-inline">--publish-all</strong> option to publish all the ports that are exposed by the container image to random ports on the host interfaces. Podman will run through the container image's metadata looking for the exposed ports. These ports are usually defined in a Dockerfile or Containerfile with the <strong class="source-inline">EXPOSE</strong> instruction, as shown here:</p>
			<p class="source-code">EXPOSE 80/tcp</p>
			<p class="source-code">EXPOSE 80/udp</p>
			<p>With the previous keyword, we can instruct the container engine that will run the final container of which network ports will be exposed and used by it.</p>
			<p>However, we can leverage an easy but insecure alternative, as shown in the next section.</p>
			<h2 id="_idParaDest-238"><a id="_idTextAnchor238"/>Attaching a host network</h2>
			<p>To expose a container service to the outside world, we can attach the whole host network to the running <a id="_idIndexMarker1175"/>container. As you can imagine, this method could lead to the unauthorized use of host resources so, for this reason, it is not recommended and should be used carefully.</p>
			<p>As we anticipated, attaching the host network to a running container is quite simple. Using the right Podman option, we can easily get rid of any network isolation:</p>
			<p class="source-code"># podman run --network=host -dt docker.io/library/httpd</p>
			<p class="source-code">2cb80369e53761601a41a4c004a485139de280c3738d1b7131c241f4001 f78a6</p>
			<p>Here, we used the <strong class="source-inline">--network</strong> option while specifying the <strong class="source-inline">host</strong> value. This informs Podman that we want to let the container attach to the host network.</p>
			<p>After running the previous command, we can check that the running container is bound to the host system's network interfaces since it can access all of them:</p>
			<p class="source-code"># netstat -nap|grep ::80</p>
			<p class="source-code">tcp6       0      0 :::80                   :::*                    LISTEN      37304/httpd</p>
			<p class="source-code"># curl localhost:80</p>
			<p class="source-code">&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;</p>
			<p>Here, we executed a <strong class="source-inline">curl</strong> command from the host system and it worked – the <strong class="source-inline">httpd</strong> process running in the container just replied to us.</p>
			<p>The process of exposing containers outside the underlying host does not stop here. In the next section, we'll learn how to complete this job.</p>
			<h2 id="_idParaDest-239"><a id="_idTextAnchor239"/>Host firewall configuration</h2>
			<p>Whether we choose to <a id="_idIndexMarker1176"/>leverage Port Publishing or attach the host network to the container, the process of exposing containers outside the underlying host does not stop here – we have reached the base OS of our host machine. In most cases, we will also need to allow the incoming connections to flow in the host's underlying machine, which will be interacting with the system firewall.</p>
			<p>The following example shows a non-comprehensive way to interact with the base OS firewall. If we're using a Fedora operating system or any other Linux distribution that's leveraging Firewalld as its firewall daemon manager, we can allow incoming connections on port <strong class="source-inline">80</strong> by running the following commands:</p>
			<p class="source-code"># firewall-cmd --add-port=80/tcp</p>
			<p class="source-code">success</p>
			<p class="source-code"># firewall-cmd --runtime-to-permanent</p>
			<p class="source-code">success</p>
			<p>The first command edits <a id="_idIndexMarker1177"/>the live system rules, while the second command stores the runtime rules in a permanent way that will survive system reboot or service restart.</p>
			<p class="callout-heading">Good to Know</p>
			<p class="callout">Firewalld is a firewall service daemon that provides us with an easy and fast way to customize the system <a id="_idIndexMarker1178"/>firewall. Firewalld is dynamic, which means that it can create, change, and delete the firewall rules without restarting the firewall daemon each time a change is applied.</p>
			<p>As we have seen, the process of exposing the container's services is quite simple but should be performed with a bit of consciousness and attention: opening a network port to the outside world should always be done carefully.</p>
			<h1 id="_idParaDest-240"><a id="_idTextAnchor240"/>Rootless container network behavior</h1>
			<p>As we saw in the previous sections, Podman relies on CNI plugins or Netavark for containers running <a id="_idIndexMarker1179"/>as root and has the privileges to alter network configurations in the host network namespace. For rootless containers, Podman uses the <strong class="source-inline">slirp4netns</strong> project, which allows you to create container network configurations without the need for root privileges; the network interfaces are created inside a rootless network namespace where the standard user has sufficient privileges. This approach allows you to transparently and flexibly manage rootless container networking.</p>
			<p>In the previous sections, we saw how container network namespaces can be connected to a bridge using a veth pair. Being able to create a veth pair in the host network namespace requires root privileges that are not allowed for standard users. </p>
			<p>In the simplest scenario, <strong class="source-inline">slirp4netns</strong> aims to overcome these privilege limitations by allowing a tap device to <a id="_idIndexMarker1180"/>be created that's attached to a user-mode network namespace. This tap device is created in the rootless network namespace.</p>
			<p>For every new rootless container, a new <strong class="source-inline">slirp4netns</strong> process is executed on the host. The process creates a network namespace for the container and a <strong class="source-inline">tap0</strong> device is created and configured with the <strong class="source-inline">10.0.2.100/24</strong> address (from the default slirp4netns <strong class="source-inline">10.0.2.0/24</strong> subnet). This prevents two containers from directly communicating with each other on the same network since there would be an IP address overlap.</p>
			<p>The following example demonstrates the network behavior of a rootless <strong class="source-inline">busybox</strong> container:</p>
			<p class="source-code">$ podman run -i busybox sh -c 'ip addr show tap0'</p>
			<p class="source-code">2: tap0: &lt;BROADCAST,UP,LOWER_UP&gt; mtu 65520 qdisc fq_codel state UNKNOWN group default qlen 1000</p>
			<p class="source-code">    link/ether 2a:c7:86:66:e9:20 brd ff:ff:ff:ff:ff:ff</p>
			<p class="source-code">    inet 10.0.2.100/24 brd 10.0.2.255 scope global tap0</p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">    inet6 fd00::28c7:86ff:fe66:e920/64 scope global dynamic mngtmpaddr </p>
			<p class="source-code">       valid_lft 86117sec preferred_lft 14117sec</p>
			<p class="source-code">    inet6 fe80::28c7:86ff:fe66:e920/64 scope link </p>
			<p class="source-code">       valid_lft forever preferred_lft forever </p>
			<p>It is possible to inspect the rootless network namespace and find the corresponding <strong class="source-inline">tap0</strong> device:</p>
			<p class="source-code">$ podman unshare --rootless-netns ip addr show tap0</p>
			<p class="source-code">2: tap0: &lt;BROADCAST,UP,LOWER_UP&gt; mtu 65520 qdisc fq_codel state UNKNOWN group default qlen 1000</p>
			<p class="source-code">    link/ether 1a:eb:82:6a:82:8d brd ff:ff:ff:ff:ff:ff</p>
			<p class="source-code">    inet 10.0.2.100/24 brd 10.0.2.255 scope global tap0</p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p class="source-code">    inet6 fd00::18eb:82ff:fe6a:828d/64 scope global dynamic mngtmpaddr </p>
			<p class="source-code">       valid_lft 86311sec preferred_lft 14311sec</p>
			<p class="source-code">    inet6 fe80::18eb:82ff:fe6a:828d/64 scope link </p>
			<p class="source-code">       valid_lft forever preferred_lft forever</p>
			<p>Since rootless containers do not own independent IP addresses, we have two ways to let two or more <a id="_idIndexMarker1181"/>containers communicate with each other:</p>
			<ul>
				<li>The easiest way could be to put all the containers in a single Pod so that the containers can communicate using the localhost interface, without the need to open any ports. </li>
				<li>The second way is to attach the container to a custom network and have its interfaces managed in the rootless network namespace.</li>
				<li>If we want to keep all the containers independent, we could use the port mapping technique to publish all the necessary ports and then use those ports to let the containers communicate with each other.</li>
			</ul>
			<p>Using a Podman 4 network backend, let's quickly focus on the second scenario, where two pods are attached on a rootless network. First, we need to create the network and attach a couple of test containers:</p>
			<p class="source-code">$ podman network create rootless-net</p>
			<p class="source-code">$ podman run -d --net rootless-net --name endpoint1 --cap-add=net_admin,net_raw busybox /bin/sleep 10000</p>
			<p class="source-code">$ podman run -d --net rootless-net --name endpoint2 --cap-add=net_admin,net_raw busybox /bin/sleep 10000</p>
			<p>Let's try to ping the <strong class="source-inline">endpoint2</strong> container from <strong class="source-inline">endpoint1</strong>:</p>
			<p class="source-code">$ podman exec -it endpoint1 ping -c1 endpoint1</p>
			<p class="source-code">PING endpoint1 (10.89.1.2): 56 data bytes</p>
			<p class="source-code">64 bytes from 10.89.1.2: seq=0 ttl=64 time=0.023 ms </p>
			<p class="source-code">--- endpoint1 ping statistics ---</p>
			<p class="source-code">1 packets transmitted, 1 packets received, 0% packet loss</p>
			<p class="source-code">round-trip min/avg/max = 0.023/0.023/0.023 ms</p>
			<p>These two containers can communicate on the common network and have different IPv4 addresses. To prove this, we can inspect the contents of the <strong class="source-inline">aardvark-dns</strong> configuration for the rootless containers:</p>
			<p class="source-code">$ cat /run/user/1000/containers/networks/aardvark-dns/rootless-net </p>
			<p class="source-code">10.89.1.1</p>
			<p class="source-code">fe27f8d653384fc191d5c580d18d874d480a7e8ef74c2626ae21b118eedb f1e6 10.89.1.2  endpoint1,fe27f8d65338</p>
			<p class="source-code">19a4307516ce1ece32ce58753e70da5e5abf9cf70feea7b981917ae399ef 934d 10.89.1.3  endpoint2,19a4307516ce</p>
			<p>Finally, let's demonstrate that the custom network bypasses the <strong class="source-inline">tap0</strong> interface and allows <a id="_idIndexMarker1182"/>dedicated veth pairs and bridges to be created in the rootless network namespace. The following command will show a Linux bridge for the <strong class="source-inline">rootless-net</strong> network and two attached veth pairs:</p>
			<p class="source-code">$ podman unshare --rootless-netns ip link | grep 'podman'</p>
			<p class="source-code">3: <strong class="bold">podman2</strong>: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000</p>
			<p class="source-code">4: <strong class="bold">vethdca7cdc6@if2</strong>: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master podman2 state UP mode DEFAULT group default qlen 1000</p>
			<p class="source-code">5: <strong class="bold">veth912bd229@if2</strong>: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master podman2 state UP mode DEFAULT group default qlen 1000</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">If you're running this code on a CNI network backend, use the <strong class="source-inline">podman unshare –rootless-cni</strong> command.</p>
			<p>Another limitation of rootless containers is regarding the <strong class="source-inline">ping</strong> command. Usually, on Linux distributions, standard non-root users lack the <strong class="source-inline">CAP_NET_RAW</strong> security capability. This inhibits the execution of the <strong class="source-inline">ping</strong> command, which leverages the send/receive of ICMP packets. If we want to use the <strong class="source-inline">ping</strong> command in a rootless container, we can enable the missing security capability through the <strong class="source-inline">sysctl</strong> command:</p>
			<p class="source-code"># sysctl -w "net.ipv4.ping_group_range=0 2000000"</p>
			<p>Note that this <a id="_idIndexMarker1183"/>could allow any process that will be executed by a user on these groups to send ping packets.</p>
			<p>Finally, while using rootless containers, we also need to consider that the Port Publishing technique can only be used for ports above <strong class="source-inline">1024</strong>. This is because, on Linux operating systems, all the ports below <strong class="source-inline">1024</strong> are privileged and cannot be used by standard non-root users. </p>
			<h1 id="_idParaDest-241"><a id="_idTextAnchor241"/>Summary</h1>
			<p>In this chapter, we learned how container network isolation can be leveraged to allow network segregation for each container that's running through network namespaces. These activities seem complex but thankfully, with the help of a container runtime, the steps are almost automated. We learned how to manage container networking with Podman and how to interconnect two or more containers. Finally, we learned how to expose a container's network ports outside of the underlying host and what kind of limitations we can expect while networking for rootless containers.</p>
			<p>In the next chapter, we will discover the main differences between Docker and Podman. This will be useful for advanced users, but also for novice ones, to understand what we can expect by comparing these two container engines.</p>
			<h1 id="_idParaDest-242"><a id="_idTextAnchor242"/>Further reading</h1>
			<p>To learn more about the topics that were covered in this chapter, take a look at the following resources:</p>
			<ul>
				<li>Container Network Interface: <a href="https://github.com/containernetworking/cni">https://github.com/containernetworking/cni</a></li>
				<li>The Netavark project on GitHub: <a href="https://github.com/containers/netavark">https://github.com/containers/netavark</a></li>
				<li>The <strong class="source-inline">Aardvark-dns</strong> project on GitHub: https://github.com/containers/aardvark-dns </li>
				<li>CNI reference plugins: https://www.cni.dev/plugins/current/</li>
				<li>CNI third-party plugins: <a href="https://github.com/containernetworking/cni#3rd-party-plugins">https://github.com/containernetworking/cni#3rd-party-plugins</a></li>
				<li>Kubernetes Pod definition: <a href="https://kubernetes.io/docs/concepts/workloads/pods/">https://kubernetes.io/docs/concepts/workloads/pods/</a></li>
				<li>The <strong class="source-inline">Slirp4netns</strong> project repository: <a href="https://github.com/rootless-containers/slirp4netns">https://github.com/rootless-containers/slirp4netns</a></li>
			</ul>
		</div>
	</body></html>