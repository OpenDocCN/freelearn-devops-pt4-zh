- en: '*Chapter 12*: Implementing Container Networking Concepts'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Container network isolation leverages network namespaces to provide separate
    network stacks for each container. Without a container runtime, managing network
    interfaces across multiple namespaces would be complex. Podman provides flexible
    network management that allows users to customize how containers communicate with
    external containers and other containers inside the same host.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn about the common configuration practices for
    managing container networking, along with the differences between rootless and
    rootfull containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Container networking and Podman setup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interconnecting two or more containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exposing containers outside our underlying host
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rootless container network behavior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To complete this chapter, you will need a machine with a working Podman installation.
    As we mentioned in [*Chapter 3*](B17908_03_epub.xhtml#_idTextAnchor068), *Running
    the First Container*, all the examples in this book can be executed on a Fedora
    34 system or later but can be reproduced on your **operating system** (**OS**)
    of choice. The examples in this chapter will be related to both Podman v3.4.z
    and Podman v4.0.0 since they provide different network implementations.
  prefs: []
  type: TYPE_NORMAL
- en: A good understanding of the topics that were covered in [*Chapter 4*](B17908_04_epub.xhtml#_idTextAnchor083),
    *Managing Running Containers*, [*Chapter 5*](B17908_05_epub.xhtml#_idTextAnchor101),
    *Implementing Storage for the Container's Data*, and [*Chapter 9*](B17908_09_epub.xhtml#_idTextAnchor167),
    *Pushing Images to a Container Registry*, will help you grasp the container networking
    topics we'll be covering.
  prefs: []
  type: TYPE_NORMAL
- en: You must also have a good understanding of basic networking concepts to understand
    topics such as routing, the IP protocol, DNS, and firewalling.
  prefs: []
  type: TYPE_NORMAL
- en: Container networking and Podman setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll cover Podman's networking implementation and how to configure
    networks. Podman 4.0.0 introduced an important change to the network stack. However,
    Podman 3 is still widely used in the community. For this reason, we will cover
    both implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Podman 3 leverages the **Container Network Interface** (**CNI**) to manage local
    networks that are created on the host. The CNI provides a standard set of specifications
    and libraries to create and configure plugin-based network interfaces in a container
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: CNI specifications were created for Kubernetes to provide a network configuration
    format that's used by the container runtime to set up the defined plugins, as
    well as an execution protocol between plugin binaries and runtimes. The great
    advantage of this plugin-based approach is that vendors and communities can develop
    third-party plugins that satisfy the CNI's specifications.
  prefs: []
  type: TYPE_NORMAL
- en: The Podman 4 network stack is based on a brand new project called **Netavark**,
    a container-native networking implementation completely written in Rust and designed
    to work with Podman. Rust is a great programming language for developing system
    and network components thanks to its efficient memory management and high performance,
    similar to the C programming language. Netavark provides better support for dual-stack
    networking (IPv4/IPv6) and inter-container DNS resolution, along with a tighter
    bond with the Podman project development roadmap.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Users upgrading from Podman 3 to Podman 4 will continue to use CNI by default
    and preserve their previous configuration. New Podman 4 installations will use
    Netavark by default. Users can revert to the CNI network backend by upgrading
    the `network_backend` field in the `/usr/share/containers/containers.conf` file.
  prefs: []
  type: TYPE_NORMAL
- en: In the next subsection, we'll focus on the CNI configuration that's used by
    Podman 3 to orchestrate container networking.
  prefs: []
  type: TYPE_NORMAL
- en: CNI configuration quick start
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A typical CNI configuration file defines a list of plugins and their related
    configuration. The following example shows the default CNI configuration of a
    fresh Podman installation on Fedora:'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter12/podman_cni_conf.json
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the `plugins` list in this file contains a set of plugins that
    are used by the runtime to orchestrate container networking.
  prefs: []
  type: TYPE_NORMAL
- en: The CNI community curates a repository of reference plugins that can be used
    by container runtimes. CNI reference plugins are organized into **interface-creating**,
    **IP address management** (**IPAM**), and **Meta** plugins. Interface-creating
    plugins can make use of IPAM and Meta plugins.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following non-exhaustive list describes the most commonly used interface-creating
    plugins:'
  prefs: []
  type: TYPE_NORMAL
- en: '`bridge`: This plugin creates a dedicated Linux bridge on the host for the
    network. Container interfaces are attached to the managed bridge to communicate
    between each other and with the external systems. This plugin is currently supported
    by Podman and by the `podman network` CLI tools and is the default interface-creating
    plugin that''s configured when Podman is installed or a new network is created.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ipvlan`: This plugin allows you to attach an IPVLAN interface to the container.
    The IPVLAN solution is an alternative to the traditional Linux bridge networking
    solution for containers, where a single parent interface is shared across multiple
    sub-interfaces, each with an IP address. This plugin is currently supported by
    Podman but you can still manually create and edit the CNI configuration file if
    necessary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`macvlan`: This plugin allows a MACVLAN configuration, which is an approach
    similar to IPVLAN with one main difference: in this configuration, each container
    sub-interface also gets a MAC address. This plugin is currently supported by Podman
    and by the `podman network` CLI tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`host-device`: This plugin allows you to directly pass an existing interface
    into a container. This is currently not supported by Podman.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CNI IPAM plugins are related to the IP address management inside containers.
    There are only three reference IPAM plugins:'
  prefs: []
  type: TYPE_NORMAL
- en: '`dhcp`: This plugin lets you execute a daemon on the host that manages the
    `dhcp` leases on behalf of the running containers. It also implies that a running
    `dhcp` server is already running on the host network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`host-local`: This plugin is used to allocate IP addresses to containers using
    a defined address range. The allocation data is stored in the host filesystem.
    It is optimal for local container execution and is the default IPAM plugin that''s
    used by Podman in the network bridge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`static`: This is a basic plugin that manages a discrete list of static addresses
    that are assigned to containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NI Meta plugins are used to configure specific behaviors in the host, such
    as tuning, firewall rules, and port mapping, and are executed as chained plugins
    along with the interface-creating plugins. The current Meta plugins that are maintained
    in the reference plugins repository are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`portmap`: This plugin is used to manage port mapping between the container
    and the host. It applies configuration using the host firewall (`iptables`) and
    is responsible for creating **Source NAT** (**SNAT**) and **Destination Nat**
    (**DNAT**) rules. This plugin is enabled by default in Podman.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`firewall`: This plugin configures firewall rules to allow container ingress
    and egress traffic. It''s enabled by default in Podman.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tuning`: This plugin customizes system tuning (using `sysctl` parameters)
    and interface attributes in the network namespace. It''s enabled by default in
    Podman.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bandwidth`: This plugin can be used to configure traffic rate limiting on
    containers using the Linux traffic control subsystem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sbr`: This plugin is used to configure `/usr/libexec/cni` folder and are provided
    by the `containernetworking-plugins` package, installed as a Podman dependency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going back to the CNI configuration example, we can see that the default Podman
    configuration uses a `bridge` plugin with `host-local` IP address management and
    that the `portmap`, `tuning`, and `firewall` plugins are chained together with
    it.
  prefs: []
  type: TYPE_NORMAL
- en: In the default network that was created for Podman, the subnet that's been allocated
    for container networking is `10.88.0.0/16` and the bridge, called `cni-podman0`,
    acts as the default gateway to containers on `10.88.0.1`, implying that all outbound
    traffic from a container is directed to the bridge's interface.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: This configuration is applied to rootfull containers only. Later in this chapter,
    we'll learn that Podman uses a different networking approach for rootless containers
    to overcome the user's limited privileges. We will see that this approach has
    many limitations on host interfaces and IP address management.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see what happens on the host when a new rootfull container is created.
  prefs: []
  type: TYPE_NORMAL
- en: Podman CNI walkthrough
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this subsection, we will investigate the most peculiar network events that
    occur when a new container is created when CNI is used as a network backend.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: All the examples in this subsection are executed as the `root` user. Ensure
    that you clean up the existing running containers to have a clearer view of the
    network interfaces and firewall rules.
  prefs: []
  type: TYPE_NORMAL
- en: We will try to run an example using the Nginx container and map its default
    internal port, `80/tcp`, to the host port, `8080/tcp`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin, we want to verify the current host''s IP configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Along with the host's main interface, `eth0`, we can see a `cni-podman0` bridge
    interface with an address of `10.88.0.1/16`. Also, notice that the bridge's state
    is set to `DOWN`.
  prefs: []
  type: TYPE_NORMAL
- en: Important
  prefs: []
  type: TYPE_NORMAL
- en: If the host that's being used for the test is a fresh install and Podman has
    never been executed before, the `cni-podman0` bridge interface will not be listed.
    This is not a problem – it will be created when a rootfull container is created
    for the first time.
  prefs: []
  type: TYPE_NORMAL
- en: 'If no other container is running on the host, we should see no interface attached
    to the virtual bridge. To verify this, we are going to use the `bridge link show`
    command, whose output is expected to be empty:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the firewall rules, we do not expect to see rules related to containers
    in the `filter` and `nat` tables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: The output of the preceding commands has been omitted for the sake of brevity,
    but it is worth noting that the `filter` table should already contain two CNI-related
    chains named `CNI-ADMIN` and `CNI-FORWARD`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we want to inspect the routing rules for the `cni-podman0` interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This command says that all traffic going to the `10.88.0.0/16` network goes
    through the `cni-podman0` interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run our Nginx container and see what happens to the network interfaces,
    routing, and firewall configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The first and most interesting event is a new network interface being created,
    as shown in the output of the `ip addr show` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This new interface is part of a `man 4 veth`), a couple of virtual Ethernet
    devices that act like a local tunnel. Veth pairs are native Linux kernel virtual
    interfaces that don't depend on a container runtime and can be applied to use
    cases that go beyond container execution.
  prefs: []
  type: TYPE_NORMAL
- en: The interesting part of veth pairs is that they can be spawned across multiple
    network namespaces and that a packet that's sent to one side of the pair is immediately
    received on the other side.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `vethcf8b2132@if2` interface is linked to a device that resides in a network
    namespace named `cni-df380fb0-b8a6-4f39-0d19-99a0535c2f2d`. Since Linux offers
    us the option to inspect network namespaces using the `ip netns` command, we can
    check if the namespace exists and inspect its network stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Hint
  prefs: []
  type: TYPE_NORMAL
- en: When a new network namespace is created, a file with the same name under `/var/run/netns/`
    is created. This file has also the same inode number that's pointed to by the
    symlink under `/proc/<PID>/ns/net`. When the file is opened, the returned file
    descriptor gives access to the namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding command confirms that the network namespace exists. Now, we want
    to inspect the network interfaces that have been defined inside it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we executed an `ip addr show` command that''s nested inside the `ip netns
    exec` command. The output shows us an interface that is on the other side of our
    veth pair. This also tells us something valuable: the container''s IPv4 address,
    set to `10.88.0.3`.'
  prefs: []
  type: TYPE_NORMAL
- en: Hint
  prefs: []
  type: TYPE_NORMAL
- en: If you're curious, the container IP configuration, when using Podman's default
    network with the `host-local` IPAM plugin, is persisted to the `/var/lib/cni/networks/podman`
    folder. Here, a file named after the assigned IP address is created and written
    with the container-generated ID.
  prefs: []
  type: TYPE_NORMAL
- en: If a new network is created and used by a container, its configuration will
    be persisted in the `/var/lib/cni/networks/<NETWORK_NAME>` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also inspect the container''s routing tables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: All the outbound traffic that's directed to the external networks will go through
    the `10.88.0.1` address, which has been assigned to the `cni-podman0` bridge.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a new container is created, the `firewall` and `portmapper` CNI plugins
    apply the necessary rules in the host filter and NAT tables. In the following
    code, we can see the rules that have been applied to the container IP address
    in the `nat` table, where SNAT, DNAT, and masquerading rules have been applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The bolder line shows a DNAT rule in a custom chain named `CNI-DN-fb51a7bfa5365a8a89e76`.
    This rule says that all the TCP packets whose destination is the `8080/tcp` port
    on the host should be redirected to the `10.88.0.3:80` port, which is the network
    socket that's exposed by the container. This rule matches the`–p 8080:80` option
    that we passed during container creation.
  prefs: []
  type: TYPE_NORMAL
- en: 'But how does the container communicate with the external world? Let''s inspect
    the `cni-podman0` bridge again while looking for notable changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The aforementioned interface is connected to the virtual bridge, which also
    happens to have an IP address assigned to it (`10.88.0.1`) that acts as the default
    gateway for all the containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to trace the path of an ICMP packet from the container to a well-known
    host, `1.1.1.1` (Cloudflare public DNS). To do so, we must run the `traceroute`
    utility from the container network''s namespace using the `ip netns exec` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: The traceroute program could be installed on the host by default. To install
    it on Fedora, run the `sudo dnf install traceroute` command.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding output shows a series of `10.88.0.1`), moving to the host's network
    stack.
  prefs: []
  type: TYPE_NORMAL
- en: The second hop is the host's default gateway (`192.168.121.1`), which is assigned
    to a virtual bridge in a hypervisor and connected to our lab's host VM.
  prefs: []
  type: TYPE_NORMAL
- en: The third hop is a private network default gateway (`192.168.1.1`) that's assigned
    to a physical router that's connected to the lab's hypervisor network.
  prefs: []
  type: TYPE_NORMAL
- en: This demonstrates that all the traffic goes through the `cni-podman0` bridge
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: We can create more than one network, either using Podman native commands or
    our favorite editor to manage JSON files directly.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've explored CNI's implementation and configuration details, let's
    look at the new Netavark implementation in Podman 4\.
  prefs: []
  type: TYPE_NORMAL
- en: Netavark configuration quick start
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Podman''s 4.0.0 release introduced Netavark as the default network backend.
    The advantages of Netavark are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Support for dual IPv4/IPv6 stacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for DNS native resolution using the **aardvark-dns** companion project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for rootless containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for different firewall implementations, including iptables, firewalld,
    and nftables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The configuration files that are used by Netavark are not very different from
    the ones that were shown for CNI. Netavark still uses JSON format to configure
    networks; files are stored under the `/etc/containers/networks` path for rootfull
    containers and the `~/.local/share/containers/storage/networks` path for rootless
    containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following configuration file shows an example network that''s been created
    and managed under Netavark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The first noticeable element is the more compact size of the configuration
    file compared to a CNI configuration. The following fields are defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '`name`: The name of the network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`id`: The unique network ID.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`driver`: This specifies the kind of network driver that''s being used. The
    default is `bridge`. Netavark also supports MACVLAN drivers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`network_interface`: This is the name of the network interface associated with
    the network. If `bridge` is the configured driver, this will be the name of the
    Linux bridge. In the preceding example, a bridge is created called `podman1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`created`: The network creation timestamp.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subnets`: This provides a list of subnet and gateway objects. Subnets are
    assigned automatically. However, when you''re creating a new network with Podman,
    users can provide a custom CIDR. Netavark allows you to manage multiple subnets
    and gateways on a network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ipv6_enabled`: Native support for IPv6 in Netavark can be enabled or disabled
    with this boolean.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`internal`: This boolean is used to configure a network for internal use only
    and to block external routing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dns_enabled`: This boolean enables DNS resolution for the network and is served
    by the `aardvark-dns` daemon.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ipam_options`: This object defines a series of `ipam` parameters. In the preceding
    example, the only option is the kind of IPAM driver, `host-local`, which behaves
    in a way similar to the CNI host-local plugin.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The default Podman 4 network, named `podman`, implements a bridge driver (the
    bridge's name is `podman0`). Here, DNS support is disabled, similar to what happens
    with the default CNI configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Netavark is also an executable binary that's installed by default in the `/usr/libexec/podman/netavark`
    path. It has a simple `setup` and `teardown` commands, applying the network configuration
    to a given network namespace (see `man netavark`).
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at the effects of creating a new container with Netavark.
  prefs: []
  type: TYPE_NORMAL
- en: Podman Netavark walkthrough
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like CNI, Netavark manages the creation of network configurations in the container
    network namespace and the host network namespace, including the creation of veth
    pairs and the Linux bridge that's defined in the config file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before the first container is created in the default Podman network, no bridges
    are created and the host interfaces are the only ones available, along with the
    loopback interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run a new Nginx container and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'When the container is started, the `podman0` bridge and a veth interface appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: There are no particular changes for the end user in terms of network namespaces,
    mixing context between version management, firewall rules, or routing compared
    to the CNI walkthrough provided previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, a network namespace in the host is created for the `nginx-netavark`
    container. Let''s inspect the contents of the network namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Once again, it is possible to find the internal IP address that's been assigned
    to the container.
  prefs: []
  type: TYPE_NORMAL
- en: If the container is executed in rootless mode, the bridge and veth pairs will
    be created in a rootless network namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: The rootless network namespace can be inspected in Podman 4 with the `podman
    unshare --rootless-netns` command.
  prefs: []
  type: TYPE_NORMAL
- en: Users running Podman 3 and CNI can use the `--rootless-cni` option to obtain
    the same results.
  prefs: []
  type: TYPE_NORMAL
- en: In the next subsection, we will learn how to manage and customize container
    networks with the CLI tools that are offered by Podman.
  prefs: []
  type: TYPE_NORMAL
- en: Managing networks with Podman
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `podman network` command provides the necessary tools for managing container
    networks. The following subcommands are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '`create`: Creates a new network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`connect`: Connects to a given network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`disconnect`: Disconnects from a network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exists`: Checks if a network exists'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inspect`: Dumps the CNI configuration of a network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prune`: Removes unused networks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reload`: Reloads container firewall rules'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rm`: Removes a given network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, you will learn how to create a new network and connect a container
    to it. For Podman 3, all the generated CNI config files are written to the `/etc/cni/net.d`
    folder in the host.
  prefs: []
  type: TYPE_NORMAL
- en: For Podman 4, all the generated Netavark config files for rootfull networks
    are written to `/etc/containers/networks`, while the config files for rootless
    networks are written to `~/.local/share/containers/storage/networks`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command creates a new network called `example1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Here, we provided subnet and gateway information, along with the driver type
    that corresponds to the CNI interface-creating plugin. The resulting network configuration
    is written in the aforementioned paths according to the kind of network backend
    and can be inspected with the `podman network inspect` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows the configuration for a CNI network backend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The new network CNI configuration shows that a bridge called `cni-podman1` will
    be created for this network and that containers will allocate IPs from the `10.89.0.0/16`
    subnet.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other fields of the configuration are pretty similar to the default one,
    except for the `dnsname` plugin (project''s repository: [https://github.com/containers/dnsname](https://github.com/containers/dnsname)),
    which is used to enable internal container name resolution. This feature provides
    an advantage in cross-container communication that we will look at in the next
    subsection.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows the generated configuration for a Netavark network
    backend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the bridge naming convention with Netavark is slightly different
    since it uses the `podmanN` pattern, with *N >= 0*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To list all the existing networks, we can use the `podman network ls` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output shows the name, ID, CNI version, and active plugins of
    each active network.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Podman 4, the output is slightly more compact since there are no CNI plugins
    to be shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it''s time to spin up a container that''s attached to the new network.
    The following code creates a PostgreSQL database that''s attached to the `example1`
    network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The new container receives an address from the `10.89.0.0/16` subnet, as shown
    by the `podman inspect` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'When we''re using the CNI network backend, we can double-check this information
    by looking at the contents of the new `/var/lib/cni/networks/example1` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the content of the `10.89.0.3` file, we find the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The file holds the container ID of our `postgres` container, which is used to
    track the mapping with the assigned IP address. As we mentioned previously, this
    behavior is managed by the `host-local` plugin, the default IPAM choice for Podman
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: The Netavark network backend tracks IPAM configuration in the `/run/containers/networks/ipam.db`
    file for rootfull containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also see that a new Linux bridge has been created (notice the `cni-`
    prefix that is used for CNI network backends):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The new device is connected to one peer of the PostgreSQL container''s veth
    pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that `vethf03ed735@eth0` is attached to the `cni-podman1`
    bridge. The interface has the following configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding output also shows that the other side of the veth pair is located
    in the container''s network namespace – that is, `cni-77bfb1c0-af07-1170-4cc8-eb56d15511ac`.
    We can inspect the container''s network configuration and confirm the IP address
    that''s been allocated from the new subnet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: The network namespace naming pattern for the Netavark backend in Podman 4 is
    `netns-<UID>`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to connect a running container to another network without stopping
    and restarting it. In this way, the container will keep an interface attached
    to the original network and a second interface, attached to the new network, will
    be created. This feature, which is useful for use cases such as reverse proxies,
    can be achieved with the `podman network connect` command. Let''s try to run a
    new `net_example` container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'To verify that the container has been attached to the new network, we can run
    the `podman inspect` command and look at the networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that the container now has two interfaces attached to the `podman`
    and `example1` networks, with IP addresses allocated from each network's subnet.
  prefs: []
  type: TYPE_NORMAL
- en: 'To disconnect a container from a network, we can use the `podman network disconnect`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'When a network is not necessary anymore and is disconnected from running containers,
    we can delete it with the `podman network rm` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The command's output shows the list of removed networks. Here, the network's
    CNI configuration is removed from the host's `/etc/cni/net.d` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'If the network has associated containers that are either running or have been
    stopped, the previous message will fail with `Error: "example1" has associated
    containers with it`. To work around this issue, remove or disconnect the associated
    containers before using the command.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `podman network rm` command is useful when we need to remove a specific
    network. To remove all unused networks, the `podman network prune` command is
    a better choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we learned about the CNI specification and how Podman leverages
    its interface to simplify container networking. In a multi-tier or microservices
    scenario, we need to let containers communicate with each other. In the next section,
    we will learn how to manage container-to-container communication.
  prefs: []
  type: TYPE_NORMAL
- en: Interconnecting two or more containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using our knowledge from the previous section, we should be aware that two or
    more containers that have been created inside the same network can reach each
    other on the same subnet without the need for external routing.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, two or more containers that belong to different networks will
    be able to reach each other on different subnets by routing packets through their
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate this, let''s create a couple of `busybox` containers in the
    same default network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In our lab, the two containers have `10.88.0.14` (`endpoint1`) and `10.88.0.15`
    (`endpoint2`) as their addresses. These two addresses are subject to change and
    can be collected using the methods illustrated previously with the `podman inspect`
    or the `nsenter` commands.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding capabilities customization, we added the `CAP_NET_ADMIN` and `CAP_NET_RAW`
    capabilities to let the containers run commands such as `ping` or `traceroute`
    seamlessly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to run a `traceroute` command from `endpoint1` to `endpoint2` to
    see the path of a packet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the packet stays on the internal network and reaches the node
    without additional hops.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create a new network, `net1`, and connect a container called `endpoint3`
    to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The container in our lab gets an IP address of `10.90.0.2`. Let''s see the
    network path from `endpoint1` to `endpoint3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: This time, the packet has traversed the `endpoint1` container's default gateway
    (`10.88.0.1`) and reached the `endpoint3` container, which is routed from the
    host to the associated `net1` Linux bridge.
  prefs: []
  type: TYPE_NORMAL
- en: 'Connectivity across containers in the same host is very easy to manage and
    understand. However, we are still missing an important aspect for container-to-container
    communication: DNS resolution.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's learn how to leverage this feature with Podman networks.
  prefs: []
  type: TYPE_NORMAL
- en: Container DNS resolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Despite its many configuration caveats, DNS resolution is a very simple concept:
    a service is queried to provide the IP address associated with a given hostname.
    The amount of information that can be provided by a DNS server is far richer than
    this, but we want to focus on simple IP resolution in this example.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, let's imagine a scenario where a web application running on a container
    named `webapp` needs read/write access to a database running on a second container
    named `db`. DNS resolution enables `webapp` to query for the `db` container's
    IP address before contacting it.
  prefs: []
  type: TYPE_NORMAL
- en: Previously, we learned that Podman's default network does not provide DNS resolution,
    while new user-created networks have DNS resolution enabled by default. On a CNI
    network backend, the `dnsname` plugin automatically configures a `dnsmasq` service,
    which is started when containers are connected to the network, to provide DNS
    resolution. On a Netavark network backend, the DNS resolution is delivered by
    `aarvark-dns`.
  prefs: []
  type: TYPE_NORMAL
- en: To test this feature, we are going to reuse the **students** web application
    that we illustrated in [*Chapter 10*](B17908_10_epub.xhtml#_idTextAnchor193),
    *Troubleshooting and Monitoring Containers*, since it provides an adequate client-server
    example with a minimal REST service and a database backend based on PostgreSQL.
  prefs: []
  type: TYPE_NORMAL
- en: Info
  prefs: []
  type: TYPE_NORMAL
- en: The source code is available in this book's GitHub repository at [https://github.com/PacktPublishing/Podman-for-DevOps/tree/main/Chapter10/students](https://github.com/PacktPublishing/Podman-for-DevOps/tree/main/Chapter10/students).
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the web application simply prints some output in JSON as the
    result of an HTTP GET that triggers a query to a PostgreSQL database. For our
    demonstration, we will run both the database and the web application on the same
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must create the PostgreSQL database pod while providing a generic
    username and password:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we must restore the data from the SQL dump in the `students` folder to
    the database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'If you haven''t already built it in the previous chapters, then you need to
    build the `students` container image and run it on the host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice the highlighted part of the command: the `students` application accepts
    the `-host`, `-port`, `-username`, and `-password` options to customize the database''s
    endpoints and credentials.'
  prefs: []
  type: TYPE_NORMAL
- en: We did not provide any IP address in the host field. Instead, the Postgres container
    name, `db`, along with the default `5432` port, were used to identify the database.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, notice that the `db` container was created without any kind of port mapping:
    we expect to directly reach the database over the `net1` container network, where
    both containers were created.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to call the `students` application API and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The query worked fine, meaning that the application successfully queried the
    database. But how did this happen? How did it resolve the container IP address
    by only knowing its name? In the next section, we'll look at the different behaviors
    on CNI and Netavark network backends.
  prefs: []
  type: TYPE_NORMAL
- en: DNS resolution on a CNI network backend
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'On Podman 3 or Podman 4 with a CNI backend, the `dnsname` plugin is enabled
    in the `net1` network and a dedicated `dnsmasq` service is spawned that is in
    charge of resolving container names to their assigned IP addresses. Let''s start
    by finding the container''s IP addresses first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to look for `dnsmasq` processes running on the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding output shows an instance of the `dnsmasq` process running with
    a config file that''s been created under the `/run/containers/cni/dnsname/net1/`
    directory. Let''s inspect its contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '`/run/containers/cni/dnsname/net1/dnsmasq.conf` defines the `dnsmasq` configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The process listens on the `cni-podman1` interface (the `net1` network bridge,
    which has an IP address of `10.90.0.1`) and is authoritative for the `dns.podman`
    domain. The host''s records are kept in the `/run/containers/cni/dnsname/net1/addnhosts`
    file, which contains the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'When a container in the `net1` network attempts DNS resolution, it uses its
    `/etc/resolv.conf` file to find out the DNS server to direct the query to. The
    file''s content in the `webapp` container is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: This shows that the container contacts the `10.90.0.1` address (which is also
    the container default gateway and the `cni-podman1` bridge) to query hostname
    resolution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The search domain allows processes to search for a `db.dns.podman` would be
    resolved correctly by the DNS service. The search domain for a CNI network configuration
    can be customized by editing the related config file under `/etc/cni/net.d/`.
    The default configuration for the `dnsname` plugin in the `net1` config is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: When you update the `domainName` field to a new value, the changes are not effective
    immediately. To regenerate the updated `dnsmasq.conf`, all the containers in the
    network must be stopped to let the `dnsname` plugin clean up the current network
    configuration. When containers are restarted, the `dnsmasq` configuration is regenerated
    accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: DNS resolution on a Netavark network backend
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the preceding example was executed on Podman 4 with a Netavark network backend,
    the `aardvark-dns` daemon would be responsible for container resolution in a similar
    way to `dnsmasq`.
  prefs: []
  type: TYPE_NORMAL
- en: The `aardvark-dns` project is a companion project of Netavark written in Rust.
    It is a lightweight authoritative DNS service that can work on both IPv4 A records
    and IPv6 AAAA records.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a new network with DNS resolution enabled is created, a new `aardvark-dns`
    process is created, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The process listens on port `53/udp` of the host network namespace for rootfull
    containers and on port `53/udp` of the rootless network namespace for rootless
    containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the `ps` command also shows the default configuration path –
    the `/run/containers/networks/aardvark-dns` directory – where the `aardvark-dns`
    process stores the resolution configurations under different files, named after
    the associated network. For example, for the `net1` network, we will find content
    similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The file stores IPv4 addresses (and IPv6 addresses, if present) for every container.
    Here, we can see the containers' names and short IDs resolved to the IPv4 addresses.
  prefs: []
  type: TYPE_NORMAL
- en: The first line tells us the address where `aardvark-dns` is listening for incoming
    requests. Once again, it corresponds to the default gateway address for the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Connecting containers across the same network allows for fast and simple communication
    across different services running in separate network namespaces. However, there
    are use cases where containers must share the same network namespace. Podman offers
    a solution to achieve this goal easily: Pods.'
  prefs: []
  type: TYPE_NORMAL
- en: Running containers inside a Pod
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concept of a Pod comes from the Kubernetes architecture. According to the
    official upstream documentation, "*A Pod ... is a group of one or more containers,
    with shared storage and network resources, and a specification for how to run
    the containers*."
  prefs: []
  type: TYPE_NORMAL
- en: A Pod is also the smallest deployable unit in Kubernetes scheduling. All the
    containers inside a Pod share the same network, UTC, IPC, and (optionally) PID
    namespace. This means that all the services running on the different containers
    can refer to each other as **localhost**, while external containers will continue
    to contact the Pod's IP address. A Pod receives one IP address that is shared
    across all the containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many adoption use cases. A very common one is sidecar containers:
    in this case, a reverse proxy or an OAuth proxy runs alongside the main container
    to provide authentication or service mesh functionalities.'
  prefs: []
  type: TYPE_NORMAL
- en: Podman provides the basic tooling for manipulating Pods with the `podman pod`
    command. The following example shows how to create a basic Pod with two containers
    and demonstrates network namespace sharing across the two containers in the Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: To understand the following example, stop and remove all the running containers
    and Pods and start with a clean environment.
  prefs: []
  type: TYPE_NORMAL
- en: '`podman pod create` initializes a new, empty Pod from scratch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: When a new, empty Pod is created, Podman also creates an `infra` container,
    which is used to initialize the namespaces when the Pod is started. This container
    is based on the `k8s.gcr.io/pause` image for Podman 3 and a locally-built `podman-pause`
    image for Podman 4.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can create two basic `busybox` containers inside the Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can start the Pod (and its associated containers) with the `podman
    pod start` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have a running Pod with two containers (plus an infra one) running.
    To verify its status, we can use the `podman pod ps` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'With the `podman pod top` command, we can see the resources that are being
    consumed by each container in the Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'After creating the Pod, we can inspect the network''s behavior. First, we will
    see that only one network namespace has been created in the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check the IP configuration for this namespace and its related network
    stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'To verify that the `c1` and `c2` containers share the same network namespace
    and are running with an IP address of `10.88.0.3`, we can run the same `ip addr
    show` command inside the containers using the `podman exec` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: These two containers are expected to return the same output as the `netns-17b9bb67-5ce6-d533-ecf0-9d7f339e6ebd`
    network namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example pod can be stopped and removed with the `podman pod stop` and `podman
    pod rm` commands, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: We will cover pods in more detail in [*Chapter 14*](B17908_14_epub.xhtml#_idTextAnchor257),
    *Interacting with systemd and Kubernetes*, where we will also discuss name resolution
    and multi-pod orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we focused on communication across two or more containers inside
    the same host or Pod, regardless of the number and type of networks involved.
    However, containers are a platform where you can run services that are generally
    accessed by the external world. For this reason, in the next section, we will
    investigate the best practices that can be applied to expose containers outside
    their hosts and make their services accessible to other clients/consumers.
  prefs: []
  type: TYPE_NORMAL
- en: Exposing containers outside our underlying host
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Container adoption in an enterprise company or a community project could be
    a hard thing to do that could require time. For this reason, we may not have all
    the required services running as containers during our adoption journey. This
    is why exposing containers outside our underlying host could be a nice solution
    for interconnecting services that live in containers to services that run in the
    legacy world.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we briefly saw earlier in this chapter, Podman uses two different networking
    stacks, depending on the container: rootless or rootfull.'
  prefs: []
  type: TYPE_NORMAL
- en: Even though the underlying mechanism is slightly different, depending on if
    you are using a rootless or a rootfull container, Podman's command-line options
    for exposing network ports are the same for both container types.
  prefs: []
  type: TYPE_NORMAL
- en: Good to Know
  prefs: []
  type: TYPE_NORMAL
- en: Note that the example we are going to see in this section will be executed as
    a root user. This choice was necessary because the main objective of this section
    is to show you some of the firewall configurations that could be mandatory for
    exposing a container service to the outside world.
  prefs: []
  type: TYPE_NORMAL
- en: Exposing a container starts with Port Publishing activities. We'll learn what
    this is in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Port Publishing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Port Publishing consists of instructing Podman to create a temporary mapping
    between the container's ports and some random or custom host's ports.
  prefs: []
  type: TYPE_NORMAL
- en: 'The option to instruct Podman to publish a port is really simple – it consists
    of adding the `-p` or `--publish` option to the `run` command. Let''s see how
    it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: The previous option publishes a container's port, or range of ports, to the
    host. When we are specifying ranges for `hostPort` or `containerPort`, the number
    must be equal for both ranges.
  prefs: []
  type: TYPE_NORMAL
- en: We can even omit `ip`. In that case, the port will be bound on all the IPs of
    the underlying host. If we do not set the host port, the container's port will
    be randomly assigned a port on the host.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example of the port publishing option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we have told Podman to run a container starting from the `httpd`
    base image. Then, we allocated a pseudo-tty (`-t`) in detached mode (`-d`) before
    setting the port mapping to bind the underlying host port, `80`, to port `80`
    of the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can use the `podman port` command to see the actual mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we requested the list of running containers and then passed the correct
    container ID to the `podman port` command. We can check if the mapping is working
    properly like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Here, we executed a `curl` command from the host system and it worked – the
    `httpd` process running in the container just replied to us.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have multiple ports and we do not care about their assignment on the
    underlying host system, we can easily leverage the`–P` or `--publish-all` option
    to publish all the ports that are exposed by the container image to random ports
    on the host interfaces. Podman will run through the container image''s metadata
    looking for the exposed ports. These ports are usually defined in a Dockerfile
    or Containerfile with the `EXPOSE` instruction, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: With the previous keyword, we can instruct the container engine that will run
    the final container of which network ports will be exposed and used by it.
  prefs: []
  type: TYPE_NORMAL
- en: However, we can leverage an easy but insecure alternative, as shown in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Attaching a host network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To expose a container service to the outside world, we can attach the whole
    host network to the running container. As you can imagine, this method could lead
    to the unauthorized use of host resources so, for this reason, it is not recommended
    and should be used carefully.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we anticipated, attaching the host network to a running container is quite
    simple. Using the right Podman option, we can easily get rid of any network isolation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Here, we used the `--network` option while specifying the `host` value. This
    informs Podman that we want to let the container attach to the host network.
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the previous command, we can check that the running container
    is bound to the host system''s network interfaces since it can access all of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Here, we executed a `curl` command from the host system and it worked – the
    `httpd` process running in the container just replied to us.
  prefs: []
  type: TYPE_NORMAL
- en: The process of exposing containers outside the underlying host does not stop
    here. In the next section, we'll learn how to complete this job.
  prefs: []
  type: TYPE_NORMAL
- en: Host firewall configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whether we choose to leverage Port Publishing or attach the host network to
    the container, the process of exposing containers outside the underlying host
    does not stop here – we have reached the base OS of our host machine. In most
    cases, we will also need to allow the incoming connections to flow in the host's
    underlying machine, which will be interacting with the system firewall.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows a non-comprehensive way to interact with the base
    OS firewall. If we''re using a Fedora operating system or any other Linux distribution
    that''s leveraging Firewalld as its firewall daemon manager, we can allow incoming
    connections on port `80` by running the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: The first command edits the live system rules, while the second command stores
    the runtime rules in a permanent way that will survive system reboot or service
    restart.
  prefs: []
  type: TYPE_NORMAL
- en: Good to Know
  prefs: []
  type: TYPE_NORMAL
- en: Firewalld is a firewall service daemon that provides us with an easy and fast
    way to customize the system firewall. Firewalld is dynamic, which means that it
    can create, change, and delete the firewall rules without restarting the firewall
    daemon each time a change is applied.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have seen, the process of exposing the container''s services is quite
    simple but should be performed with a bit of consciousness and attention: opening
    a network port to the outside world should always be done carefully.'
  prefs: []
  type: TYPE_NORMAL
- en: Rootless container network behavior
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in the previous sections, Podman relies on CNI plugins or Netavark
    for containers running as root and has the privileges to alter network configurations
    in the host network namespace. For rootless containers, Podman uses the `slirp4netns`
    project, which allows you to create container network configurations without the
    need for root privileges; the network interfaces are created inside a rootless
    network namespace where the standard user has sufficient privileges. This approach
    allows you to transparently and flexibly manage rootless container networking.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous sections, we saw how container network namespaces can be connected
    to a bridge using a veth pair. Being able to create a veth pair in the host network
    namespace requires root privileges that are not allowed for standard users.
  prefs: []
  type: TYPE_NORMAL
- en: In the simplest scenario, `slirp4netns` aims to overcome these privilege limitations
    by allowing a tap device to be created that's attached to a user-mode network
    namespace. This tap device is created in the rootless network namespace.
  prefs: []
  type: TYPE_NORMAL
- en: For every new rootless container, a new `slirp4netns` process is executed on
    the host. The process creates a network namespace for the container and a `tap0`
    device is created and configured with the `10.0.2.100/24` address (from the default
    slirp4netns `10.0.2.0/24` subnet). This prevents two containers from directly
    communicating with each other on the same network since there would be an IP address
    overlap.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrates the network behavior of a rootless `busybox`
    container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'It is possible to inspect the rootless network namespace and find the corresponding
    `tap0` device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Since rootless containers do not own independent IP addresses, we have two
    ways to let two or more containers communicate with each other:'
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way could be to put all the containers in a single Pod so that the
    containers can communicate using the localhost interface, without the need to
    open any ports.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second way is to attach the container to a custom network and have its interfaces
    managed in the rootless network namespace.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we want to keep all the containers independent, we could use the port mapping
    technique to publish all the necessary ports and then use those ports to let the
    containers communicate with each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using a Podman 4 network backend, let''s quickly focus on the second scenario,
    where two pods are attached on a rootless network. First, we need to create the
    network and attach a couple of test containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try to ping the `endpoint2` container from `endpoint1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'These two containers can communicate on the common network and have different
    IPv4 addresses. To prove this, we can inspect the contents of the `aardvark-dns`
    configuration for the rootless containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s demonstrate that the custom network bypasses the `tap0` interface
    and allows dedicated veth pairs and bridges to be created in the rootless network
    namespace. The following command will show a Linux bridge for the `rootless-net`
    network and two attached veth pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: If you're running this code on a CNI network backend, use the `podman unshare
    –rootless-cni` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another limitation of rootless containers is regarding the `ping` command.
    Usually, on Linux distributions, standard non-root users lack the `CAP_NET_RAW`
    security capability. This inhibits the execution of the `ping` command, which
    leverages the send/receive of ICMP packets. If we want to use the `ping` command
    in a rootless container, we can enable the missing security capability through
    the `sysctl` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Note that this could allow any process that will be executed by a user on these
    groups to send ping packets.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, while using rootless containers, we also need to consider that the
    Port Publishing technique can only be used for ports above `1024`. This is because,
    on Linux operating systems, all the ports below `1024` are privileged and cannot
    be used by standard non-root users.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how container network isolation can be leveraged
    to allow network segregation for each container that's running through network
    namespaces. These activities seem complex but thankfully, with the help of a container
    runtime, the steps are almost automated. We learned how to manage container networking
    with Podman and how to interconnect two or more containers. Finally, we learned
    how to expose a container's network ports outside of the underlying host and what
    kind of limitations we can expect while networking for rootless containers.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discover the main differences between Docker and
    Podman. This will be useful for advanced users, but also for novice ones, to understand
    what we can expect by comparing these two container engines.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Container Network Interface: [https://github.com/containernetworking/cni](https://github.com/containernetworking/cni)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Netavark project on GitHub: [https://github.com/containers/netavark](https://github.com/containers/netavark)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `Aardvark-dns` project on GitHub: https://github.com/containers/aardvark-dns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CNI reference plugins: https://www.cni.dev/plugins/current/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CNI third-party plugins: [https://github.com/containernetworking/cni#3rd-party-plugins](https://github.com/containernetworking/cni#3rd-party-plugins)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes Pod definition: [https://kubernetes.io/docs/concepts/workloads/pods/](https://kubernetes.io/docs/concepts/workloads/pods/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `Slirp4netns` project repository: [https://github.com/rootless-containers/slirp4netns](https://github.com/rootless-containers/slirp4netns)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
