<html><head></head><body>
		<div id="_idContainer056">
			<h1 id="_idParaDest-257"><em class="italic"><a id="_idTextAnchor257"/>Chapter 14</em>: Interacting with systemd and Kubernetes</h1>
			<p>In the previous chapters, we learned how to initialize and manage containers, starting with simple concepts and arriving at advanced ones. Containers represent a key technology for application development in the latest Linux operating system releases. For this reason, containers are only the starting point for advanced developers and system administrators. Once this technology becomes widely adopted in an enterprise company or a technical project, the next step will be to integrate it with the base operating system and with -system orchestration platforms.</p>
			<p>In this chapter, we're going to cover the following main topics: </p>
			<ul>
				<li>Setting up the prerequisites for the host operating system</li>
				<li>Creating the systemd unit files</li>
				<li>Managing container-based systemd services</li>
				<li>Generating Kubernetes YAML resources</li>
				<li>Running Kubernetes resource files in Podman</li>
				<li>Testing the results in Kubernetes</li>
			</ul>
			<h1 id="_idParaDest-258"><a id="_idTextAnchor258"/>Technical requirements</h1>
			<p>To complete this chapter, you will need a machine with a working Podman installation. As we mentioned in <a href="B17908_03_epub.xhtml#_idTextAnchor068"><em class="italic">Chapter 3</em></a>, <em class="italic">Running the First Container</em>, all the examples in this book were executed on a Fedora 34 system or later but can be reproduced on your choice of <strong class="bold">operating system</strong> (<strong class="bold">OS</strong>).</p>
			<p>Having a good understanding of the topics that were covered in <a href="B17908_04_epub.xhtml#_idTextAnchor083"><em class="italic">Chapter 4</em></a>, <em class="italic">Managing Running Containers</em>, <a href="B17908_05_epub.xhtml#_idTextAnchor101"><em class="italic">Chapter 5</em></a>, <em class="italic">Implementing Storage for the Container's Data</em>, and <a href="B17908_09_epub.xhtml#_idTextAnchor167"><em class="italic">Chapter 9</em></a>, <em class="italic">Pushing Images to a Container Registry</em>, will help you grasp the topics we'll cover regarding advanced containers.</p>
			<p>You should also have a good understanding of system administration and Kubernetes container orchestration.</p>
			<p>For the examples related to the Kubernetes section, you will require Podman version 4.0.0 because of a bug in version 3.4.z that prevents container environment variables from being created (<a href="https://github.com/containers/podman/issues/12781">https://github.com/containers/podman/issues/12781</a>). This bug was fixed in v4.0.0 but it hasn't been backported to Podman v3 at the time of writing.</p>
			<h1 id="_idParaDest-259"><a id="_idTextAnchor259"/>Setting up the prerequisites for the host operating system</h1>
			<p>As we saw in <a href="B17908_01_epub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to Container Technology</em>, containers were born to help simplify and create <a id="_idIndexMarker1263"/>system services that can be distributed on standalone hosts.</p>
			<p>In the following sections, we will learn how to run MariaDB and a GIT service in containers while managing those containers like any other service – that is, through Systemd and the <strong class="source-inline">systemctl</strong> command.</p>
			<p>First, let's introduce systemd, a system and service manager for Linux that runs as the first process on boot (as PID 1) and acts as an init system that brings up and maintains userspace services. Once a new user logs in to the host system, separate instances are executed to start their services. </p>
			<p>The systemd daemon starts services and ensures priority with a dependency system between various entities called <em class="italic">units</em>. There are 11 <a id="_idIndexMarker1264"/>different types of units.</p>
			<p>Fedora 34 and later has systemd enabled and running by default. We can check if it is running properly by using the following command:</p>
			<p class="source-code"># systemctl is-system-running</p>
			<p class="source-code">running</p>
			<p>In the following sections, we are going to work with system unit files of the <strong class="source-inline">service</strong> type. We can check the current ones by running the following command:</p>
			<p class="source-code"># systemctl list-units --type=service | head</p>
			<p class="source-code">  UNIT                      LOAD   ACTIVE SUB     DESCRIPTION</p>
			<p class="source-code">  abrt-journal-core.service  loaded active running Creates ABRT problems from coredumpctl messages</p>
			<p class="source-code">  abrt-oops.service  loaded active running ABRT kernel log watcher</p>
			<p class="source-code">  abrt-xorg.service  loaded active running ABRT Xorg log watcher</p>
			<p class="source-code">  abrtd.service  loaded active running ABRT Automated Bug Reporting Tool</p>
			<p class="callout-heading">Please Note</p>
			<p class="callout">The systemd service and its internals are more complex, so they cannot be summarized in a few lines. For additional information, please refer to the related Linux manual.</p>
			<p>In the next section, we are <a id="_idIndexMarker1265"/>going to learn how to create the systemd unit files for any running container service on our operating system.</p>
			<h1 id="_idParaDest-260"><a id="_idTextAnchor260"/>Creating the systemd unit files</h1>
			<p>The unit files on our system <a id="_idIndexMarker1266"/>define how systemd starts and runs services. </p>
			<p>Each unit file represents a single component as a simple text file that describes its behavior, what needs to run before or afterward, and more.</p>
			<p>Unit files are stored in a few different places on a system and systemd looks for them in this order:</p>
			<ol>
				<li><strong class="source-inline">/etc/systemd/system</strong></li>
				<li><strong class="source-inline">/run/systemd/system</strong></li>
				<li><strong class="source-inline">/usr/lib/systemd/system</strong></li>
			</ol>
			<p>Unit files that are in the earlier directories override the later ones. This lets us change what we need in the <strong class="source-inline">/etc</strong> directory, where configuration is expected, leaving the default configuration files in <strong class="source-inline">/usr</strong>, for example.</p>
			<p>But what does a unit file look like? Let's find out.</p>
			<p>First, we can get the location of a default unit file by asking systemd about it:</p>
			<p class="source-code"># systemctl status sshd</p>
			<p class="source-code">○ sshd.service - OpenSSH server daemon</p>
			<p class="source-code">     Loaded: loaded (/usr/lib/systemd/system/sshd.service; disabled; vendor preset: disabled)</p>
			<p class="source-code">     Active: inactive (dead)</p>
			<p class="source-code">       Docs: man:sshd(8)</p>
			<p class="source-code">             man:sshd_config(5)</p>
			<p>Here, we executed the <strong class="source-inline">status</strong> command while passing the <strong class="source-inline">sshd</strong> service name as a filter. </p>
			<p>In the systemd output, the<a id="_idIndexMarker1267"/> default unit file path can be inspected with the following example command:</p>
			<p class="source-code"># cat /usr/lib/systemd/system/sshd.service</p>
			<p>But what about Podman? Well, Podman makes systemd integration easier with its dedicated sub-command:</p>
			<p class="source-code"># podman generate systemd -h</p>
			<p class="source-code">Generate systemd units.</p>
			<p class="source-code">Description:</p>
			<p class="source-code">  Generate systemd units for a pod or container.</p>
			<p class="source-code">  The generated units can later be controlled via systemctl(1).</p>
			<p class="source-code">Usage:</p>
			<p class="source-code">  podman generate systemd [options] {CONTAINER|POD}</p>
			<p class="source-code">...</p>
			<p>The <strong class="source-inline">podman generate systemd</strong> command will output a text file representing the unit file that was created. As we can see from the help output, we can set up several options to adjust our settings.</p>
			<p>We should always save the generated<a id="_idIndexMarker1268"/> file and place it on the right path, as described in the previous output. We'll explore this command by providing a full example in the next section.</p>
			<h1 id="_idParaDest-261"><a id="_idTextAnchor261"/>Managing container-based systemd services</h1>
			<p>In this section, you will learn how to <a id="_idIndexMarker1269"/>use the <strong class="source-inline">podman generate systemd</strong> command through a practical example. We will create two system services based on containers to create a GIT repository.</p>
			<p>For this example, we will leverage two well-known open source projects:</p>
			<ul>
				<li><strong class="bold">Gitea</strong>: The GIT repository, which also offers a<a id="_idIndexMarker1270"/> nice web interface for code management</li>
				<li><strong class="bold">MariaDB</strong>: The SQL database <a id="_idIndexMarker1271"/>for holding the data that's produced by the Gitea service</li>
			</ul>
			<p>Let's start with the example. First, we need to generate a password for our database's user:</p>
			<p class="source-code"># export MARIADB_PASSWORD=my-secret-pw</p>
			<p class="source-code"># podman secret create --env MARIADB_PASSWORD</p>
			<p class="source-code">53149b678d0dbd34fb56800cc</p>
			<p>Here, we exported the environment variable with the secret password we are going to use and then leveraged a useful secrets management command that we did not introduce previously: <strong class="source-inline">podman secret create</strong>. Unfortunately, this command holds the secret in plain text, though this is good enough for our purpose. Since we are running these containers as root, these secrets are stored on the filesystem with root-only permissions.</p>
			<p>We can inspect the secret with the following commands:</p>
			<p class="source-code"># podman secret ls</p>
			<p class="source-code">ID                         NAME              DRIVER      CREATED       UPDATED       </p>
			<p class="source-code">53149b678d0dbd34fb56800cc  MARIADB_PASSWORD  file        10 hours ago  10 hours ago  </p>
			<p class="source-code"># podman secret inspect 53149b678d0dbd34fb56800cc</p>
			<p class="source-code">[</p>
			<p class="source-code">    {</p>
			<p class="source-code">        "ID": "53149b678d0dbd34fb56800cc",</p>
			<p class="source-code">        "CreatedAt": "2022-02-16T00:54:21.01087091+01:00",</p>
			<p class="source-code">        "UpdatedAt": "2022-02-16T00:54:21.01087091+01:00",</p>
			<p class="source-code">        "Spec": {</p>
			<p class="source-code">            "Name": "MARIADB_PASSWORD",</p>
			<p class="source-code">            "Driver": {</p>
			<p class="source-code">                "Name": "file",</p>
			<p class="source-code">                "Options": {</p>
			<p class="source-code">                    "path": "/var/lib/containers/storage/secrets/filedriver"</p>
			<p class="source-code">                }</p>
			<p class="source-code">            }</p>
			<p class="source-code">        }</p>
			<p class="source-code">    }</p>
			<p class="source-code">]</p>
			<p class="source-code"># cat /var/lib/containers/storage/secrets/filedriver/secretsdata.json </p>
			<p class="source-code">{</p>
			<p class="source-code">  "53149b678d0dbd34fb56800cc": "bXktc2VjcmV0LXB3"</p>
			<p class="source-code">}</p>
			<p class="source-code"># ls -l /var/lib/containers/storage/secrets/filedriver/secretsdata.json </p>
			<p class="source-code">-rw-------. 1 root root 53 16 feb 00.54 /var/lib/containers/storage/secrets/filedriver/secretsdata.json</p>
			<p>Here, we have asked Podman to list and inspect the secret we created previously and looked at the underlying filesystem for the file holding the secrets.</p>
			<p>The file holding the secrets is a file in JSON<a id="_idIndexMarker1272"/> format and, as we mentioned previously, is in plain text. The first string of the couple is the secret ID, while the second string is the value Base64 encoded. If we try to decode it with the <strong class="source-inline">BASE64</strong> algorithm, we would see that it represents the password we just added – that is, <strong class="source-inline">my-secret-pw</strong>.</p>
			<p>Even though the password is stored in plain text, it is good enough for our example because we are using the root user and this filestore has root-only permission, as we can verify with the last command of the previous output.</p>
			<p>Now, we can continue setting up the database container. We will start with the database setup because it is a dependency on our GIT server.</p>
			<p>We must create a local folder in the host system where we can store container data:</p>
			<p class="source-code"># mkdir -p /opt/var/lib/mariadb</p>
			<p>We can also look at the public documentation of the container image to find out the right volume path and the various environment variables to use to start our container:</p>
			<p class="source-code"># podman run -d --network host --name mariadb-service –v \</p>
			<p class="source-code"> /opt/var/lib/mariadb:/var/lib/mysql:Z –e \</p>
			<p class="source-code"> MARIADB_DATABASE=gitea -e MARIADB_USER=gitea –e \</p>
			<p class="source-code"> MARIADB_RANDOM_ROOT_PASSWORD=true \</p>
			<p class="source-code">--secret=MARIADB_PASSWORD,type=env docker.io/mariadb:latest</p>
			<p class="source-code">61ae055ef6512cb34c4b3fe1d8feafe6ec174a25547728873932f0649217 62d1</p>
			<p>We are going to run and test the container as standalone first to check if there are any errors; then, we will transform it into a system service.</p>
			<p>In the preceding Podman command, we did the following:</p>
			<ul>
				<li>We ran the container in detached mode.</li>
				<li>We assigned it a name – that is, <strong class="source-inline">mariadb-service</strong>.</li>
				<li>We exposed the host network for simplicity; of course, we could limit and filter this connectivity.</li>
				<li>We mapped the storage volume with the newly created local directory while also specifying the <strong class="source-inline">:Z</strong> option to correctly assign the SELinux labels.</li>
				<li>We defined the environment variables to use at runtime by the container's processes, also providing the password's secret with the <strong class="source-inline">--secret</strong> option.</li>
				<li>We used the container image name we want to use – that is, <strong class="source-inline">docker.io/mariadb:latest</strong>.</li>
			</ul>
			<p>We can also check if the container is <a id="_idIndexMarker1273"/>up and running by using the following command:</p>
			<p class="source-code"># podman ps</p>
			<p class="source-code">CONTAINER ID  IMAGE                             COMMAND     CREATED         STATUS             PORTS       NAMES</p>
			<p class="source-code">61ae055ef651  docker.io/library/mariadb:latest  mariadbd    56 seconds ago  Up 57 seconds ago              mariadb-service</p>
			<p>Now, we are ready to check the output of the <strong class="source-inline">podman generate systemd</strong> command:</p>
			<p class="source-code"># podman generate systemd --name mariadb-service</p>
			<p class="source-code">...</p>
			<p class="source-code">[Unit]</p>
			<p class="source-code">Description=Podman container-mariadb-service.service</p>
			<p class="source-code">Documentation=man:podman-generate-systemd(1)</p>
			<p class="source-code">Wants=network-online.target</p>
			<p class="source-code">After=network-online.target</p>
			<p class="source-code">RequiresMountsFor=/run/containers/storage</p>
			<p class="source-code">[Service]</p>
			<p class="source-code">Environment=PODMAN_SYSTEMD_UNIT=%n</p>
			<p class="source-code">Restart=on-failure</p>
			<p class="source-code">TimeoutStopSec=70</p>
			<p class="source-code">ExecStart=/usr/bin/podman start mariadb-service</p>
			<p class="source-code">ExecStop=/usr/bin/podman stop -t 10 mariadb-service</p>
			<p class="source-code">ExecStopPost=/usr/bin/podman stop -t 10 mariadb-service</p>
			<p class="source-code">PIDFile=/run/containers/storage/overlay-containers/61ae055ef6512cb34c4b3fe1d8feafe6ec174a25547728873932f064921762d1/userdata/conmon.pid</p>
			<p class="source-code">Type=forking</p>
			<p class="source-code">[Install]</p>
			<p class="source-code">WantedBy=default.target</p>
			<p>As you can see, the output has been published directly in the console. Here, we used the <strong class="source-inline">--name</strong> option to instruct Podman that we want to manage the container with that name through systemd.</p>
			<p>Podman generated a unit file with all the required command instructions to integrate our container into the operating system. </p>
			<p>In the <strong class="source-inline">[Unit]</strong> section, we can see that it declared a dependency of this service from the network through the <strong class="source-inline">network-online.target</strong> unit. It also states the need for the storage mount point for <strong class="source-inline">/run/containers/storage</strong> containers.</p>
			<p>In the <strong class="source-inline">[Service]</strong> section, Podman defined all the instructions for describing how to start and stop the containerized service.</p>
			<p>Now, let's look at the GIT <a id="_idIndexMarker1274"/>service. First, we will create the storage directory:</p>
			<p class="source-code"># mkdir -p /opt/var/lib/gitea/data</p>
			<p>After that, we can look at the project documentation for any configuration that's needed for the Gitea container image to be built correctly and complete the <strong class="source-inline">podman run</strong> command:</p>
			<p class="source-code"># podman run -d --network host --name gitea-service \</p>
			<p class="source-code">-v /opt/var/lib/gitea/data:/data:Z \</p>
			<p class="source-code">docker.io/gitea/gitea:latest</p>
			<p class="source-code">ee96f8276038f750ee3b956cbf9d3700fe46e6e2bae93605a67e623717e 206dd</p>
			<p>In the previous Podman command, we did the following:</p>
			<ul>
				<li>We ran the container in detached mode.</li>
				<li>We assigned it a name – that is, <strong class="source-inline">gitea-service</strong>.</li>
				<li>We exposed the host network for simplicity; of course, we can limit and filter this connectivity.</li>
				<li>We mapped the storage volume with the newly created local directory while specifying the <strong class="source-inline">:Z</strong> option to correctly assign the SELinux labels.</li>
			</ul>
			<p>Finally, we can check if the service<a id="_idIndexMarker1275"/> is running properly by inspecting its logs:</p>
			<p class="source-code"># podman logs gitea-service</p>
			<p class="source-code">Server listening on :: port 22.</p>
			<p class="source-code">Server listening on 0.0.0.0 port 22.</p>
			<p class="source-code">2022/02/16 00:01:55 cmd/web.go:102:runWeb() [I] Starting Gitea on PID: 12</p>
			<p class="source-code">...</p>
			<p class="source-code">2022/02/16 00:01:56 cmd/web.go:208:listen() [I] Listen: http://0.0.0.0:3000</p>
			<p class="source-code">2022/02/16 00:01:56 cmd/web.go:212:listen() [I] AppURL(ROOT_URL): http://localhost:3000/</p>
			<p class="source-code">2022/02/16 00:01:56 ...s/graceful/server.go:61:NewServer() [I] Starting new Web server: tcp:0.0.0.0:3000 on PID: 12</p>
			<p>As we can see, the Gitea service is listening on port <strong class="source-inline">3000</strong>. Let's point our web browser to <strong class="source-inline">http://localhost:3000</strong> to install it with the required configuration:</p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B17908_14_01.jpg" alt="Figure 14.1 – Gitea service installation page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.1 – Gitea service installation page</p>
			<p>In the preceding screenshot, we <a id="_idIndexMarker1276"/>defined the database's type, address, username, and password to complete the installation. Once done, we should be redirected to the login page, as follows:</p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B17908_14_02.jpg" alt="Figure 14.2 – Gitea service login page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.2 – Gitea service login page</p>
			<p>Once the configuration is complete, we can generate and add the systemd unit files to the right configuration <a id="_idIndexMarker1277"/>path:</p>
			<p class="source-code"># podman generate systemd --name gitea-service &gt; /etc/systemd/system/container-gitea-service.service</p>
			<p class="source-code"># podman generate systemd --name mariadb-service &gt; /etc/systemd/system/container-mariadb-service.service</p>
			<p>Then, we can manually edit the Gitea service unit file by adding a depending order to the MariaDB service through the special <strong class="source-inline">Requires</strong> instruction:</p>
			<p class="source-code"># cat /etc/systemd/system/container-gitea-service.service</p>
			<p class="source-code">...</p>
			<p class="source-code">[Unit]</p>
			<p class="source-code">Description=Podman container-gitea-service.service</p>
			<p class="source-code">Documentation=man:podman-generate-systemd(1)</p>
			<p class="source-code">Wants=network-online.target</p>
			<p class="source-code">After=network-online.target</p>
			<p class="source-code">RequiresMountsFor=/run/containers/storage</p>
			<p class="source-code"><strong class="bold">Requires=container-mariadb-service.service</strong></p>
			<p class="source-code">...</p>
			<p>Thanks to the <strong class="source-inline">Requires</strong> instruction, systemd will start the MariaDB service first, then the Gitea service.</p>
			<p>Now, we can stop the containers by starting them through the systemd units:</p>
			<p class="source-code"># podman stop mariadb-service gitea-service</p>
			<p class="source-code">mariadb-service</p>
			<p class="source-code">gitea-service</p>
			<p>Don't worry about the data – previously, we <a id="_idIndexMarker1278"/>mapped both containers to a dedicated storage volume that holds the data.</p>
			<p>We need to let the systemd daemon know about the new unit files we just added. So, first, we need to run the following command:</p>
			<p class="source-code"># systemctl daemon-reload</p>
			<p>After that, we can start the services through systemd and check their statuses:</p>
			<p class="source-code"># systemctl start container-mariadb-service.service</p>
			<p class="source-code"># systemctl status container-mariadb-service.service</p>
			<p class="source-code">● container-mariadb-service.service - Podman container-mariadb-service.service</p>
			<p class="source-code">     Loaded: loaded (/etc/systemd/system/container-mariadb-service.service; disabled; vendor preset: disabled)</p>
			<p class="source-code">     Active: active (running) since Wed 2022-02-16 01:11:50 CET; 13s ago</p>
			<p class="source-code">...</p>
			<p class="source-code"># systemctl start container-gitea-service.service</p>
			<p class="source-code"># systemctl status container-gitea-service.service </p>
			<p class="source-code">● container-gitea-service.service - Podman container-gitea-service.service</p>
			<p class="source-code">     Loaded: loaded (/etc/systemd/system/container-gitea-service.service; disabled; vendor preset: disabled)</p>
			<p class="source-code">     Active: active (running) since Wed 2022-02-16 01:11:57 CET; 18s ago</p>
			<p class="source-code">...</p>
			<p>Finally, we can enable the service to <a id="_idIndexMarker1279"/>start them when the OS boots:</p>
			<p class="source-code"># systemctl enable container-mariadb-service.service </p>
			<p class="source-code">Created symlink /etc/systemd/system/default.target.wants/container-mariadb-service.service → /etc/systemd/system/container-mariadb-service.service.</p>
			<p class="source-code"># systemctl enable container-gitea-service.service </p>
			<p class="source-code">Created symlink /etc/systemd/system/default.target.wants/container-gitea-service.service → /etc/systemd/system/container-gitea-service.service.</p>
			<p>With that, we have set up and enabled two containerized system services on our host OS. This process is simple and could be useful for leveraging the containers' features and capabilities, extending them to system services.</p>
			<p>Now, we are ready to move on to the next advanced topic, where we will learn how to generate Kubernetes resources.</p>
			<h1 id="_idParaDest-262"><a id="_idTextAnchor262"/>Generating Kubernetes YAML resources</h1>
			<p>Kubernetes has become the <a id="_idIndexMarker1280"/>de facto standard for multi-node container orchestration. Kubernetes clusters allow multiple pods to be executed across nodes according to scheduling policies that reflect the node's load, labels, capabilities, or hardware resources (for example, GPUs).</p>
			<p>We have already described the concept of a pod – a single execution group of one or more containers that share common namespaces (network, IPC, and, optionally, PID namespaces). In other words, we can think of pods as sandboxes for containers. Containers inside a Pod are executed and thus started, stopped, or paused simultaneously. </p>
			<p>One of the most promising features that was introduced by Podman is the capability to generate Kubernetes resources in YAML format. Podman can intercept the configuration of running containers or pods and generate a <strong class="source-inline">Pod</strong> resource that is compliant with Kubernetes API specifications.</p>
			<p>Along with pods, we can generate <strong class="source-inline">Service</strong> and <strong class="source-inline">PersistentVolumeClaim</strong> resources as well, which reflect the configurations of the port mappings and volumes that are mounted inside containers.</p>
			<p>We can use the generated <a id="_idIndexMarker1281"/>Kubernetes resources inside Podman itself as an alternative to the Docker Compose stacks or apply them inside a Kubernetes cluster to orchestrate the execution of simple pods. </p>
			<p>Kubernetes has many ways to orchestrate how workloads are executed: <strong class="source-inline">Deployments</strong>, <strong class="source-inline">StatefulSets</strong>, <strong class="source-inline">DaemonSets</strong>, <strong class="source-inline">Jobs</strong>, and <strong class="source-inline">CronJobs</strong>. In every case, Pods are their workload-minimal execution units and the orchestration logic changes based on that specific behavior. This means that we can take a Pod resource that's been generated by Podman and easily adapt it to be orchestrated in a more complex object, such as <strong class="source-inline">Deployments</strong>, which manages replicas and version rollouts of our applications, or <strong class="source-inline">DaemonSets</strong>, which guarantees that a singleton pod instance is created for every cluster node.</p>
			<p>Now, let's learn how to generate Kubernetes YAML resources with Podman. </p>
			<h2 id="_idParaDest-263"><a id="_idTextAnchor263"/>Generating basic Pod resources from running containers</h2>
			<p>The basic command to generate Kubernetes<a id="_idIndexMarker1282"/> resource from Podman is <strong class="source-inline">podman generate kube</strong>, followed by various options and<a id="_idIndexMarker1283"/> arguments, as shown in the following code:</p>
			<p class="source-code">$ podman generate kube [options] {CONTAINER|POD|VOLUME}</p>
			<p>We can apply this command to a running container, pod, or existing volume. The command also allows you to use the <strong class="source-inline">-s, --service</strong> option to generate <strong class="source-inline">Service</strong> resources and <strong class="source-inline">-f, --filename</strong> to export contents to a file (the default is to standard output).</p>
			<p>Let's start with a basic example of a <strong class="source-inline">Pod</strong> resource that's been generated from a running container. First, we will start a rootless Nginx container:</p>
			<p class="source-code">$ podman run –d \</p>
			<p class="source-code">  -p 8080:80 --name nginx \</p>
			<p class="source-code">  docker.io/library/nginx</p>
			<p>When the container is created, we <a id="_idIndexMarker1284"/>can generate our <a id="_idIndexMarker1285"/>Kubernetes <strong class="source-inline">Pod</strong> resource:</p>
			<p class="source-code">$ podman generate kube nginx</p>
			<p class="source-code"># Save the output of this file and use kubectl create -f to import</p>
			<p class="source-code"># it into Kubernetes.</p>
			<p class="source-code">#</p>
			<p class="source-code"># Created with podman-4.0.0-rc4</p>
			<p class="source-code">apiVersion: v1</p>
			<p class="source-code">kind: Pod</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  creationTimestamp: "2022-02-10T23:14:25Z"</p>
			<p class="source-code">  labels:</p>
			<p class="source-code">    app: nginxpod</p>
			<p class="source-code">  name: nginx_pod</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  containers:</p>
			<p class="source-code">  - args:</p>
			<p class="source-code">    - nginx</p>
			<p class="source-code">    - -g</p>
			<p class="source-code">    - daemon off;</p>
			<p class="source-code">    image: docker.io/library/nginx:latest</p>
			<p class="source-code">    name: nginx</p>
			<p class="source-code">    ports:</p>
			<p class="source-code">    - containerPort: 80</p>
			<p class="source-code">      hostPort: 8080</p>
			<p class="source-code">    securityContext:</p>
			<p class="source-code">      capabilities:</p>
			<p class="source-code">        drop:</p>
			<p class="source-code">        - CAP_MKNOD</p>
			<p class="source-code">        - CAP_NET_RAW</p>
			<p class="source-code">        - CAP_AUDIT_WRITE</p>
			<p>Let's describe the generated output. Every<a id="_idIndexMarker1286"/> new Kubernetes resource is always composed of at least four fields: </p>
			<ul>
				<li><strong class="source-inline">apiVersion</strong>: This field describes the API version schema of the resource. The <strong class="source-inline">Pod</strong> object belongs to<a id="_idIndexMarker1287"/> the <strong class="source-inline">v1</strong> version of the <strong class="source-inline">core</strong> APIs of Kubernetes.</li>
				<li><strong class="source-inline">kind</strong>: This field defines the resource kind, which is <strong class="source-inline">Pod</strong> in our example.</li>
				<li><strong class="source-inline">metadata</strong>: This field is an object that holds a set of resource metadata that usually includes <strong class="source-inline">name</strong>, <strong class="source-inline">namespace</strong>, <strong class="source-inline">labels</strong>, and <strong class="source-inline">annotations</strong>, along with additional dynamic metadata that's created at runtime, such as <strong class="source-inline">creationTimestamp</strong>, <strong class="source-inline">resourceVersion</strong>, or the resource's <strong class="source-inline">uid</strong>.</li>
				<li><strong class="source-inline">spec</strong>: This field holds resource specifications and varies among different resources. For example, a <strong class="source-inline">Pod</strong> resource will contain a list of <strong class="source-inline">containers</strong>, along with their startup arguments, volumes, ports, or security contexts. </li>
			</ul>
			<p>All the information that's embedded inside a Pod resource is enough to start the pod inside a Kubernetes cluster. Along with the fields described previously, a fifth <strong class="source-inline">status</strong> field is dynamically created when the pod is running to describe its execution status.</p>
			<p>From the generated output, we can notice an <strong class="source-inline">args</strong> list for every container, along with their startup commands, arguments, and options.</p>
			<p>When you're generating a Pod from a<a id="_idIndexMarker1288"/> container with mapped ports, the following <strong class="source-inline">ports</strong> list is created inside the Pod resource:</p>
			<p class="source-code">ports:</p>
			<p class="source-code">    - containerPort: 80</p>
			<p class="source-code">      hostPort: 8080</p>
			<p>This means that port <strong class="source-inline">80</strong> must be<a id="_idIndexMarker1289"/> exposed to the container and port <strong class="source-inline">8080</strong> must be exposed on the host running it. This information will be used by Podman when we create containers and pods with the <strong class="source-inline">podman play kube</strong> command, as we will see in the next section.</p>
			<p>The <strong class="source-inline">securityContext</strong> object defines capabilities that must be dropped for this container. This means that the <strong class="source-inline">CAP_MKNOD</strong>, <strong class="source-inline">CAP_NET_RAW</strong>, and <strong class="source-inline">CAP_AUDIT_WRITE</strong> capabilities won't be enabled on a pod that's created from this configuration.</p>
			<p>We can apply the output of the <strong class="source-inline">podman generate kube</strong> command directly to a Kubernetes cluster or save it to a file. To save it to a file, we can use the <strong class="source-inline">-f</strong> option:</p>
			<p class="source-code">$ podman generate kube nginx –f nginx-pod.yaml</p>
			<p>To apply the generated output to a running Kubernetes cluster, we can use the Kubernetes CLI tool, <strong class="source-inline">kubectl</strong>. The <strong class="source-inline">kubectl create</strong> command applies a resource object inside the cluster:</p>
			<p class="source-code">$ podman generate kube nginx | kubectl create -f -</p>
			<p>The basic Pod generation command can be enriched by creating the related Kubernetes services, as described in the next subsection.</p>
			<h2 id="_idParaDest-264"><a id="_idTextAnchor264"/>Generating Pods and services from running containers</h2>
			<p>Pods running inside a Kubernetes<a id="_idIndexMarker1290"/> cluster obtain <a id="_idIndexMarker1291"/>unique IP addresses on a software-defined network that's managed by the default CNI plugin. </p>
			<p>These IPs are not routed externally – we can only reach the Pod's IP address from within the cluster. However, we <a id="_idIndexMarker1292"/>need a layer to balance multiple replicas of the same pods and provide a DNS resolution for a single abstraction frontend. In other words, our<a id="_idIndexMarker1293"/> application must be able to query for a given service name and receive a unique IP address that abstracts from the <a id="_idIndexMarker1294"/>pods' IPs, regardless of the number of replicas.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Native, cluster-scoped DNS name resolution in Kubernetes is implemented with the <strong class="bold">CoreDNS</strong> service, which is started when the <a id="_idIndexMarker1295"/>cluster's control plane is bootstrapped. CoreDNS is delegated to resolve internal requests and to forward ones for external names to authoritative DNS servers outside the cluster.</p>
			<p>The resource that describes the<a id="_idIndexMarker1296"/> abstraction in one or more pods in Kubernetes is called <strong class="source-inline">Service</strong>. </p>
			<p>For example, we can have three replicas of the Nginx pod running inside our cluster and expose them with a unique IP. It belongs to a <strong class="source-inline">ClusterIP</strong> type, and its allocation is dynamic when the service is created. <strong class="source-inline">ClusterIP</strong> services are the default in Kubernetes and their assigned IPs are only local to the cluster.</p>
			<p>We can also create <strong class="source-inline">NodePort</strong> type services <a id="_idIndexMarker1297"/>that use <strong class="bold">Network Address Translation</strong> (<strong class="bold">NAT</strong>) so that the service can be reached from the external world. We can do this by mapping the service VIP and port to a local port on the cluster worker nodes.</p>
			<p>If we have a cluster running on an infrastructure that allows dynamic load balancing (such as a public cloud provider), we can create <strong class="source-inline">LoadBalancer</strong> type services and have the provider manage ingress traffic load balancing for us.</p>
			<p>Podman allows you to create services along with pods by adding the <strong class="source-inline">-s</strong> option to the <strong class="source-inline">podman generate kube</strong> command. This allows them to be potentially reused inside a Kubernetes cluster. The following example is a variation of the previous one and generates the Service resource<a id="_idIndexMarker1298"/> along with the previously<a id="_idIndexMarker1299"/> described Pod:</p>
			<p class="source-code">$ podman generate kube -s nginx</p>
			<p class="source-code"># Save the output of this file and use kubectl create -f to import</p>
			<p class="source-code"># it into Kubernetes.</p>
			<p class="source-code">#</p>
			<p class="source-code"># Created with podman-4.0.0-rc4</p>
			<p class="source-code">apiVersion: v1</p>
			<p class="source-code">kind: Service</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  creationTimestamp: "2022-02-12T21:54:02Z"</p>
			<p class="source-code">  labels:</p>
			<p class="source-code">    app: nginxpod</p>
			<p class="source-code">  name: nginx_pod</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  ports:</p>
			<p class="source-code">  - name: "80"</p>
			<p class="source-code">    nodePort: 30582</p>
			<p class="source-code">    port: 80</p>
			<p class="source-code">    targetPort: 80</p>
			<p class="source-code">  selector:</p>
			<p class="source-code">    app: nginxpod</p>
			<p class="source-code">  type: NodePort</p>
			<p class="source-code">---</p>
			<p class="source-code">apiVersion: v1</p>
			<p class="source-code">kind: Pod</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  creationTimestamp: "2022-02-12T21:54:02Z"</p>
			<p class="source-code">  labels:</p>
			<p class="source-code">    app: nginxpod</p>
			<p class="source-code">  name: nginx_pod</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  containers:</p>
			<p class="source-code">  - args:</p>
			<p class="source-code">    - nginx</p>
			<p class="source-code">    - -g</p>
			<p class="source-code">    - daemon off;</p>
			<p class="source-code">    image: docker.io/library/nginx:latest</p>
			<p class="source-code">    name: nginx</p>
			<p class="source-code">    ports:</p>
			<p class="source-code">    - containerPort: 80</p>
			<p class="source-code">      hostPort: 8080</p>
			<p class="source-code">    securityContext:</p>
			<p class="source-code">      capabilities:</p>
			<p class="source-code">        drop:</p>
			<p class="source-code">        - CAP_MKNOD</p>
			<p class="source-code">        - CAP_NET_RAW</p>
			<p class="source-code">        - CAP_AUDIT_WRITE</p>
			<p>The<a id="_idIndexMarker1300"/> generated output <a id="_idIndexMarker1301"/>contains, along <a id="_idIndexMarker1302"/>with the Pod resource, a Service resource<a id="_idIndexMarker1303"/> that exposes the Nginx pod using a selector field. The selector matches all the pods with the <strong class="source-inline">app: nginxpod</strong> label.</p>
			<p>When the service is created inside a Kubernetes cluster, an internal, non-routed VIP is allocated for the service. Since this is a <strong class="source-inline">NodePort</strong> type service, a <strong class="bold">destination NAT</strong> (<strong class="bold">DNAT</strong>) rule is created to match incoming traffic on all the cluster nodes on port <strong class="source-inline">30582</strong> and forward it to the service IP.</p>
			<p>By default, Podman generates <strong class="source-inline">NodePort</strong> type services. Whenever a container or pod is decorated with a port mapping, Podman populates the <strong class="source-inline">ports</strong> object with a list of ports and their related <strong class="source-inline">nodePort</strong> mappings inside the manifest.</p>
			<p>In our use case, we created the Nginx <a id="_idIndexMarker1304"/>container by mapping its port, <strong class="source-inline">80</strong>, to port <strong class="source-inline">8080</strong> on the host. Here, Podman generated a Service that maps the container's port, <strong class="source-inline">80</strong>, to port <strong class="source-inline">30582</strong> on the cluster nodes. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The <strong class="source-inline">nodePort</strong> mapping is applied to Kubernetes cluster nodes only, not to standalone hosts running Podman.</p>
			<p>The value of creating Kubernetes <a id="_idIndexMarker1305"/>services and pods from Podman is the<a id="_idIndexMarker1306"/> ability to port to a Kubernetes platform. </p>
			<p>In many cases, we work with composite, multi-tier applications that need to be exported and recreated together. Podman allows us to export multiple containers into a single Kubernetes Pod object <a id="_idIndexMarker1307"/>or to create and export multiple pods to gain more control over our application. In the next two subsections, we will see both cases applied to a WordPress application and try to find out what the best approach is. </p>
			<h2 id="_idParaDest-265"><a id="_idTextAnchor265"/>Generating a composite application in a single Pod</h2>
			<p>In this first scenario, we <a id="_idIndexMarker1308"/>will implement a multi-tier application in a single pod. The advantage of this approach is that we can leverage the pod as a single unit that will execute multiple containers and that resource sharing across them is simplified. </p>
			<p>We will launch two containers – one for MySQL and one for WordPress – and export them as a single Pod resource. We will learn how to work around some minor adjustments to make it work seamlessly later during run tests.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The following examples have been created in a rootless context but can be seamlessly applied to rootfull containers too.</p>
			<p class="callout">A set of scripts that will be useful for launching the stacks and the generated Kubernetes YAML files are available in this book's GitHub repository at <a href="https://github.com/PacktPublishing/Podman-for-DevOps/tree/main/Chapter14/kube">https://github.com/PacktPublishing/Podman-for-DevOps/tree/main/Chapter14/kube</a>.</p>
			<p>First, we must create<a id="_idIndexMarker1309"/> two volumes that will be used later by the WordPress and MySQL containers:</p>
			<p class="source-code">$ for vol in dbvol wpvol; do podman volume create $vol; done</p>
			<p>Then, we must create an empty pod named <strong class="source-inline">wordpress-pod</strong> with the necessary pre-defined port mappings:</p>
			<p class="source-code">$ podman pod create --name wordpress-pod -p 8080:80</p>
			<p>Now, we can populate our pod by creating the WordPress and MySQL containers. Let's begin with the MySQL container:</p>
			<p class="source-code">$ podman create \</p>
			<p class="source-code">  --pod wordpress-pod --name db \</p>
			<p class="source-code">  -v dbvol:/var/lib/mysql</p>
			<p class="source-code">  -e MYSQL_ROOT_PASSWORD=myrootpasswd \</p>
			<p class="source-code">  -e MYSQL_DATABASE=wordpress \</p>
			<p class="source-code">  -e MYSQL_USER=wordpress \</p>
			<p class="source-code">  -e MYSQL_PASSWORD=wordpress \</p>
			<p class="source-code">  docker.io/library/mysql</p>
			<p>Now, we can create the WordPress container:</p>
			<p class="source-code">$ podman create \</p>
			<p class="source-code">  --pod wordpress-pod --name wordpress \</p>
			<p class="source-code">  -v wpvol:/var/www/html</p>
			<p class="source-code">  -e WORDPRESS_DB_HOST=127.0.0.1 \</p>
			<p class="source-code">  -e WORDPRESS_DB_USER=wordpress \</p>
			<p class="source-code">  -e WORDPRESS_DB_PASSWORD=wordpress \</p>
			<p class="source-code">  -e WORDPRESS_DB_NAME=wordpress \</p>
			<p class="source-code">  docker.io/library/wordpress</p>
			<p>Here, we can see that the <strong class="source-inline">WORDPRESS_DB_HOST</strong> variable has been set to <strong class="source-inline">127.0.0.1</strong> (the address of the loopback device) since the two containers are going to run in the same pod and share the same network namespace. For this reason, we let the WordPress container know that the MySQL service is listening on the same loopback device.</p>
			<p>Finally, we can start the <a id="_idIndexMarker1310"/>pod with the <strong class="source-inline">podman pod start</strong> command:</p>
			<p class="source-code">$ podman pod start wordpress-pod</p>
			<p>We can inspect the running containers with <strong class="source-inline">podman ps</strong>: </p>
			<p class="source-code">$ podman ps</p>
			<p class="source-code">CONTAINER ID  IMAGE                                        COMMAND               CREATED            STATUS                PORTS                 NAMES</p>
			<p class="source-code">19bf706f0eb8  localhost/podman-pause:4.0.0-rc4-1643988335                        About an hour ago  Up About an hour ago  0.0.0.0:8080-&gt;80/tcp  0400f8770627-infra</p>
			<p class="source-code">f1da755a846c  docker.io/library/mysql:latest               mysqld                About an hour ago  Up About an hour ago  0.0.0.0:8080-&gt;80/tcp  db</p>
			<p class="source-code">1f28ef82d58f  docker.io/library/wordpress:latest           apache2-foregroun...  About an hour ago  Up About an hour ago  0.0.0.0:8080-&gt;80/tcp  wordpress</p>
			<p>Now, we can point our browser to <strong class="source-inline">http://localhost:8080</strong> and confirm the appearance of the WordPress setup dialog screen:</p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B17908_14_03.jpg" alt="Figure 14.3 – WordPress setup dialog screen&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.3 – WordPress setup dialog screen</p>
			<p class="callout-heading">Important Note </p>
			<p class="callout">The pod also started a third <strong class="bold">infra</strong> container. It is<a id="_idIndexMarker1311"/> based on a minimal <strong class="source-inline">podman-pause</strong> image that initializes the pod's network and the IPC namespaces of our example. The image is built directly in the background on the host the first time a pod is created and executes a <strong class="source-inline">catatonit</strong> process, an <strong class="source-inline">init</strong> micro container written in C that's designed to handle system signals and zombie process reaping. </p>
			<p class="callout">This behavior of the pod's infra image is directly inherited from Kubernetes's design.</p>
			<p>Now, we are ready to<a id="_idIndexMarker1312"/> generate our Pod YAML manifest with the <strong class="source-inline">podman generate kube</strong> command and save it to a file for reuse:</p>
			<p class="source-code">$ podman generate kube wordpress-pod \</p>
			<p class="source-code">  -f wordpress-single-pod.yaml</p>
			<p>The preceding command<a id="_idIndexMarker1313"/> generates a file with the following content:</p>
			<p class="source-code"># Save the output of this file and use kubectl create -f to import</p>
			<p class="source-code"># it into Kubernetes.</p>
			<p class="source-code">#</p>
			<p class="source-code"># Created with podman-4.0.0-rc4</p>
			<p class="source-code">apiVersion: v1</p>
			<p class="source-code">kind: Pod</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  creationTimestamp: "2022-02-13T11:06:38Z"</p>
			<p class="source-code">  labels:</p>
			<p class="source-code">    app: wordpress-pod</p>
			<p class="source-code">  name: wordpress-pod</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  containers:</p>
			<p class="source-code">  - args:</p>
			<p class="source-code">    - mysqld</p>
			<p class="source-code">    env:</p>
			<p class="source-code">    - name: MYSQL_PASSWORD</p>
			<p class="source-code">      value: wordpress</p>
			<p class="source-code">    - name: MYSQL_USER</p>
			<p class="source-code">      value: wordpress</p>
			<p class="source-code">    - name: MYSQL_ROOT_PASSWORD</p>
			<p class="source-code">      value: myrootpasswd</p>
			<p class="source-code">    - name: MYSQL_DATABASE</p>
			<p class="source-code">      value: wordpress</p>
			<p class="source-code">    image: docker.io/library/mysql:latest</p>
			<p class="source-code">    name: db</p>
			<p class="source-code">    ports:</p>
			<p class="source-code">    - containerPort: 80</p>
			<p class="source-code">      hostPort: 8080</p>
			<p class="source-code">    resources: {}</p>
			<p class="source-code">    securityContext:</p>
			<p class="source-code">      capabilities:</p>
			<p class="source-code">        drop:</p>
			<p class="source-code">        - CAP_MKNOD</p>
			<p class="source-code">        - CAP_NET_RAW</p>
			<p class="source-code">        - CAP_AUDIT_WRITE</p>
			<p class="source-code">    volumeMounts:</p>
			<p class="source-code">    - mountPath: /var/lib/mysql</p>
			<p class="source-code">      name: dbvol-pvc</p>
			<p class="source-code">  - args:</p>
			<p class="source-code">    - apache2-foreground</p>
			<p class="source-code">    env:</p>
			<p class="source-code">    - name: WORDPRESS_DB_HOST</p>
			<p class="source-code">      value: 127.0.0.1</p>
			<p class="source-code">    - name: WORDPRESS_DB_PASSWORD</p>
			<p class="source-code">      value: wordpress</p>
			<p class="source-code">    - name: WORDPRESS_DB_USER</p>
			<p class="source-code">      value: wordpress</p>
			<p class="source-code">    - name: WORDPRESS_DB_NAME</p>
			<p class="source-code">      value: wordpress</p>
			<p class="source-code">    image: docker.io/library/wordpress:latest</p>
			<p class="source-code">    name: wordpress</p>
			<p class="source-code">    resources: {}</p>
			<p class="source-code">    securityContext:</p>
			<p class="source-code">      capabilities:</p>
			<p class="source-code">        drop:</p>
			<p class="source-code">        - CAP_MKNOD</p>
			<p class="source-code">        - CAP_NET_RAW</p>
			<p class="source-code">        - CAP_AUDIT_WRITE</p>
			<p class="source-code">    volumeMounts:</p>
			<p class="source-code">    - mountPath: /var/www/html</p>
			<p class="source-code">      name: wpvol-pvc</p>
			<p class="source-code">  restartPolicy: Never</p>
			<p class="source-code">  volumes:</p>
			<p class="source-code">  - name: wpvol-pvc</p>
			<p class="source-code">    persistentVolumeClaim:</p>
			<p class="source-code">      claimName: wpvol</p>
			<p class="source-code">  - name: dbvol-pvc</p>
			<p class="source-code">    persistentVolumeClaim:</p>
			<p class="source-code">      claimName: dbvol</p>
			<p class="source-code">status: {}</p>
			<p>Our YAML file holds a single Pod resource with two containers inside. Note that the previously defined environment variables have been created correctly inside our containers (when using Podman v4.0.0 or later).</p>
			<p>Also, notice that the two container volumes have been mapped to <strong class="source-inline">PersistentVolumeClaim</strong> objects, often referred to as <strong class="source-inline">PVC</strong> objects. </p>
			<p>PVCs are Kubernetes<a id="_idIndexMarker1314"/> resources that are used to request (in other words, claim) a storage volume resource that satisfies a specific capacity and consumption modes. The attached storage volume resource is called a <strong class="source-inline">PersistentVolume</strong> (<strong class="source-inline">PV</strong>) and can be created manually or automatically by a <strong class="source-inline">StorageClass</strong> resource that leverages a<a id="_idIndexMarker1315"/> storage driver that's compliant with the <strong class="bold">Container Storage Interface</strong> (<strong class="bold">CSI</strong>).</p>
			<p>When we create a PVC, <strong class="source-inline">StorageClass</strong> provisions a <strong class="source-inline">PersistentVolume</strong> that satisfied our storage requests, and the two resources are bound together. This approach decouples the storage request from storage provisioning and makes storage consumption in Kubernetes more portable.</p>
			<p>When Podman generates Kubernetes YAML files, PVC resources are not exported by default. However, we can also export the PVC resources to recreate them in Kubernetes with the <strong class="source-inline">podman generate kube &lt;VOLUME_NAME&gt;</strong> command.</p>
			<p>The following command exports the WordPress application, along with its volume definitions, as a PVC:</p>
			<p class="source-code">$ podman generate kube wordpress-pod wpvol dbvol </p>
			<p>The following is an example of the <strong class="source-inline">dbvol</strong> volume translated into a <strong class="source-inline">PersistentVolumeClaim</strong>:</p>
			<p class="source-code">apiVersion: v1</p>
			<p class="source-code">kind: PersistentVolumeClaim</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  annotations:</p>
			<p class="source-code">    volume.podman.io/driver: local</p>
			<p class="source-code">  creationTimestamp: "2022-02-13T14:51:05Z"</p>
			<p class="source-code">  name: dbvol</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  accessModes:</p>
			<p class="source-code">  - ReadWriteOnce</p>
			<p class="source-code">  resources:</p>
			<p class="source-code">    requests:</p>
			<p class="source-code">      storage: 1Gi</p>
			<p class="source-code">status: {}</p>
			<p>This approach has the advantage <a id="_idIndexMarker1316"/>of providing the necessary PVC definitions to recreate the whole application in a Kubernetes cluster, but it is not necessary to recreate the volume resources in Podman: if they're not available, an empty volume with the same name will be created automatically.</p>
			<p>To recreate all the resource dependencies in a Kubernetes cluster, we can also export the application's <strong class="source-inline">Service</strong> resource.</p>
			<p>The following command exports everything in our WordPress example, including pods, services, and volumes:</p>
			<p class="source-code">$ podman generate kube -s wordpress-pod wpvol dbvol</p>
			<p>Before we move on, let's briefly dig into the single pod approach logic that was described in this subsection and look at its advantages and possible limitations.</p>
			<p>One great advantage of executing all the containers in a single pod is the simpler networking configuration – one network namespace is shared by all the running containers. This also means we don't have to create a dedicated Podman network to let the containers communicate with each other.</p>
			<p>On the other hand, this approach does not reflect the common Kubernetes pattern of executing pods. In Kubernetes, we would prefer to split the WordPress pod and the MySQL pod to manage them independently and have different services associated with them. More separation implies more control and the chance to update independently.</p>
			<p>In the next subsection, you'll learn how to replicate this approach and generate multiple pods for every application tier.</p>
			<h2 id="_idParaDest-266"><a id="_idTextAnchor266"/>Generating composite applications with multiple Pods</h2>
			<p>One of the features of Docker Compose is that you<a id="_idIndexMarker1317"/> can create different independent containers that communicate with each other using a service abstraction concept that is decoupled from the container's execution.</p>
			<p>The Podman community (and many of its users) believe that a standardization toward Kubernetes YAML manifests to describe complex workloads is useful to get closer to the mainstream orchestration solution. </p>
			<p>For this reason, the approach we'll describe in this section can become a full replacement for Docker Compose while providing Kubernetes portability at the same time. First, we will learn how to prepare an environment that can be used to generate the YAML manifests. After that, we can get rid of the workloads and only use the Kubernetes YAML to run our workloads.</p>
			<p>The following example can be executed with rootless containers and networks.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Before continuing, make sure that the previous example pod and containers have been completely removed, along with their volumes, to prevent any issues with port assignment or WordPress content initialization. Please refer to the commands in this book's GitHub repository as a reference: <a href="https://github.com/PacktPublishing/Podman-for-DevOps/tree/main/AdditionalMaterial">https://github.com/PacktPublishing/Podman-for-DevOps/tree/main/AdditionalMaterial</a>.</p>
			<p>First, we need to create a network. We have chosen the name <strong class="source-inline">kubenet</strong> to identify it easily and leave it with the default configuration for the sake of our example:</p>
			<p class="source-code">$ podman network create kubenet</p>
			<p>Once the network has been created, the two <strong class="source-inline">dbvol</strong> and <strong class="source-inline">wpvol</strong> volumes must be created:</p>
			<p class="source-code">$ for vol in wpvol dbvol; do podman volume create $vol; done</p>
			<p>We want to generate two distinct pods – one for each container. First, we must create the MySQL pod and its related container:</p>
			<p class="source-code">$ podman pod create –p 3306:3306 \</p>
			<p class="source-code">  --network kubenet \</p>
			<p class="source-code">  --name mysql-pod </p>
			<p class="source-code">$ podman create --name db \</p>
			<p class="source-code">  --pod mysql-pod \</p>
			<p class="source-code">  -v dbvol:/var/lib/mysql \</p>
			<p class="source-code">  -e MYSQL_ROOT_PASSWORD=myrootpasswd\</p>
			<p class="source-code">  -e MYSQL_DATABASE=wordpress \</p>
			<p class="source-code">  -e MYSQL_USER=wordpress \</p>
			<p class="source-code">  -e MYSQL_PASSWORD=wordpress \</p>
			<p class="source-code">  docker.io/library/mysql</p>
			<p>Notice the port mapping, which we can use to access the MySQL service from a client and create the correct port mapping<a id="_idIndexMarker1318"/> later in the Kubernetes service.</p>
			<p>Now, let's create the WordPress pod and container:</p>
			<p class="source-code">$ podman pod create -p 8080:80 \</p>
			<p class="source-code">  --network kubenet \</p>
			<p class="source-code">  --name wordpress-pod</p>
			<p class="source-code">$ podman create --name wordpress \</p>
			<p class="source-code">  --pod wordpress-pod \</p>
			<p class="source-code">  -v wpvol:/var/www/html \</p>
			<p class="source-code">  -e WORDPRESS_DB_HOST=<strong class="bold">mysql-pod</strong> \</p>
			<p class="source-code">  -e WORDPRESS_DB_USER=wordpress \</p>
			<p class="source-code">  -e WORDPRESS_DB_PASSWORD=wordpress \</p>
			<p class="source-code">  -e WORDPRESS_DB_NAME=wordpress \</p>
			<p class="source-code">  docker.io/library/wordpress</p>
			<p>There is a very important variable in the preceding command that can be considered the key to this approach: <strong class="source-inline">WORDPRESS_DB_HOST</strong> is populated with the <strong class="source-inline">mysql-pod</strong> string, which is the name that's been given to the MySQL pod. </p>
			<p>In Podman, the pod's name will act as the service name of the application and the DNS daemon associated with the network (<strong class="source-inline">dnsmasq</strong> in Podman 3 or <strong class="source-inline">aardvark-dns</strong> in Podman 4) will directly resolve the pod name to the associated IP address. This is a key feature that makes multi-pod applications a perfect replacement for Compose stacks.</p>
			<p>Now, we can start the two pods and have all the containers up and running:</p>
			<p class="source-code">$ podman pod start mysql-pod &amp;&amp; </p>
			<p class="source-code">  podman pod start wordpress-pod</p>
			<p>Once again, pointing our browsers to <strong class="source-inline">http://localhost:8080</strong> should lead us to the WordPress first setup page (if everything was set up correctly).</p>
			<p>Now, we are ready to <a id="_idIndexMarker1319"/>export our Kubernetes YAML manifest. We can choose to simply export the two Pod resources or create a full export that also includes services and volumes. This is useful if you need to import to a Kubernetes cluster. </p>
			<p>Let's start with the basic version:</p>
			<p class="source-code">$ podman generate kube \</p>
			<p class="source-code">  -f wordpress-multi-pod-basic.yaml \</p>
			<p class="source-code">  wordpress-pod \</p>
			<p class="source-code">  mysql-pod</p>
			<p>The output of the preceding code will contain nothing but the two Pod resources:</p>
			<p class="source-code"># Save the output of this file and use kubectl create -f to import</p>
			<p class="source-code"># it into Kubernetes.</p>
			<p class="source-code">#</p>
			<p class="source-code"># Created with podman-4.0.0-rc4</p>
			<p class="source-code">apiVersion: v1</p>
			<p class="source-code">kind: Pod</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  creationTimestamp: "2022-02-13T21:32:48Z"</p>
			<p class="source-code">  labels:</p>
			<p class="source-code">    app: wordpress-pod</p>
			<p class="source-code">  name: wordpress-pod</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  containers:</p>
			<p class="source-code">  - args:</p>
			<p class="source-code">    - apache2-foreground</p>
			<p class="source-code">    env:</p>
			<p class="source-code">    - name: WORDPRESS_DB_NAME</p>
			<p class="source-code">      value: wordpress</p>
			<p class="source-code">    - name: WORDPRESS_DB_HOST</p>
			<p class="source-code">      value: mysql-pod</p>
			<p class="source-code">    - name: WORDPRESS_DB_PASSWORD</p>
			<p class="source-code">      value: wordpress</p>
			<p class="source-code">    - name: WORDPRESS_DB_USER</p>
			<p class="source-code">      value: wordpress</p>
			<p class="source-code">    image: docker.io/library/wordpress:latest</p>
			<p class="source-code">    name: wordpress</p>
			<p class="source-code">    ports:</p>
			<p class="source-code">    - containerPort: 80</p>
			<p class="source-code">      hostPort: 8080</p>
			<p class="source-code">    resources: {}</p>
			<p class="source-code">    securityContext:</p>
			<p class="source-code">      capabilities:</p>
			<p class="source-code">        drop:</p>
			<p class="source-code">        - CAP_MKNOD</p>
			<p class="source-code">        - CAP_NET_RAW</p>
			<p class="source-code">        - CAP_AUDIT_WRITE</p>
			<p class="source-code">    volumeMounts:</p>
			<p class="source-code">    - mountPath: /var/www/html</p>
			<p class="source-code">      name: wpvol-pvc</p>
			<p class="source-code">  restartPolicy: Never</p>
			<p class="source-code">  volumes:</p>
			<p class="source-code">  - name: wpvol-pvc</p>
			<p class="source-code">    persistentVolumeClaim:</p>
			<p class="source-code">      claimName: wpvol</p>
			<p class="source-code">status: {}</p>
			<p class="source-code">---</p>
			<p class="source-code">apiVersion: v1</p>
			<p class="source-code">kind: Pod</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  creationTimestamp: "2022-02-13T21:32:48Z"</p>
			<p class="source-code">  labels:</p>
			<p class="source-code">    app: mysql-pod</p>
			<p class="source-code">  name: mysql-pod</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  containers:</p>
			<p class="source-code">  - args:</p>
			<p class="source-code">    - mysqld</p>
			<p class="source-code">    env:</p>
			<p class="source-code">    - name: MYSQL_ROOT_PASSWORD</p>
			<p class="source-code">      value: myrootpasswd</p>
			<p class="source-code">    - name: MYSQL_DATABASE</p>
			<p class="source-code">      value: wordpress</p>
			<p class="source-code">    - name: MYSQL_USER</p>
			<p class="source-code">      value: wordpress</p>
			<p class="source-code">    - name: MYSQL_PASSWORD</p>
			<p class="source-code">      value: wordpress</p>
			<p class="source-code">    image: docker.io/library/mysql:latest</p>
			<p class="source-code">    name: db</p>
			<p class="source-code">    ports:</p>
			<p class="source-code">    - containerPort: 3306</p>
			<p class="source-code">      hostPort: 3306</p>
			<p class="source-code">    resources: {}</p>
			<p class="source-code">    securityContext:</p>
			<p class="source-code">      capabilities:</p>
			<p class="source-code">        drop:</p>
			<p class="source-code">        - CAP_MKNOD</p>
			<p class="source-code">        - CAP_NET_RAW</p>
			<p class="source-code">        - CAP_AUDIT_WRITE</p>
			<p class="source-code">    volumeMounts:</p>
			<p class="source-code">    - mountPath: /var/lib/mysql</p>
			<p class="source-code">      name: dbvol-pvc</p>
			<p class="source-code">  restartPolicy: Never</p>
			<p class="source-code">  volumes:</p>
			<p class="source-code">  - name: dbvol-pvc</p>
			<p class="source-code">    persistentVolumeClaim:</p>
			<p class="source-code">      claimName: dbvol</p>
			<p class="source-code">status: {}</p>
			<p>The resulting file is also available in this book's GitHub repository:</p>
			<p><a href="https://github.com/PacktPublishing/Podman-for-DevOps/blob/main/Chapter14/kube/wordpress-multi-pod-basic.yaml">https://github.com/PacktPublishing/Podman-for-DevOps/blob/main/Chapter14/kube/wordpress-multi-pod-basic.yaml</a></p>
			<p>As we will see in the <a id="_idIndexMarker1320"/>next section, this YAML file is enough to recreate a fully working WordPress application on Podman from scratch. We can persist and version it on a source control repository such as Git for future reuse.</p>
			<p>The following code exports the two <strong class="source-inline">Pod</strong> resources, along with the <strong class="source-inline">PersistentVolumeClaim</strong> and <strong class="source-inline">Service</strong> resources:</p>
			<p class="source-code">$ podman generate kube -s \</p>
			<p class="source-code">  -f wordpress-multi-pod-full.yaml \</p>
			<p class="source-code">  wordpress-pod \</p>
			<p class="source-code">  mysql-pod \</p>
			<p class="source-code">  dbvol \</p>
			<p class="source-code">  wpvol</p>
			<p>The output of this command is also available in this book's GitHub repository: </p>
			<p><a href="https://github.com/PacktPublishing/Podman-for-DevOps/blob/main/Chapter14/kube/wordpress-multi-pod-full.yaml">https://github.com/PacktPublishing/Podman-for-DevOps/blob/main/Chapter14/kube/wordpress-multi-pod-full.yaml</a></p>
			<p>This full manifest is useful for importing and testing our application on a Kubernetes cluster, where the <strong class="source-inline">Service</strong> and <strong class="source-inline">PersistentVolumeClaim</strong> resources are necessary.</p>
			<p>Now, we are ready to test our <a id="_idIndexMarker1321"/>generated resources in Podman and learn how to reproduce full stack deployments with simple operations.</p>
			<h1 id="_idParaDest-267"><a id="_idTextAnchor267"/>Running Kubernetes resource files in Podman</h1>
			<p>Now that we've learned how to<a id="_idIndexMarker1322"/> generate Kubernetes YAML files containing the necessary resources to deploy our applications, we want to test<a id="_idIndexMarker1323"/> them in a real scenario.</p>
			<p>For this book, we will use the WordPress application again, both in its simple form with a single container and in its multi-pod variation. </p>
			<p>The following examples are also available in this book's GitHub repository – you can choose to use the resources that have been generated from your labs or use the prepared manifests in this book's repository.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Don't forget to clean up all the previous workloads before testing the creation of Kubernetes resources with Podman.</p>
			<p>For all our examples, we will use the <strong class="source-inline">podman play kube</strong> command. It offers us an easy and intuitive interface for managing the execution of complex stacks with a good degree of customization.</p>
			<p>The first example will be based on the single-pod manifest: </p>
			<p class="source-code">$ podman play kube wordpress-single-pod.yaml</p>
			<p>The preceding command creates a pod called <strong class="source-inline">wordpress-pod</strong> that's composed of the two containers, along with the necessary volumes. Let's inspect the results and see what happened:</p>
			<p class="source-code">$ podman pod ps</p>
			<p class="source-code">POD ID        NAME           STATUS      CREATED         INFRA ID      # OF CONTAINERS</p>
			<p class="source-code">5f8ecfe66acd  wordpress-pod  Running     4 minutes ago  46b4bdfe6a08  3</p>
			<p>We can also check the running containers. Here, we expect to see the two WordPress and MySQL containers and the third infra-related <strong class="source-inline">podman-pause</strong>:</p>
			<p class="source-code">$ podman ps</p>
			<p class="source-code">CONTAINER ID  IMAGE                                        COMMAND               CREATED         STATUS             PORTS                 NAMES</p>
			<p class="source-code">46b4bdfe6a08  localhost/podman-pause:4.0.0-rc4-1643988335                        4 minutes ago  Up 4 minutes ago  0.0.0.0:8080-&gt;80/tcp  5f8ecfe66acd-infra</p>
			<p class="source-code">ef88a5c8d1e5  docker.io/library/mysql:latest               mysqld                4 minutes ago  Up 4 minutes ago  0.0.0.0:8080-&gt;80/tcp  wordpress-pod-db</p>
			<p class="source-code">76c6b6328653  docker.io/library/wordpress:latest           apache2-foregroun...  4 minutes ago  Up 4 minutes ago  0.0.0.0:8080-&gt;80/tcp  wordpress-pod-wordpress</p>
			<p>Finally, we can verify<a id="_idIndexMarker1324"/> if the <strong class="source-inline">dbvol</strong> and <strong class="source-inline">wpvol</strong> volumes <a id="_idIndexMarker1325"/>have been created:</p>
			<p class="source-code">$ podman volume ls</p>
			<p class="source-code">DRIVER      VOLUME NAME</p>
			<p class="source-code">local       dbvol</p>
			<p class="source-code">local       wpvol</p>
			<p>Before we look at the more articulated (and interesting) example with the multi-pod manifest, we must clean up the environment. We can do this manually or by using the <strong class="source-inline">--down</strong> option of the <strong class="source-inline">podman play kube</strong> command, which immediately stops and removes the running pods:</p>
			<p class="source-code">$ podman play kube --down wordpress-single-pod.yaml </p>
			<p class="source-code">Pods stopped:</p>
			<p class="source-code">5f8ecfe66acd01b705f38cd175fad222890ab612bf572807082f30ab37fd 0b88</p>
			<p class="source-code">Pods removed:</p>
			<p class="source-code">5f8ecfe66acd01b705f38cd175fad222890ab612bf572807082f30ab37fd 0b88</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Volumes are not removed by default since it can be useful to keep them if containers have already written data on them. To remove unused volumes, use the <strong class="source-inline">podman volume prune</strong> command.</p>
			<p>Now, let's run the multi-pod <a id="_idIndexMarker1326"/>example using the basic exported manifest:</p>
			<p class="source-code">$ podman play kube --network kubenet \</p>
			<p class="source-code">  wordpress-multi-pod-basic.yaml</p>
			<p>Notice the additional <strong class="source-inline">--network</strong> argument, which is used to specify the network that the pods will be attached to. This is <a id="_idIndexMarker1327"/>necessary information since the Kubernetes YAML file contains no information about Podman networks. Our pods will be executed in rootless mode and attached to the rootless <strong class="source-inline">kubenet</strong> network.</p>
			<p>We can check that the two pods have been created correctly by using the following command:</p>
			<p class="source-code">$ podman pod ps</p>
			<p class="source-code">POD ID        NAME           STATUS      CREATED        INFRA ID      # OF CONTAINERS</p>
			<p class="source-code">c9d775da0379  <strong class="bold">mysql-pod</strong>      Running     8 minutes ago  71c93fa6080b  2</p>
			<p class="source-code">3b497cbaeebc  <strong class="bold">wordpress-pod</strong>  Running     8 minutes ago  0c52ee133f0f  2</p>
			<p>Now, we can<a id="_idIndexMarker1328"/> inspect the running containers. The strings that are highlighted in the following code represent the main workload to differentiate from the infra containers:</p>
			<p class="source-code">$ podman ps --format "{{.Image }} {{.Names}}"</p>
			<p class="source-code">localhost/podman-pause:4.0.0-rc5-1644672408 3b497cbaeebc-infra</p>
			<p class="source-code">docker.io/library/wordpress:latest <strong class="bold">wordpress-pod-wordpress</strong></p>
			<p class="source-code">localhost/podman-pause:4.0.0-rc5-1644672408 c9d775da0379-infra</p>
			<p class="source-code">docker.io/library/mysql:latest <strong class="bold">mysql-pod-db</strong></p>
			<p>The <strong class="source-inline">podman volume ls</strong> command confirms the existence of the two volumes:</p>
			<p class="source-code">$ podman volume ls</p>
			<p class="source-code">DRIVER      VOLUME NAME</p>
			<p class="source-code">local       dbvol</p>
			<p class="source-code">local       wpvol</p>
			<p>The rootless network configuration <a id="_idIndexMarker1329"/>can be inspected with the <strong class="source-inline">podman unshare</strong> command: </p>
			<p class="source-code">$ podman unshare --rootless-netns ip addr show</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The <strong class="source-inline">--rootless-netns</strong> option is only available on Podman 4, which is the recommended version for this chapter.</p>
			<p>Finally, let's inspect the DNS<a id="_idIndexMarker1330"/> behavior. On Podman 4, the name resolution for custom networks is managed by the <strong class="source-inline">aardvark-dns</strong> daemon, while on Podman 3, it is managed by <strong class="source-inline">dnsmasq</strong>. Since we assume you're using Podman 4 for these examples, let's look at its DNS configuration. For rootless networks, we can find the managed records in the <strong class="source-inline">/run/user/&lt;UID&gt;/containers/networks/aardvark-dns/&lt;NETWORK_NAME&gt;</strong> file.</p>
			<p>In our example, the configuration for the <strong class="source-inline">kubenet</strong> network is as follows:</p>
			<p class="source-code">$ cat /run/user/1000/containers/networks/aardvark-dns/kubenet</p>
			<p class="source-code">10.89.0.1</p>
			<p class="source-code">0c52ee133f0fec5084f25bd89ad8bd0f6af2fc46d696e2b8161864567b0a92 0b 10.89.0.4  <strong class="bold">wordpress-pod</strong>,0c52ee133f0f</p>
			<p class="source-code">71c93fa6080b6a3bfe1ebad3e164594c5fa7ea584e180113d2893eb67f6f3b 56 10.89.0.5  <strong class="bold">mysql-pod</strong>,71c93fa6080b</p>
			<p>The most amazing thing from this output is the confirmation that the name resolution now works at the pod<a id="_idIndexMarker1331"/> level, not at the container level. This is fair if we think that the pod initialized the namespaces, including the network namespace. For this reason, we can treat the pod name in Podman as a service name. </p>
			<p>Here, we demonstrated how the Kubernetes manifests that are generated with Podman can become a great<a id="_idIndexMarker1332"/> replacement for the Docker Compose<a id="_idIndexMarker1333"/> approach while being more portable. Now, let's learn how to import our generated resources into a test Kubernetes cluster.</p>
			<h1 id="_idParaDest-268"><a id="_idTextAnchor268"/>Testing the results in Kubernetes</h1>
			<p>In this section, we want to import the <a id="_idIndexMarker1334"/>multi-pod YAML file, which is enriched with the Services and PVC configurations, on Kubernetes. </p>
			<p>To provide a repeatable environment, we <a id="_idIndexMarker1335"/>will use <strong class="bold">minikube</strong> (with a lowercase m), a portable solution, to create an all-in-one Kubernetes cluster as the local infrastructure.</p>
			<p>The minikube project aims to provide a local Kubernetes cluster on Linux, Windows, and macOS. It uses host virtualization to spin up a VM that runs the all-in-one cluster or containerization to create a control plane that runs inside a container. It also provides a large set of add-ons to extend cluster functionalities, such as ingress controllers, service meshes, registries, logging, and more.</p>
			<p>Another widely adopted alternative to spinning up a local Kubernetes cluster is the <strong class="bold">Kubernetes in Docker</strong> (<strong class="bold">KinD</strong>) project, which is not described in this book. KinD runs a Kubernetes control plane inside a container that's driven by Docker or Podman.</p>
			<p>To set up minikube, users need <a id="_idIndexMarker1336"/>virtualization support (KVM, VirtualBox, Hyper-V, Parallels, or VMware) or a container runtime such as Docker or Podman. </p>
			<p>For brevity, we will not cover the technical steps necessary to configure the virtualization support for the different OSs; instead, we will use a GNU/Linux distribution.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">If you already own a running Kubernetes cluster or want to set up one in an alternative way, you can skip the next minikube configuration quick start and go to the <em class="italic">Running generated resource files in Kubernetes</em> subsection.</p>
			<h2 id="_idParaDest-269"><a id="_idTextAnchor269"/>Setting up minikube</h2>
			<p>Run the<a id="_idIndexMarker1337"/> following commands to <a id="_idIndexMarker1338"/>download and install the latest <strong class="source-inline">minikube</strong> binary:</p>
			<p class="source-code">$ curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64</p>
			<p class="source-code">$ sudo install minikube-linux-amd64 /usr/local/bin/minikube</p>
			<p>You can choose to run minikube with a virtualization or containerization driver. To run minikube as a virtual machine on the KVM driver, you <a id="_idIndexMarker1339"/>must install the <strong class="bold">Qemu/KVM</strong> and <strong class="bold">libvirt</strong> <a id="_idIndexMarker1340"/>packages. </p>
			<p>On Fedora, run the following command to install all the mandatory and default packages using the <strong class="source-inline">@virtualization</strong> package group:</p>
			<p class="source-code">$ sudo dnf install @virtualization</p>
			<p>Now, start and enable the <strong class="source-inline">libvirtd</strong> service:</p>
			<p class="source-code">$ sudo systemctl enable --now libvirtd</p>
			<p>To grant the user running minikube the proper permissions, append it to the <strong class="source-inline">libvirt</strong> supplementary group (this operation requires a new login to load the new group):</p>
			<p class="source-code">$ sudo usermod -aG libvirt $(whoami)</p>
			<p>The following command statically configures the <strong class="source-inline">kvm2</strong> driver as the default:</p>
			<p class="source-code">$ minikube config set driver kvm2</p>
			<p>When the preceding command is executed for the first time, minikube will automatically download the proper <strong class="source-inline">kvm2</strong> driver binary before starting the VM.</p>
			<p>Alternatively, you can<a id="_idIndexMarker1341"/> choose to run minikube as a containerized service with Docker or Podman. Assuming Podman is already installed, we only need to ensure that the user running minikube can run passwordless sudo. This is necessary since the Kubernetes cluster must run in a rootfull container, so<a id="_idIndexMarker1342"/> privilege escalation is necessary. To allow passwordless privilege escalation for Podman, edit the <strong class="source-inline">/etc/sudoers</strong> file with the following command:</p>
			<p class="source-code">$ sudo visudo</p>
			<p>Once opened, add the following line to the end of the file to grant passwordless escalation for the Podman binary and save it. Remember to replace <strong class="source-inline">&lt;username&gt;</strong> with your user's name:</p>
			<p class="source-code">&lt;username&gt; ALL=(ALL) NOPASSWD: /usr/bin/podman</p>
			<p>The following command statically configures the <strong class="source-inline">podman</strong> driver as the default:</p>
			<p class="source-code">$ minikube config set driver podman</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">If your host is a virtual machine running on a hypervisor such as KVM and Podman is installed on the host, minikube will detect the environment and set up the default driver as <strong class="source-inline">podman</strong> automatically.</p>
			<p>To use minikube, users also need to install the Kubernetes CLI tool, kubectl. The following commands download and install the latest Linux release:</p>
			<p class="source-code">$ version=$(curl -L -s https://dl.k8s.io/release/stable.txt) curl -LO "https://dl.k8s.io/release/${version}/bin/linux/amd64/kubectl $ sudo install -o root -g root \</p>
			<p class="source-code">  -m 0755 kubectl \</p>
			<p class="source-code">  /usr/local/bin/kubectl</p>
			<p>Now, we are ready to run our<a id="_idIndexMarker1343"/> Kubernetes cluster with minikube.</p>
			<h2 id="_idParaDest-270"><a id="_idTextAnchor270"/>Starting minikube</h2>
			<p>To start minikube as a VM, use the CRI-O container<a id="_idIndexMarker1344"/> runtime inside the Kubernetes cluster:</p>
			<p class="source-code">$ minikube start --driver=kvm2 --container-runtime=cri-o</p>
			<p>The <strong class="source-inline">--driver</strong> option is not necessary if <strong class="source-inline">kvm2</strong> has already been configured as the default driver with the <strong class="source-inline">minikube config set driver</strong> command.</p>
			<p>To start minikube with Podman, use the CRI-O <a id="_idIndexMarker1345"/>container runtime inside the cluster:</p>
			<p class="source-code">$ minikube start --driver=podman --container-runtime=cri-o</p>
			<p>Again, the <strong class="source-inline">--driver</strong> option is not necessary if <strong class="source-inline">podman</strong> has been already configured as the default driver with the <strong class="source-inline">minikube config set driver</strong> command.</p>
			<p>To ensure that the cluster has been created correctly, run the following command with the <strong class="source-inline">kubectl</strong> CLI. All the pods should have the <strong class="source-inline">Running</strong> status:</p>
			<p class="source-code">$ kubectl get pods –A</p>
			<p class="source-code">NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE</p>
			<p class="source-code">kube-system   coredns-64897985d-gqnrn            1/1     Running   0          19s</p>
			<p class="source-code">kube-system   etcd-minikube                      1/1     Running   0          27s</p>
			<p class="source-code">kube-system   kube-apiserver-minikube            1/1     Running   0          27s</p>
			<p class="source-code">kube-system   kube-controller-manager-minikube   1/1     Running   0          27s</p>
			<p class="source-code">kube-system   kube-proxy-sj7xn                   1/1     Running   0          20s</p>
			<p class="source-code">kube-system   kube-scheduler-minikube            1/1     Running   0          33s</p>
			<p class="source-code">kube-system   storage-provisioner                1/1     Running   0          30s</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">If one or more containers still have the <strong class="source-inline">ContainerCreating</strong> status, wait a little longer for the images to be pulled. </p>
			<p class="callout">Also, notice that the output may differ slightly if you're running minikube with a Podman driver. In that case, an additional pod named <strong class="source-inline">kindnet</strong> will be created to help manage CNI networking inside the cluster.</p>
			<p>With that, we<a id="_idIndexMarker1346"/> have set everything up for <a id="_idIndexMarker1347"/>a local Kubernetes environment and are ready to test our generated manifests.</p>
			<h2 id="_idParaDest-271"><a id="_idTextAnchor271"/>Running generated resource files in Kubernetes</h2>
			<p>In the <em class="italic">Generating a composite application with multiple Pods</em> section, we learned how to export a manifest file from Podman that <a id="_idIndexMarker1348"/>included the <strong class="source-inline">Pod</strong> resources, along with the <strong class="source-inline">Service</strong> and <strong class="source-inline">PersistentVolumeClaim</strong> resources. The need to export this set of resources is related to the way Kubernetes handles workloads, storage, and exposed services. </p>
			<p>Kubernetes services are needed to provide a resolution mechanism, as well as internal load balancing. In our example, the <strong class="source-inline">mysql-pod</strong> pod will be mapped to a homonymous <strong class="source-inline">mysql-pod</strong> service.</p>
			<p>PVCs are required to define a storage claim that starts provisioning persistent volumes for our pods. In minikube, automated provisioning is implemented by a local <strong class="source-inline">StorageClass</strong> named <strong class="source-inline">minikube-hostpath</strong>; it creates local directories in the VM/container filesystem that are later bind-mounted inside the pods' containers.</p>
			<p>We can roll out our WordPress stack by using the <strong class="source-inline">kubectl create</strong> command:</p>
			<p class="source-code">$ kubectl create –f wordpress-multi-pod-full.yaml</p>
			<p>If not specified, all the resources will be created in the <strong class="source-inline">default</strong> Kubernetes namespace. Let's wait for the pods to reach the <strong class="source-inline">Running</strong> status and inspect the results.</p>
			<p>First, we can inspect the pods and services that have been created:</p>
			<p class="source-code">$ kubectl get pods </p>
			<p class="source-code">NAME            READY   STATUS    RESTARTS   AGE</p>
			<p class="source-code">mysql-pod       1/1     Running   0          48m</p>
			<p class="source-code">wordpress-pod   1/1     Running   0          48m</p>
			<p class="source-code">$ kubectl get svc</p>
			<p class="source-code">NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE</p>
			<p class="source-code">kubernetes      ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP          53m</p>
			<p class="source-code">mysql-pod       NodePort    10.108.34.77   &lt;none&gt;        3306:30284/TCP   52m</p>
			<p class="source-code">wordpress-pod   NodePort    10.96.63.142   &lt;none&gt;        80:30408/TCP     52m</p>
			<p>Notice that the two <strong class="source-inline">mysql-pod</strong> and <strong class="source-inline">wordpress-pod</strong> services have been created with the <strong class="source-inline">NodePort</strong> type and mapped to a port on a <strong class="source-inline">30000</strong> or upper range. We will use the <strong class="source-inline">30408</strong> port to test the<a id="_idIndexMarker1349"/> WordPress frontend.</p>
			<p>The pods are mapped by the services using label matching logic. If the labels that have been defined in the service's <strong class="source-inline">selector</strong> field exist in the pod, it becomes an <strong class="source-inline">endpoint</strong> to the service itself. Let's view the current endpoints in our project:</p>
			<p class="source-code">$ kubectl get endpoints</p>
			<p class="source-code">NAME            ENDPOINTS         AGE</p>
			<p class="source-code">kubernetes      10.88.0.6:8443    84m</p>
			<p class="source-code">mysql-pod       10.244.0.5:3306   4m9s</p>
			<p class="source-code">wordpress-pod   10.244.0.6:80     4m9s</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The <strong class="source-inline">kubernetes</strong> service and its related endpoint provide API access to internal workloads. However, it is not part of this book's examples, so it can be ignored in this context.</p>
			<p>Let's also inspect the claims and their related volumes:</p>
			<p class="source-code">$ kubectl get pvc</p>
			<p class="source-code">NAME    STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</p>
			<p class="source-code">dbvol   Bound    pvc-4d4a047b-bd20-4bef-879c-c3d80f96d712   1Gi        RWO            standard       54m</p>
			<p class="source-code">wpvol   Bound    pvc-accd7947-1499-44b5-bac8-9345da7edc23   1Gi        RWO            standard       54m</p>
			<p class="source-code">$ kubectl get pv</p>
			<p class="source-code">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM           STORAGECLASS   REASON   AGE</p>
			<p class="source-code">pvc-4d4a047b-bd20-4bef-879c-c3d80f96d712   1Gi        RWO            Delete           Bound    default/dbvol   standard                60m</p>
			<p class="source-code">pvc-accd7947-1499-44b5-bac8-9345da7edc23   1Gi        RWO            Delete           Bound    default/wpvol   standard                60m</p>
			<p>The two PVC resources have been created and bound to two dynamically provisioned persistent volumes. So long as<a id="_idIndexMarker1350"/> the PVC objects exist, the related PV will stay untouched, even if the pods are destroyed and recreated.</p>
			<p>Now, the WordPress application can be tested. By default, minikube does not deploy an ingress controller (even though this can be enabled with the <strong class="source-inline">minikube addons enable ingress</strong> command), so we will use the simple NodePort service to test the functionalities of our application.</p>
			<p>The current minikube VM/container IP must be obtained to reach the exposed NodePort service. Port <strong class="source-inline">30408</strong>, which is associated with the <strong class="source-inline">wordpress-pod</strong> service, listens to the IP address that's produced by the following command:</p>
			<p class="source-code">$ minikube ip</p>
			<p class="source-code">10.88.0.6</p>
			<p>Now, we can point our browser to <strong class="source-inline">http://10.88.0.6:30408</strong> and see the WordPress first setup screen.</p>
			<p>To remove the WordPress application and all its related content, use the <strong class="source-inline">kubectl delete</strong> command in the YAML manifest file:</p>
			<p class="source-code">$ kubectl delete –f wordpress-multi-pod-full.yaml</p>
			<p>This command removes<a id="_idIndexMarker1351"/> all the resources that have been defined in the file, including the generated PVs.</p>
			<h1 id="_idParaDest-272"><a id="_idTextAnchor272"/>Summary</h1>
			<p>With that, we have reached the end of this book about Podman and its companion tools.</p>
			<p>First, we learned how to generate Systemd unit files and control containerized workloads as Systemd services, which allows us to, for example, automate container execution at system startup.</p>
			<p>After that, we learned how to generate Kubernetes YAML resources. Starting with basic concepts and examples, we learned how to generate complex application stacks using both single-pod and multiple pods approaches and illustrated how the latter can provide a great alternative (and Kubernetes compliant) to the Docker Compose methodology.</p>
			<p>Finally, we tested our results on Podman and a local Kubernetes cluster that had been created with <strong class="source-inline">minikube</strong> to show the great portability of this approach.</p>
			<p>This book's journey finishes here, but Podman's amazing evolution continues thanks to its growing adoption in many contexts and its vibrant and helpful community.</p>
			<p>Before you move on, don't forget to join the community on IRC, Matrix, or Discord and subscribe to the relevant mailing lists. Feel free to ask for and give feedback and contribute to help with the growth of the project.</p>
			<p>Thank you for your interest and dedication.</p>
			<h1 id="_idParaDest-273"><a id="_idTextAnchor273"/>Further reading</h1>
			<p>To learn more about the topics that were covered in this chapter, take a look at the following resources:</p>
			<ul>
				<li>The Catatonit repository on GitHub: <a href="https://github.com/openSUSE/catatonit">https://github.com/openSUSE/catatonit</a></li>
				<li>Kubernetes persistent volumes definition: <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">https://kubernetes.io/docs/concepts/storage/persistent-volumes/</a></li>
				<li>The minikube project's home page: <a href="https://minikube.sigs.k8s.io/">https://minikube.sigs.k8s.io/</a></li>
				<li>The KinD project's home page: <a href="https://kind.sigs.k8s.io/">https://kind.sigs.k8s.io/</a></li>
				<li>Podman community links: <a href="https://podman.io/community/">https://podman.io/community/</a></li>
			</ul>
		</div>
	</body></html>